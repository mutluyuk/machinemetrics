<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Optimization Algorithms - Basics | MachineMetrics</title>
  <meta name="description" content="Chapter 12 Optimization Algorithms - Basics | MachineMetrics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Optimization Algorithms - Basics | MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Optimization Algorithms - Basics | MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="hyperparameter-tuning.html"/>
<link rel="next" href="prediction-intervals.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> INTRODUCTION:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html"><i class="fa fa-check"></i><b>2</b> Statistical Models and Simulations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#distinguishing-between-statistical-modeling-and-machine-learning-in-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#goals-and-objectives"><i class="fa fa-check"></i><b>2.1.1</b> Goals and Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#prediction-vs.-inference"><i class="fa fa-check"></i><b>2.1.2</b> Prediction vs. Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#conclusion"><i class="fa fa-check"></i><b>2.1.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#model-selection-and-approaches-in-data-modeling"><i class="fa fa-check"></i><b>2.4</b> Model Selection and Approaches in Data Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#mle"><i class="fa fa-check"></i><b>3.5.3</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error---mse"><i class="fa fa-check"></i><b>5.1</b> Estimation error - MSE</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.2</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#technical-points-about-mse-and-mspe"><i class="fa fa-check"></i><b>5.3</b> Technical points about MSE and MSPE</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.1</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.1</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.2</b> LPM</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
<li class="chapter" data-level="10.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#knn"><i class="fa fa-check"></i><b>10.3</b> Knn</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#adult-dataset"><i class="fa fa-check"></i><b>10.3.1</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="11.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#grid-search"><i class="fa fa-check"></i><b>11.4</b> Grid Search</a></li>
<li class="chapter" data-level="11.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>11.5</b> Cross-validated grid search</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#one-variable"><i class="fa fa-check"></i><b>12.4.1</b> One-variable</a></li>
<li class="chapter" data-level="12.4.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>12.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="12.4.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#multivariable"><i class="fa fa-check"></i><b>12.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>16.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.3</b> Pruning</a></li>
<li class="chapter" data-level="16.4" data-path="regression-trees.html"><a href="regression-trees.html#classification-with-titanic"><i class="fa fa-check"></i><b>16.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="16.5" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>17.2</b> Random Forest</a></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>17.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="17.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="17.3.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.3.3</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.4</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.5</b> Classification</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.6</b> Regression</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.7</b> Exploration</a></li>
<li class="chapter" data-level="17.8" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.8</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression-1"><i class="fa fa-check"></i><b>17.8.1</b> Regression</a></li>
<li class="chapter" data-level="17.8.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.8.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.8.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.8.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.8.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-1"><i class="fa fa-check"></i><b>17.8.4</b> Classification</a></li>
<li class="chapter" data-level="17.8.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost.m1"><i class="fa fa-check"></i><b>17.8.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="17.8.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.8.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#out-sample-prediction-accuracy"><i class="fa fa-check"></i><b>20.3</b> out-sample prediction accuracy</a></li>
<li class="chapter" data-level="20.4" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-2.html"><a href="classification-2.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-2.html"><a href="classification-2.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-2.html"><a href="classification-2.html#mnist-dataset"><i class="fa fa-check"></i><b>21.2</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="21.3" data-path="classification-2.html"><a href="classification-2.html#linear-classifiers-again"><i class="fa fa-check"></i><b>21.3</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="21.4" data-path="classification-2.html"><a href="classification-2.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.4</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.5" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret"><i class="fa fa-check"></i><b>21.5</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="classification-2.html"><a href="classification-2.html#mnist_27"><i class="fa fa-check"></i><b>21.5.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="21.5.2" data-path="classification-2.html"><a href="classification-2.html#adult-dataset-1"><i class="fa fa-check"></i><b>21.5.2</b> Adult dataset</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="classification-2.html"><a href="classification-2.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.6</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.7" data-path="classification-2.html"><a href="classification-2.html#confusion-matrix"><i class="fa fa-check"></i><b>21.7</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.8" data-path="classification-2.html"><a href="classification-2.html#performance-measures"><i class="fa fa-check"></i><b>21.8</b> Performance measures</a></li>
<li class="chapter" data-level="21.9" data-path="classification-2.html"><a href="classification-2.html#roc-curve"><i class="fa fa-check"></i><b>21.9</b> ROC Curve</a></li>
<li class="chapter" data-level="21.10" data-path="classification-2.html"><a href="classification-2.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.10</b> AUC - Area Under the Curve</a></li>
<li class="chapter" data-level="21.11" data-path="classification-2.html"><a href="classification-2.html#classification-example"><i class="fa fa-check"></i><b>21.11</b> Classification Example</a></li>
<li class="chapter" data-level="21.12" data-path="classification-2.html"><a href="classification-2.html#lpm-1"><i class="fa fa-check"></i><b>21.12</b> LPM</a></li>
<li class="chapter" data-level="21.13" data-path="classification-2.html"><a href="classification-2.html#logistic-regression-1"><i class="fa fa-check"></i><b>21.13</b> Logistic Regression</a></li>
<li class="chapter" data-level="21.14" data-path="classification-2.html"><a href="classification-2.html#knn-1"><i class="fa fa-check"></i><b>21.14</b> kNN</a>
<ul>
<li class="chapter" data-level="21.14.1" data-path="classification-2.html"><a href="classification-2.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>21.14.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="21.14.2" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret-1"><i class="fa fa-check"></i><b>21.14.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>22</b> Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="time-series.html"><a href="time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="time-series.html"><a href="time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="time-series.html"><a href="time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="time-series.html"><a href="time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="time-series.html"><a href="time-series.html#stationarity"><i class="fa fa-check"></i><b>22.5</b> Stationarity</a></li>
<li class="chapter" data-level="22.6" data-path="time-series.html"><a href="time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.6</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="time-series.html"><a href="time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.7</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.8" data-path="time-series.html"><a href="time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.8</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.9" data-path="time-series.html"><a href="time-series.html#speed"><i class="fa fa-check"></i><b>22.9</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="forecast.html"><a href="forecast.html"><i class="fa fa-check"></i><b>23</b> Forecast</a>
<ul>
<li class="chapter" data-level="23.1" data-path="forecast.html"><a href="forecast.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="forecast.html"><a href="forecast.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="forecast.html"><a href="forecast.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="forecast.html"><a href="forecast.html#random-forest-1"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="forecast.html"><a href="forecast.html#univariate"><i class="fa fa-check"></i><b>23.5</b> Univariate</a></li>
<li class="chapter" data-level="23.6" data-path="forecast.html"><a href="forecast.html#multivariate"><i class="fa fa-check"></i><b>23.6</b> Multivariate</a></li>
<li class="chapter" data-level="23.7" data-path="forecast.html"><a href="forecast.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.7</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>24.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-margin"><i class="fa fa-check"></i><b>24.1.1</b> The Margin</a></li>
<li class="chapter" data-level="24.1.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-non-separable-case"><i class="fa fa-check"></i><b>24.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>24.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="24.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#application-with-svm"><i class="fa fa-check"></i><b>24.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#precision-matrix"><i class="fa fa-check"></i><b>27.4</b> Precision Matrix</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.5</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.6" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.6</b> Regularized Covariance Matrix</a></li>
<li class="chapter" data-level="27.7" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>27.7</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="27.8" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#high-dimensional-data"><i class="fa fa-check"></i><b>27.8</b> High-dimensional data</a></li>
<li class="chapter" data-level="27.9" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>27.9</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#rankr-approximations"><i class="fa fa-check"></i><b>28.4</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="28.5" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.5</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html"><i class="fa fa-check"></i><b>29</b> PCA (Principle Component Analysis)</a>
<ul>
<li class="chapter" data-level="29.1" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.1</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>30.1</b> Using bins</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>30.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="30.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>30.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="30.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="31.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>31.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="31.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>31.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="text-analysis.html"><a href="text-analysis.html"><i class="fa fa-check"></i><b>32</b> Text Analysis</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#other-nonparametric-estimation-methods-1"><i class="fa fa-check"></i><b>33.1</b> Other Nonparametric Estimation methods</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.2</b> Regression splines</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.3</b> MARS</a></li>
<li class="chapter" data-level="33.4" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.4</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuk/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="optimization-algorithms---basics" class="section level1 hasAnchor" number="12">
<h1><span class="header-section-number">Chapter 12</span> Optimization Algorithms - Basics<a href="optimization-algorithms---basics.html#optimization-algorithms---basics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Here is a definition of algorithmic optimization from <a href="https://en.wikipedia.org/wiki/Mathematical_optimization#Optimization_algorithms">Wikipedia</a>:</p>
<blockquote>
<p>An optimization algorithm is a procedure which is executed iteratively by comparing various solutions until an optimum or a satisfactory solution is found. Optimization algorithms help us to minimize or maximize an objective function <span class="math inline">\(F(x)\)</span> with respect to the internal parameters of a model mapping a set of predictors (<span class="math inline">\(X\)</span>) to target values(<span class="math inline">\(Y\)</span>). There are three types of optimization algorithms which are widely used; <strong><em>Zero-Order Algorithms, First-Order Optimization Algorithms, and Second-Order Optimization Algorithms</em></strong>. Zero-order (or derivative-free) algorithms use only the criterion value at some positions. <strong><em>It is popular when the gradient and Hessian information are difficult to obtain, e.g., no explicit function forms are given</em></strong>. First Order Optimization Algorithms minimize or maximize a Loss function <span class="math inline">\(F(x)\)</span> using its Gradient values with respect to the parameters. Most widely used First order optimization algorithm is Gradient Descent. The First order derivative displays whether the function is decreasing or increasing at a particular point.</p>
</blockquote>
<p>In this appendix, we will review some important concepts in algorithmic optimization.</p>
<div id="brute-force-optimization" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> Brute-force optimization<a href="optimization-algorithms---basics.html#brute-force-optimization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s look at a simplified example about optimal retirement-plan and solve it with a zero-order algorithm.</p>
<p>Suppose that there are 2 groups of workers who are planning for their retirement at the age of 65. Both consider spending 40,000 dollars each year for the rest of their lives after retirement. On average, people in both groups expect to live 20 more years after retirement with some uncertainty. The people in the first group (A) have the following risk profile: 85% chance to live 20 years and 15% chance to live 30 years. The same risk profile for the people in the second group (B) is: 99% for 20 years and 1% for 30 years. Suppose that in each group, their utility (objective) function is <span class="math inline">\(U=C^{0.5}\)</span>.</p>
<p>What’s the maximum premium (lump-sum payment) that a person in each group would be willing to pay for a life-time annuity of 40K?</p>
<p>Without a pension plan, people in each group have the following utilities:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="optimization-algorithms---basics.html#cb180-1" tabindex="-1"></a><span class="co">#For people in group A</span></span>
<span id="cb180-2"><a href="optimization-algorithms---basics.html#cb180-2" tabindex="-1"></a>U_A <span class="ot">=</span> <span class="fl">0.85</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">40000</span><span class="sc">*</span><span class="dv">20</span>) <span class="sc">+</span> <span class="fl">0.15</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">10</span><span class="sc">*</span><span class="dv">0</span>)</span>
<span id="cb180-3"><a href="optimization-algorithms---basics.html#cb180-3" tabindex="-1"></a>U_A</span></code></pre></div>
<pre><code>## [1] 760.2631</code></pre>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="optimization-algorithms---basics.html#cb182-1" tabindex="-1"></a><span class="co">#For people in group B</span></span>
<span id="cb182-2"><a href="optimization-algorithms---basics.html#cb182-2" tabindex="-1"></a>U_B <span class="ot">=</span> <span class="fl">0.99</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">40000</span><span class="sc">*</span><span class="dv">20</span>) <span class="sc">+</span> <span class="fl">0.01</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">10</span><span class="sc">*</span><span class="dv">0</span>)</span>
<span id="cb182-3"><a href="optimization-algorithms---basics.html#cb182-3" tabindex="-1"></a>U_B</span></code></pre></div>
<pre><code>## [1] 885.4829</code></pre>
<p>For example, they would not pay 200,000 dollars to cover their retirement because that would make them worse than their current situation (without a pension plan).</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="optimization-algorithms---basics.html#cb184-1" tabindex="-1"></a><span class="co">#For people in group A</span></span>
<span id="cb184-2"><a href="optimization-algorithms---basics.html#cb184-2" tabindex="-1"></a>U_A <span class="ot">=</span> <span class="fl">0.85</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">40000</span><span class="sc">*</span><span class="dv">20</span> <span class="sc">-</span> <span class="dv">200000</span>) <span class="sc">+</span> <span class="fl">0.15</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">40000</span><span class="sc">*</span><span class="dv">10</span> <span class="sc">-</span> <span class="dv">200000</span>) </span>
<span id="cb184-3"><a href="optimization-algorithms---basics.html#cb184-3" tabindex="-1"></a>U_A</span></code></pre></div>
<pre><code>## [1] 725.4892</code></pre>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="optimization-algorithms---basics.html#cb186-1" tabindex="-1"></a><span class="co">#For people in group B</span></span>
<span id="cb186-2"><a href="optimization-algorithms---basics.html#cb186-2" tabindex="-1"></a>U_B <span class="ot">=</span> <span class="fl">0.99</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">40000</span><span class="sc">*</span><span class="dv">20</span> <span class="sc">-</span> <span class="dv">200000</span>) <span class="sc">+</span> <span class="fl">0.01</span><span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">40000</span><span class="sc">*</span><span class="dv">10</span> <span class="sc">-</span> <span class="dv">200000</span>)</span>
<span id="cb186-3"><a href="optimization-algorithms---basics.html#cb186-3" tabindex="-1"></a>U_B</span></code></pre></div>
<pre><code>## [1] 771.3228</code></pre>
<p>Hence, the payment they would be willing to make for reduction in uncertainty during their retirement should not make them worse off. Or more technically, their utility should not be lower than their current utility levels. Therefore <code>Pmax</code>, the maximum premium that a person would be willing to pay, can be found by minimizing the following <strong>cost function</strong> for people, for example, in Group A:</p>
<p><span class="math display">\[
f(Pmax) = p \times \sqrt{40000 \times 20~\text{years}-Pmax}+ \\
(1-p) \times \sqrt{40000 \times 10~ \text{years}-Pmax} - p \times \sqrt{ 40000 \times 20~\text{years}}
\]</span></p>
<p>Here is the iteration to solve for <code>Pmax</code> for people in Group A. We created a cost function, <code>costf</code>, that we try to minimize. Change the parameters to play with it. The same algorithm can be used to find <code>Pmax</code> for people in Group B.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="optimization-algorithms---basics.html#cb188-1" tabindex="-1"></a><span class="fu">library</span>(stats)</span>
<span id="cb188-2"><a href="optimization-algorithms---basics.html#cb188-2" tabindex="-1"></a></span>
<span id="cb188-3"><a href="optimization-algorithms---basics.html#cb188-3" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.85</span></span>
<span id="cb188-4"><a href="optimization-algorithms---basics.html#cb188-4" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> <span class="dv">800000</span></span>
<span id="cb188-5"><a href="optimization-algorithms---basics.html#cb188-5" tabindex="-1"></a>w2 <span class="ot">&lt;-</span> <span class="dv">400000</span></span>
<span id="cb188-6"><a href="optimization-algorithms---basics.html#cb188-6" tabindex="-1"></a></span>
<span id="cb188-7"><a href="optimization-algorithms---basics.html#cb188-7" tabindex="-1"></a>converged <span class="ot">=</span> F</span>
<span id="cb188-8"><a href="optimization-algorithms---basics.html#cb188-8" tabindex="-1"></a>iterations <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb188-9"><a href="optimization-algorithms---basics.html#cb188-9" tabindex="-1"></a>maxiter <span class="ot">&lt;-</span> <span class="dv">600000</span></span>
<span id="cb188-10"><a href="optimization-algorithms---basics.html#cb188-10" tabindex="-1"></a>learnrate <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb188-11"><a href="optimization-algorithms---basics.html#cb188-11" tabindex="-1"></a>Pmax <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb188-12"><a href="optimization-algorithms---basics.html#cb188-12" tabindex="-1"></a></span>
<span id="cb188-13"><a href="optimization-algorithms---basics.html#cb188-13" tabindex="-1"></a><span class="cf">while</span>(converged <span class="sc">==</span> <span class="cn">FALSE</span>){</span>
<span id="cb188-14"><a href="optimization-algorithms---basics.html#cb188-14" tabindex="-1"></a>  costf <span class="ot">&lt;-</span> p<span class="sc">*</span><span class="fu">sqrt</span>(w1 <span class="sc">-</span> Pmax) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> p)<span class="sc">*</span><span class="fu">sqrt</span>(w2 <span class="sc">-</span> Pmax) <span class="sc">-</span> p<span class="sc">*</span><span class="fu">sqrt</span>(w1)</span>
<span id="cb188-15"><a href="optimization-algorithms---basics.html#cb188-15" tabindex="-1"></a>  <span class="cf">if</span>(costf <span class="sc">&gt;</span> <span class="dv">0</span>){</span>
<span id="cb188-16"><a href="optimization-algorithms---basics.html#cb188-16" tabindex="-1"></a>    Pmax <span class="ot">&lt;-</span> Pmax <span class="sc">+</span> learnrate</span>
<span id="cb188-17"><a href="optimization-algorithms---basics.html#cb188-17" tabindex="-1"></a>    iterations <span class="ot">=</span> iterations <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb188-18"><a href="optimization-algorithms---basics.html#cb188-18" tabindex="-1"></a>  </span>
<span id="cb188-19"><a href="optimization-algorithms---basics.html#cb188-19" tabindex="-1"></a>    <span class="cf">if</span>(iterations <span class="sc">&gt;</span> maxiter) { </span>
<span id="cb188-20"><a href="optimization-algorithms---basics.html#cb188-20" tabindex="-1"></a>      <span class="fu">print</span>(<span class="st">&quot;It cannot converge before finding the optimal Pmax&quot;</span>)</span>
<span id="cb188-21"><a href="optimization-algorithms---basics.html#cb188-21" tabindex="-1"></a>      <span class="cf">break</span></span>
<span id="cb188-22"><a href="optimization-algorithms---basics.html#cb188-22" tabindex="-1"></a>    }  </span>
<span id="cb188-23"><a href="optimization-algorithms---basics.html#cb188-23" tabindex="-1"></a>    converged <span class="ot">=</span> <span class="cn">FALSE</span></span>
<span id="cb188-24"><a href="optimization-algorithms---basics.html#cb188-24" tabindex="-1"></a>  }<span class="cf">else</span>{</span>
<span id="cb188-25"><a href="optimization-algorithms---basics.html#cb188-25" tabindex="-1"></a>    converged <span class="ot">=</span> <span class="cn">TRUE</span></span>
<span id="cb188-26"><a href="optimization-algorithms---basics.html#cb188-26" tabindex="-1"></a>    <span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">&quot;Maximum Premium:&quot;</span>,</span>
<span id="cb188-27"><a href="optimization-algorithms---basics.html#cb188-27" tabindex="-1"></a>                Pmax, <span class="st">&quot;achieved with&quot;</span>,</span>
<span id="cb188-28"><a href="optimization-algorithms---basics.html#cb188-28" tabindex="-1"></a>                iterations, <span class="st">&quot;iterations&quot;</span>))</span>
<span id="cb188-29"><a href="optimization-algorithms---basics.html#cb188-29" tabindex="-1"></a>  }</span>
<span id="cb188-30"><a href="optimization-algorithms---basics.html#cb188-30" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## [1] &quot;Maximum Premium: 150043 achieved with 280086 iterations&quot;</code></pre>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="optimization-algorithms---basics.html#cb190-1" tabindex="-1"></a><span class="co">#let&#39;s verify it by `uniroot()` which finds the roots for f(x) = 0</span></span>
<span id="cb190-2"><a href="optimization-algorithms---basics.html#cb190-2" tabindex="-1"></a>costf <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb190-3"><a href="optimization-algorithms---basics.html#cb190-3" tabindex="-1"></a>  p <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">800000</span> <span class="sc">-</span> x) <span class="sc">+</span></span>
<span id="cb190-4"><a href="optimization-algorithms---basics.html#cb190-4" tabindex="-1"></a>    (<span class="dv">1</span><span class="sc">-</span>p) <span class="sc">*</span> <span class="fu">sqrt</span>(<span class="dv">400000</span> <span class="sc">-</span> x) <span class="sc">-</span></span>
<span id="cb190-5"><a href="optimization-algorithms---basics.html#cb190-5" tabindex="-1"></a>    p<span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">800000</span>)</span>
<span id="cb190-6"><a href="optimization-algorithms---basics.html#cb190-6" tabindex="-1"></a>  }</span>
<span id="cb190-7"><a href="optimization-algorithms---basics.html#cb190-7" tabindex="-1"></a></span>
<span id="cb190-8"><a href="optimization-algorithms---basics.html#cb190-8" tabindex="-1"></a><span class="fu">paste</span>(<span class="st">&quot;Unitroot for f(x) = 0 is &quot;</span>,</span>
<span id="cb190-9"><a href="optimization-algorithms---basics.html#cb190-9" tabindex="-1"></a>      <span class="fu">uniroot</span>(costf, <span class="fu">c</span>(<span class="dv">10000</span>, <span class="dv">200000</span>))<span class="sc">$</span>root)</span></code></pre></div>
<pre><code>## [1] &quot;Unitroot for f(x) = 0 is  150042.524874307&quot;</code></pre>
<p>There are better functions that we could use for this purpose, but this example works well for our experiment.</p>
<p>There several of important parameters in our algorithm. The first one is the starting <code>Pmax</code>, which can be set up manually. If the starting value is too low, iteration could not converge. If it’s too high, it can give us an error. Another issue is that our iteration does not know if the learning rate should increase or decrease when the starting value is too high or too low. This can be done with additional lines of code, but we will not address it here.</p>
<p>This situation leads us to the learning rate: the incremental change in the value of the parameter. This parameter should be conditioned on the value of cost function. If the cost function for a given <code>Pmax</code> is negative, for example, the learning rate should be negative. Secondly, the number of maximum iterations must be set properly, otherwise the algorithm may not converge or take too long to converge. In the next section, we will address these issues with a smarter algorithm.</p>
<p>There are other types of approaches. For example, the algorithm may create a grid of <code>Pmax</code> and then try all the possible values to see which one approximately makes the cost function minimum.</p>
</div>
<div id="derivative-based-methods" class="section level2 hasAnchor" number="12.2">
<h2><span class="header-section-number">12.2</span> Derivative-based methods<a href="optimization-algorithms---basics.html#derivative-based-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the derivative-based methods is the <strong>Newton-Raphson</strong> method. If we assume that the function is differentiable and has only one minimum (maximum), we can develop an optimization algorithm that looks for the point in parameter space where the derivative of the function is zero. There are other methods, like <a href="https://en.wikipedia.org/wiki/Scoring_algorithm">Fisher Scoring</a> and <a href="https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares">Iteratively Reweighted Least Squares</a>, that we will not see here.</p>
<p>First, let’s see the Newton-Raphson method. This is a well-known extension of your calculus class about derivatives in High School. The method is very simple and used to find the roots of <span class="math inline">\(f(x)=0\)</span> by iterations. In first-year computer science courses, this method is used to teach loop algorithms that calculate the value of, for example, <span class="math inline">\(e^{0.71}\)</span> or <span class="math inline">\(\sqrt{12}\)</span>. It is a simple iteration that converges in a few steps.</p>
<p><span class="math display">\[
x_{n+1}=x_{n}-\frac{f\left(x_{n}\right)}{f^{\prime}\left(x_{n}\right)}
\]</span>
To understand it, let’s look at the function <span class="math inline">\(y=f(x)\)</span> shown in the following graph:</p>
<p><img src="png/RN.png" width="80%" height="80%" /></p>
<p>It has a zero at <span class="math inline">\(x=x_r\)</span>, which is not known. To find it, we start with <span class="math inline">\(x_0\)</span> as an initial estimate of <span class="math inline">\(X_r\)</span>. The tangent line to the graph at the point <span class="math inline">\(\left(x_0, f\left(x_0\right)\right)\)</span> has the point <span class="math inline">\(x_1\)</span> at which the tangent crosses the <span class="math inline">\(x\)</span>-axis. The slope of this line can be defined as</p>
<p><span class="math display">\[
\frac{y-f\left(x_0\right)}{x-x_0}=f^{\prime}\left(x_0\right)
\]</span>
Hence,</p>
<p><span class="math display">\[
y-f\left(x_0\right)=f^{\prime}\left(x_0\right)\left(x-x_0\right)
\]</span>
At the point where the tangent line cross the <span class="math inline">\(x\)</span>-axis, <span class="math inline">\(y=0\)</span> and <span class="math inline">\(x=x_1\)</span>. Hence solving the equation for <span class="math inline">\(x_1\)</span>, we get</p>
<p><span class="math display">\[
x_{1}=x_{0}-\frac{f\left(x_{0}\right)}{f^{\prime}\left(x_{0}\right)}
\]</span>
And the second approximations:</p>
<p><span class="math display">\[
x_{2}=x_{1}-\frac{f\left(x_{1}\right)}{f^{\prime}\left(x_{1}\right)}
\]</span>
And with multiple iterations one can find the solution. Here is the example:</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="optimization-algorithms---basics.html#cb192-1" tabindex="-1"></a>newton <span class="ot">&lt;-</span> <span class="cf">function</span>(f, x0, <span class="at">tol =</span> <span class="fl">1e-5</span>, <span class="at">n =</span> <span class="dv">1000</span>) {</span>
<span id="cb192-2"><a href="optimization-algorithms---basics.html#cb192-2" tabindex="-1"></a>  <span class="fu">require</span>(numDeriv) <span class="co"># Package for computing f&#39;(x)</span></span>
<span id="cb192-3"><a href="optimization-algorithms---basics.html#cb192-3" tabindex="-1"></a>  </span>
<span id="cb192-4"><a href="optimization-algorithms---basics.html#cb192-4" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb192-5"><a href="optimization-algorithms---basics.html#cb192-5" tabindex="-1"></a>    dx <span class="ot">&lt;-</span> <span class="fu">genD</span>(<span class="at">func =</span> f, <span class="at">x =</span> x0)<span class="sc">$</span>D[<span class="dv">1</span>] <span class="co"># First-order derivative f&#39;(x0)</span></span>
<span id="cb192-6"><a href="optimization-algorithms---basics.html#cb192-6" tabindex="-1"></a>    x1 <span class="ot">&lt;-</span> x0 <span class="sc">-</span> (<span class="fu">f</span>(x0) <span class="sc">/</span> dx) <span class="co"># Calculate next value x1</span></span>
<span id="cb192-7"><a href="optimization-algorithms---basics.html#cb192-7" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">abs</span>(x1 <span class="sc">-</span> x0) <span class="sc">&lt;</span> tol) {</span>
<span id="cb192-8"><a href="optimization-algorithms---basics.html#cb192-8" tabindex="-1"></a>      res <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;Root approximation is &quot;</span>, x1, <span class="st">&quot; in &quot;</span>, i, <span class="st">&quot; iterations&quot;</span>)</span>
<span id="cb192-9"><a href="optimization-algorithms---basics.html#cb192-9" tabindex="-1"></a>      <span class="fu">return</span>(res)</span>
<span id="cb192-10"><a href="optimization-algorithms---basics.html#cb192-10" tabindex="-1"></a>    }</span>
<span id="cb192-11"><a href="optimization-algorithms---basics.html#cb192-11" tabindex="-1"></a>    <span class="co"># If Newton-Raphson has not yet reached convergence set x1 as x0 and continue</span></span>
<span id="cb192-12"><a href="optimization-algorithms---basics.html#cb192-12" tabindex="-1"></a>    x0 <span class="ot">&lt;-</span> x1</span>
<span id="cb192-13"><a href="optimization-algorithms---basics.html#cb192-13" tabindex="-1"></a>  }</span>
<span id="cb192-14"><a href="optimization-algorithms---basics.html#cb192-14" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">&#39;Too many iterations in method&#39;</span>)</span>
<span id="cb192-15"><a href="optimization-algorithms---basics.html#cb192-15" tabindex="-1"></a>}</span>
<span id="cb192-16"><a href="optimization-algorithms---basics.html#cb192-16" tabindex="-1"></a></span>
<span id="cb192-17"><a href="optimization-algorithms---basics.html#cb192-17" tabindex="-1"></a>func2 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb192-18"><a href="optimization-algorithms---basics.html#cb192-18" tabindex="-1"></a>  x<span class="sc">^</span><span class="dv">15</span> <span class="sc">-</span> <span class="dv">2</span></span>
<span id="cb192-19"><a href="optimization-algorithms---basics.html#cb192-19" tabindex="-1"></a>}</span>
<span id="cb192-20"><a href="optimization-algorithms---basics.html#cb192-20" tabindex="-1"></a><span class="fu">newton</span>(func2, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Root approximation is  1.04729412282063  in  5  iterations&quot;</code></pre>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="optimization-algorithms---basics.html#cb194-1" tabindex="-1"></a><span class="co">#Check it</span></span>
<span id="cb194-2"><a href="optimization-algorithms---basics.html#cb194-2" tabindex="-1"></a><span class="fu">paste</span>(<span class="st">&quot;Calculator result: &quot;</span>, <span class="dv">2</span><span class="sc">^</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">15</span>))</span></code></pre></div>
<pre><code>## [1] &quot;Calculator result:  1.04729412282063&quot;</code></pre>
<p>Newton’s method is often used to solve two different, but related, problems:</p>
<ol style="list-style-type: decimal">
<li>Finding <span class="math inline">\(x\)</span> such that <span class="math inline">\(f(x)=0\)</span> (try to solve our insurance problem with this method)</li>
<li>Finding <span class="math inline">\(x\)</span> that <span class="math inline">\(g&#39;(x)=0\)</span>, or find <span class="math inline">\(x\)</span> that minimizes/maximizes <span class="math inline">\(g(x)\)</span>.</li>
</ol>
<p>The relation between these two problems is obvious when we define <span class="math inline">\(f(x) = g&#39;(x)\)</span>. Hence, for the second problem, the Newton-Raphson method becomes:</p>
<p><span class="math display">\[
x_{n+1}=x_{n}-\frac{g^{\prime}\left(x_{n}\right)}{g^{\prime \prime}\left(x_{n}\right)}
\]</span></p>
<p>Connection between these two problems are defined in this <a href="https://stats.stackexchange.com/questions/376191/why-is-the-second-derivative-required-for-newtons-method-for-back-propagation">post</a> <span class="citation">(<a href="#ref-Gulzar_2018"><strong>Gulzar_2018?</strong></a>)</span> very nicely.</p>
<p>Let’s pretend that we are interested in determining the parameters of a random variable <span class="math inline">\(X \sim N(\mu, \sigma^{2})\)</span>. Here is the log-likelihood function for <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
\log (\mathcal{L}(\mu, \sigma))=-\frac{n}{2} \log \left(2 \pi \sigma^{2}\right)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}
\]</span></p>
<p>We have seen it in Chapter 2 before. But this time we will use <code>dnorm()</code> which calculates the pdf of a normal variable. First let’s have the data and the log-likelihood:</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="optimization-algorithms---basics.html#cb196-1" tabindex="-1"></a><span class="co"># Let&#39;s create a sample of normal variables</span></span>
<span id="cb196-2"><a href="optimization-algorithms---basics.html#cb196-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2019</span>)</span>
<span id="cb196-3"><a href="optimization-algorithms---basics.html#cb196-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb196-4"><a href="optimization-algorithms---basics.html#cb196-4" tabindex="-1"></a></span>
<span id="cb196-5"><a href="optimization-algorithms---basics.html#cb196-5" tabindex="-1"></a><span class="co"># And the log-likelihood of this function.</span></span>
<span id="cb196-6"><a href="optimization-algorithms---basics.html#cb196-6" tabindex="-1"></a><span class="co"># Remember likelihood function would be prod(dnorm()) with log=F</span></span>
<span id="cb196-7"><a href="optimization-algorithms---basics.html#cb196-7" tabindex="-1"></a>normalLL <span class="ot">&lt;-</span> <span class="cf">function</span>(prmt){</span>
<span id="cb196-8"><a href="optimization-algorithms---basics.html#cb196-8" tabindex="-1"></a>  <span class="fu">sum</span>(<span class="fu">dnorm</span>(X, <span class="at">mean =</span> prmt[<span class="dv">1</span>], <span class="at">sd =</span> prmt[<span class="dv">2</span>], <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb196-9"><a href="optimization-algorithms---basics.html#cb196-9" tabindex="-1"></a>}</span>
<span id="cb196-10"><a href="optimization-algorithms---basics.html#cb196-10" tabindex="-1"></a></span>
<span id="cb196-11"><a href="optimization-algorithms---basics.html#cb196-11" tabindex="-1"></a><span class="co"># Let&#39;s try several parameters</span></span>
<span id="cb196-12"><a href="optimization-algorithms---basics.html#cb196-12" tabindex="-1"></a><span class="fu">normalLL</span>(<span class="at">prmt =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="fl">1.5</span>))</span></code></pre></div>
<pre><code>## [1] -176.078</code></pre>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="optimization-algorithms---basics.html#cb198-1" tabindex="-1"></a><span class="fu">normalLL</span>(<span class="at">prmt =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] -347.4119</code></pre>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="optimization-algorithms---basics.html#cb200-1" tabindex="-1"></a><span class="fu">normalLL</span>(<span class="at">prmt =</span> <span class="fu">c</span>(<span class="fu">mean</span>(X),<span class="fu">sd</span>(X)))</span></code></pre></div>
<pre><code>## [1] -131.4619</code></pre>
<p>As you can see, the last one is the best. And we can verify it because we had created <span class="math inline">\(X\)</span> with 0 mean and 1 sd, approximately. Now we will use the Newton-Raphson method to calculate those parameters that minimize the negative log-likelihood.</p>
<p>First, let’s build a function that estimates the slope of the function (first-derivative) numerically at any arbitrary point in parameter space for mean and sd, separately. Don’t forget, <strong>the log-likelihood is a function of parameters (mean and sd) not X</strong>.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="optimization-algorithms---basics.html#cb202-1" tabindex="-1"></a><span class="co"># First partial (numerical) derivative w.r.t. mean  </span></span>
<span id="cb202-2"><a href="optimization-algorithms---basics.html#cb202-2" tabindex="-1"></a>firstM <span class="ot">&lt;-</span> <span class="cf">function</span>(p1, p2, <span class="at">change =</span> <span class="fl">0.0001</span>){      </span>
<span id="cb202-3"><a href="optimization-algorithms---basics.html#cb202-3" tabindex="-1"></a>  prmt <span class="ot">&lt;-</span> <span class="fu">c</span>(p1, p2)</span>
<span id="cb202-4"><a href="optimization-algorithms---basics.html#cb202-4" tabindex="-1"></a>  high <span class="ot">&lt;-</span> <span class="fu">normalLL</span>(prmt <span class="sc">+</span> <span class="fu">c</span>(change,<span class="dv">0</span>))</span>
<span id="cb202-5"><a href="optimization-algorithms---basics.html#cb202-5" tabindex="-1"></a>  low <span class="ot">&lt;-</span> <span class="fu">normalLL</span>(prmt <span class="sc">-</span> <span class="fu">c</span>(change,<span class="dv">0</span>))</span>
<span id="cb202-6"><a href="optimization-algorithms---basics.html#cb202-6" tabindex="-1"></a>  slope <span class="ot">&lt;-</span> (high<span class="sc">-</span>low)<span class="sc">/</span>(change<span class="sc">*</span><span class="dv">2</span>)</span>
<span id="cb202-7"><a href="optimization-algorithms---basics.html#cb202-7" tabindex="-1"></a>  <span class="fu">return</span>(slope)</span>
<span id="cb202-8"><a href="optimization-algorithms---basics.html#cb202-8" tabindex="-1"></a>}</span>
<span id="cb202-9"><a href="optimization-algorithms---basics.html#cb202-9" tabindex="-1"></a><span class="fu">firstM</span>(<span class="fu">mean</span>(X), <span class="fu">sd</span>(X))</span></code></pre></div>
<pre><code>## [1] 1.421085e-10</code></pre>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="optimization-algorithms---basics.html#cb204-1" tabindex="-1"></a><span class="co"># First partial (numerical) derivative w.r.t. sd  </span></span>
<span id="cb204-2"><a href="optimization-algorithms---basics.html#cb204-2" tabindex="-1"></a>firstSD <span class="ot">&lt;-</span> <span class="cf">function</span>(p1, p2, <span class="at">change =</span> <span class="fl">0.0001</span>){      </span>
<span id="cb204-3"><a href="optimization-algorithms---basics.html#cb204-3" tabindex="-1"></a>  prmt <span class="ot">&lt;-</span> <span class="fu">c</span>(p1, p2)</span>
<span id="cb204-4"><a href="optimization-algorithms---basics.html#cb204-4" tabindex="-1"></a>  high <span class="ot">&lt;-</span> <span class="fu">normalLL</span>(prmt <span class="sc">+</span> <span class="fu">c</span>(<span class="dv">0</span>, change))</span>
<span id="cb204-5"><a href="optimization-algorithms---basics.html#cb204-5" tabindex="-1"></a>  low <span class="ot">&lt;-</span> <span class="fu">normalLL</span>(prmt <span class="sc">-</span> <span class="fu">c</span>(<span class="dv">0</span>, change))</span>
<span id="cb204-6"><a href="optimization-algorithms---basics.html#cb204-6" tabindex="-1"></a>  slope <span class="ot">&lt;-</span> (high<span class="sc">-</span>low)<span class="sc">/</span>(change<span class="sc">*</span><span class="dv">2</span>)</span>
<span id="cb204-7"><a href="optimization-algorithms---basics.html#cb204-7" tabindex="-1"></a>  <span class="fu">return</span>(slope)</span>
<span id="cb204-8"><a href="optimization-algorithms---basics.html#cb204-8" tabindex="-1"></a>}</span>
<span id="cb204-9"><a href="optimization-algorithms---basics.html#cb204-9" tabindex="-1"></a></span>
<span id="cb204-10"><a href="optimization-algorithms---basics.html#cb204-10" tabindex="-1"></a><span class="fu">firstSD</span>(<span class="fu">mean</span>(X), <span class="fu">sd</span>(X))</span></code></pre></div>
<pre><code>## [1] -1.104417</code></pre>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="optimization-algorithms---basics.html#cb206-1" tabindex="-1"></a><span class="co">#Verify them with the grad()</span></span>
<span id="cb206-2"><a href="optimization-algorithms---basics.html#cb206-2" tabindex="-1"></a><span class="fu">library</span>(numDeriv)</span>
<span id="cb206-3"><a href="optimization-algorithms---basics.html#cb206-3" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb206-4"><a href="optimization-algorithms---basics.html#cb206-4" tabindex="-1"></a>    a <span class="ot">&lt;-</span> x[<span class="dv">1</span>]; b <span class="ot">&lt;-</span> x[<span class="dv">2</span>]  </span>
<span id="cb206-5"><a href="optimization-algorithms---basics.html#cb206-5" tabindex="-1"></a>    <span class="fu">sum</span>(<span class="fu">dnorm</span>(X, <span class="at">mean =</span> a, <span class="at">sd =</span> b, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb206-6"><a href="optimization-algorithms---basics.html#cb206-6" tabindex="-1"></a>    }</span>
<span id="cb206-7"><a href="optimization-algorithms---basics.html#cb206-7" tabindex="-1"></a><span class="fu">grad</span>(f,<span class="fu">c</span>(<span class="fu">mean</span>(X),<span class="fu">sd</span>(X)))[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>## [1] -1.367073e-12</code></pre>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="optimization-algorithms---basics.html#cb208-1" tabindex="-1"></a><span class="fu">grad</span>(f,<span class="fu">c</span>(<span class="fu">mean</span>(X),<span class="fu">sd</span>(X)))[<span class="dv">2</span>]</span></code></pre></div>
<pre><code>## [1] -1.104419</code></pre>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="optimization-algorithms---basics.html#cb210-1" tabindex="-1"></a><span class="co"># Or better</span></span>
<span id="cb210-2"><a href="optimization-algorithms---basics.html#cb210-2" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">jacobian</span>(f,<span class="fu">c</span>(<span class="fu">mean</span>(X),<span class="fu">sd</span>(X))), <span class="dv">4</span>) <span class="co">#First derivatives</span></span></code></pre></div>
<pre><code>##      [,1]    [,2]
## [1,]    0 -1.1044</code></pre>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="optimization-algorithms---basics.html#cb212-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">hessian</span>(f,<span class="fu">c</span>(<span class="fu">mean</span>(X),<span class="fu">sd</span>(X))), <span class="dv">4</span>) <span class="co">#Second derivatives</span></span></code></pre></div>
<pre><code>##           [,1]     [,2]
## [1,] -121.9741    0.000
## [2,]    0.0000 -240.289</code></pre>
<p>Let’s try them now in the Newton-Raphson method.</p>
<p><span class="math display">\[
x_{n+1}=x_{n}-\frac{g^{\prime}\left(x_{n}\right)}{g^{\prime \prime}\left(x_{n}\right)}
\]</span></p>
<p>Similar to the first one, we can also develop a function that calculates the second derivatives. However, instead of using our own functions, let’s use <code>grad()</code> and <code>hessian()</code> from the <code>numDeriv</code> package.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="optimization-algorithms---basics.html#cb214-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2019</span>)</span>
<span id="cb214-2"><a href="optimization-algorithms---basics.html#cb214-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb214-3"><a href="optimization-algorithms---basics.html#cb214-3" tabindex="-1"></a></span>
<span id="cb214-4"><a href="optimization-algorithms---basics.html#cb214-4" tabindex="-1"></a>NR <span class="ot">&lt;-</span> <span class="cf">function</span>(f, x0, y0, <span class="at">tol =</span> <span class="fl">1e-5</span>, <span class="at">n =</span> <span class="dv">1000</span>) {</span>
<span id="cb214-5"><a href="optimization-algorithms---basics.html#cb214-5" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb214-6"><a href="optimization-algorithms---basics.html#cb214-6" tabindex="-1"></a>    dx <span class="ot">&lt;-</span> <span class="fu">grad</span>(f,<span class="fu">c</span>(x0, y0))[<span class="dv">1</span>] <span class="co"># First-order derivative f&#39;(x0)</span></span>
<span id="cb214-7"><a href="optimization-algorithms---basics.html#cb214-7" tabindex="-1"></a>    ddx <span class="ot">&lt;-</span> <span class="fu">hessian</span>(f,<span class="fu">c</span>(x0, y0))[<span class="dv">1</span>,<span class="dv">1</span>] <span class="co"># Second-order derivative f&#39;&#39;(x0)</span></span>
<span id="cb214-8"><a href="optimization-algorithms---basics.html#cb214-8" tabindex="-1"></a>    x1 <span class="ot">&lt;-</span> x0 <span class="sc">-</span> (dx <span class="sc">/</span> ddx) <span class="co"># Calculate next value x1</span></span>
<span id="cb214-9"><a href="optimization-algorithms---basics.html#cb214-9" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">abs</span>(x1 <span class="sc">-</span> x0) <span class="sc">&lt;</span> tol) {</span>
<span id="cb214-10"><a href="optimization-algorithms---basics.html#cb214-10" tabindex="-1"></a>        res <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;The mean approximation is &quot;</span>, x1, <span class="st">&quot; in &quot;</span>, i, <span class="st">&quot; iterations&quot;</span>)</span>
<span id="cb214-11"><a href="optimization-algorithms---basics.html#cb214-11" tabindex="-1"></a>        <span class="fu">return</span>(res)</span>
<span id="cb214-12"><a href="optimization-algorithms---basics.html#cb214-12" tabindex="-1"></a>      }</span>
<span id="cb214-13"><a href="optimization-algorithms---basics.html#cb214-13" tabindex="-1"></a>    <span class="co"># If Newton-Raphson has not yet reached convergence set x1 as x0 and continue</span></span>
<span id="cb214-14"><a href="optimization-algorithms---basics.html#cb214-14" tabindex="-1"></a>    x0 <span class="ot">&lt;-</span> x1</span>
<span id="cb214-15"><a href="optimization-algorithms---basics.html#cb214-15" tabindex="-1"></a>  }</span>
<span id="cb214-16"><a href="optimization-algorithms---basics.html#cb214-16" tabindex="-1"></a>  <span class="fu">print</span>(<span class="st">&#39;Too many iterations in method&#39;</span>)</span>
<span id="cb214-17"><a href="optimization-algorithms---basics.html#cb214-17" tabindex="-1"></a>}</span>
<span id="cb214-18"><a href="optimization-algorithms---basics.html#cb214-18" tabindex="-1"></a></span>
<span id="cb214-19"><a href="optimization-algorithms---basics.html#cb214-19" tabindex="-1"></a>func <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb214-20"><a href="optimization-algorithms---basics.html#cb214-20" tabindex="-1"></a>    a <span class="ot">&lt;-</span> x[<span class="dv">1</span>]; b <span class="ot">&lt;-</span> x[<span class="dv">2</span>]  </span>
<span id="cb214-21"><a href="optimization-algorithms---basics.html#cb214-21" tabindex="-1"></a>    <span class="fu">sum</span>(<span class="fu">dnorm</span>(X, <span class="at">mean =</span> a, <span class="at">sd =</span> b, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb214-22"><a href="optimization-algorithms---basics.html#cb214-22" tabindex="-1"></a>    }</span>
<span id="cb214-23"><a href="optimization-algorithms---basics.html#cb214-23" tabindex="-1"></a></span>
<span id="cb214-24"><a href="optimization-algorithms---basics.html#cb214-24" tabindex="-1"></a><span class="fu">NR</span>(func, <span class="sc">-</span><span class="dv">3</span>, <span class="fl">1.5</span>)</span></code></pre></div>
<pre><code>## [1] &quot;The mean approximation is  1.85333200301108  in  2  iterations&quot;</code></pre>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="optimization-algorithms---basics.html#cb216-1" tabindex="-1"></a><span class="co">#Let;s verify it</span></span>
<span id="cb216-2"><a href="optimization-algorithms---basics.html#cb216-2" tabindex="-1"></a><span class="fu">mean</span>(X)</span></code></pre></div>
<pre><code>## [1] 1.853332</code></pre>
<p>Finding sd is left to the practice questions. But the way to do it should be obvious. Use our approximation of the mean (1.853332) as a fixed parameter in the function and run the same algorithm for finding sd. <strong>When the power of computers and the genius of mathematics intercepts, beautiful magics happen</strong>.</p>
</div>
<div id="ml-estimation-with-logistic-regression" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> ML Estimation with logistic regression<a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The pdf of Bernoulli distribution is</p>
<p><span class="math display">\[
P(Y=y)=p^y(1-p)^{1-y}
\]</span>
It’s likelihood</p>
<p><span class="math display">\[
\begin{aligned}
L(\boldsymbol{\beta} \mid \mathbf{y} ; \mathbf{x}) &amp;=L\left(\beta_0, \beta_1 \mid\left(y_1, \ldots, y_n\right) ;\left(x_1, \ldots, x_n\right)\right) \\
&amp;=\prod_{i=1}^n p_i^{y_i}\left(1-p_i\right)^{1-y_i}
\end{aligned}
\]</span>
And log-likelihood</p>
<p><span class="math display">\[
\begin{aligned}
\ell(\boldsymbol{\beta} \mid \mathbf{y} ; \mathbf{x}) &amp;=\log \left(\prod_{i=1}^n p_i^{y_i}\left(1-p_i\right)^{1-y_i}\right) \\
&amp;=\sum_{i=1}^n\left( \log \left(p_i^{y_i}\right)+\log \left(1-p_i\right)^{1-y_i}\right) \\
&amp;=\sum_{i=1}^n y_i \left(\log \left(p_i\right)+\left(1-y_i\right) \log \left(1-p_i\right)\right)
\end{aligned}
\]</span>
where</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{L}\left(p_i\right) &amp;=\log \left(\frac{p_i}{1-p_i}\right) \\
&amp;=\beta_0+\beta_1 x_1
\end{aligned}
\]</span></p>
<p>So,</p>
<p><span class="math display">\[
p_i=\frac{\exp \left(\beta_0+x_1 \beta_1\right)}{1+\exp \left(\beta_0+x_1 \beta_1\right)}
\]</span>
First partial derivative with respect to <span class="math inline">\(\beta_0\)</span>
<span class="math display">\[
\begin{aligned}
\frac{\partial p_i}{\partial \beta_0} &amp;=\frac{\exp \left(\beta_0+x_1 \beta_1\right)}{\left(1+\exp \left(\beta_0+x_1 \beta_1\right)\right)^2} \\
&amp;=p_i\left(1-p_i\right)
\end{aligned}
\]</span>
And</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial p_i}{\partial \beta_1} &amp;=\frac{x_1 \exp \left(\beta_0+x_1 \beta_1\right)}{\left(1+\exp \left(\beta_0+x_1 \beta_1\right)\right)^2} \\
&amp;=x_1 p_i\left(1-p_i\right)
\end{aligned}
\]</span>
Newton-Raphson’s equation is</p>
<p><span class="math display">\[
\boldsymbol{\beta}^{(t+1)}=\boldsymbol{\beta}^{(t)}-\left(\boldsymbol{H}^{(t)}\right)^{-1} \boldsymbol{u}^{(t)},
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\boldsymbol{\beta}^{(t)}=\left[\begin{array}{c}
\beta_0^{(t)} \\
\beta_1^{(t)}
\end{array}\right]
\]</span></p>
<p><span class="math display">\[
\boldsymbol{u}^{(t)}=\left[\begin{array}{c}
u_0^{(t)} \\
u_1^{(t)}
\end{array}\right]=\left[\begin{array}{c}
\frac{\partial \ell\left(\beta^{(t)} \mid y ; x\right)}{\partial \beta_0} \\
\frac{\partial \ell\left(\beta^{(t)} \mid y ; x\right)}{\partial \beta_1}
\end{array}\right]=\left[\begin{array}{c}
\sum_{i=1}^n \left(y_i-p_i^{(t)}\right) \\
\sum_{i=1}^n x_i\left(y_i-p_i^{(t)}\right)
\end{array}\right]
\]</span>
where,</p>
<p><span class="math display">\[
p_i^{(t)}=\frac{\exp \left(\beta_0^{(t)}+x_1 \beta_1^{(t)}\right)}{1+\exp \left(\beta_0^{(t)}+x_1 \beta_1^{(t)}\right)}
\]</span></p>
<p><span class="math inline">\(\boldsymbol{H}^{(t)}\)</span> can be considered as Jacobian matrix of <span class="math inline">\(\boldsymbol{u}(\cdot)\)</span>,</p>
<p><span class="math display">\[
\boldsymbol{H}^{(t)}=\left[\begin{array}{ll}
\frac{\partial u_0^{(t)}}{\partial \beta_0} &amp; \frac{\partial u_0^{(t)}}{\partial \beta_1} \\
\frac{\partial u_1^{(t)}}{\partial \beta_0} &amp; \frac{\partial u_1^{(t)}}{\partial \beta_1}
\end{array}\right]
\]</span>
Let’s simulate data and solve it the Newton-Raphson’s method described above.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="optimization-algorithms---basics.html#cb218-1" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list=</span><span class="fu">ls</span>())</span>
<span id="cb218-2"><a href="optimization-algorithms---basics.html#cb218-2" tabindex="-1"></a></span>
<span id="cb218-3"><a href="optimization-algorithms---basics.html#cb218-3" tabindex="-1"></a><span class="co">#Simulating data</span></span>
<span id="cb218-4"><a href="optimization-algorithms---basics.html#cb218-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) </span>
<span id="cb218-5"><a href="optimization-algorithms---basics.html#cb218-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span> </span>
<span id="cb218-6"><a href="optimization-algorithms---basics.html#cb218-6" tabindex="-1"></a>X <span class="ot">=</span> <span class="fu">rnorm</span>(n) <span class="co"># this is our x</span></span>
<span id="cb218-7"><a href="optimization-algorithms---basics.html#cb218-7" tabindex="-1"></a>z <span class="ot">=</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> X</span>
<span id="cb218-8"><a href="optimization-algorithms---basics.html#cb218-8" tabindex="-1"></a></span>
<span id="cb218-9"><a href="optimization-algorithms---basics.html#cb218-9" tabindex="-1"></a><span class="co">#Prob. is defined by logistic function</span></span>
<span id="cb218-10"><a href="optimization-algorithms---basics.html#cb218-10" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>z))</span>
<span id="cb218-11"><a href="optimization-algorithms---basics.html#cb218-11" tabindex="-1"></a></span>
<span id="cb218-12"><a href="optimization-algorithms---basics.html#cb218-12" tabindex="-1"></a><span class="co">#Bernoulli is the special case of the binomial distribution with size = 1</span></span>
<span id="cb218-13"><a href="optimization-algorithms---basics.html#cb218-13" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> p)</span>
<span id="cb218-14"><a href="optimization-algorithms---basics.html#cb218-14" tabindex="-1"></a></span>
<span id="cb218-15"><a href="optimization-algorithms---basics.html#cb218-15" tabindex="-1"></a><span class="co">#And we create our data</span></span>
<span id="cb218-16"><a href="optimization-algorithms---basics.html#cb218-16" tabindex="-1"></a>df <span class="ot">&lt;-</span>  <span class="fu">data.frame</span>(y, X)</span>
<span id="cb218-17"><a href="optimization-algorithms---basics.html#cb218-17" tabindex="-1"></a><span class="fu">head</span>(df)</span></code></pre></div>
<pre><code>##   y          X
## 1 0 -0.6264538
## 2 0  0.1836433
## 3 0 -0.8356286
## 4 0  1.5952808
## 5 0  0.3295078
## 6 0 -0.8204684</code></pre>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="optimization-algorithms---basics.html#cb220-1" tabindex="-1"></a>logis <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> X, <span class="at">data =</span> df, <span class="at">family =</span> binomial)</span>
<span id="cb220-2"><a href="optimization-algorithms---basics.html#cb220-2" tabindex="-1"></a><span class="fu">summary</span>(logis)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ X, family = binomial, data = df)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -1.8253     0.1867  -9.776   &lt;2e-16 ***
## X             2.7809     0.2615  10.635   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 605.69  on 499  degrees of freedom
## Residual deviance: 328.13  on 498  degrees of freedom
## AIC: 332.13
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="optimization-algorithms---basics.html#cb222-1" tabindex="-1"></a><span class="fu">library</span>(numDeriv)</span>
<span id="cb222-2"><a href="optimization-algorithms---basics.html#cb222-2" tabindex="-1"></a></span>
<span id="cb222-3"><a href="optimization-algorithms---basics.html#cb222-3" tabindex="-1"></a>func_u <span class="ot">&lt;-</span> <span class="cf">function</span>(b) {</span>
<span id="cb222-4"><a href="optimization-algorithms---basics.html#cb222-4" tabindex="-1"></a>  <span class="fu">c</span>(<span class="fu">sum</span>(df<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">exp</span>(b[<span class="dv">1</span>] <span class="sc">+</span> b[<span class="dv">2</span>] <span class="sc">*</span> df<span class="sc">$</span>X)<span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(b[<span class="dv">1</span>] <span class="sc">+</span> b[<span class="dv">2</span>] <span class="sc">*</span> df<span class="sc">$</span>X))),</span>
<span id="cb222-5"><a href="optimization-algorithms---basics.html#cb222-5" tabindex="-1"></a>    <span class="fu">sum</span>(df<span class="sc">$</span>X <span class="sc">*</span> (df<span class="sc">$</span>y <span class="sc">-</span> <span class="fu">exp</span>(b[<span class="dv">1</span>] <span class="sc">+</span> b[<span class="dv">2</span>] <span class="sc">*</span> df<span class="sc">$</span>X)<span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(b[<span class="dv">1</span>] <span class="sc">+</span> b[<span class="dv">2</span>] <span class="sc">*</span> df<span class="sc">$</span>X)))))</span>
<span id="cb222-6"><a href="optimization-algorithms---basics.html#cb222-6" tabindex="-1"></a>}</span>
<span id="cb222-7"><a href="optimization-algorithms---basics.html#cb222-7" tabindex="-1"></a></span>
<span id="cb222-8"><a href="optimization-algorithms---basics.html#cb222-8" tabindex="-1"></a><span class="co"># Starting points</span></span>
<span id="cb222-9"><a href="optimization-algorithms---basics.html#cb222-9" tabindex="-1"></a>delta <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="at">nrow =</span> <span class="dv">2</span>) <span class="co"># starting delta container (with any number &gt; 0) </span></span>
<span id="cb222-10"><a href="optimization-algorithms---basics.html#cb222-10" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb222-11"><a href="optimization-algorithms---basics.html#cb222-11" tabindex="-1"></a></span>
<span id="cb222-12"><a href="optimization-algorithms---basics.html#cb222-12" tabindex="-1"></a><span class="cf">while</span>(<span class="fu">abs</span>(<span class="fu">sum</span>(delta)) <span class="sc">&gt;</span> <span class="fl">0.0001</span>){</span>
<span id="cb222-13"><a href="optimization-algorithms---basics.html#cb222-13" tabindex="-1"></a>  B <span class="ot">&lt;-</span> b <span class="co">#current b </span></span>
<span id="cb222-14"><a href="optimization-algorithms---basics.html#cb222-14" tabindex="-1"></a>  b <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(b) <span class="sc">-</span> <span class="fu">solve</span>(<span class="fu">jacobian</span>(func_u, <span class="at">x =</span> b)) <span class="sc">%*%</span> <span class="fu">func_u</span>(b) <span class="co">#new b </span></span>
<span id="cb222-15"><a href="optimization-algorithms---basics.html#cb222-15" tabindex="-1"></a>  delta <span class="ot">&lt;-</span> b <span class="sc">-</span> <span class="fu">as.matrix</span>(B)</span>
<span id="cb222-16"><a href="optimization-algorithms---basics.html#cb222-16" tabindex="-1"></a>}</span>
<span id="cb222-17"><a href="optimization-algorithms---basics.html#cb222-17" tabindex="-1"></a>b</span></code></pre></div>
<pre><code>##           [,1]
## [1,] -1.825347
## [2,]  2.780929</code></pre>
</div>
<div id="gradient-descent-algorithm" class="section level2 hasAnchor" number="12.4">
<h2><span class="header-section-number">12.4</span> Gradient Descent Algorithm<a href="optimization-algorithms---basics.html#gradient-descent-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s start with a regression problem. The cost function in OLS is the residual sum of squares, <span class="math inline">\(\mathrm{RSS}=\sum_{i=1}^n\left(\widehat{e}_i\right)^2=\sum_{i=1}^n\left(y_i-\hat{y}\right)^2=\sum_{i=1}^n\left(y_i-\left(b_1+b_2 x_i\right)\right)^2\)</span>, which is a convex function. Our objective to find <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> that minimize RSS. How can we find those parameters to minimize a cost function if we don’t know much about it? The trick is to start with some point and move a bit (locally) in the direction that reduces the value of the cost function. In general, this search process for finding the minimizing point has two components: the direction and the step size. The direction tells us which direction we move next, and the step size determines how far we move in that direction. For example, the iterative search for <span class="math inline">\(b_2\)</span> of gradient descent can be described by the following recursive rule:</p>
<p><span class="math display">\[
b_2^{(k+1)}=b_2^{(k)}-lr \nabla RSS^{k}
\]</span>
Here, <span class="math inline">\(lr\)</span> is <em>learning rate</em> and <span class="math inline">\(\nabla RSS^{k}\)</span> is the slope of RSS at step <span class="math inline">\(k\)</span>. Hence, <span class="math inline">\(lr \nabla RSS^{k}\)</span> is the total step size at step <span class="math inline">\(k\)</span>. Note that, as we move from either directions towards <span class="math inline">\(b^*_2\)</span>, <span class="math inline">\(\nabla RSS^{k}\)</span> gets smaller. In fact, it becomes zero at <span class="math inline">\(b^*_2\)</span>. Therefore, <span class="math inline">\(\nabla RSS^{k}\)</span> helps iterations find the proper adjustment in each step in terms of direction and magnitude. Since RSS is a convex function, it’s easy to see how sign of <span class="math inline">\(\nabla RSS^{k}\)</span> will direct the arbitrary <span class="math inline">\(b_2^{&#39;&#39;}\)</span> towards the optimal <span class="math inline">\(b_2\)</span>.</p>
<p><img src="png/gradient.png" width="85%" height="80%" /></p>
<p>Since first-order approximation at <span class="math inline">\(b_2^{&#39;&#39;}\)</span> is good only for small <span class="math inline">\(\Delta b_2\)</span>, a small <span class="math inline">\(lr&gt;0\)</span> is needed to o make <span class="math inline">\(\Delta b_2\)</span> small in magnitude. Moreover, when a high learning rate used it leads to “overshooting” past the local minima and may result in diverging algorithm.</p>
<p>Below, we first use a simple linear regression function on simulated data and estimate its parameters with <code>lm()</code>. Let’s simulate a sample with our DGM.</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="optimization-algorithms---basics.html#cb224-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1001</span>)</span>
<span id="cb224-2"><a href="optimization-algorithms---basics.html#cb224-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb224-3"><a href="optimization-algorithms---basics.html#cb224-3" tabindex="-1"></a>int <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, N)</span>
<span id="cb224-4"><a href="optimization-algorithms---basics.html#cb224-4" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb224-5"><a href="optimization-algorithms---basics.html#cb224-5" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">2</span><span class="sc">*</span>x1 <span class="sc">+</span> int, <span class="dv">1</span>)</span>
<span id="cb224-6"><a href="optimization-algorithms---basics.html#cb224-6" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> x1)</span>
<span id="cb224-7"><a href="optimization-algorithms---basics.html#cb224-7" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">coef</span>(model)</span>
<span id="cb224-8"><a href="optimization-algorithms---basics.html#cb224-8" tabindex="-1"></a>b</span></code></pre></div>
<pre><code>## (Intercept)          x1 
##    1.209597    1.979643</code></pre>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="optimization-algorithms---basics.html#cb226-1" tabindex="-1"></a><span class="fu">plot</span>(x1, Y, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">20</span>)</span>
<span id="cb226-2"><a href="optimization-algorithms---basics.html#cb226-2" tabindex="-1"></a><span class="fu">abline</span>(b)</span></code></pre></div>
<p><img src="12-AlgorithmicOpt_files/figure-html/opt12-1.png" width="672" /></p>
<p>The cost function that we want to minimize is</p>
<p><span class="math display">\[
y_i = 1 + 2x_i + \epsilon_i \\
RSS = \sum{\epsilon_i^2}=\sum{(y_i-1-2x_i)^2}
\]</span>
And, its plot for a range of coefficients is already shown earlier.</p>
<div id="one-variable" class="section level3 hasAnchor" number="12.4.1">
<h3><span class="header-section-number">12.4.1</span> One-variable<a href="optimization-algorithms---basics.html#one-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Below, we create a function, <code>grdescent</code>, to show how sensitive gradient descent algorithms would be to different calibrations:</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="optimization-algorithms---basics.html#cb227-1" tabindex="-1"></a>grdescent <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y, lr, maxiter) {</span>
<span id="cb227-2"><a href="optimization-algorithms---basics.html#cb227-2" tabindex="-1"></a>  <span class="co">#starting points</span></span>
<span id="cb227-3"><a href="optimization-algorithms---basics.html#cb227-3" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">234</span>)</span>
<span id="cb227-4"><a href="optimization-algorithms---basics.html#cb227-4" tabindex="-1"></a>  b <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb227-5"><a href="optimization-algorithms---basics.html#cb227-5" tabindex="-1"></a>  c <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb227-6"><a href="optimization-algorithms---basics.html#cb227-6" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb227-7"><a href="optimization-algorithms---basics.html#cb227-7" tabindex="-1"></a>  </span>
<span id="cb227-8"><a href="optimization-algorithms---basics.html#cb227-8" tabindex="-1"></a>  <span class="co">#function</span></span>
<span id="cb227-9"><a href="optimization-algorithms---basics.html#cb227-9" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> c <span class="sc">+</span> b <span class="sc">*</span> x</span>
<span id="cb227-10"><a href="optimization-algorithms---basics.html#cb227-10" tabindex="-1"></a>  </span>
<span id="cb227-11"><a href="optimization-algorithms---basics.html#cb227-11" tabindex="-1"></a>  <span class="co">#gradient</span></span>
<span id="cb227-12"><a href="optimization-algorithms---basics.html#cb227-12" tabindex="-1"></a>  MSE <span class="ot">&lt;-</span> <span class="fu">sum</span>((y <span class="sc">-</span> yhat) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> n</span>
<span id="cb227-13"><a href="optimization-algorithms---basics.html#cb227-13" tabindex="-1"></a>  converged <span class="ot">=</span> F</span>
<span id="cb227-14"><a href="optimization-algorithms---basics.html#cb227-14" tabindex="-1"></a>  iterations <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb227-15"><a href="optimization-algorithms---basics.html#cb227-15" tabindex="-1"></a>  </span>
<span id="cb227-16"><a href="optimization-algorithms---basics.html#cb227-16" tabindex="-1"></a>  <span class="co">#while loop</span></span>
<span id="cb227-17"><a href="optimization-algorithms---basics.html#cb227-17" tabindex="-1"></a>  <span class="cf">while</span> (converged <span class="sc">==</span> F) {</span>
<span id="cb227-18"><a href="optimization-algorithms---basics.html#cb227-18" tabindex="-1"></a>    b_new <span class="ot">&lt;-</span> b <span class="sc">-</span> ((lr <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">/</span> n)) <span class="sc">*</span> (<span class="fu">sum</span>((y <span class="sc">-</span> yhat) <span class="sc">*</span> x <span class="sc">*</span> (<span class="sc">-</span><span class="dv">1</span>))))</span>
<span id="cb227-19"><a href="optimization-algorithms---basics.html#cb227-19" tabindex="-1"></a>    c_new <span class="ot">&lt;-</span> c <span class="sc">-</span> ((lr <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">/</span> n)) <span class="sc">*</span> (<span class="fu">sum</span>(y <span class="sc">-</span> yhat) <span class="sc">*</span> (<span class="sc">-</span><span class="dv">1</span>)))</span>
<span id="cb227-20"><a href="optimization-algorithms---basics.html#cb227-20" tabindex="-1"></a>    b <span class="ot">&lt;-</span> b_new</span>
<span id="cb227-21"><a href="optimization-algorithms---basics.html#cb227-21" tabindex="-1"></a>    c <span class="ot">&lt;-</span> c_new</span>
<span id="cb227-22"><a href="optimization-algorithms---basics.html#cb227-22" tabindex="-1"></a>    yhat <span class="ot">&lt;-</span> b <span class="sc">*</span> x <span class="sc">+</span> c</span>
<span id="cb227-23"><a href="optimization-algorithms---basics.html#cb227-23" tabindex="-1"></a>    </span>
<span id="cb227-24"><a href="optimization-algorithms---basics.html#cb227-24" tabindex="-1"></a>    MSE_new <span class="ot">&lt;-</span> <span class="fu">sum</span>((y <span class="sc">-</span> yhat) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> n</span>
<span id="cb227-25"><a href="optimization-algorithms---basics.html#cb227-25" tabindex="-1"></a>    MSE <span class="ot">&lt;-</span> <span class="fu">c</span>(MSE, MSE_new)</span>
<span id="cb227-26"><a href="optimization-algorithms---basics.html#cb227-26" tabindex="-1"></a>    d <span class="ot">=</span> <span class="fu">tail</span>(<span class="fu">abs</span>(<span class="fu">diff</span>(MSE)), <span class="dv">1</span>)</span>
<span id="cb227-27"><a href="optimization-algorithms---basics.html#cb227-27" tabindex="-1"></a>    </span>
<span id="cb227-28"><a href="optimization-algorithms---basics.html#cb227-28" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">round</span>(d, <span class="dv">12</span>) <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb227-29"><a href="optimization-algorithms---basics.html#cb227-29" tabindex="-1"></a>      converged <span class="ot">=</span> T</span>
<span id="cb227-30"><a href="optimization-algorithms---basics.html#cb227-30" tabindex="-1"></a>      <span class="fu">return</span>(<span class="fu">paste</span>(<span class="st">&quot;Iterations: &quot;</span>,</span>
<span id="cb227-31"><a href="optimization-algorithms---basics.html#cb227-31" tabindex="-1"></a>                   iterations, <span class="st">&quot;Intercept: &quot;</span>,</span>
<span id="cb227-32"><a href="optimization-algorithms---basics.html#cb227-32" tabindex="-1"></a>                   c, <span class="st">&quot;Slope: &quot;</span>, b))</span>
<span id="cb227-33"><a href="optimization-algorithms---basics.html#cb227-33" tabindex="-1"></a>    }</span>
<span id="cb227-34"><a href="optimization-algorithms---basics.html#cb227-34" tabindex="-1"></a>    iterations <span class="ot">=</span> iterations <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb227-35"><a href="optimization-algorithms---basics.html#cb227-35" tabindex="-1"></a>    <span class="cf">if</span> (iterations <span class="sc">&gt;</span> maxiter) {</span>
<span id="cb227-36"><a href="optimization-algorithms---basics.html#cb227-36" tabindex="-1"></a>      converged <span class="ot">=</span> T</span>
<span id="cb227-37"><a href="optimization-algorithms---basics.html#cb227-37" tabindex="-1"></a>      <span class="fu">return</span>(<span class="fu">paste</span>(<span class="st">&quot;Max. iter. reached, &quot;</span>, <span class="st">&quot;Intercept:&quot;</span>,</span>
<span id="cb227-38"><a href="optimization-algorithms---basics.html#cb227-38" tabindex="-1"></a>                   c, <span class="st">&quot;Slope:&quot;</span>, b))</span>
<span id="cb227-39"><a href="optimization-algorithms---basics.html#cb227-39" tabindex="-1"></a>    }</span>
<span id="cb227-40"><a href="optimization-algorithms---basics.html#cb227-40" tabindex="-1"></a>  }</span>
<span id="cb227-41"><a href="optimization-algorithms---basics.html#cb227-41" tabindex="-1"></a>}</span></code></pre></div>
<p>Note that the key part in this algorithm is <code>b_new &lt;- b + (learnrate * (1 / n)) * sum((y - yhat) * x*(-1)</code>.
The first <span class="math inline">\(b\)</span> that is picked randomly by <code>b &lt;- runif(1, 0, 1)</code> is adjusted by <code>learnrate * (1 / n) * (sum((y - yhat) * -x))</code>.</p>
<p>Note that <code>sum((y - yhat) * x)</code> is the first order condition of the cost function (MSE - Residual Sum of Squares) for the slope coefficient. The cost function is a convex function where the minimum can be achieved by the optimal <span class="math inline">\(b\)</span>. It is a linear Taylor approximation of MSE at <span class="math inline">\(b\)</span> that provides the <strong>steepest</strong> descent, that is just a simple adjustment for identifying the direction of the adjustment of <span class="math inline">\(b\)</span> until the minimum MSE is reached.</p>
<p>Now we will see if this function will give us the same intercept and slope coefficients already calculated with <code>lm()</code> above.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="optimization-algorithms---basics.html#cb228-1" tabindex="-1"></a><span class="fu">grdescent</span>(x1, Y, <span class="fl">0.01</span>, <span class="dv">100000</span>) </span></code></pre></div>
<pre><code>## [1] &quot;Iterations:  16388 Intercept:  1.20949474169039 Slope:  1.97965284879392&quot;</code></pre>
<p>This is good. But, if start a very low number with a small learning rate, then we need more iteration</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="optimization-algorithms---basics.html#cb230-1" tabindex="-1"></a><span class="fu">grdescent</span>(x1, Y, <span class="fl">0.005</span>, <span class="dv">1000000</span>)</span></code></pre></div>
<pre><code>## [1] &quot;Iterations:  31363 Intercept:  1.20945256472045 Slope:  1.97965686098386&quot;</code></pre>
<p>Yes, the main question is how do we find out what the learning rate should be? A general suggestion is to keep it small and tune it within the training process. Obviously, we can have an adaptive learning rate that changes at each iteration depending on the change in the MSE. If the change is positive, for example, the learning rate can be reduced to keep the descent.</p>
</div>
<div id="adjustable-lr-and-sgd" class="section level3 hasAnchor" number="12.4.2">
<h3><span class="header-section-number">12.4.2</span> Adjustable <code>lr</code> and SGD<a href="optimization-algorithms---basics.html#adjustable-lr-and-sgd" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An adjustable learning rate has several advantages over a fixed learning rate in gradient-based optimization algorithms like stochastic gradient descent:</p>
<ul>
<li>Faster convergence: An adjustable learning rate can help the algorithm converge faster by starting with a larger learning rate, allowing the model to make bigger steps in the initial phase. This can help escape local minima or saddle points more quickly and reach the vicinity of the global minimum.</li>
<li>Improved precision: As the learning rate decreases over time, the algorithm takes smaller steps, allowing for more precise updates to the model parameters. This can help the model fine-tune its parameters and potentially achieve a lower loss value compared to a fixed learning rate.</li>
<li>Prevent oscillations: A fixed learning rate might cause oscillations around the optimal solution, whereas an adjustable learning rate can help dampen these oscillations by gradually reducing the step size. This can result in a more stable convergence.</li>
<li>Adaptive to problem complexity: Some optimization problems might require different learning rates for different stages of the optimization process. An adjustable learning rate can adapt to the problem’s complexity, allowing the model to learn at a more suitable pace for each stage.</li>
<li>Robustness: An adjustable learning rate can make the optimization algorithm more robust to the choice of the initial learning rate. Even if the initial learning rate is not perfect, the algorithm can adapt over time and still reach a reasonable solution.</li>
</ul>
<p>Although an adjustable learning rate can lead to faster convergence, improved precision, and better overall performance in gradient-based optimization algorithms, it also introduces additional hyperparameters (e.g., decay rate, annealing schedule) that need to be tuned for optimal performance.</p>
<p>Below, we made some changes to earlier gradient descent to make it stochastic with an adaptive learning rate. In this modified code, we have implemented the following changes:</p>
<ul>
<li>Shuffled the data points using the <code>sample()</code> function.</li>
<li>Iterated over the data points in mini-batches of size 1 (<code>batch_size = 1</code>). This makes it <strong>stochastic gradient descent</strong>.</li>
<li>Re-calculated the gradients and updated the weights for each mini-batch.</li>
</ul>
<p>This should give us a simple stochastic gradient descent implementation for our linear regression problem. To implement an adjustable learning rate, we can use a learning rate scheduler or a learning rate annealing method. The following example shows how to use a simple exponential learning rate annealing method, which will decrease the learning rate over time:</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="optimization-algorithms---basics.html#cb232-1" tabindex="-1"></a><span class="co"># Set the seed</span></span>
<span id="cb232-2"><a href="optimization-algorithms---basics.html#cb232-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1001</span>)</span>
<span id="cb232-3"><a href="optimization-algorithms---basics.html#cb232-3" tabindex="-1"></a></span>
<span id="cb232-4"><a href="optimization-algorithms---basics.html#cb232-4" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb232-5"><a href="optimization-algorithms---basics.html#cb232-5" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb232-6"><a href="optimization-algorithms---basics.html#cb232-6" tabindex="-1"></a>int <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, N)</span>
<span id="cb232-7"><a href="optimization-algorithms---basics.html#cb232-7" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb232-8"><a href="optimization-algorithms---basics.html#cb232-8" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">2</span><span class="sc">*</span>x1 <span class="sc">+</span> int, <span class="dv">1</span>)</span>
<span id="cb232-9"><a href="optimization-algorithms---basics.html#cb232-9" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> x1)</span>
<span id="cb232-10"><a href="optimization-algorithms---basics.html#cb232-10" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">coef</span>(model)</span>
<span id="cb232-11"><a href="optimization-algorithms---basics.html#cb232-11" tabindex="-1"></a>b</span></code></pre></div>
<pre><code>## (Intercept)          x1 
##    1.209597    1.979643</code></pre>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="optimization-algorithms---basics.html#cb234-1" tabindex="-1"></a><span class="co"># Starting points</span></span>
<span id="cb234-2"><a href="optimization-algorithms---basics.html#cb234-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">234</span>)</span>
<span id="cb234-3"><a href="optimization-algorithms---basics.html#cb234-3" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb234-4"><a href="optimization-algorithms---basics.html#cb234-4" tabindex="-1"></a>c <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb234-5"><a href="optimization-algorithms---basics.html#cb234-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(x1)</span>
<span id="cb234-6"><a href="optimization-algorithms---basics.html#cb234-6" tabindex="-1"></a></span>
<span id="cb234-7"><a href="optimization-algorithms---basics.html#cb234-7" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb234-8"><a href="optimization-algorithms---basics.html#cb234-8" tabindex="-1"></a>initial_learning_rate <span class="ot">&lt;-</span> <span class="fl">0.01</span></span>
<span id="cb234-9"><a href="optimization-algorithms---basics.html#cb234-9" tabindex="-1"></a>decay_rate <span class="ot">&lt;-</span> <span class="fl">0.99999</span></span>
<span id="cb234-10"><a href="optimization-algorithms---basics.html#cb234-10" tabindex="-1"></a>batch_size <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb234-11"><a href="optimization-algorithms---basics.html#cb234-11" tabindex="-1"></a>max_iterations <span class="ot">&lt;-</span> <span class="dv">300000</span></span>
<span id="cb234-12"><a href="optimization-algorithms---basics.html#cb234-12" tabindex="-1"></a>tolerance <span class="ot">&lt;-</span> <span class="fl">1e-12</span></span>
<span id="cb234-13"><a href="optimization-algorithms---basics.html#cb234-13" tabindex="-1"></a></span>
<span id="cb234-14"><a href="optimization-algorithms---basics.html#cb234-14" tabindex="-1"></a><span class="co"># Function</span></span>
<span id="cb234-15"><a href="optimization-algorithms---basics.html#cb234-15" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> c <span class="sc">+</span> b <span class="sc">*</span> x1</span>
<span id="cb234-16"><a href="optimization-algorithms---basics.html#cb234-16" tabindex="-1"></a></span>
<span id="cb234-17"><a href="optimization-algorithms---basics.html#cb234-17" tabindex="-1"></a><span class="co"># Gradient</span></span>
<span id="cb234-18"><a href="optimization-algorithms---basics.html#cb234-18" tabindex="-1"></a>MSE <span class="ot">&lt;-</span> <span class="fu">sum</span>((Y <span class="sc">-</span> yhat) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> n</span>
<span id="cb234-19"><a href="optimization-algorithms---basics.html#cb234-19" tabindex="-1"></a>converged <span class="ot">=</span> F</span>
<span id="cb234-20"><a href="optimization-algorithms---basics.html#cb234-20" tabindex="-1"></a>iterations <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb234-21"><a href="optimization-algorithms---basics.html#cb234-21" tabindex="-1"></a>num_batches <span class="ot">&lt;-</span> <span class="fu">ceiling</span>(n <span class="sc">/</span> batch_size)</span>
<span id="cb234-22"><a href="optimization-algorithms---basics.html#cb234-22" tabindex="-1"></a></span>
<span id="cb234-23"><a href="optimization-algorithms---basics.html#cb234-23" tabindex="-1"></a><span class="co"># While loop</span></span>
<span id="cb234-24"><a href="optimization-algorithms---basics.html#cb234-24" tabindex="-1"></a><span class="cf">while</span> (converged <span class="sc">==</span> F) {</span>
<span id="cb234-25"><a href="optimization-algorithms---basics.html#cb234-25" tabindex="-1"></a>  <span class="co"># Shuffle data points</span></span>
<span id="cb234-26"><a href="optimization-algorithms---basics.html#cb234-26" tabindex="-1"></a>  indices <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n)</span>
<span id="cb234-27"><a href="optimization-algorithms---basics.html#cb234-27" tabindex="-1"></a>  </span>
<span id="cb234-28"><a href="optimization-algorithms---basics.html#cb234-28" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq</span>(<span class="dv">1</span>, n, <span class="at">by =</span> batch_size)) {</span>
<span id="cb234-29"><a href="optimization-algorithms---basics.html#cb234-29" tabindex="-1"></a>    idx <span class="ot">&lt;-</span> indices[i<span class="sc">:</span><span class="fu">min</span>(i <span class="sc">+</span> batch_size <span class="sc">-</span> <span class="dv">1</span>, n)]</span>
<span id="cb234-30"><a href="optimization-algorithms---basics.html#cb234-30" tabindex="-1"></a>    x_batch <span class="ot">&lt;-</span> x1[idx]</span>
<span id="cb234-31"><a href="optimization-algorithms---basics.html#cb234-31" tabindex="-1"></a>    y_batch <span class="ot">&lt;-</span> Y[idx]</span>
<span id="cb234-32"><a href="optimization-algorithms---basics.html#cb234-32" tabindex="-1"></a>    </span>
<span id="cb234-33"><a href="optimization-algorithms---basics.html#cb234-33" tabindex="-1"></a>    yhat_batch <span class="ot">&lt;-</span> c <span class="sc">+</span> b <span class="sc">*</span> x_batch</span>
<span id="cb234-34"><a href="optimization-algorithms---basics.html#cb234-34" tabindex="-1"></a>    </span>
<span id="cb234-35"><a href="optimization-algorithms---basics.html#cb234-35" tabindex="-1"></a>    learning_rate <span class="ot">&lt;-</span> initial_learning_rate <span class="sc">*</span> decay_rate<span class="sc">^</span>iterations</span>
<span id="cb234-36"><a href="optimization-algorithms---basics.html#cb234-36" tabindex="-1"></a>    </span>
<span id="cb234-37"><a href="optimization-algorithms---basics.html#cb234-37" tabindex="-1"></a>    b_new <span class="ot">&lt;-</span> b <span class="sc">-</span> learning_rate <span class="sc">*</span> ((<span class="dv">1</span> <span class="sc">/</span> <span class="fu">length</span>(idx)) <span class="sc">*</span> </span>
<span id="cb234-38"><a href="optimization-algorithms---basics.html#cb234-38" tabindex="-1"></a>                                    <span class="fu">sum</span>((y_batch <span class="sc">-</span> yhat_batch) <span class="sc">*</span></span>
<span id="cb234-39"><a href="optimization-algorithms---basics.html#cb234-39" tabindex="-1"></a>                                          x_batch <span class="sc">*</span> (<span class="sc">-</span><span class="dv">1</span>)))</span>
<span id="cb234-40"><a href="optimization-algorithms---basics.html#cb234-40" tabindex="-1"></a>    c_new <span class="ot">&lt;-</span> c <span class="sc">-</span> learning_rate <span class="sc">*</span> ((<span class="dv">1</span> <span class="sc">/</span> <span class="fu">length</span>(idx)) <span class="sc">*</span></span>
<span id="cb234-41"><a href="optimization-algorithms---basics.html#cb234-41" tabindex="-1"></a>                                    <span class="fu">sum</span>(y_batch <span class="sc">-</span> yhat_batch) <span class="sc">*</span> (<span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb234-42"><a href="optimization-algorithms---basics.html#cb234-42" tabindex="-1"></a>    </span>
<span id="cb234-43"><a href="optimization-algorithms---basics.html#cb234-43" tabindex="-1"></a>    b <span class="ot">&lt;-</span> b_new</span>
<span id="cb234-44"><a href="optimization-algorithms---basics.html#cb234-44" tabindex="-1"></a>    c <span class="ot">&lt;-</span> c_new</span>
<span id="cb234-45"><a href="optimization-algorithms---basics.html#cb234-45" tabindex="-1"></a>  }</span>
<span id="cb234-46"><a href="optimization-algorithms---basics.html#cb234-46" tabindex="-1"></a>  </span>
<span id="cb234-47"><a href="optimization-algorithms---basics.html#cb234-47" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> b <span class="sc">*</span> x1 <span class="sc">+</span> c</span>
<span id="cb234-48"><a href="optimization-algorithms---basics.html#cb234-48" tabindex="-1"></a>  MSE_new <span class="ot">&lt;-</span> <span class="fu">sum</span>((Y <span class="sc">-</span> yhat) <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">/</span> n</span>
<span id="cb234-49"><a href="optimization-algorithms---basics.html#cb234-49" tabindex="-1"></a>  d <span class="ot">=</span> <span class="fu">abs</span>(MSE_new <span class="sc">-</span> <span class="fu">tail</span>(MSE, <span class="dv">1</span>))</span>
<span id="cb234-50"><a href="optimization-algorithms---basics.html#cb234-50" tabindex="-1"></a>  </span>
<span id="cb234-51"><a href="optimization-algorithms---basics.html#cb234-51" tabindex="-1"></a>  <span class="cf">if</span> (d <span class="sc">&lt;</span> tolerance) converged <span class="ot">=</span> T</span>
<span id="cb234-52"><a href="optimization-algorithms---basics.html#cb234-52" tabindex="-1"></a>  MSE <span class="ot">&lt;-</span> <span class="fu">c</span>(MSE, MSE_new)</span>
<span id="cb234-53"><a href="optimization-algorithms---basics.html#cb234-53" tabindex="-1"></a>  </span>
<span id="cb234-54"><a href="optimization-algorithms---basics.html#cb234-54" tabindex="-1"></a>  iterations <span class="ot">=</span> iterations <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb234-55"><a href="optimization-algorithms---basics.html#cb234-55" tabindex="-1"></a>  <span class="cf">if</span> (iterations <span class="sc">&gt;</span> max_iterations) converged <span class="ot">=</span> T</span>
<span id="cb234-56"><a href="optimization-algorithms---basics.html#cb234-56" tabindex="-1"></a>}</span>
<span id="cb234-57"><a href="optimization-algorithms---basics.html#cb234-57" tabindex="-1"></a></span>
<span id="cb234-58"><a href="optimization-algorithms---basics.html#cb234-58" tabindex="-1"></a><span class="fu">c</span>(iterations, c, b)</span></code></pre></div>
<pre><code>## [1] 3.000010e+05 1.205426e+00 1.966007e+00</code></pre>
<p>Stochastic Gradient Descent (SGD) tends to be faster than plain Gradient Descent (GD) when working with large datasets. The main reason for this is that SGD updates the model parameters more frequently, using only a random subset of data points (or even a single data point) in each update, while GD uses the entire dataset for each update.</p>
<p>The main advantage of using SGD over plain GD is related to the convergence speed and the ability to escape local minima. In SGD, the model parameters are updated after each mini-batch (in this case, a single data point), whereas in GD, the updates happen after going through the entire dataset. As a result, SGD can converge faster than GD because it performs more frequent updates, which can be especially beneficial when working with large datasets.</p>
<p>Moreover, SGD introduces randomness in the optimization process due to the random sampling of data points. This stochastic nature can help the algorithm to escape local minima and find a better (global) minimum. In the case of plain GD, the algorithm always follows the true gradient, which can cause it to get stuck in sharp, non-optimal minima.</p>
<p>However, there are some trade-offs when using SGD. The updates in SGD can be noisy because they are based on a random subset of data points, which can lead to fluctuations in the learning process. This can make the algorithm’s convergence path look less smooth than in the case of plain GD. Further, SGD often requires more careful tuning of hyperparameters, such as the learning rate and batch size. In some cases, a learning rate schedule (decreasing the learning rate over time) can be used to improve convergence.</p>
<p>In summary, while SGD can offer faster convergence and better ability to escape local minima, it comes with the trade-off of noisier updates and may require more careful hyperparameter tuning. When working with large datasets, we can also consider using mini-batch gradient descent, which is a compromise between GD and SGD. Mini-batch gradient descent uses a small batch of data points to compute the gradient, rather than the entire dataset (GD) or a single data point (SGD). This can offer a good balance between computational efficiency and convergence properties.</p>
<p>In the SGD code above, the decay rate is a hyperparameter that controls the rate at which the learning rate decreases over time in an adjustable learning rate schedule. Choosing an appropriate decay rate depends on the specific problem, the model, and the optimization algorithm being used. In practice, the decay rate is often chosen empirically through experimentation or by using techniques such as cross-validation or grid search.</p>
<p>Here are some guidelines to help us choose an appropriate decay rate:</p>
<ul>
<li>A common starting point for the decay rate is 0.99, as it provides a relatively slow decay of the learning rate. However, this value might not be optimal for all problems, so you should treat it as a starting point and experiment with different values to see what works best for your specific problem.</li>
<li>If the optimization problem is complex or has a highly non-convex loss surface, you might want to choose a smaller decay rate (e.g., 0.9 or 0.95) to allow for a faster reduction in the learning rate. This can help the model escape local minima or saddle points more quickly. On the other hand, if the problem is relatively simple, you might want to choose a larger decay rate (e.g., 0.995 or 0.999) to keep the learning rate higher for a longer period.</li>
<li>Since there is no one-size-fits-all answer for the decay rate, it is essential to experiment with different values and observe how they affect the optimization process. You can use techniques such as cross-validation or grid search to systematically explore different decay rate values and choose the one that yields the best performance.</li>
<li>It can be helpful to monitor the learning rate during training to ensure that it is decaying at an appropriate pace. If the learning rate is decreasing too quickly, it might result in slow convergence or getting stuck in local minima. If the learning rate is decreasing too slowly, it might cause oscillations around the optimal solution and prevent the model from converging.</li>
</ul>
<p>Learning rate scheduler is a more general concept than the specific exponential decay method we demonstrated here. A learning rate scheduler is a technique used to adjust the learning rate during the training process according to a pre-defined schedule or rule. The exponential decay method is just one example of a learning rate scheduler.</p>
<p>There are various learning rate scheduler strategies:</p>
<ul>
<li>Exponential decay: The learning rate is multiplied by a fixed decay rate at each iteration or epoch, as demonstrated in the previous example.</li>
<li>Step decay: The learning rate is reduced by a fixed factor at specific intervals, such as every N epochs. For example, the learning rate could be reduced by a factor of 0.5 every 10 epochs.</li>
<li>Time-based decay: The learning rate is reduced according to a function of the elapsed training time or the number of iterations. For example, the learning rate could be reduced by a factor proportional to the inverse of the square root of the number of iterations.</li>
<li>Cosine annealing: The learning rate is reduced following a cosine function, which allows for periodic “restarts” of the learning rate, helping the optimization process escape local minima or saddle points.</li>
<li>Cyclic learning rates: The learning rate is varied cyclically within a predefined range, allowing the model to explore different areas of the loss surface more effectively.</li>
<li>Adaptive learning rates: These learning rate schedulers adjust the learning rate based on the progress of the optimization process, such as the improvement in the loss function or the validation accuracy. Some well-known adaptive learning rate methods include AdaGrad, RMSprop, and Adam.</li>
</ul>
<p>The choice of the learning rate scheduler depends on the specific problem, the model, and the optimization algorithm being used. It’s essential to experiment with different learning rate schedulers and monitor the training progress to find the best strategy for a particular problem.</p>
</div>
<div id="multivariable" class="section level3 hasAnchor" number="12.4.3">
<h3><span class="header-section-number">12.4.3</span> Multivariable<a href="optimization-algorithms---basics.html#multivariable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will expand the gradient descent algorithms with an multivariable example using matrix algebra. First, the data and model simulation:</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="optimization-algorithms---basics.html#cb236-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1001</span>)</span>
<span id="cb236-2"><a href="optimization-algorithms---basics.html#cb236-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb236-3"><a href="optimization-algorithms---basics.html#cb236-3" tabindex="-1"></a>int <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, N)</span>
<span id="cb236-4"><a href="optimization-algorithms---basics.html#cb236-4" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb236-5"><a href="optimization-algorithms---basics.html#cb236-5" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb236-6"><a href="optimization-algorithms---basics.html#cb236-6" tabindex="-1"></a>x3 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(N, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb236-7"><a href="optimization-algorithms---basics.html#cb236-7" tabindex="-1"></a>x4 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(N, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb236-8"><a href="optimization-algorithms---basics.html#cb236-8" tabindex="-1"></a>x5 <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(N, <span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb236-9"><a href="optimization-algorithms---basics.html#cb236-9" tabindex="-1"></a>x6 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">1</span>, <span class="fl">0.25</span>)</span>
<span id="cb236-10"><a href="optimization-algorithms---basics.html#cb236-10" tabindex="-1"></a>x7 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">1</span>, <span class="fl">0.2</span>)</span>
<span id="cb236-11"><a href="optimization-algorithms---basics.html#cb236-11" tabindex="-1"></a>x2x3 <span class="ot">&lt;-</span> x2<span class="sc">*</span>x3</span>
<span id="cb236-12"><a href="optimization-algorithms---basics.html#cb236-12" tabindex="-1"></a>x4x5 <span class="ot">&lt;-</span> x4<span class="sc">*</span>x5</span>
<span id="cb236-13"><a href="optimization-algorithms---basics.html#cb236-13" tabindex="-1"></a>x4x6 <span class="ot">&lt;-</span> x5<span class="sc">*</span>x6</span>
<span id="cb236-14"><a href="optimization-algorithms---basics.html#cb236-14" tabindex="-1"></a>x3x7 <span class="ot">&lt;-</span> x3<span class="sc">*</span>x7</span>
<span id="cb236-15"><a href="optimization-algorithms---basics.html#cb236-15" tabindex="-1"></a></span>
<span id="cb236-16"><a href="optimization-algorithms---basics.html#cb236-16" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">2</span><span class="sc">*</span>x1 <span class="sc">+</span> <span class="sc">-</span><span class="fl">0.5</span><span class="sc">*</span>x2 <span class="sc">-</span> <span class="fl">1.75</span><span class="sc">*</span>x2x3 <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x4x5 <span class="sc">-</span> <span class="dv">3</span><span class="sc">*</span>x4x6 <span class="sc">+</span> <span class="fl">1.2</span><span class="sc">*</span>x3x7 <span class="sc">+</span> int, <span class="dv">1</span>)</span>
<span id="cb236-17"><a href="optimization-algorithms---basics.html#cb236-17" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(int, x1, x2, x2x3, x4x5, x4x6, x3x7)</span></code></pre></div>
<p>We can solve it with linear algebra manually:</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="optimization-algorithms---basics.html#cb237-1" tabindex="-1"></a>betaOLS <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">%*%</span><span class="fu">t</span>(X)<span class="sc">%*%</span>Y</span>
<span id="cb237-2"><a href="optimization-algorithms---basics.html#cb237-2" tabindex="-1"></a><span class="fu">print</span>(betaOLS)</span></code></pre></div>
<pre><code>##            [,1]
## int   0.4953323
## x1    1.9559022
## x2   -0.3511182
## x2x3 -1.9112623
## x4x5  1.7424723
## x4x6 -2.8323934
## x3x7  2.1015442</code></pre>
<p>We can also solve it with <code>lm()</code></p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="optimization-algorithms---basics.html#cb239-1" tabindex="-1"></a>model1.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb239-2"><a href="optimization-algorithms---basics.html#cb239-2" tabindex="-1"></a><span class="fu">summary</span>(model1.lm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X - 1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.84941 -0.45289 -0.09686  0.57679  2.07154 
## 
## Coefficients:
##       Estimate Std. Error t value Pr(&gt;|t|)    
## Xint   0.49533    0.75410   0.657  0.51290    
## Xx1    1.95590    0.03868  50.571  &lt; 2e-16 ***
## Xx2   -0.35112    0.12600  -2.787  0.00645 ** 
## Xx2x3 -1.91126    0.13358 -14.308  &lt; 2e-16 ***
## Xx4x5  1.74247    0.24396   7.143 2.01e-10 ***
## Xx4x6 -2.83239    0.18831 -15.041  &lt; 2e-16 ***
## Xx3x7  2.10154    0.64210   3.273  0.00149 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8448 on 93 degrees of freedom
## Multiple R-squared:  0.9972, Adjusted R-squared:  0.997 
## F-statistic:  4677 on 7 and 93 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Now the function for gradient descent:</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="optimization-algorithms---basics.html#cb241-1" tabindex="-1"></a>grdescentM <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y, lr, maxiter) {</span>
<span id="cb241-2"><a href="optimization-algorithms---basics.html#cb241-2" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb241-3"><a href="optimization-algorithms---basics.html#cb241-3" tabindex="-1"></a>  b <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="fu">ncol</span>(x), <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb241-4"><a href="optimization-algorithms---basics.html#cb241-4" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> x<span class="sc">%*%</span>b</span>
<span id="cb241-5"><a href="optimization-algorithms---basics.html#cb241-5" tabindex="-1"></a>  e <span class="ot">&lt;-</span> y <span class="sc">-</span> yhat</span>
<span id="cb241-6"><a href="optimization-algorithms---basics.html#cb241-6" tabindex="-1"></a>  RSS <span class="ot">&lt;-</span> <span class="fu">t</span>(e)<span class="sc">%*%</span>e</span>
<span id="cb241-7"><a href="optimization-algorithms---basics.html#cb241-7" tabindex="-1"></a>  converged <span class="ot">=</span> F</span>
<span id="cb241-8"><a href="optimization-algorithms---basics.html#cb241-8" tabindex="-1"></a>  iterations <span class="ot">=</span> <span class="dv">0</span></span>
<span id="cb241-9"><a href="optimization-algorithms---basics.html#cb241-9" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(y)</span>
<span id="cb241-10"><a href="optimization-algorithms---basics.html#cb241-10" tabindex="-1"></a>  </span>
<span id="cb241-11"><a href="optimization-algorithms---basics.html#cb241-11" tabindex="-1"></a>  <span class="cf">while</span>(converged <span class="sc">==</span> F) {</span>
<span id="cb241-12"><a href="optimization-algorithms---basics.html#cb241-12" tabindex="-1"></a>    b_new <span class="ot">&lt;-</span> b <span class="sc">-</span> (lr<span class="sc">*</span>(<span class="dv">1</span><span class="sc">/</span>n))<span class="sc">*</span><span class="fu">t</span>(x)<span class="sc">%*%</span>(x<span class="sc">%*%</span>b <span class="sc">-</span> y)</span>
<span id="cb241-13"><a href="optimization-algorithms---basics.html#cb241-13" tabindex="-1"></a>    b <span class="ot">&lt;-</span> b_new</span>
<span id="cb241-14"><a href="optimization-algorithms---basics.html#cb241-14" tabindex="-1"></a>    yhat <span class="ot">&lt;-</span> x<span class="sc">%*%</span>b</span>
<span id="cb241-15"><a href="optimization-algorithms---basics.html#cb241-15" tabindex="-1"></a>    e <span class="ot">&lt;-</span> y <span class="sc">-</span> yhat</span>
<span id="cb241-16"><a href="optimization-algorithms---basics.html#cb241-16" tabindex="-1"></a>    </span>
<span id="cb241-17"><a href="optimization-algorithms---basics.html#cb241-17" tabindex="-1"></a>    RSS_new <span class="ot">&lt;-</span> <span class="fu">t</span>(e)<span class="sc">%*%</span>e</span>
<span id="cb241-18"><a href="optimization-algorithms---basics.html#cb241-18" tabindex="-1"></a>    RSS <span class="ot">&lt;-</span> <span class="fu">c</span>(RSS, RSS_new)</span>
<span id="cb241-19"><a href="optimization-algorithms---basics.html#cb241-19" tabindex="-1"></a>    d <span class="ot">=</span> <span class="fu">tail</span>(<span class="fu">abs</span>(<span class="fu">diff</span>(RSS)), <span class="dv">1</span>)</span>
<span id="cb241-20"><a href="optimization-algorithms---basics.html#cb241-20" tabindex="-1"></a>    </span>
<span id="cb241-21"><a href="optimization-algorithms---basics.html#cb241-21" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">round</span>(d, <span class="dv">12</span>) <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb241-22"><a href="optimization-algorithms---basics.html#cb241-22" tabindex="-1"></a>      converged <span class="ot">=</span> T</span>
<span id="cb241-23"><a href="optimization-algorithms---basics.html#cb241-23" tabindex="-1"></a>      <span class="fu">return</span>(b)</span>
<span id="cb241-24"><a href="optimization-algorithms---basics.html#cb241-24" tabindex="-1"></a>    }</span>
<span id="cb241-25"><a href="optimization-algorithms---basics.html#cb241-25" tabindex="-1"></a>    iterations <span class="ot">=</span> iterations <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb241-26"><a href="optimization-algorithms---basics.html#cb241-26" tabindex="-1"></a>    <span class="cf">if</span>(iterations <span class="sc">&gt;</span> maxiter) { </span>
<span id="cb241-27"><a href="optimization-algorithms---basics.html#cb241-27" tabindex="-1"></a>      converged <span class="ot">=</span> T</span>
<span id="cb241-28"><a href="optimization-algorithms---basics.html#cb241-28" tabindex="-1"></a>      <span class="fu">return</span>(b)</span>
<span id="cb241-29"><a href="optimization-algorithms---basics.html#cb241-29" tabindex="-1"></a>   }</span>
<span id="cb241-30"><a href="optimization-algorithms---basics.html#cb241-30" tabindex="-1"></a>  }</span>
<span id="cb241-31"><a href="optimization-algorithms---basics.html#cb241-31" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="optimization-algorithms---basics.html#cb242-1" tabindex="-1"></a><span class="fu">grdescentM</span>(X, Y, <span class="fl">0.01</span>, <span class="dv">100000</span>) </span></code></pre></div>
<pre><code>##            [,1]
## int   0.4953843
## x1    1.9559009
## x2   -0.3511257
## x2x3 -1.9112548
## x4x5  1.7424746
## x4x6 -2.8323944
## x3x7  2.1015069</code></pre>
</div>
</div>
<div id="optimization-with-r" class="section level2 hasAnchor" number="12.5">
<h2><span class="header-section-number">12.5</span> Optimization with R<a href="optimization-algorithms---basics.html#optimization-with-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A good summary of tools for optimization in R given in this guide: <a href="https://cran.r-project.org/web/views/Optimization.html">Optimization and Mathematical Programming</a>. There are many optimization methods, each of which would only be appropriate for specific cases. In choosing a numerical optimization method, we need to consider following points:</p>
<ol style="list-style-type: decimal">
<li>We need to know if it is a constrained or unconstrained problem. For example, the MLE method is an unconstrained problem. Most regularization problems, like Lasso or Ridge, are constraint optimization problems.<br />
</li>
<li>Do we know how the objective function is shaped a priori? MLE and OLS methods have well-known objective functions (Residual Sum of Squares and Log-Likelihood). Maximization and minimization problems can be used in both cases by flipping the sign of the objective function.<br />
</li>
<li>Multivariate optimization problems are much harder than single-variable optimization problems. There is, however, a large set of available optimization methods for multivariate problems.<br />
</li>
<li>In multivariate cases, the critical point is whether the objective function has available gradients. If only the objective function is available without gradient or Hessian, the Nelder-Mead algorithm is the most common method for numerical optimization. If gradients are available, the best and most used method is the gradient descent algorithm. We have seen its application for OLS. This method can be applied to MLE as well. It is also called a Steepest Descent Algorithm. In general, the gradient descent method has three types: Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent.<br />
</li>
<li>If the gradient and Hessian are available, we can also use the Newton-Raphson Method. This is only possible if the dataset is not high-dimensional, as calculating the Hessian would otherwise be very expensive.<br />
</li>
<li>Usually the Nelder-Mead method is slower than the Gradient Descent method. <code>Optim()</code> uses the Nelder-Mead method, but the optimization method can be chosen in its arguments.</li>
</ol>
<p>More details can be found in this educational <a href="http://bigdatasummerinst.sph.umich.edu/wiki/images/a/ad/Bdsi_optimization_2019.pdf">slides</a>. The most detailed and advance source is <a href="https://en.wikipedia.org/wiki/Numerical_Recipes">Numerical Recipes</a>, which uses <code>C++</code> and <code>R</code>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hyperparameter-tuning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="prediction-intervals.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuk/machinemetrics/edit/master/12-AlgorithmicOpt.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
