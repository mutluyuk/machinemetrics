<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 26 Deep Learning | MachineMetrics</title>
  <meta name="description" content="Chapter 26 Deep Learning | MachineMetrics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 26 Deep Learning | MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 26 Deep Learning | MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="neural-networks.html"/>
<link rel="next" href="graphical-network-analysis.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html"><i class="fa fa-check"></i><b>2</b> Statistical Models and Simulations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#distinguishing-between-statistical-modeling-and-machine-learning-in-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#goals-and-objectives"><i class="fa fa-check"></i><b>2.1.1</b> Goals and Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#prediction-vs.-inference"><i class="fa fa-check"></i><b>2.1.2</b> Prediction vs. Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#conclusion"><i class="fa fa-check"></i><b>2.1.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#model-selection-and-approaches-in-data-modeling"><i class="fa fa-check"></i><b>2.4</b> Model Selection and Approaches in Data Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#mle"><i class="fa fa-check"></i><b>3.5.3</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error---mse"><i class="fa fa-check"></i><b>5.1</b> Estimation error - MSE</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.2</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#technical-points-about-mse-and-mspe"><i class="fa fa-check"></i><b>5.3</b> Technical points about MSE and MSPE</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.1</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.1</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.2</b> LPM</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
<li class="chapter" data-level="10.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#knn"><i class="fa fa-check"></i><b>10.3</b> Knn</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#adult-dataset"><i class="fa fa-check"></i><b>10.3.1</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="11.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#grid-search"><i class="fa fa-check"></i><b>11.4</b> Grid Search</a></li>
<li class="chapter" data-level="11.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>11.5</b> Cross-validated grid search</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#one-variable"><i class="fa fa-check"></i><b>12.4.1</b> One-variable</a></li>
<li class="chapter" data-level="12.4.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>12.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="12.4.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#multivariable"><i class="fa fa-check"></i><b>12.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>16.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.3</b> Pruning</a></li>
<li class="chapter" data-level="16.4" data-path="regression-trees.html"><a href="regression-trees.html#classification-with-titanic"><i class="fa fa-check"></i><b>16.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="16.5" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>17.2</b> Random Forest</a></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>17.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="17.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="17.3.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.3.3</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.4</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.5</b> Classification</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.6</b> Regression</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.7</b> Exploration</a></li>
<li class="chapter" data-level="17.8" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.8</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression-1"><i class="fa fa-check"></i><b>17.8.1</b> Regression</a></li>
<li class="chapter" data-level="17.8.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.8.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.8.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.8.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.8.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-1"><i class="fa fa-check"></i><b>17.8.4</b> Classification</a></li>
<li class="chapter" data-level="17.8.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost.m1"><i class="fa fa-check"></i><b>17.8.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="17.8.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.8.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#out-sample-prediction-accuracy"><i class="fa fa-check"></i><b>20.3</b> out-sample prediction accuracy</a></li>
<li class="chapter" data-level="20.4" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-2.html"><a href="classification-2.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-2.html"><a href="classification-2.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-2.html"><a href="classification-2.html#mnist-dataset"><i class="fa fa-check"></i><b>21.2</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="21.3" data-path="classification-2.html"><a href="classification-2.html#linear-classifiers-again"><i class="fa fa-check"></i><b>21.3</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="21.4" data-path="classification-2.html"><a href="classification-2.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.4</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.5" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret"><i class="fa fa-check"></i><b>21.5</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="classification-2.html"><a href="classification-2.html#mnist_27"><i class="fa fa-check"></i><b>21.5.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="21.5.2" data-path="classification-2.html"><a href="classification-2.html#adult-dataset-1"><i class="fa fa-check"></i><b>21.5.2</b> Adult dataset</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="classification-2.html"><a href="classification-2.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.6</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.7" data-path="classification-2.html"><a href="classification-2.html#confusion-matrix"><i class="fa fa-check"></i><b>21.7</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.8" data-path="classification-2.html"><a href="classification-2.html#performance-measures"><i class="fa fa-check"></i><b>21.8</b> Performance measures</a></li>
<li class="chapter" data-level="21.9" data-path="classification-2.html"><a href="classification-2.html#roc-curve"><i class="fa fa-check"></i><b>21.9</b> ROC Curve</a></li>
<li class="chapter" data-level="21.10" data-path="classification-2.html"><a href="classification-2.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.10</b> AUC - Area Under the Curve</a></li>
<li class="chapter" data-level="21.11" data-path="classification-2.html"><a href="classification-2.html#classification-example"><i class="fa fa-check"></i><b>21.11</b> Classification Example</a></li>
<li class="chapter" data-level="21.12" data-path="classification-2.html"><a href="classification-2.html#lpm-1"><i class="fa fa-check"></i><b>21.12</b> LPM</a></li>
<li class="chapter" data-level="21.13" data-path="classification-2.html"><a href="classification-2.html#logistic-regression-1"><i class="fa fa-check"></i><b>21.13</b> Logistic Regression</a></li>
<li class="chapter" data-level="21.14" data-path="classification-2.html"><a href="classification-2.html#knn-1"><i class="fa fa-check"></i><b>21.14</b> kNN</a>
<ul>
<li class="chapter" data-level="21.14.1" data-path="classification-2.html"><a href="classification-2.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>21.14.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="21.14.2" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret-1"><i class="fa fa-check"></i><b>21.14.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>22</b> Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="time-series.html"><a href="time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="time-series.html"><a href="time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="time-series.html"><a href="time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="time-series.html"><a href="time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="time-series.html"><a href="time-series.html#stationarity"><i class="fa fa-check"></i><b>22.5</b> Stationarity</a></li>
<li class="chapter" data-level="22.6" data-path="time-series.html"><a href="time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.6</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="time-series.html"><a href="time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.7</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.8" data-path="time-series.html"><a href="time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.8</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.9" data-path="time-series.html"><a href="time-series.html#speed"><i class="fa fa-check"></i><b>22.9</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="forecast.html"><a href="forecast.html"><i class="fa fa-check"></i><b>23</b> Forecast</a>
<ul>
<li class="chapter" data-level="23.1" data-path="forecast.html"><a href="forecast.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="forecast.html"><a href="forecast.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="forecast.html"><a href="forecast.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="forecast.html"><a href="forecast.html#random-forest-1"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="forecast.html"><a href="forecast.html#univariate"><i class="fa fa-check"></i><b>23.5</b> Univariate</a></li>
<li class="chapter" data-level="23.6" data-path="forecast.html"><a href="forecast.html#multivariate"><i class="fa fa-check"></i><b>23.6</b> Multivariate</a></li>
<li class="chapter" data-level="23.7" data-path="forecast.html"><a href="forecast.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.7</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>24.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-margin"><i class="fa fa-check"></i><b>24.1.1</b> The Margin</a></li>
<li class="chapter" data-level="24.1.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-non-separable-case"><i class="fa fa-check"></i><b>24.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>24.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="24.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#application-with-svm"><i class="fa fa-check"></i><b>24.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#precision-matrix"><i class="fa fa-check"></i><b>27.4</b> Precision Matrix</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.5</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.6" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.6</b> Regularized Covariance Matrix</a></li>
<li class="chapter" data-level="27.7" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>27.7</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="27.8" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#high-dimensional-data"><i class="fa fa-check"></i><b>27.8</b> High-dimensional data</a></li>
<li class="chapter" data-level="27.9" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>27.9</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#rankr-approximations"><i class="fa fa-check"></i><b>28.4</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="28.5" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.5</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html"><i class="fa fa-check"></i><b>29</b> PCA (Principle Component Analysis)</a>
<ul>
<li class="chapter" data-level="29.1" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.1</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>30.1</b> Using bins</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>30.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="30.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>30.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="30.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="31.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>31.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="31.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>31.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="text-analysis.html"><a href="text-analysis.html"><i class="fa fa-check"></i><b>32</b> Text Analysis</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#other-nonparametric-estimation-methods-1"><i class="fa fa-check"></i><b>33.1</b> Other Nonparametric Estimation methods</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.2</b> Regression splines</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.3</b> MARS</a></li>
<li class="chapter" data-level="33.4" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.4</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuk/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="deep-learning" class="section level1 hasAnchor" number="26">
<h1><span class="header-section-number">Chapter 26</span> Deep Learning<a href="deep-learning.html#deep-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Simply, a Deep Neural Network (DNN), or Deep Learning, is an artificial neural network that has two or more hidden layers. Even greater flexibility is achieved via composition of activation functions:</p>
<p><span class="math display">\[
y \approx \alpha+\sum_{m=1}^M \beta_m f\left(\alpha_m^{(1)}+\underbrace{\sum_{p=1}^P f\left(\alpha_p^{(2)}+\textbf{X} \delta_p^{(2)}\right)}_{\text {it replaces } \textbf{X}} \delta_m^{(1)}\right)
\]</span></p>
<p>Before having an application, we should note the number of available packages that offer ANN implementations in R and with Python. For example, CRAN hosts more than 80 packages related to neural network modeling. Above, we just saw one example with <code>neuralnet</code>. The work by <a href="https://www.inmodelia.com/exemples/2021-0103-RJournal-SM-AV-CD-PK-JN.pdf">Mahdi et al, 2021</a> surveys and ranks these packages for their accuracy, reliability, and ease-of-use.</p>
<div class="sourceCode" id="cb805"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb805-1"><a href="deep-learning.html#cb805-1" tabindex="-1"></a>mse.test <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb805-2"><a href="deep-learning.html#cb805-2" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>){ </span>
<span id="cb805-3"><a href="deep-learning.html#cb805-3" tabindex="-1"></a>  </span>
<span id="cb805-4"><a href="deep-learning.html#cb805-4" tabindex="-1"></a>  <span class="fu">set.seed</span>(i<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb805-5"><a href="deep-learning.html#cb805-5" tabindex="-1"></a>  trainid <span class="ot">&lt;-</span> <span class="fu">unique</span>(<span class="fu">sample</span>(<span class="fu">nrow</span>(ddf), <span class="fu">nrow</span>(ddf), <span class="at">replace =</span> <span class="cn">TRUE</span>))</span>
<span id="cb805-6"><a href="deep-learning.html#cb805-6" tabindex="-1"></a>  train <span class="ot">&lt;-</span> ddf[trainid,]</span>
<span id="cb805-7"><a href="deep-learning.html#cb805-7" tabindex="-1"></a>  test <span class="ot">&lt;-</span> ddf[<span class="sc">-</span>trainid,]</span>
<span id="cb805-8"><a href="deep-learning.html#cb805-8" tabindex="-1"></a>  </span>
<span id="cb805-9"><a href="deep-learning.html#cb805-9" tabindex="-1"></a>  <span class="co"># Models</span></span>
<span id="cb805-10"><a href="deep-learning.html#cb805-10" tabindex="-1"></a>  fit.nn22 <span class="ot">&lt;-</span> <span class="fu">neuralnet</span>(frmnn, <span class="at">data =</span> train, <span class="at">hidden =</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>), <span class="at">threshold =</span> <span class="fl">0.05</span>)</span>
<span id="cb805-11"><a href="deep-learning.html#cb805-11" tabindex="-1"></a>  mse.test[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test<span class="sc">$</span>wage <span class="sc">-</span> <span class="fu">predict</span>(fit.nn22, test))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb805-12"><a href="deep-learning.html#cb805-12" tabindex="-1"></a> }</span>
<span id="cb805-13"><a href="deep-learning.html#cb805-13" tabindex="-1"></a></span>
<span id="cb805-14"><a href="deep-learning.html#cb805-14" tabindex="-1"></a><span class="fu">mean</span>(mse.test)</span></code></pre></div>
<pre><code>## [1] 1.211114</code></pre>
<p>The overfitting gets worse with an increased complexity! Here is the plot for our DNN:</p>
<div class="sourceCode" id="cb807"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb807-1"><a href="deep-learning.html#cb807-1" tabindex="-1"></a><span class="fu">plot</span>(fit.nn22, <span class="at">rep =</span> <span class="st">&quot;best&quot;</span>)</span></code></pre></div>
<p><img src="25-NeuralNetworks_files/figure-html/nn12-1.png" width="672" /></p>
<p>A better plot could be obtained by using the <code>NeuralNetTools</code> package:</p>
<div class="sourceCode" id="cb808"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb808-1"><a href="deep-learning.html#cb808-1" tabindex="-1"></a><span class="fu">library</span>(NeuralNetTools)</span>
<span id="cb808-2"><a href="deep-learning.html#cb808-2" tabindex="-1"></a><span class="fu">plotnet</span>(fit.nn22)</span></code></pre></div>
<p><img src="25-NeuralNetworks_files/figure-html/nn13-1.png" width="672" /></p>
<p>Training DNN is an important concept and we leave it to Chapter 25. As we see, deep neural networks can model complex non-linear relationships.
With very complex problems, such as detecting hundreds of types of objects in high-resolution images, we need to train deeper NNs, perhaps with 10 layers or more each with hundreds of neurons. Therefore, training a fully-connected DNN is a very slow process facing a severe risk of overfitting with millions of parameters. Moreover, gradients problems make lower layers very hard to train. A solution to these problems came with a different NN architect such as convolutional Neural Networks (CNN or ConvNets) and Recurrent Neural Networks (RNN), which we will see in Chapter 25.</p>
<p>Moreover, the interpretability of an artificial neural network (ANN), which is known to be a “blackbox” method, can be an issue regardless of the complexity of the network. However, it is generally easier to understand the decisions made by a simple ANN than by a more complex one.</p>
<p>A simple ANN might have only a few layers and a relatively small number of neurons, making it easier to understand how the input data is processed and how the final output is produced. However, even a simple ANN can still be a black box in the sense that the specific calculations and decisions made by the individual neurons within the network are not fully visible or understood. On the other hand, a more complex ANN with many layers and a large number of neurons can be more difficult to interpret, as the internal workings of the network are more complex and harder to understand. In these cases, it can be more challenging to understand how the ANN is making its decisions or to identify any biases or errors in its output. Overall, the interpretability of an ANN depends on the complexity of the network and the specific task it is being used for. Simple ANNs may be more interpretable, but even they can be considered black boxes to some extent.</p>
<p>Here are a few resources that provide information about the interpretability of artificial neural networks (ANNs):</p>
<ul>
<li><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a> by Christoph Molnar is a online book that provides an overview of interpretability in machine learning, including techniques for interpreting ANNs.</li>
<li><a href="https://ieeexplore.ieee.org/document/8397411">Interpretability of Deep Neural Networks</a> by Chakraborty is a survey paper that discusses the interpretability of deep neural networks and presents an overview of the various techniques and approaches that have been developed to improve their interpretability.</li>
</ul>
<p>Before concluding this section we apply DNN to a classification problem using the same data that we have in Chapter 14.4.4.</p>
<div class="sourceCode" id="cb809"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb809-1"><a href="deep-learning.html#cb809-1" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb809-2"><a href="deep-learning.html#cb809-2" tabindex="-1"></a>df <span class="ot">&lt;-</span> Carseats</span>
<span id="cb809-3"><a href="deep-learning.html#cb809-3" tabindex="-1"></a><span class="fu">str</span>(df)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  11 variables:
##  $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...
##  $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...
##  $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...
##  $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...
##  $ Population : num  276 260 269 466 340 501 45 425 108 131 ...
##  $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...
##  $ ShelveLoc  : Factor w/ 3 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Medium&quot;: 1 2 3 3 1 1 3 2 3 3 ...
##  $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...
##  $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...
##  $ Urban      : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 2 2 1 1 ...
##  $ US         : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 1 2 1 2 1 2 ...</code></pre>
<div class="sourceCode" id="cb811"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb811-1"><a href="deep-learning.html#cb811-1" tabindex="-1"></a><span class="co">#Change SALES to a factor variable</span></span>
<span id="cb811-2"><a href="deep-learning.html#cb811-2" tabindex="-1"></a>df<span class="sc">$</span>Sales <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(Carseats<span class="sc">$</span>Sales <span class="sc">&lt;=</span> <span class="dv">8</span>, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb811-3"><a href="deep-learning.html#cb811-3" tabindex="-1"></a>dff <span class="ot">&lt;-</span> df[, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb811-4"><a href="deep-learning.html#cb811-4" tabindex="-1"></a></span>
<span id="cb811-5"><a href="deep-learning.html#cb811-5" tabindex="-1"></a><span class="co"># Scaling and Dummy coding</span></span>
<span id="cb811-6"><a href="deep-learning.html#cb811-6" tabindex="-1"></a>dff[,<span class="fu">sapply</span>(dff, is.numeric)] <span class="ot">&lt;-</span> <span class="fu">scale</span>((dff[, <span class="fu">sapply</span>(dff, is.numeric)]))</span>
<span id="cb811-7"><a href="deep-learning.html#cb811-7" tabindex="-1"></a>ddf <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>.<span class="sc">-</span><span class="dv">1</span>, <span class="at">data=</span> dff, <span class="at">contrasts.arg =</span></span>
<span id="cb811-8"><a href="deep-learning.html#cb811-8" tabindex="-1"></a>                       <span class="fu">lapply</span>(dff[, <span class="fu">sapply</span>(dff, is.factor)],</span>
<span id="cb811-9"><a href="deep-learning.html#cb811-9" tabindex="-1"></a>                              contrasts, <span class="at">contrasts =</span> <span class="cn">FALSE</span>))</span>
<span id="cb811-10"><a href="deep-learning.html#cb811-10" tabindex="-1"></a></span>
<span id="cb811-11"><a href="deep-learning.html#cb811-11" tabindex="-1"></a>ddf <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Sales =</span> df<span class="sc">$</span>Sales, ddf)</span>
<span id="cb811-12"><a href="deep-learning.html#cb811-12" tabindex="-1"></a></span>
<span id="cb811-13"><a href="deep-learning.html#cb811-13" tabindex="-1"></a><span class="co"># Formula</span></span>
<span id="cb811-14"><a href="deep-learning.html#cb811-14" tabindex="-1"></a>w.ind <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(ddf) <span class="sc">==</span> <span class="st">&quot;Sales&quot;</span>)</span>
<span id="cb811-15"><a href="deep-learning.html#cb811-15" tabindex="-1"></a>frm <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">paste</span>(<span class="st">&quot;Sales~&quot;</span>, <span class="fu">paste</span>(<span class="fu">colnames</span>(ddf[<span class="sc">-</span>w.ind]),</span>
<span id="cb811-16"><a href="deep-learning.html#cb811-16" tabindex="-1"></a>                                        <span class="at">collapse =</span> <span class="st">&#39;+&#39;</span>)))</span></code></pre></div>
<div class="sourceCode" id="cb812"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb812-1"><a href="deep-learning.html#cb812-1" tabindex="-1"></a><span class="fu">library</span>(ROCR)</span>
<span id="cb812-2"><a href="deep-learning.html#cb812-2" tabindex="-1"></a></span>
<span id="cb812-3"><a href="deep-learning.html#cb812-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb812-4"><a href="deep-learning.html#cb812-4" tabindex="-1"></a>AUC1 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb812-5"><a href="deep-learning.html#cb812-5" tabindex="-1"></a>AUC2 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb812-6"><a href="deep-learning.html#cb812-6" tabindex="-1"></a></span>
<span id="cb812-7"><a href="deep-learning.html#cb812-7" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb812-8"><a href="deep-learning.html#cb812-8" tabindex="-1"></a>  <span class="fu">set.seed</span>(i)</span>
<span id="cb812-9"><a href="deep-learning.html#cb812-9" tabindex="-1"></a>  ind <span class="ot">&lt;-</span> <span class="fu">unique</span>(<span class="fu">sample</span>(<span class="fu">nrow</span>(ddf), <span class="fu">nrow</span>(ddf), <span class="at">replace =</span> <span class="cn">TRUE</span>))</span>
<span id="cb812-10"><a href="deep-learning.html#cb812-10" tabindex="-1"></a>  train <span class="ot">&lt;-</span> ddf[ind, ]</span>
<span id="cb812-11"><a href="deep-learning.html#cb812-11" tabindex="-1"></a>  test <span class="ot">&lt;-</span> ddf[<span class="sc">-</span>ind, ]</span>
<span id="cb812-12"><a href="deep-learning.html#cb812-12" tabindex="-1"></a>  </span>
<span id="cb812-13"><a href="deep-learning.html#cb812-13" tabindex="-1"></a>  <span class="co"># Models</span></span>
<span id="cb812-14"><a href="deep-learning.html#cb812-14" tabindex="-1"></a>  fit.ln <span class="ot">&lt;-</span> <span class="fu">glm</span>(frm, <span class="at">data =</span> train, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb812-15"><a href="deep-learning.html#cb812-15" tabindex="-1"></a>  fit.dnn <span class="ot">&lt;-</span> <span class="fu">neuralnet</span>(frm, <span class="at">data =</span> train, <span class="at">hidden =</span> <span class="dv">2</span>, <span class="at">threshold =</span> <span class="fl">0.05</span>,</span>
<span id="cb812-16"><a href="deep-learning.html#cb812-16" tabindex="-1"></a>                       <span class="at">linear.output =</span> <span class="cn">FALSE</span>, <span class="at">err.fct =</span> <span class="st">&quot;ce&quot;</span>)</span>
<span id="cb812-17"><a href="deep-learning.html#cb812-17" tabindex="-1"></a>  </span>
<span id="cb812-18"><a href="deep-learning.html#cb812-18" tabindex="-1"></a>  <span class="co">#Predictions</span></span>
<span id="cb812-19"><a href="deep-learning.html#cb812-19" tabindex="-1"></a>  phat.ln <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.ln, test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb812-20"><a href="deep-learning.html#cb812-20" tabindex="-1"></a>  phat.dnn <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.dnn, test, <span class="at">type =</span> <span class="st">&quot;repsonse&quot;</span>)[,<span class="dv">2</span>]</span>
<span id="cb812-21"><a href="deep-learning.html#cb812-21" tabindex="-1"></a></span>
<span id="cb812-22"><a href="deep-learning.html#cb812-22" tabindex="-1"></a>  <span class="co">#AUC for predicting Y = 1</span></span>
<span id="cb812-23"><a href="deep-learning.html#cb812-23" tabindex="-1"></a>  pred_rocr1 <span class="ot">&lt;-</span> ROCR<span class="sc">::</span><span class="fu">prediction</span>(phat.ln, test<span class="sc">$</span>Sales)</span>
<span id="cb812-24"><a href="deep-learning.html#cb812-24" tabindex="-1"></a>  auc_ROCR1 <span class="ot">&lt;-</span> ROCR<span class="sc">::</span><span class="fu">performance</span>(pred_rocr1, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb812-25"><a href="deep-learning.html#cb812-25" tabindex="-1"></a>  AUC1[i] <span class="ot">&lt;-</span> auc_ROCR1<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb812-26"><a href="deep-learning.html#cb812-26" tabindex="-1"></a></span>
<span id="cb812-27"><a href="deep-learning.html#cb812-27" tabindex="-1"></a>  pred_rocr2 <span class="ot">&lt;-</span> ROCR<span class="sc">::</span><span class="fu">prediction</span>(phat.dnn, test<span class="sc">$</span>Sales)</span>
<span id="cb812-28"><a href="deep-learning.html#cb812-28" tabindex="-1"></a>  auc_ROCR2 <span class="ot">&lt;-</span> ROCR<span class="sc">::</span><span class="fu">performance</span>(pred_rocr2, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb812-29"><a href="deep-learning.html#cb812-29" tabindex="-1"></a>  AUC2[i] <span class="ot">&lt;-</span> auc_ROCR2<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb812-30"><a href="deep-learning.html#cb812-30" tabindex="-1"></a>}</span>
<span id="cb812-31"><a href="deep-learning.html#cb812-31" tabindex="-1"></a></span>
<span id="cb812-32"><a href="deep-learning.html#cb812-32" tabindex="-1"></a>(<span class="fu">c</span>(<span class="fu">mean</span>(AUC1), <span class="fu">mean</span>(AUC2)))</span></code></pre></div>
<pre><code>## [1] 0.9471081 0.9186785</code></pre>
<p>Again the results are not very convincing to use DNN in this example.</p>
<p>Let’s have a more complex task with the <a href="https://www.kaggle.com/datasets/piyushgoyal443/red-wine-dataset?resource=download">Red Wine</a> dataset from Kaggle (<a href="https://www.sciencedirect.com/science/article/abs/pii/S0167923609001377">Cortez et.al, 2009</a>). Our job us to use 11 attributes to classify each wine.</p>
<div class="sourceCode" id="cb814"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb814-1"><a href="deep-learning.html#cb814-1" tabindex="-1"></a>dfr <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;wineQualityReds.csv&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)</span>
<span id="cb814-2"><a href="deep-learning.html#cb814-2" tabindex="-1"></a>dfr <span class="ot">&lt;-</span> dfr[, <span class="sc">-</span><span class="dv">1</span>] <span class="co"># removing the index</span></span>
<span id="cb814-3"><a href="deep-learning.html#cb814-3" tabindex="-1"></a><span class="fu">table</span>(dfr<span class="sc">$</span>quality)</span></code></pre></div>
<pre><code>## 
##   3   4   5   6   7   8 
##  10  53 681 638 199  18</code></pre>
<div class="sourceCode" id="cb816"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb816-1"><a href="deep-learning.html#cb816-1" tabindex="-1"></a><span class="co"># Let&#39;s remove the outlier qualities:</span></span>
<span id="cb816-2"><a href="deep-learning.html#cb816-2" tabindex="-1"></a>indo <span class="ot">&lt;-</span> <span class="fu">which</span>(dfr<span class="sc">$</span>quality <span class="sc">==</span> <span class="st">&quot;3&quot;</span> <span class="sc">|</span> dfr<span class="sc">$</span>quality <span class="sc">==</span> <span class="st">&quot;8&quot;</span>)</span>
<span id="cb816-3"><a href="deep-learning.html#cb816-3" tabindex="-1"></a>dfr <span class="ot">&lt;-</span> dfr[<span class="sc">-</span>indo, ]</span>
<span id="cb816-4"><a href="deep-learning.html#cb816-4" tabindex="-1"></a>dfr<span class="sc">$</span>quality <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(dfr<span class="sc">$</span>quality)</span>
<span id="cb816-5"><a href="deep-learning.html#cb816-5" tabindex="-1"></a><span class="fu">table</span>(dfr<span class="sc">$</span>quality)</span></code></pre></div>
<pre><code>## 
##   4   5   6   7 
##  53 681 638 199</code></pre>
<p>Then scale the data and get the formula,</p>
<div class="sourceCode" id="cb818"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb818-1"><a href="deep-learning.html#cb818-1" tabindex="-1"></a><span class="co"># Scaling and Dummy coding</span></span>
<span id="cb818-2"><a href="deep-learning.html#cb818-2" tabindex="-1"></a>dfr[, <span class="fu">sapply</span>(dfr, is.numeric)] <span class="ot">&lt;-</span></span>
<span id="cb818-3"><a href="deep-learning.html#cb818-3" tabindex="-1"></a>  <span class="fu">scale</span>((dfr[, <span class="fu">sapply</span>(dfr, is.numeric)]))</span>
<span id="cb818-4"><a href="deep-learning.html#cb818-4" tabindex="-1"></a></span>
<span id="cb818-5"><a href="deep-learning.html#cb818-5" tabindex="-1"></a>ddf <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>( <span class="sc">~</span> quality <span class="sc">-</span> <span class="dv">1</span>, <span class="at">data =</span> dfr)</span>
<span id="cb818-6"><a href="deep-learning.html#cb818-6" tabindex="-1"></a>w.ind <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">colnames</span>(dfr) <span class="sc">==</span> <span class="st">&quot;quality&quot;</span>)</span>
<span id="cb818-7"><a href="deep-learning.html#cb818-7" tabindex="-1"></a>dfr <span class="ot">&lt;-</span> dfr[, <span class="sc">-</span>w.ind] <span class="co"># removing &#39;quality`</span></span>
<span id="cb818-8"><a href="deep-learning.html#cb818-8" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">cbind</span>(ddf, dfr)</span>
<span id="cb818-9"><a href="deep-learning.html#cb818-9" tabindex="-1"></a></span>
<span id="cb818-10"><a href="deep-learning.html#cb818-10" tabindex="-1"></a>frm <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(<span class="fu">paste</span>(</span>
<span id="cb818-11"><a href="deep-learning.html#cb818-11" tabindex="-1"></a>  <span class="fu">paste</span>(<span class="fu">colnames</span>(ddf), <span class="at">collapse =</span> <span class="st">&#39;+&#39;</span>),</span>
<span id="cb818-12"><a href="deep-learning.html#cb818-12" tabindex="-1"></a>  <span class="st">&quot;~&quot;</span>,</span>
<span id="cb818-13"><a href="deep-learning.html#cb818-13" tabindex="-1"></a>  <span class="fu">paste</span>(<span class="fu">colnames</span>(dfr), <span class="at">collapse =</span> <span class="st">&#39;+&#39;</span>)</span>
<span id="cb818-14"><a href="deep-learning.html#cb818-14" tabindex="-1"></a>))</span>
<span id="cb818-15"><a href="deep-learning.html#cb818-15" tabindex="-1"></a>frm</span></code></pre></div>
<pre><code>## quality4 + quality5 + quality6 + quality7 ~ fixed.acidity + volatile.acidity + 
##     citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + 
##     total.sulfur.dioxide + density + pH + sulphates + alcohol</code></pre>
<p>And, our simple DNN application</p>
<div class="sourceCode" id="cb820"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb820-1"><a href="deep-learning.html#cb820-1" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df) <span class="sc">*</span> .<span class="dv">7</span>)</span>
<span id="cb820-2"><a href="deep-learning.html#cb820-2" tabindex="-1"></a>train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb820-3"><a href="deep-learning.html#cb820-3" tabindex="-1"></a>test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb820-4"><a href="deep-learning.html#cb820-4" tabindex="-1"></a></span>
<span id="cb820-5"><a href="deep-learning.html#cb820-5" tabindex="-1"></a>fit.nn <span class="ot">&lt;-</span></span>
<span id="cb820-6"><a href="deep-learning.html#cb820-6" tabindex="-1"></a>  <span class="fu">neuralnet</span>(</span>
<span id="cb820-7"><a href="deep-learning.html#cb820-7" tabindex="-1"></a>    frm,</span>
<span id="cb820-8"><a href="deep-learning.html#cb820-8" tabindex="-1"></a>    <span class="at">data =</span> train,</span>
<span id="cb820-9"><a href="deep-learning.html#cb820-9" tabindex="-1"></a>    <span class="at">hidden =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">2</span>),</span>
<span id="cb820-10"><a href="deep-learning.html#cb820-10" tabindex="-1"></a>    <span class="at">threshold =</span> <span class="fl">0.05</span>,</span>
<span id="cb820-11"><a href="deep-learning.html#cb820-11" tabindex="-1"></a>    <span class="at">linear.output =</span> <span class="cn">FALSE</span>,</span>
<span id="cb820-12"><a href="deep-learning.html#cb820-12" tabindex="-1"></a>    <span class="at">err.fct =</span> <span class="st">&quot;ce&quot;</span></span>
<span id="cb820-13"><a href="deep-learning.html#cb820-13" tabindex="-1"></a>  )</span>
<span id="cb820-14"><a href="deep-learning.html#cb820-14" tabindex="-1"></a></span>
<span id="cb820-15"><a href="deep-learning.html#cb820-15" tabindex="-1"></a><span class="fu">plot</span>(fit.nn, <span class="at">rep =</span> <span class="st">&quot;best&quot;</span>)</span></code></pre></div>
<p><img src="25-NeuralNetworks_files/figure-html/nn18-1.png" width="672" /></p>
<p>And our prediction:</p>
<div class="sourceCode" id="cb821"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb821-1"><a href="deep-learning.html#cb821-1" tabindex="-1"></a><span class="fu">library</span>(utiml)</span>
<span id="cb821-2"><a href="deep-learning.html#cb821-2" tabindex="-1"></a></span>
<span id="cb821-3"><a href="deep-learning.html#cb821-3" tabindex="-1"></a>phat <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit.nn, test)</span>
<span id="cb821-4"><a href="deep-learning.html#cb821-4" tabindex="-1"></a><span class="fu">head</span>(phat)</span></code></pre></div>
<pre><code>##          [,1]      [,2]      [,3]        [,4]
## 11 0.04931942 0.6859165 0.2825146 0.007453298
## 14 0.02015208 0.2300976 0.5716617 0.050629158
## 16 0.08538068 0.8882124 0.0550048 0.009824072
## 17 0.03592572 0.5136539 0.5273030 0.006685037
## 22 0.03616818 0.5173671 0.5263112 0.006541895
## 27 0.03092853 0.4318134 0.5389403 0.011412135</code></pre>
<div class="sourceCode" id="cb823"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb823-1"><a href="deep-learning.html#cb823-1" tabindex="-1"></a><span class="co"># Assigning label by selecting the highest phat</span></span>
<span id="cb823-2"><a href="deep-learning.html#cb823-2" tabindex="-1"></a>label.hat <span class="ot">&lt;-</span> <span class="fu">t</span>(<span class="fu">apply</span>(phat, <span class="dv">1</span>, <span class="cf">function</span>(x) <span class="fu">as.numeric</span>(x <span class="sc">==</span> <span class="fu">max</span>(x))))</span>
<span id="cb823-3"><a href="deep-learning.html#cb823-3" tabindex="-1"></a><span class="fu">head</span>(label.hat)</span></code></pre></div>
<pre><code>##    [,1] [,2] [,3] [,4]
## 11    0    1    0    0
## 14    0    0    1    0
## 16    0    1    0    0
## 17    0    0    1    0
## 22    0    0    1    0
## 27    0    0    1    0</code></pre>
<div class="sourceCode" id="cb825"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb825-1"><a href="deep-learning.html#cb825-1" tabindex="-1"></a><span class="co"># Confusion Table</span></span>
<span id="cb825-2"><a href="deep-learning.html#cb825-2" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">apply</span>(phat, <span class="dv">1</span>, which.max)</span>
<span id="cb825-3"><a href="deep-learning.html#cb825-3" tabindex="-1"></a>fck <span class="ot">&lt;-</span> <span class="fu">colnames</span>(test)[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]</span>
<span id="cb825-4"><a href="deep-learning.html#cb825-4" tabindex="-1"></a>predicted <span class="ot">&lt;-</span> fck[pred]</span>
<span id="cb825-5"><a href="deep-learning.html#cb825-5" tabindex="-1"></a></span>
<span id="cb825-6"><a href="deep-learning.html#cb825-6" tabindex="-1"></a>act<span class="ot">&lt;-</span> <span class="fu">apply</span>(test[,<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>], <span class="dv">1</span>, which.max)</span>
<span id="cb825-7"><a href="deep-learning.html#cb825-7" tabindex="-1"></a>actual <span class="ot">&lt;-</span> fck[act]</span>
<span id="cb825-8"><a href="deep-learning.html#cb825-8" tabindex="-1"></a></span>
<span id="cb825-9"><a href="deep-learning.html#cb825-9" tabindex="-1"></a><span class="fu">table</span>(predicted, actual)</span></code></pre></div>
<pre><code>##           actual
## predicted  quality4 quality5 quality6 quality7
##   quality5        4      107       32        3
##   quality6        7       78      160       51
##   quality7        1        0       11       18</code></pre>
<p>This is just an example and the results are not reflecting a trained model.</p>
<p>Advance DNN applications with a proper training requires a longer time and more capable machines. We can do a grid search on different number of hidden layers and neurons. However, a large datasets and more complex DNNs need better applications, like, <code>Keras</code> that uses GPU with capable operating systems allowing a much better efficiency in training. So far, we have used the <code>neuralnet</code> package. There are several packages in R that are also capable of implementing and training artificial neural networks. The most suitable one for our needs will depend on our specific requirements and preferences. For a powerful and flexible package for building and training ANNs, <code>neuralnet</code> or <code>deepnet</code> may be good options. When we just need a simple and easy-to-use package for training feedforward networks and making predictions, <code>nnet</code> may be another good choice. If we want a general-purpose package that can handle a wide range of machine learning tasks, including ANNs, <code>caret</code> would be a good option.</p>
<p>Deep neural networks (DNNs) are neural networks with many layers, which can be difficult to train because of the large number of parameters that need to be optimized. This can make the training process computationally intensive and prone to overfitting. Convolutional neural networks (CNNs), on the other hand, are specifically designed to process data that has a grid-like structure, such as an image. One key aspect of CNNs is that they use convolutional layers, which apply a set of filters to the input data and produce a set of transformed feature maps. These filters are able to detect specific features in the input data, such as edges, corners, or textures, and are able to share these features across the input data. This means that the number of parameters in a CNN is typically much smaller than in a DNN, which makes the model easier to train and less prone to overfitting. Overall, CNNs are well-suited for tasks such as image classification, object detection and, speech recognition. We will not cover the details of CNN here. There are several packages available in R for working with CNNs.</p>
<p>Finally, in a deep neural network, “dropout” and “regularization” are techniques used to prevent overfitting. Dropout is a regularization technique that randomly drops out, or removes, a certain percentage of neurons from the network during training. This has the effect of reducing the complexity of the model, as it can’t rely on any one neuron or group of neurons to make predictions. Regularization is a general term that refers to any method used to prevent overfitting in a machine learning model. There are many types of regularization techniques, which add a penalty term to the parameters of the the activation functions.</p>
<p>We will be back to ANN later in Section VII - Time Series.</p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="neural-networks.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="graphical-network-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuk/machinemetrics/edit/master/25-NeuralNetworks.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
