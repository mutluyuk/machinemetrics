<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Hyperparameter Tuning | MachineMetrics</title>
  <meta name="description" content="Chapter 11 Hyperparameter Tuning | MachineMetrics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Hyperparameter Tuning | MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Hyperparameter Tuning | MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="nonparametric-estimations---basics.html"/>
<link rel="next" href="optimization-algorithms---basics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> INTRODUCTION:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html"><i class="fa fa-check"></i><b>2</b> Statistical Models and Simulations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#distinguishing-between-statistical-modeling-and-machine-learning-in-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#goals-and-objectives"><i class="fa fa-check"></i><b>2.1.1</b> Goals and Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#prediction-vs.-inference"><i class="fa fa-check"></i><b>2.1.2</b> Prediction vs. Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#conclusion"><i class="fa fa-check"></i><b>2.1.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#model-selection-and-approaches-in-data-modeling"><i class="fa fa-check"></i><b>2.4</b> Model Selection and Approaches in Data Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#mle"><i class="fa fa-check"></i><b>3.5.3</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error---mse"><i class="fa fa-check"></i><b>5.1</b> Estimation error - MSE</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.2</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#technical-points-about-mse-and-mspe"><i class="fa fa-check"></i><b>5.3</b> Technical points about MSE and MSPE</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.1</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.1</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.2</b> LPM</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
<li class="chapter" data-level="10.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#knn"><i class="fa fa-check"></i><b>10.3</b> Knn</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#adult-dataset"><i class="fa fa-check"></i><b>10.3.1</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="11.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#grid-search"><i class="fa fa-check"></i><b>11.4</b> Grid Search</a></li>
<li class="chapter" data-level="11.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>11.5</b> Cross-validated grid search</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#one-variable"><i class="fa fa-check"></i><b>12.4.1</b> One-variable</a></li>
<li class="chapter" data-level="12.4.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>12.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="12.4.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#multivariable"><i class="fa fa-check"></i><b>12.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>16.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.3</b> Pruning</a></li>
<li class="chapter" data-level="16.4" data-path="regression-trees.html"><a href="regression-trees.html#classification-with-titanic"><i class="fa fa-check"></i><b>16.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="16.5" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>17.2</b> Random Forest</a></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>17.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="17.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="17.3.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.3.3</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.4</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.5</b> Classification</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.6</b> Regression</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.7</b> Exploration</a></li>
<li class="chapter" data-level="17.8" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.8</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression-1"><i class="fa fa-check"></i><b>17.8.1</b> Regression</a></li>
<li class="chapter" data-level="17.8.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.8.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.8.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.8.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.8.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-1"><i class="fa fa-check"></i><b>17.8.4</b> Classification</a></li>
<li class="chapter" data-level="17.8.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost.m1"><i class="fa fa-check"></i><b>17.8.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="17.8.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.8.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#out-sample-prediction-accuracy"><i class="fa fa-check"></i><b>20.3</b> out-sample prediction accuracy</a></li>
<li class="chapter" data-level="20.4" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-2.html"><a href="classification-2.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-2.html"><a href="classification-2.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-2.html"><a href="classification-2.html#mnist-dataset"><i class="fa fa-check"></i><b>21.2</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="21.3" data-path="classification-2.html"><a href="classification-2.html#linear-classifiers-again"><i class="fa fa-check"></i><b>21.3</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="21.4" data-path="classification-2.html"><a href="classification-2.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.4</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.5" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret"><i class="fa fa-check"></i><b>21.5</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="classification-2.html"><a href="classification-2.html#mnist_27"><i class="fa fa-check"></i><b>21.5.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="21.5.2" data-path="classification-2.html"><a href="classification-2.html#adult-dataset-1"><i class="fa fa-check"></i><b>21.5.2</b> Adult dataset</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="classification-2.html"><a href="classification-2.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.6</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.7" data-path="classification-2.html"><a href="classification-2.html#confusion-matrix"><i class="fa fa-check"></i><b>21.7</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.8" data-path="classification-2.html"><a href="classification-2.html#performance-measures"><i class="fa fa-check"></i><b>21.8</b> Performance measures</a></li>
<li class="chapter" data-level="21.9" data-path="classification-2.html"><a href="classification-2.html#roc-curve"><i class="fa fa-check"></i><b>21.9</b> ROC Curve</a></li>
<li class="chapter" data-level="21.10" data-path="classification-2.html"><a href="classification-2.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.10</b> AUC - Area Under the Curve</a></li>
<li class="chapter" data-level="21.11" data-path="classification-2.html"><a href="classification-2.html#classification-example"><i class="fa fa-check"></i><b>21.11</b> Classification Example</a></li>
<li class="chapter" data-level="21.12" data-path="classification-2.html"><a href="classification-2.html#lpm-1"><i class="fa fa-check"></i><b>21.12</b> LPM</a></li>
<li class="chapter" data-level="21.13" data-path="classification-2.html"><a href="classification-2.html#logistic-regression-1"><i class="fa fa-check"></i><b>21.13</b> Logistic Regression</a></li>
<li class="chapter" data-level="21.14" data-path="classification-2.html"><a href="classification-2.html#knn-1"><i class="fa fa-check"></i><b>21.14</b> kNN</a>
<ul>
<li class="chapter" data-level="21.14.1" data-path="classification-2.html"><a href="classification-2.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>21.14.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="21.14.2" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret-1"><i class="fa fa-check"></i><b>21.14.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>22</b> Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="time-series.html"><a href="time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="time-series.html"><a href="time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="time-series.html"><a href="time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="time-series.html"><a href="time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="time-series.html"><a href="time-series.html#stationarity"><i class="fa fa-check"></i><b>22.5</b> Stationarity</a></li>
<li class="chapter" data-level="22.6" data-path="time-series.html"><a href="time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.6</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="time-series.html"><a href="time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.7</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.8" data-path="time-series.html"><a href="time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.8</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.9" data-path="time-series.html"><a href="time-series.html#speed"><i class="fa fa-check"></i><b>22.9</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="forecast.html"><a href="forecast.html"><i class="fa fa-check"></i><b>23</b> Forecast</a>
<ul>
<li class="chapter" data-level="23.1" data-path="forecast.html"><a href="forecast.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="forecast.html"><a href="forecast.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="forecast.html"><a href="forecast.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="forecast.html"><a href="forecast.html#random-forest-1"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="forecast.html"><a href="forecast.html#univariate"><i class="fa fa-check"></i><b>23.5</b> Univariate</a></li>
<li class="chapter" data-level="23.6" data-path="forecast.html"><a href="forecast.html#multivariate"><i class="fa fa-check"></i><b>23.6</b> Multivariate</a></li>
<li class="chapter" data-level="23.7" data-path="forecast.html"><a href="forecast.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.7</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>24.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-margin"><i class="fa fa-check"></i><b>24.1.1</b> The Margin</a></li>
<li class="chapter" data-level="24.1.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-non-separable-case"><i class="fa fa-check"></i><b>24.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>24.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="24.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#application-with-svm"><i class="fa fa-check"></i><b>24.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#precision-matrix"><i class="fa fa-check"></i><b>27.4</b> Precision Matrix</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.5</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.6" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.6</b> Regularized Covariance Matrix</a></li>
<li class="chapter" data-level="27.7" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>27.7</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="27.8" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#high-dimensional-data"><i class="fa fa-check"></i><b>27.8</b> High-dimensional data</a></li>
<li class="chapter" data-level="27.9" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>27.9</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#rankr-approximations"><i class="fa fa-check"></i><b>28.4</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="28.5" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.5</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html"><i class="fa fa-check"></i><b>29</b> PCA (Principle Component Analysis)</a>
<ul>
<li class="chapter" data-level="29.1" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.1</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>30.1</b> Using bins</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>30.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="30.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>30.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="30.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="31.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>31.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="31.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>31.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="text-analysis.html"><a href="text-analysis.html"><i class="fa fa-check"></i><b>32</b> Text Analysis</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#other-nonparametric-estimation-methods-1"><i class="fa fa-check"></i><b>33.1</b> Other Nonparametric Estimation methods</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.2</b> Regression splines</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.3</b> MARS</a></li>
<li class="chapter" data-level="33.4" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.4</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuk/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hyperparameter-tuning" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">Chapter 11</span> Hyperparameter Tuning<a href="hyperparameter-tuning.html#hyperparameter-tuning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>How do we know that an estimated regression model is generalizable beyond the sample data used to fit it? Ideally, we can obtain new independent data with which to validate our model. For example, we could refit the model to the new dataset to see if the various characteristics of the model (e.g., estimates regression coefficients) are consistent with the model fit to the original dataset. Alternatively, we could use the regression equation of the model fit to the original dataset to make predictions of the response variable for the new dataset. Then we can calculate the prediction errors (differences between the actual response values and the predictions) and summarize the predictive ability of the model by the mean squared prediction error (MSPE). This gives an indication of how well the model will predict in the future. Sometimes the MSPE is rescaled to provide a cross-validation R2.</p>
<p>However, most of the time we cannot obtain new independent data to validate our model. An alternative is to partition the sample data into a training (or model-building) set, which we can use to develop the model, and a validation (or prediction) set, which is used to evaluate the predictive ability of the model. This is called cross-validation. Again, we can compare the model fit to the training set to the model refit to the validation set to assess consistency. Or we can calculate the MSPE for the validation set to assess the predictive ability of the model.</p>
<p>Another way to employ cross-validation is to use the validation set to help determine the final selected model. Suppose we have found a handful of “good” models that each provide a satisfactory fit to the training data and satisfy the model (LINE) conditions. We can calculate the MSPE for each model on the validation set. Our final selected model is the one with the smallest MSPE.</p>
<p>The simplest approach to cross-validation is to partition the sample observations randomly with 50% of the sample in each set. This assumes there is sufficient data to have 6-10 observations per potential predictor variable in the training set; if not, then the partition can be set to, say, 60%/40% or 70%/30%, to satisfy this constraint.</p>
<p>If the dataset is too small to satisfy this constraint even by adjusting the partition allocation then K-fold cross-validation can be used. This partitions the sample dataset into K parts which are (roughly) equal in size. For each part, we use the remaining K – 1 parts to estimate the model of interest (i.e., the training sample) and test the predictability of the model with the remaining part (i.e., the validation sample). We then calculate the sum of squared prediction errors, and combine the K estimates of prediction error to produce a K-fold cross-validation estimate.</p>
<p>When K = 2, this is a simple extension of the 50%/50% partition method described above. The advantage of this method is that it is usually preferable to residual diagnostic methods and takes not much longer to compute. However, its evaluation can have high variance since evaluation may depend on which data points end up in the training sample and which end up in the test sample.</p>
<p>When K = n, this is called leave-one-out cross-validation. That means that n separate data sets are trained on all of the data (except one point) and then prediction is made for that one point. The evaluation of this method is very good, but often computationally expensive. Note that the K-fold cross-validation estimate of prediction error is identical to the PRESS statistic.</p>
<p>In general, there are multiple <strong>tuning</strong> parameters or so-called <strong>hyperparameters</strong> associated with each prediction method. The value of the hyperparameter has to be set before the learning process begins because those tuning parameters are external to the model and their value cannot be estimated from data.</p>
<p>Therefore, we usually need to perform a grid search to identify the optimal combination of these parameters that minimizes the prediction error. For example, <span class="math inline">\(k\)</span> in kNN, the number of hidden layers in Neural Networks, even the degree of of polynomials in a linear regression have to be tuned before the learning process starts. In contrast, a parameter (in a paramteric model) is an internal characteristic of the model and its value can be estimated from data for any given hyperparameter.</p>
<p>For example, <span class="math inline">\(\lambda\)</span>, the penalty parameter that shrinks the number of variables in Lasso, which we will see in Section 5, is a hyperparameter and has to be set before the estimation. When it’s set, the coefficients of Lasso are estimated from the process. We start with <strong>k-fold cross validation</strong> process and perform a cross-validated grid search to identify the optimal mix of those parameters.</p>
<p>Here, we will learn the rules and simple application how to set up a grid serach that evaluates many different combinations of hyperparameters. This chapter covers the key concept in modern machine learning applications and many learning algorithms.</p>
<div id="training-and-validation" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">11.1</span> Training and Validation<a href="hyperparameter-tuning.html#training-and-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before learning how to split the data into subsections randomly, we need to know what these sets are for and how we define them properly. This section is inspired by the article, <a href="https://machinelearningmastery.com/difference-test-validation-datasets/"><em>What is the Difference Between Test and Validation Datasets?</em></a>, by Jason Brownlee <span class="citation">(<a href="#ref-Brownlee_2017"><strong>Brownlee_2017?</strong></a>)</span>. The article clarifies how validation and test datasets are different, which can be confusing in practice.</p>
<p>Let’s define them formally first:</p>
<ul>
<li><strong>Training Dataset</strong>: The sample of data used to fit (train) the predictive model.<br />
</li>
<li><strong>Validation Dataset</strong>: The sample of data used for tuning model hyperparameters, and selecting variables (feature selection).<br />
</li>
<li><strong>Test Dataset</strong>: The sample of data reserved to provide an unbiased evaluation of a final model fit on the training dataset.</li>
</ul>
<p>However, in practice, validation and test datasets are not named separately. Let’s summarize a usual process in modern machine learning applications:</p>
<ol style="list-style-type: decimal">
<li>You have a dataset for building a predictive model.</li>
<li>Given the data and the prediction problem on your hand, you usually have multiple alternatives or <em>competing</em> models to start with.<br />
</li>
<li>Each model needs a training, which is a process of tuning their hyperparameters and selecting their features (variables) for the best predictive performance.</li>
</ol>
<p>Therefore, this process requires two different datasets: <strong>training</strong> and <strong>validation</strong> datasets. The intuition behind this split is very simple: the prediction is an out-of-sample problem. If we use the same sample that we use to fit the model for assessing the prediction accuracy of our model, we face the infamous overfitting problem. Since we usually don’t have another unseen dataset available to us, we split the data and leave one part out of our original dataset. We literally pretend that one that is left out is “unseen” by us. Now the question is how we do this split. Would it be 50-50?. The general approach is k-fold cross validation with a grid search. Here are the main steps:</p>
<ol style="list-style-type: decimal">
<li>Suppose Model 1 requires to pick a value for <span class="math inline">\(\lambda\)</span>, perhaps it is a degree of polynomials in the model.<br />
</li>
<li>We establish a grid, a set of sequential numbers, that is a set of possible values of <span class="math inline">\(\lambda\)</span>.<br />
</li>
<li>We split the data into <span class="math inline">\(k\)</span> random sections, let’s say 10 proportionally equal sections.</li>
<li>We leave one section out and use 9 sections. The combination of these 9 sections is our <strong>training set</strong>. The one that is left out is our <strong>validation set</strong>.<br />
</li>
<li>We fit the model using each value in the set of possible values of <span class="math inline">\(\lambda\)</span>. For example, if we have 100 values of <span class="math inline">\(\lambda\)</span>, we fit the model to the training set 100 times, once for each possible value of <span class="math inline">\(\lambda\)</span>.</li>
<li>We evaluate each of 100 models by using their predictive accuracy on the validation set, the one that is left out. We pick a <span class="math inline">\(\lambda\)</span> that gives the highest prediction accuracy.<br />
</li>
<li>We do this process 10 times (if it is 10-fold cross validation) with each time using a different section of the data as the validation set. So, note that each time our training and validation sets are going to be different. At the end, in total we will have 10 best <span class="math inline">\(\lambda\)</span>s.<br />
</li>
<li>We pick the average or modal value of <span class="math inline">\(\lambda\)</span> as our optimal hyperparameter that tunes our predictive model for its best performance.</li>
</ol>
<p>Note that the term <em>validation</em> is sometimes is mixed-up with <em>test</em> for the dataset we left out from our sample. This point often confuses practitioners. So what is the <strong>test set</strong>?</p>
<p>We have now Model 1 tuned with the optimal <span class="math inline">\(\lambda\)</span>. This is Model 1 among several alternative models (there are more than 300 predictive models and growing in practice). Besides, the steps above we followed provides a limited answer whether if Model 1 has a good or “acceptable” prediction accuracy or not. In other words, tuning Model 1 doesn’t mean that it does a good or a bad job in prediction. How do we know and measure its performance in prediction?</p>
<p>Usually, if the outcome that we try to predict is quantitative variable, we use root mean squared prediction error (RMSPE). There are several other metrics we will see later. If it’s an indicator outcome, we have to apply some other methods, one of which is called as Receiver Operating Curve (ROC). We will see and learn all of them all in detail shortly. But, for now, let’s pretend that we know a metric that measures the prediction accuracy of Model 1 as well as other alternative models.</p>
<p>The only sensible way to do it would be to test the “tuned” model on a new dataset. In other words, you need to use the trained model on a real and new dataset and calculate the prediction accuracy of Model 1 by RMSPE or ROC. But, we do not have one. <strong>That’s why we have to go the beginning and create a split before starting the training process: training and test datasets. We use the training data for the feature selection and tuning the parameter.</strong> After you “trained” the model by validation, we can use the <strong>test set</strong> to see its performance.</p>
<p>Finally, you follow the same steps for other alternative learning algorithms and then pick the winner. Having trained each model using the training set, and chosen the best model using the validation set, the test set tells you how good your final choice of model is.</p>
<p>Here is a visualization of the split:</p>
<p><img src="png/split.png" width="130%" height="130%" /></p>
<p>Before seeing every step with an application in this chapter, let’s have a more intuitive and simpler explanation about “training” a model. First, what’s learning? We can summarize it this way: <strong>observe the facts, do some generalizations, use these generalizations to predict previously unseen facts, evaluate your predictions, and adjust your generalizations (knowledge) for better predictions</strong>. It’s an infinite loop.</p>
<p>Here is the basic paradigm:</p>
<ul>
<li>Observe the facts (<strong>training data</strong>),</li>
<li>Make generalizations (<strong>build prediction models</strong>),</li>
<li>Adjust your prediction to make them better (<strong>train your model with validation data</strong>),</li>
<li>Test your predictions on unseen data to see how they hold up (<strong>test data</strong>)</li>
</ul>
<p>As the distinction between validation and test datasets is now clear, we can conclude that, even if we have the best possible predictive model given the training dataset, our generalization of the seen data for prediction of unseen facts would be fruitless in practice. In fact, we may learn nothing at the end of this process and remain unknowledgeable about the unseen facts. Why would this happen? The main reason would be the lack of enough information in training set. If we do not have enough data to make and test models that are applicable to real life, our predictions may not be valid. The second reason would be modeling inefficiencies in a sense that it requires a very large computing power and storage capacity. This subject is also getting more interesting everyday. The Google’s quantum computers are one of them.</p>
</div>
<div id="splitting-the-data-randomly" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">11.2</span> Splitting the data randomly<a href="hyperparameter-tuning.html#splitting-the-data-randomly" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We already know how to sample a set of observation by using <code>sample()</code>. We can use this function again to sort the data into k-fold sections. Here is an example with just 2 sections:</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="hyperparameter-tuning.html#cb121-1" tabindex="-1"></a><span class="co">#We can create a simple dataset with X and Y using a DGM</span></span>
<span id="cb121-2"><a href="hyperparameter-tuning.html#cb121-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb121-3"><a href="hyperparameter-tuning.html#cb121-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb121-4"><a href="hyperparameter-tuning.html#cb121-4" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">3</span>, <span class="dv">6</span>)</span>
<span id="cb121-5"><a href="hyperparameter-tuning.html#cb121-5" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">13</span><span class="sc">*</span>X <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb121-6"><a href="hyperparameter-tuning.html#cb121-6" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Y, X)</span>
<span id="cb121-7"><a href="hyperparameter-tuning.html#cb121-7" tabindex="-1"></a></span>
<span id="cb121-8"><a href="hyperparameter-tuning.html#cb121-8" tabindex="-1"></a><span class="co">#We need to shuffle it</span></span>
<span id="cb121-9"><a href="hyperparameter-tuning.html#cb121-9" tabindex="-1"></a>random <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb121-10"><a href="hyperparameter-tuning.html#cb121-10" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[random, ]</span>
<span id="cb121-11"><a href="hyperparameter-tuning.html#cb121-11" tabindex="-1"></a><span class="co">#Now we have a dataset shuffled randomly.</span></span>
<span id="cb121-12"><a href="hyperparameter-tuning.html#cb121-12" tabindex="-1"></a></span>
<span id="cb121-13"><a href="hyperparameter-tuning.html#cb121-13" tabindex="-1"></a><span class="co">#Since the order of data is now completely random,</span></span>
<span id="cb121-14"><a href="hyperparameter-tuning.html#cb121-14" tabindex="-1"></a><span class="co">#we can divide it as many slices as we wish</span></span>
<span id="cb121-15"><a href="hyperparameter-tuning.html#cb121-15" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="co">#2-fold (slices-sections)</span></span>
<span id="cb121-16"><a href="hyperparameter-tuning.html#cb121-16" tabindex="-1"></a>nslice <span class="ot">&lt;-</span> <span class="fu">floor</span>(n<span class="sc">/</span>k) <span class="co">#number of observations in each fold/slice</span></span>
<span id="cb121-17"><a href="hyperparameter-tuning.html#cb121-17" tabindex="-1"></a></span>
<span id="cb121-18"><a href="hyperparameter-tuning.html#cb121-18" tabindex="-1"></a><span class="co">#Since we have only 2 slices of data</span></span>
<span id="cb121-19"><a href="hyperparameter-tuning.html#cb121-19" tabindex="-1"></a><span class="co">#we can call one slice as a &quot;validation set&quot; the other one as a &quot;training set&quot;</span></span>
<span id="cb121-20"><a href="hyperparameter-tuning.html#cb121-20" tabindex="-1"></a>train <span class="ot">&lt;-</span> data[<span class="dv">1</span><span class="sc">:</span>nslice, ]</span>
<span id="cb121-21"><a href="hyperparameter-tuning.html#cb121-21" tabindex="-1"></a><span class="fu">str</span>(train)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    5000 obs. of  2 variables:
##  $ Y: num  -92.7 52.35 -114 133.6 7.39 ...
##  $ X: num  -7.344 3.868 -9.006 9.978 0.468 ...</code></pre>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="hyperparameter-tuning.html#cb123-1" tabindex="-1"></a>val <span class="ot">&lt;-</span> data[(nslice<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n,]</span>
<span id="cb123-2"><a href="hyperparameter-tuning.html#cb123-2" tabindex="-1"></a><span class="fu">str</span>(val)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    5000 obs. of  2 variables:
##  $ Y: num  -49.1 -53.7 -25 -46.2 135.2 ...
##  $ X: num  -3.9 -4.22 -1.99 -3.67 10.37 ...</code></pre>
<p>This is good, we now have it split into a 2-fold with 50-50% splitting. But what if we want a slicing that gives a 10% validation set and a 90% training set? How can we do that? One of the most common ways to do 10%-90% splitting is 10-fold slicing. Here is how:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="hyperparameter-tuning.html#cb125-1" tabindex="-1"></a><span class="co">#Again, first, we need to shuffle it</span></span>
<span id="cb125-2"><a href="hyperparameter-tuning.html#cb125-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb125-3"><a href="hyperparameter-tuning.html#cb125-3" tabindex="-1"></a>random <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb125-4"><a href="hyperparameter-tuning.html#cb125-4" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[random, ]</span>
<span id="cb125-5"><a href="hyperparameter-tuning.html#cb125-5" tabindex="-1"></a></span>
<span id="cb125-6"><a href="hyperparameter-tuning.html#cb125-6" tabindex="-1"></a><span class="co">#number of folds</span></span>
<span id="cb125-7"><a href="hyperparameter-tuning.html#cb125-7" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb125-8"><a href="hyperparameter-tuning.html#cb125-8" tabindex="-1"></a>nslice <span class="ot">&lt;-</span> <span class="fu">floor</span>(n<span class="sc">/</span>k) <span class="co">#number of observations in each fold/slice</span></span>
<span id="cb125-9"><a href="hyperparameter-tuning.html#cb125-9" tabindex="-1"></a></span>
<span id="cb125-10"><a href="hyperparameter-tuning.html#cb125-10" tabindex="-1"></a><span class="co">#Now we have  10 slices of data and each slice has 10% of data.</span></span>
<span id="cb125-11"><a href="hyperparameter-tuning.html#cb125-11" tabindex="-1"></a><span class="co">#We can call one slice as a &quot;validation set&quot;</span></span>
<span id="cb125-12"><a href="hyperparameter-tuning.html#cb125-12" tabindex="-1"></a><span class="co">#And we group the other 9 slices as a &quot;training set&quot;</span></span>
<span id="cb125-13"><a href="hyperparameter-tuning.html#cb125-13" tabindex="-1"></a></span>
<span id="cb125-14"><a href="hyperparameter-tuning.html#cb125-14" tabindex="-1"></a>val <span class="ot">&lt;-</span> data[<span class="dv">1</span><span class="sc">:</span>nslice, ]</span>
<span id="cb125-15"><a href="hyperparameter-tuning.html#cb125-15" tabindex="-1"></a><span class="fu">str</span>(val)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    1000 obs. of  2 variables:
##  $ Y: num  -9.14 -87.79 -74.7 130.82 31.2 ...
##  $ X: num  -0.908 -6.95 -5.881 9.809 2.21 ...</code></pre>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="hyperparameter-tuning.html#cb127-1" tabindex="-1"></a>train <span class="ot">&lt;-</span> data[(nslice<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n, ]</span>
<span id="cb127-2"><a href="hyperparameter-tuning.html#cb127-2" tabindex="-1"></a><span class="fu">str</span>(train)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    9000 obs. of  2 variables:
##  $ Y: num  -71.6 12.1 27.9 136.1 26.3 ...
##  $ X: num  -5.59 0.83 1.98 10.37 1.92 ...</code></pre>
<p>How can we use this method to <em>tune</em> a model? Let’s use Kernel regressions applied by <code>loess()</code> that we have seen before.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="hyperparameter-tuning.html#cb129-1" tabindex="-1"></a><span class="co">#Let&#39;s simulate our data</span></span>
<span id="cb129-2"><a href="hyperparameter-tuning.html#cb129-2" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb129-3"><a href="hyperparameter-tuning.html#cb129-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb129-4"><a href="hyperparameter-tuning.html#cb129-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb129-5"><a href="hyperparameter-tuning.html#cb129-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n)<span class="sc">/</span><span class="dv">4</span></span>
<span id="cb129-6"><a href="hyperparameter-tuning.html#cb129-6" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x)</span>
<span id="cb129-7"><a href="hyperparameter-tuning.html#cb129-7" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span></code></pre></div>
<p><img src="11-HyperTuning_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="hyperparameter-tuning.html#cb130-1" tabindex="-1"></a><span class="co">#Estimation with degree = 2 (locally quadratic)</span></span>
<span id="cb130-2"><a href="hyperparameter-tuning.html#cb130-2" tabindex="-1"></a>loe0 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="fl">0.02</span>)</span>
<span id="cb130-3"><a href="hyperparameter-tuning.html#cb130-3" tabindex="-1"></a>loe1 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="fl">0.1</span>) </span>
<span id="cb130-4"><a href="hyperparameter-tuning.html#cb130-4" tabindex="-1"></a>loe2 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="dv">1</span>) </span>
<span id="cb130-5"><a href="hyperparameter-tuning.html#cb130-5" tabindex="-1"></a></span>
<span id="cb130-6"><a href="hyperparameter-tuning.html#cb130-6" tabindex="-1"></a><span class="co">#Plots</span></span>
<span id="cb130-7"><a href="hyperparameter-tuning.html#cb130-7" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> <span class="dv">700</span>)</span>
<span id="cb130-8"><a href="hyperparameter-tuning.html#cb130-8" tabindex="-1"></a>fit0 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe0, t)</span>
<span id="cb130-9"><a href="hyperparameter-tuning.html#cb130-9" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe1, t)</span>
<span id="cb130-10"><a href="hyperparameter-tuning.html#cb130-10" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe2, t)</span>
<span id="cb130-11"><a href="hyperparameter-tuning.html#cb130-11" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>)</span>
<span id="cb130-12"><a href="hyperparameter-tuning.html#cb130-12" tabindex="-1"></a><span class="fu">lines</span>(t, fit0, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>)</span>
<span id="cb130-13"><a href="hyperparameter-tuning.html#cb130-13" tabindex="-1"></a><span class="fu">lines</span>(t, fit1, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb130-14"><a href="hyperparameter-tuning.html#cb130-14" tabindex="-1"></a><span class="fu">lines</span>(t, fit2, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="11-HyperTuning_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The sensitivity of kernel regression estimations (with the locally quadratic <code>loess()</code>) to the bandwidth (<code>span</code>) is obvious from the plot. Which bandwidth should we choose for the best prediction accuracy? There are actually 2 hyperparameters in <code>loess()</code>: <code>degree</code> and <code>span</code>. We can tune both of them at the same time, but for the sake of simplicity, let’s set the <code>degree = 2</code> (locally polynomial) and tune only the bandwidth.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="hyperparameter-tuning.html#cb131-1" tabindex="-1"></a><span class="co">#Again, first, we need to shuffle it</span></span>
<span id="cb131-2"><a href="hyperparameter-tuning.html#cb131-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb131-3"><a href="hyperparameter-tuning.html#cb131-3" tabindex="-1"></a>random <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb131-4"><a href="hyperparameter-tuning.html#cb131-4" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> data2[random, ]</span>
<span id="cb131-5"><a href="hyperparameter-tuning.html#cb131-5" tabindex="-1"></a></span>
<span id="cb131-6"><a href="hyperparameter-tuning.html#cb131-6" tabindex="-1"></a><span class="co">#number of folds</span></span>
<span id="cb131-7"><a href="hyperparameter-tuning.html#cb131-7" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb131-8"><a href="hyperparameter-tuning.html#cb131-8" tabindex="-1"></a>nslice <span class="ot">&lt;-</span> <span class="fu">floor</span>(n<span class="sc">/</span>k) <span class="co">#number of observations in each fold/slice</span></span>
<span id="cb131-9"><a href="hyperparameter-tuning.html#cb131-9" tabindex="-1"></a></span>
<span id="cb131-10"><a href="hyperparameter-tuning.html#cb131-10" tabindex="-1"></a>val <span class="ot">&lt;-</span> data2[<span class="dv">1</span><span class="sc">:</span>nslice, ]</span>
<span id="cb131-11"><a href="hyperparameter-tuning.html#cb131-11" tabindex="-1"></a>train <span class="ot">&lt;-</span> data2[(nslice<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n, ]</span></code></pre></div>
<p>Our validation and train sets are ready. We are going to use the train set to train our models with different values of <code>span</code> in each one. Then, we will validate each model by looking at the RMSPE of the model results against our validation set. The winner will be the one with the lowest RMSPE. That’s the plan. Let’s use the set of <code>span</code> = 0.02, 0.1, and 1.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="hyperparameter-tuning.html#cb132-1" tabindex="-1"></a><span class="co">#Estimation with degree = 2 (locally quadratic) by training set</span></span>
<span id="cb132-2"><a href="hyperparameter-tuning.html#cb132-2" tabindex="-1"></a>loe0 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="fl">0.02</span>, <span class="at">data =</span> train)</span>
<span id="cb132-3"><a href="hyperparameter-tuning.html#cb132-3" tabindex="-1"></a>loe1 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="fl">0.1</span>, <span class="at">data =</span> train) </span>
<span id="cb132-4"><a href="hyperparameter-tuning.html#cb132-4" tabindex="-1"></a>loe2 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="dv">1</span>, <span class="at">data =</span> train)</span>
<span id="cb132-5"><a href="hyperparameter-tuning.html#cb132-5" tabindex="-1"></a></span>
<span id="cb132-6"><a href="hyperparameter-tuning.html#cb132-6" tabindex="-1"></a><span class="co">#Predicting by using validation set</span></span>
<span id="cb132-7"><a href="hyperparameter-tuning.html#cb132-7" tabindex="-1"></a>fit0 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe0, val<span class="sc">$</span>x)</span>
<span id="cb132-8"><a href="hyperparameter-tuning.html#cb132-8" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe1, val<span class="sc">$</span>x)</span>
<span id="cb132-9"><a href="hyperparameter-tuning.html#cb132-9" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe2, val<span class="sc">$</span>x)</span></code></pre></div>
<p>We must also create our performance metric, RMSPE;</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="hyperparameter-tuning.html#cb133-1" tabindex="-1"></a><span class="co">#Estimation with degree = 2 (locally quadratic) by training set</span></span>
<span id="cb133-2"><a href="hyperparameter-tuning.html#cb133-2" tabindex="-1"></a>rmspe0 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((val<span class="sc">$</span>y<span class="sc">-</span>fit0)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb133-3"><a href="hyperparameter-tuning.html#cb133-3" tabindex="-1"></a>rmspe1 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((val<span class="sc">$</span>y<span class="sc">-</span>fit1)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb133-4"><a href="hyperparameter-tuning.html#cb133-4" tabindex="-1"></a>rmspe2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((val<span class="sc">$</span>y<span class="sc">-</span>fit2)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb133-5"><a href="hyperparameter-tuning.html#cb133-5" tabindex="-1"></a></span>
<span id="cb133-6"><a href="hyperparameter-tuning.html#cb133-6" tabindex="-1"></a><span class="fu">c</span>(<span class="fu">paste</span>(<span class="st">&quot;With span = 0.02&quot;</span>, <span class="st">&quot;rmspe is &quot;</span>, rmspe0),</span>
<span id="cb133-7"><a href="hyperparameter-tuning.html#cb133-7" tabindex="-1"></a><span class="fu">paste</span>(<span class="st">&quot;With span = 0.1&quot;</span>, <span class="st">&quot;rmspe is &quot;</span>, rmspe1),</span>
<span id="cb133-8"><a href="hyperparameter-tuning.html#cb133-8" tabindex="-1"></a><span class="fu">paste</span>(<span class="st">&quot;With span = 1&quot;</span>, <span class="st">&quot;rmspe is &quot;</span>, rmspe2))</span></code></pre></div>
<pre><code>## [1] &quot;With span = 0.02 rmspe is  0.2667567209162&quot; 
## [2] &quot;With span = 0.1 rmspe is  0.247223047309939&quot;
## [3] &quot;With span = 1 rmspe is  0.309649140565879&quot;</code></pre>
<p>We are now able to see which bandwidth is better. When we set <code>span</code> = 0.1, RMSPE is the lowest. But we have several problems with this algorithm. First, we only used three arbitrary values for <code>span</code>. If we use 0.11, for example, we don’t know if its RMSPE could be better or not. Second, we only see the differences across RMSPE’s by manually comparing them. If we had tested for a large set of span values, this would have been difficult. Third, we have used only one set of validation and training sets. If we do it multiple times, we may have different results and different rankings of the models. How are we going to address these issues?</p>
<p>Let’s address this last issue, about using more than one set of training and validation sets, first:</p>
</div>
<div id="k-fold-cross-validation" class="section level2 hasAnchor" number="11.3">
<h2><span class="header-section-number">11.3</span> k-fold cross validation<a href="hyperparameter-tuning.html#k-fold-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can start here with the following figure about k-fold cross-validation. It shows 5-fold cross validation. It splits the data into k-folds, then trains the data on k-1 folds and validation on the one fold that was left out. Although, this type cross validation is the most common one, there are also several different cross validation methods, such as leave-one-out (LOOCV), leave-one-group-out, and time-series cross validation methods, which we will see later.</p>
<p><img src="png/grid3.png" width="130%" height="130%" /></p>
<p>This figure illustrates 5-k CV. We have done this with the 10-fold version for only one split and did not repeat it 10 times. The only job now is to create a loop that does the first slicing 10 times. If we repeat the same <code>loess()</code> example with 10-k cross validation, we will have 10 RMSPE’s for each <code>span</code> value. To evaluate which one is the lowest, we take the average of those 10 RSMPE’s for each model.</p>
<p>Before that, however, let’s start with a simple example of building a loop. Suppose we have a random variable <span class="math inline">\(X\)</span> and we need to calculate means of <span class="math inline">\(X\)</span> in training and validation sets to see if they are similar or not. A 10-k cross-validation example is here:</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="hyperparameter-tuning.html#cb135-1" tabindex="-1"></a><span class="co">#data</span></span>
<span id="cb135-2"><a href="hyperparameter-tuning.html#cb135-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb135-3"><a href="hyperparameter-tuning.html#cb135-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">2</span>, <span class="dv">5</span>)</span>
<span id="cb135-4"><a href="hyperparameter-tuning.html#cb135-4" tabindex="-1"></a></span>
<span id="cb135-5"><a href="hyperparameter-tuning.html#cb135-5" tabindex="-1"></a><span class="co">#Shuffle the order of observations by their index</span></span>
<span id="cb135-6"><a href="hyperparameter-tuning.html#cb135-6" tabindex="-1"></a>mysample <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb135-7"><a href="hyperparameter-tuning.html#cb135-7" tabindex="-1"></a></span>
<span id="cb135-8"><a href="hyperparameter-tuning.html#cb135-8" tabindex="-1"></a><span class="co">#Since we will have 10 means from each set</span></span>
<span id="cb135-9"><a href="hyperparameter-tuning.html#cb135-9" tabindex="-1"></a><span class="co">#we need empty &quot;containers&quot; for those values</span></span>
<span id="cb135-10"><a href="hyperparameter-tuning.html#cb135-10" tabindex="-1"></a>fold <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb135-11"><a href="hyperparameter-tuning.html#cb135-11" tabindex="-1"></a>means_validate <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb135-12"><a href="hyperparameter-tuning.html#cb135-12" tabindex="-1"></a>means_train <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb135-13"><a href="hyperparameter-tuning.html#cb135-13" tabindex="-1"></a></span>
<span id="cb135-14"><a href="hyperparameter-tuning.html#cb135-14" tabindex="-1"></a><span class="co">#Here is the loop</span></span>
<span id="cb135-15"><a href="hyperparameter-tuning.html#cb135-15" tabindex="-1"></a>nvalidate <span class="ot">&lt;-</span> <span class="fu">round</span>(n<span class="sc">/</span>fold) <span class="co">#number of observation in each set</span></span>
<span id="cb135-16"><a href="hyperparameter-tuning.html#cb135-16" tabindex="-1"></a></span>
<span id="cb135-17"><a href="hyperparameter-tuning.html#cb135-17" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>fold){</span>
<span id="cb135-18"><a href="hyperparameter-tuning.html#cb135-18" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;K-fold loop: &quot;</span>, i, <span class="st">&quot;</span><span class="sc">\r</span><span class="st">&quot;</span>) <span class="co">#This tells us which fold we are at the moment</span></span>
<span id="cb135-19"><a href="hyperparameter-tuning.html#cb135-19" tabindex="-1"></a>                                <span class="co">#No need to have it as it slows the loop speed </span></span>
<span id="cb135-20"><a href="hyperparameter-tuning.html#cb135-20" tabindex="-1"></a></span>
<span id="cb135-21"><a href="hyperparameter-tuning.html#cb135-21" tabindex="-1"></a>  <span class="cf">if</span>(i <span class="sc">&lt;</span> fold) validate <span class="ot">&lt;-</span> mysample[((i<span class="dv">-1</span>)<span class="sc">*</span>nvalidate<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(i<span class="sc">*</span>nvalidate)]</span>
<span id="cb135-22"><a href="hyperparameter-tuning.html#cb135-22" tabindex="-1"></a>        <span class="cf">else</span> validate <span class="ot">&lt;-</span> mysample[((i<span class="dv">-1</span>)<span class="sc">*</span>nvalidate<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n]</span>
<span id="cb135-23"><a href="hyperparameter-tuning.html#cb135-23" tabindex="-1"></a>  train <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, n)[<span class="sc">-</span>validate]</span>
<span id="cb135-24"><a href="hyperparameter-tuning.html#cb135-24" tabindex="-1"></a>  </span>
<span id="cb135-25"><a href="hyperparameter-tuning.html#cb135-25" tabindex="-1"></a>  X_validate <span class="ot">&lt;-</span> X[validate]</span>
<span id="cb135-26"><a href="hyperparameter-tuning.html#cb135-26" tabindex="-1"></a>  X_train <span class="ot">&lt;-</span> X[<span class="sc">-</span>validate]</span>
<span id="cb135-27"><a href="hyperparameter-tuning.html#cb135-27" tabindex="-1"></a>  </span>
<span id="cb135-28"><a href="hyperparameter-tuning.html#cb135-28" tabindex="-1"></a>  means_validate[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(X_validate)</span>
<span id="cb135-29"><a href="hyperparameter-tuning.html#cb135-29" tabindex="-1"></a>  means_train[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(X_train)</span>
<span id="cb135-30"><a href="hyperparameter-tuning.html#cb135-30" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## K-fold loop:  1 
K-fold loop:  2 
K-fold loop:  3 
K-fold loop:  4 
K-fold loop:  5 
K-fold loop:  6 
K-fold loop:  7 
K-fold loop:  8 
K-fold loop:  9 
K-fold loop:  10 </code></pre>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="hyperparameter-tuning.html#cb137-1" tabindex="-1"></a>means_validate</span></code></pre></div>
<pre><code>##  [1] 1.960626 2.078920 1.844554 2.028020 2.223252 2.090933 2.082902 1.842147
##  [9] 2.106923 2.184972</code></pre>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="hyperparameter-tuning.html#cb139-1" tabindex="-1"></a><span class="fu">mean</span>(means_validate)</span></code></pre></div>
<pre><code>## [1] 2.044325</code></pre>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="hyperparameter-tuning.html#cb141-1" tabindex="-1"></a>means_train</span></code></pre></div>
<pre><code>##  [1] 2.053625 2.040481 2.066522 2.046137 2.024444 2.039146 2.040039 2.066789
##  [9] 2.037370 2.028697</code></pre>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="hyperparameter-tuning.html#cb143-1" tabindex="-1"></a><span class="fu">mean</span>(means_train)</span></code></pre></div>
<pre><code>## [1] 2.044325</code></pre>
<p>This is impressive. The only difference between this simple example and a more complex one is some adjustments that we need to make in the type of calculations. Let’s do it with our <code>loess()</code> example:</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="hyperparameter-tuning.html#cb145-1" tabindex="-1"></a><span class="co">#Let&#39;s simulate our data</span></span>
<span id="cb145-2"><a href="hyperparameter-tuning.html#cb145-2" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10000</span></span>
<span id="cb145-3"><a href="hyperparameter-tuning.html#cb145-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb145-4"><a href="hyperparameter-tuning.html#cb145-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb145-5"><a href="hyperparameter-tuning.html#cb145-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n)<span class="sc">/</span><span class="dv">4</span></span>
<span id="cb145-6"><a href="hyperparameter-tuning.html#cb145-6" tabindex="-1"></a></span>
<span id="cb145-7"><a href="hyperparameter-tuning.html#cb145-7" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x)</span>
<span id="cb145-8"><a href="hyperparameter-tuning.html#cb145-8" tabindex="-1"></a></span>
<span id="cb145-9"><a href="hyperparameter-tuning.html#cb145-9" tabindex="-1"></a><span class="co">#Shuffle the order of observations by their index</span></span>
<span id="cb145-10"><a href="hyperparameter-tuning.html#cb145-10" tabindex="-1"></a>mysample <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb145-11"><a href="hyperparameter-tuning.html#cb145-11" tabindex="-1"></a></span>
<span id="cb145-12"><a href="hyperparameter-tuning.html#cb145-12" tabindex="-1"></a>fold <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co">#10-k CV</span></span>
<span id="cb145-13"><a href="hyperparameter-tuning.html#cb145-13" tabindex="-1"></a></span>
<span id="cb145-14"><a href="hyperparameter-tuning.html#cb145-14" tabindex="-1"></a>RMSPE <span class="ot">&lt;-</span> <span class="fu">c</span>() <span class="co"># we need an empty container to store RMSPE from validate set</span></span>
<span id="cb145-15"><a href="hyperparameter-tuning.html#cb145-15" tabindex="-1"></a></span>
<span id="cb145-16"><a href="hyperparameter-tuning.html#cb145-16" tabindex="-1"></a><span class="co">#loop</span></span>
<span id="cb145-17"><a href="hyperparameter-tuning.html#cb145-17" tabindex="-1"></a>nvalidate <span class="ot">&lt;-</span> <span class="fu">round</span>(n<span class="sc">/</span>fold)</span>
<span id="cb145-18"><a href="hyperparameter-tuning.html#cb145-18" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>fold){</span>
<span id="cb145-19"><a href="hyperparameter-tuning.html#cb145-19" tabindex="-1"></a>  <span class="cf">if</span>(i <span class="sc">&lt;</span> fold) validate <span class="ot">&lt;-</span> mysample[((i<span class="dv">-1</span>)<span class="sc">*</span>nvalidate<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(i<span class="sc">*</span>nvalidate)]</span>
<span id="cb145-20"><a href="hyperparameter-tuning.html#cb145-20" tabindex="-1"></a>        <span class="cf">else</span> validate <span class="ot">&lt;-</span> mysample[((i<span class="dv">-1</span>)<span class="sc">*</span>nvalidate<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n]</span>
<span id="cb145-21"><a href="hyperparameter-tuning.html#cb145-21" tabindex="-1"></a>  train <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, n)[<span class="sc">-</span>validate]</span>
<span id="cb145-22"><a href="hyperparameter-tuning.html#cb145-22" tabindex="-1"></a>  </span>
<span id="cb145-23"><a href="hyperparameter-tuning.html#cb145-23" tabindex="-1"></a>  data_validate <span class="ot">&lt;-</span> data[validate,]</span>
<span id="cb145-24"><a href="hyperparameter-tuning.html#cb145-24" tabindex="-1"></a>  data_train <span class="ot">&lt;-</span> data[<span class="sc">-</span>validate,]</span>
<span id="cb145-25"><a href="hyperparameter-tuning.html#cb145-25" tabindex="-1"></a>  </span>
<span id="cb145-26"><a href="hyperparameter-tuning.html#cb145-26" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">loess</span>(y <span class="sc">~</span> x, <span class="at">control=</span><span class="fu">loess.control</span>(<span class="at">surface=</span><span class="st">&quot;direct&quot;</span>), </span>
<span id="cb145-27"><a href="hyperparameter-tuning.html#cb145-27" tabindex="-1"></a>                       <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="fl">0.1</span>, <span class="at">data =</span> data_train)</span>
<span id="cb145-28"><a href="hyperparameter-tuning.html#cb145-28" tabindex="-1"></a>  <span class="co">#loess.control() is used for the adjustment</span></span>
<span id="cb145-29"><a href="hyperparameter-tuning.html#cb145-29" tabindex="-1"></a>  <span class="co">#for x values that are outside of x values used in training</span></span>
<span id="cb145-30"><a href="hyperparameter-tuning.html#cb145-30" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, data_validate<span class="sc">$</span>x)</span>
<span id="cb145-31"><a href="hyperparameter-tuning.html#cb145-31" tabindex="-1"></a>  RMSPE[i] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((data_validate<span class="sc">$</span>y<span class="sc">-</span>fit)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb145-32"><a href="hyperparameter-tuning.html#cb145-32" tabindex="-1"></a>}</span>
<span id="cb145-33"><a href="hyperparameter-tuning.html#cb145-33" tabindex="-1"></a></span>
<span id="cb145-34"><a href="hyperparameter-tuning.html#cb145-34" tabindex="-1"></a>RMSPE</span></code></pre></div>
<pre><code>##  [1] 0.2427814 0.2469909 0.2387873 0.2472059 0.2489808 0.2510570 0.2553914
##  [8] 0.2517241 0.2521053 0.2429688</code></pre>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="hyperparameter-tuning.html#cb147-1" tabindex="-1"></a><span class="fu">mean</span>(RMSPE)</span></code></pre></div>
<pre><code>## [1] 0.2477993</code></pre>
<p>How can we use this k-fold cross validation in tuning the parameters by 3 possible values of <code>span</code> = 0.02, 0.1, and 1:</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="hyperparameter-tuning.html#cb149-1" tabindex="-1"></a><span class="co">#Using the same data</span></span>
<span id="cb149-2"><a href="hyperparameter-tuning.html#cb149-2" tabindex="-1"></a>mysample <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb149-3"><a href="hyperparameter-tuning.html#cb149-3" tabindex="-1"></a>fold <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co">#10-k CV</span></span>
<span id="cb149-4"><a href="hyperparameter-tuning.html#cb149-4" tabindex="-1"></a></span>
<span id="cb149-5"><a href="hyperparameter-tuning.html#cb149-5" tabindex="-1"></a><span class="co">#Possible values for span, GRID!</span></span>
<span id="cb149-6"><a href="hyperparameter-tuning.html#cb149-6" tabindex="-1"></a>span <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.02</span>, <span class="fl">0.1</span>, <span class="dv">1</span>)</span>
<span id="cb149-7"><a href="hyperparameter-tuning.html#cb149-7" tabindex="-1"></a></span>
<span id="cb149-8"><a href="hyperparameter-tuning.html#cb149-8" tabindex="-1"></a><span class="co">#We need a container to store RMSPE from validate set</span></span>
<span id="cb149-9"><a href="hyperparameter-tuning.html#cb149-9" tabindex="-1"></a><span class="co">#but we have to have a matrix now with 3 columns for each paramater in span</span></span>
<span id="cb149-10"><a href="hyperparameter-tuning.html#cb149-10" tabindex="-1"></a>RMSPE <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> fold, <span class="at">ncol =</span> <span class="fu">length</span>(span)) </span>
<span id="cb149-11"><a href="hyperparameter-tuning.html#cb149-11" tabindex="-1"></a></span>
<span id="cb149-12"><a href="hyperparameter-tuning.html#cb149-12" tabindex="-1"></a><span class="co">#Loop</span></span>
<span id="cb149-13"><a href="hyperparameter-tuning.html#cb149-13" tabindex="-1"></a>nvalidate <span class="ot">&lt;-</span> <span class="fu">round</span>(n<span class="sc">/</span>fold)</span>
<span id="cb149-14"><a href="hyperparameter-tuning.html#cb149-14" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>fold){</span>
<span id="cb149-15"><a href="hyperparameter-tuning.html#cb149-15" tabindex="-1"></a>  <span class="cf">if</span>(i <span class="sc">&lt;</span> fold) validate <span class="ot">&lt;-</span> mysample[((i<span class="dv">-1</span>)<span class="sc">*</span>nvalidate<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(i<span class="sc">*</span>nvalidate)]</span>
<span id="cb149-16"><a href="hyperparameter-tuning.html#cb149-16" tabindex="-1"></a>        <span class="cf">else</span> validate <span class="ot">&lt;-</span> mysample[((i<span class="dv">-1</span>)<span class="sc">*</span>nvalidate<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n]</span>
<span id="cb149-17"><a href="hyperparameter-tuning.html#cb149-17" tabindex="-1"></a>  train <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, n)[<span class="sc">-</span>validate]</span>
<span id="cb149-18"><a href="hyperparameter-tuning.html#cb149-18" tabindex="-1"></a>  </span>
<span id="cb149-19"><a href="hyperparameter-tuning.html#cb149-19" tabindex="-1"></a>  data_validate <span class="ot">&lt;-</span> data[validate,]</span>
<span id="cb149-20"><a href="hyperparameter-tuning.html#cb149-20" tabindex="-1"></a>  data_train <span class="ot">&lt;-</span> data[<span class="sc">-</span>validate,]</span>
<span id="cb149-21"><a href="hyperparameter-tuning.html#cb149-21" tabindex="-1"></a>  </span>
<span id="cb149-22"><a href="hyperparameter-tuning.html#cb149-22" tabindex="-1"></a>  <span class="co">#We need another loop running each value in span:</span></span>
<span id="cb149-23"><a href="hyperparameter-tuning.html#cb149-23" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(span)) {</span>
<span id="cb149-24"><a href="hyperparameter-tuning.html#cb149-24" tabindex="-1"></a>    <span class="co">#cat(&quot;K-fold loop: &quot;, i, j,  &quot;\r&quot;) </span></span>
<span id="cb149-25"><a href="hyperparameter-tuning.html#cb149-25" tabindex="-1"></a>    model <span class="ot">&lt;-</span> <span class="fu">loess</span>(y <span class="sc">~</span> x, <span class="at">control=</span><span class="fu">loess.control</span>(<span class="at">surface=</span><span class="st">&quot;direct&quot;</span>), </span>
<span id="cb149-26"><a href="hyperparameter-tuning.html#cb149-26" tabindex="-1"></a>                       <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> span[j], <span class="at">data =</span> data_train)</span>
<span id="cb149-27"><a href="hyperparameter-tuning.html#cb149-27" tabindex="-1"></a>  </span>
<span id="cb149-28"><a href="hyperparameter-tuning.html#cb149-28" tabindex="-1"></a>    fit <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, data_validate<span class="sc">$</span>x)</span>
<span id="cb149-29"><a href="hyperparameter-tuning.html#cb149-29" tabindex="-1"></a>    RMSPE[i,j] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((data_validate<span class="sc">$</span>y<span class="sc">-</span>fit)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb149-30"><a href="hyperparameter-tuning.html#cb149-30" tabindex="-1"></a>  }</span>
<span id="cb149-31"><a href="hyperparameter-tuning.html#cb149-31" tabindex="-1"></a>}</span>
<span id="cb149-32"><a href="hyperparameter-tuning.html#cb149-32" tabindex="-1"></a>RMSPE</span></code></pre></div>
<pre><code>##            [,1]      [,2]      [,3]
##  [1,] 0.2553526 0.2537322 0.3221018
##  [2,] 0.2498332 0.2490425 0.3136585
##  [3,] 0.2457066 0.2438678 0.3156779
##  [4,] 0.2537020 0.2515660 0.3124925
##  [5,] 0.2541144 0.2506757 0.3135304
##  [6,] 0.2462051 0.2438164 0.3139374
##  [7,] 0.2589021 0.2571637 0.3147272
##  [8,] 0.2453394 0.2435748 0.3145081
##  [9,] 0.2479946 0.2461617 0.3002658
## [10,] 0.2399564 0.2393222 0.3006061</code></pre>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="hyperparameter-tuning.html#cb151-1" tabindex="-1"></a><span class="co">#Here are the values you compare to choose an optimal span</span></span>
<span id="cb151-2"><a href="hyperparameter-tuning.html#cb151-2" tabindex="-1"></a><span class="fu">colMeans</span>(RMSPE)</span></code></pre></div>
<pre><code>## [1] 0.2497106 0.2478923 0.3121506</code></pre>
</div>
<div id="grid-search" class="section level2 hasAnchor" number="11.4">
<h2><span class="header-section-number">11.4</span> Grid Search<a href="hyperparameter-tuning.html#grid-search" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The traditional way of performing hyperparameter optimization has been <em>grid search</em>, or a <em>parameter sweep</em>, which is simply an <strong>exhaustive searching</strong> through a manually specified subset of the hyperparameter space of a learning algorithm. This is how <em>grid search</em> is defined by Wikipedia.</p>
<p>Although we did not call it as grid search, we have already done it in the last example. It was a simple search for optimal <code>span</code> with three arbitrary numbers, 0.02, 0.1 and 1. Besides, we didn’t search for <code>degree</code>, which is another hyperparamater in <code>loess()</code>.</p>
<p>Hence the first job is to set the hyperparameter grid. In each hyperparameter, we need to know the maximum and the minimum values of this subset. For example, <code>span</code> in <code>loess()</code> sets the size of the neighborhood, which ranges between 0 to 1. This controls the degree of smoothing. So, the greater the value of span, smoother the fitted curve is. For example, if <span class="math inline">\(n\)</span> is the number of data points and <code>span</code> = 0.5, then for a given <span class="math inline">\(X\)</span>, loess will use the 0.5 * <span class="math inline">\(n\)</span> closest points to <span class="math inline">\(X\)</span> for the fit.</p>
<p>Additionally the <code>degree</code> argument in <code>loess()</code> is defined as <em>the degree of the polynomials to be used, normally 1 or 2. (Degree 0 is also allowed, but see the ‘Note’. in <code>?loess</code>)</em>. For each learning algorithm, the number of tuning parameters and their ranges will be different. Before running any grid search, therefore, we need to understand their function and range.</p>
<p>Let’s use our example again <strong>without</strong> cross validation first:</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="hyperparameter-tuning.html#cb153-1" tabindex="-1"></a><span class="co">#Using the same data with reduced size </span></span>
<span id="cb153-2"><a href="hyperparameter-tuning.html#cb153-2" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb153-3"><a href="hyperparameter-tuning.html#cb153-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb153-4"><a href="hyperparameter-tuning.html#cb153-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb153-5"><a href="hyperparameter-tuning.html#cb153-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n)<span class="sc">/</span><span class="dv">4</span></span>
<span id="cb153-6"><a href="hyperparameter-tuning.html#cb153-6" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x)</span>
<span id="cb153-7"><a href="hyperparameter-tuning.html#cb153-7" tabindex="-1"></a></span>
<span id="cb153-8"><a href="hyperparameter-tuning.html#cb153-8" tabindex="-1"></a><span class="co">#Creating a set of possible values for span: GRID</span></span>
<span id="cb153-9"><a href="hyperparameter-tuning.html#cb153-9" tabindex="-1"></a>span <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="fl">0.01</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.02</span>) <span class="co">#this creates 50 options</span></span>
<span id="cb153-10"><a href="hyperparameter-tuning.html#cb153-10" tabindex="-1"></a>degree <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb153-11"><a href="hyperparameter-tuning.html#cb153-11" tabindex="-1"></a></span>
<span id="cb153-12"><a href="hyperparameter-tuning.html#cb153-12" tabindex="-1"></a><span class="co">#We need a (span x degree) matrix to store RMSPE </span></span>
<span id="cb153-13"><a href="hyperparameter-tuning.html#cb153-13" tabindex="-1"></a>RMSPE <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">length</span>(span), <span class="at">ncol =</span> <span class="fu">length</span>(degree)) </span>
<span id="cb153-14"><a href="hyperparameter-tuning.html#cb153-14" tabindex="-1"></a></span>
<span id="cb153-15"><a href="hyperparameter-tuning.html#cb153-15" tabindex="-1"></a><span class="co">#loop for grid search over span</span></span>
<span id="cb153-16"><a href="hyperparameter-tuning.html#cb153-16" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(span)){</span>
<span id="cb153-17"><a href="hyperparameter-tuning.html#cb153-17" tabindex="-1"></a>  </span>
<span id="cb153-18"><a href="hyperparameter-tuning.html#cb153-18" tabindex="-1"></a>  <span class="co">#loop for grid search for degree</span></span>
<span id="cb153-19"><a href="hyperparameter-tuning.html#cb153-19" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(degree)) {</span>
<span id="cb153-20"><a href="hyperparameter-tuning.html#cb153-20" tabindex="-1"></a>  <span class="co">#cat(&quot;Loops: &quot;, i, j,  &quot;\r&quot;)   </span></span>
<span id="cb153-21"><a href="hyperparameter-tuning.html#cb153-21" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">loess</span>(y <span class="sc">~</span> x, <span class="at">control=</span><span class="fu">loess.control</span>(<span class="at">surface=</span><span class="st">&quot;direct&quot;</span>), </span>
<span id="cb153-22"><a href="hyperparameter-tuning.html#cb153-22" tabindex="-1"></a>                       <span class="at">degree=</span>degree[j], <span class="at">span =</span> span[i], <span class="at">data =</span> data)</span>
<span id="cb153-23"><a href="hyperparameter-tuning.html#cb153-23" tabindex="-1"></a>  </span>
<span id="cb153-24"><a href="hyperparameter-tuning.html#cb153-24" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, data<span class="sc">$</span>x)</span>
<span id="cb153-25"><a href="hyperparameter-tuning.html#cb153-25" tabindex="-1"></a>  RMSPE[i,j] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((data<span class="sc">$</span>y<span class="sc">-</span>fit)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb153-26"><a href="hyperparameter-tuning.html#cb153-26" tabindex="-1"></a>  }</span>
<span id="cb153-27"><a href="hyperparameter-tuning.html#cb153-27" tabindex="-1"></a>}</span></code></pre></div>
<p>The RMSPE matrix is a <span class="math inline">\(50 \times 2\)</span> matrix. Each row gives us RMSPE calculated for each <code>span</code> parameter for 2 different <code>degree</code> parameters. Note that this is not a <em>correct</em> RSMPE as it uses only one sample and calculates <em>in-sample</em> RMSPE. Moreover, it is not efficient way to search a grid as it uses double loops. We will improve it step by step.</p>
<p>Now our job is to find the smallest RMSPE in the matrix.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="hyperparameter-tuning.html#cb154-1" tabindex="-1"></a><span class="fu">head</span>(RMSPE)</span></code></pre></div>
<pre><code>##           [,1]      [,2]
## [1,] 0.2241721 0.2019041
## [2,] 0.2466408 0.2413781
## [3,] 0.2512451 0.2465098
## [4,] 0.2538281 0.2496574
## [5,] 0.2551257 0.2523178
## [6,] 0.2557593 0.2534789</code></pre>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="hyperparameter-tuning.html#cb156-1" tabindex="-1"></a><span class="co">#Here are the values you compare for an optimal degree</span></span>
<span id="cb156-2"><a href="hyperparameter-tuning.html#cb156-2" tabindex="-1"></a><span class="fu">which</span>(RMSPE <span class="sc">==</span> <span class="fu">min</span>(RMSPE), <span class="at">arr.ind =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>##      row col
## [1,]   1   2</code></pre>
<p>We will the same method in tuning our model, <code>loess()</code>, with cross validation</p>
</div>
<div id="cross-validated-grid-search" class="section level2 hasAnchor" number="11.5">
<h2><span class="header-section-number">11.5</span> Cross-validated grid search<a href="hyperparameter-tuning.html#cross-validated-grid-search" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>All we need to do is to put our grid search algorithm inside the cross-validation loop. We will improve two things: we will have a “better” grid so that the number of loops will be fewer. Here is an example:</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="hyperparameter-tuning.html#cb158-1" tabindex="-1"></a><span class="co">#Creating a set of possible values for span: GRID</span></span>
<span id="cb158-2"><a href="hyperparameter-tuning.html#cb158-2" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="fu">seq</span>(<span class="at">from =</span> <span class="fl">0.01</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.02</span>), <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb158-3"><a href="hyperparameter-tuning.html#cb158-3" tabindex="-1"></a><span class="fu">head</span>(grid)</span></code></pre></div>
<pre><code>##   Var1 Var2
## 1 0.01    1
## 2 0.03    1
## 3 0.05    1
## 4 0.07    1
## 5 0.09    1
## 6 0.11    1</code></pre>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="hyperparameter-tuning.html#cb160-1" tabindex="-1"></a><span class="co">#or</span></span>
<span id="cb160-2"><a href="hyperparameter-tuning.html#cb160-2" tabindex="-1"></a>span <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="fl">0.01</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.02</span>)</span>
<span id="cb160-3"><a href="hyperparameter-tuning.html#cb160-3" tabindex="-1"></a>degree <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,<span class="fu">length</span>(span)), <span class="fu">rep</span>(<span class="dv">2</span>,<span class="fu">length</span>(span)))</span>
<span id="cb160-4"><a href="hyperparameter-tuning.html#cb160-4" tabindex="-1"></a>grid_mine <span class="ot">&lt;-</span> <span class="fu">cbind</span>(span, degree)</span>
<span id="cb160-5"><a href="hyperparameter-tuning.html#cb160-5" tabindex="-1"></a><span class="fu">head</span>(grid)</span></code></pre></div>
<pre><code>##   Var1 Var2
## 1 0.01    1
## 2 0.03    1
## 3 0.05    1
## 4 0.07    1
## 5 0.09    1
## 6 0.11    1</code></pre>
<p>Now we need to go through each row of <code>grid</code> that contains two tuning parameters in each CV: the first column is <code>span</code> and the second column is <code>degree</code>.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="hyperparameter-tuning.html#cb162-1" tabindex="-1"></a><span class="co">#Using the same data with reduced size </span></span>
<span id="cb162-2"><a href="hyperparameter-tuning.html#cb162-2" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb162-3"><a href="hyperparameter-tuning.html#cb162-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb162-4"><a href="hyperparameter-tuning.html#cb162-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb162-5"><a href="hyperparameter-tuning.html#cb162-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n)<span class="sc">/</span><span class="dv">4</span></span>
<span id="cb162-6"><a href="hyperparameter-tuning.html#cb162-6" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x)</span>
<span id="cb162-7"><a href="hyperparameter-tuning.html#cb162-7" tabindex="-1"></a></span>
<span id="cb162-8"><a href="hyperparameter-tuning.html#cb162-8" tabindex="-1"></a><span class="co">#Setting CV</span></span>
<span id="cb162-9"><a href="hyperparameter-tuning.html#cb162-9" tabindex="-1"></a>mysample <span class="ot">&lt;-</span> <span class="fu">sample</span>(n, n, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb162-10"><a href="hyperparameter-tuning.html#cb162-10" tabindex="-1"></a>fold <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="co">#10-fold CV</span></span>
<span id="cb162-11"><a href="hyperparameter-tuning.html#cb162-11" tabindex="-1"></a></span>
<span id="cb162-12"><a href="hyperparameter-tuning.html#cb162-12" tabindex="-1"></a><span class="co">#Since we will do the same tuning 10 times</span></span>
<span id="cb162-13"><a href="hyperparameter-tuning.html#cb162-13" tabindex="-1"></a><span class="co">#we need to have a container that stores </span></span>
<span id="cb162-14"><a href="hyperparameter-tuning.html#cb162-14" tabindex="-1"></a><span class="co">#10 optimal sets of span and degree values</span></span>
<span id="cb162-15"><a href="hyperparameter-tuning.html#cb162-15" tabindex="-1"></a>OPT <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb162-16"><a href="hyperparameter-tuning.html#cb162-16" tabindex="-1"></a></span>
<span id="cb162-17"><a href="hyperparameter-tuning.html#cb162-17" tabindex="-1"></a><span class="co">#loop</span></span>
<span id="cb162-18"><a href="hyperparameter-tuning.html#cb162-18" tabindex="-1"></a>nvalidate <span class="ot">&lt;-</span> <span class="fu">round</span>(n<span class="sc">/</span>fold)</span>
<span id="cb162-19"><a href="hyperparameter-tuning.html#cb162-19" tabindex="-1"></a></span>
<span id="cb162-20"><a href="hyperparameter-tuning.html#cb162-20" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>fold){</span>
<span id="cb162-21"><a href="hyperparameter-tuning.html#cb162-21" tabindex="-1"></a>  <span class="co">#cat(&quot;K-fold loop: &quot;, i, &quot;\r&quot;) </span></span>
<span id="cb162-22"><a href="hyperparameter-tuning.html#cb162-22" tabindex="-1"></a>  </span>
<span id="cb162-23"><a href="hyperparameter-tuning.html#cb162-23" tabindex="-1"></a>  <span class="cf">if</span>(i <span class="sc">&lt;</span> fold) validate <span class="ot">&lt;-</span> mysample[((i<span class="dv">-1</span>)<span class="sc">*</span>nvalidate<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(i<span class="sc">*</span>nvalidate)]</span>
<span id="cb162-24"><a href="hyperparameter-tuning.html#cb162-24" tabindex="-1"></a>        <span class="cf">else</span> validate <span class="ot">&lt;-</span> mysample[((i<span class="dv">-1</span>)<span class="sc">*</span>nvalidate<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>n]</span>
<span id="cb162-25"><a href="hyperparameter-tuning.html#cb162-25" tabindex="-1"></a>  train <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, n)[<span class="sc">-</span>validate]</span>
<span id="cb162-26"><a href="hyperparameter-tuning.html#cb162-26" tabindex="-1"></a>  </span>
<span id="cb162-27"><a href="hyperparameter-tuning.html#cb162-27" tabindex="-1"></a>  data_validate <span class="ot">&lt;-</span> data[validate,]</span>
<span id="cb162-28"><a href="hyperparameter-tuning.html#cb162-28" tabindex="-1"></a>  data_train <span class="ot">&lt;-</span> data[<span class="sc">-</span>validate,]</span>
<span id="cb162-29"><a href="hyperparameter-tuning.html#cb162-29" tabindex="-1"></a>  </span>
<span id="cb162-30"><a href="hyperparameter-tuning.html#cb162-30" tabindex="-1"></a>  <span class="co">#we need a vector to store RMSPE of each row in the grid </span></span>
<span id="cb162-31"><a href="hyperparameter-tuning.html#cb162-31" tabindex="-1"></a>  RMSPE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb162-32"><a href="hyperparameter-tuning.html#cb162-32" tabindex="-1"></a>  </span>
<span id="cb162-33"><a href="hyperparameter-tuning.html#cb162-33" tabindex="-1"></a>  <span class="co">#we need to have another loop running each row in grid:</span></span>
<span id="cb162-34"><a href="hyperparameter-tuning.html#cb162-34" tabindex="-1"></a>    <span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(grid)){</span>
<span id="cb162-35"><a href="hyperparameter-tuning.html#cb162-35" tabindex="-1"></a>      model <span class="ot">&lt;-</span> <span class="fu">loess</span>(y <span class="sc">~</span> x, <span class="at">control=</span><span class="fu">loess.control</span>(<span class="at">surface=</span><span class="st">&quot;direct&quot;</span>), </span>
<span id="cb162-36"><a href="hyperparameter-tuning.html#cb162-36" tabindex="-1"></a>      <span class="at">degree=</span>grid[s,<span class="dv">2</span>], <span class="at">span =</span> grid[s,<span class="dv">1</span>], <span class="at">data =</span> data_train)</span>
<span id="cb162-37"><a href="hyperparameter-tuning.html#cb162-37" tabindex="-1"></a>   </span>
<span id="cb162-38"><a href="hyperparameter-tuning.html#cb162-38" tabindex="-1"></a>      fit <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, data_validate<span class="sc">$</span>x)</span>
<span id="cb162-39"><a href="hyperparameter-tuning.html#cb162-39" tabindex="-1"></a>      RMSPE[s] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((data_validate<span class="sc">$</span>y<span class="sc">-</span>fit)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb162-40"><a href="hyperparameter-tuning.html#cb162-40" tabindex="-1"></a>    }</span>
<span id="cb162-41"><a href="hyperparameter-tuning.html#cb162-41" tabindex="-1"></a>  OPT[i] <span class="ot">&lt;-</span> <span class="fu">which</span>(RMSPE <span class="sc">==</span> <span class="fu">min</span>(RMSPE), <span class="at">arr.ind =</span> <span class="cn">TRUE</span>)</span>
<span id="cb162-42"><a href="hyperparameter-tuning.html#cb162-42" tabindex="-1"></a>}</span>
<span id="cb162-43"><a href="hyperparameter-tuning.html#cb162-43" tabindex="-1"></a>opgrid <span class="ot">&lt;-</span> grid[OPT,]</span>
<span id="cb162-44"><a href="hyperparameter-tuning.html#cb162-44" tabindex="-1"></a><span class="fu">colnames</span>(opgrid) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;span&quot;</span>, <span class="st">&quot;degree&quot;</span>)</span>
<span id="cb162-45"><a href="hyperparameter-tuning.html#cb162-45" tabindex="-1"></a><span class="fu">rownames</span>(opgrid) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span>
<span id="cb162-46"><a href="hyperparameter-tuning.html#cb162-46" tabindex="-1"></a>opgrid</span></code></pre></div>
<pre><code>##    span degree
## 1  0.09      2
## 2  0.63      2
## 3  0.23      2
## 4  0.03      2
## 5  0.33      2
## 6  0.59      2
## 7  0.75      2
## 8  0.57      2
## 9  0.25      1
## 10 0.21      1</code></pre>
<p>These results are good but how are we going to pick one set, the coordinates of the optimal <code>span</code> and <code>degree</code>? It seems that most folds agree that we should use <code>degree</code> = 2, but which <code>span</code> value is the optimal? If the hyperparameter is a discrete value, we can use majority rule with the modal value, which is just the highest number of occurrences in the set. This would be appropriate for <code>degree</code> but not for <code>span</code>. Instead, we should use the mean of all 10 optimal <code>span</code> values, each of which is calculated from each fold.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="hyperparameter-tuning.html#cb164-1" tabindex="-1"></a><span class="fu">library</span>(raster)</span>
<span id="cb164-2"><a href="hyperparameter-tuning.html#cb164-2" tabindex="-1"></a>opt_degree <span class="ot">&lt;-</span> <span class="fu">modal</span>(opgrid[,<span class="dv">2</span>])</span>
<span id="cb164-3"><a href="hyperparameter-tuning.html#cb164-3" tabindex="-1"></a>opt_span <span class="ot">&lt;-</span> <span class="fu">mean</span>(opgrid[,<span class="dv">1</span>])</span>
<span id="cb164-4"><a href="hyperparameter-tuning.html#cb164-4" tabindex="-1"></a></span>
<span id="cb164-5"><a href="hyperparameter-tuning.html#cb164-5" tabindex="-1"></a>opt_degree</span></code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="hyperparameter-tuning.html#cb166-1" tabindex="-1"></a>opt_span</span></code></pre></div>
<pre><code>## [1] 0.368</code></pre>
<p>Now, the last job is to use them in predictions to see how good they are. Remember, we used the whole sample to tune our hyperparameters. At the outset, we said that this type of application should be avoided. Therefore, we need to create a test set at the beginning and put that subset a side, and use only the remaining data to tune our hyperparameters. Here is an illustration about this process:</p>
<p><img src="png/grid.png" width="140%" height="140%" /></p>
<p>Let’s do it:</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="hyperparameter-tuning.html#cb168-1" tabindex="-1"></a><span class="co"># Using the same data</span></span>
<span id="cb168-2"><a href="hyperparameter-tuning.html#cb168-2" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb168-3"><a href="hyperparameter-tuning.html#cb168-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb168-4"><a href="hyperparameter-tuning.html#cb168-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb168-5"><a href="hyperparameter-tuning.html#cb168-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n)<span class="sc">/</span><span class="dv">4</span></span>
<span id="cb168-6"><a href="hyperparameter-tuning.html#cb168-6" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x)</span>
<span id="cb168-7"><a href="hyperparameter-tuning.html#cb168-7" tabindex="-1"></a></span>
<span id="cb168-8"><a href="hyperparameter-tuning.html#cb168-8" tabindex="-1"></a><span class="co"># Creating a set of possible values for span: GRID</span></span>
<span id="cb168-9"><a href="hyperparameter-tuning.html#cb168-9" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="fu">seq</span>(<span class="at">from =</span> <span class="fl">0.01</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.02</span>), <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb168-10"><a href="hyperparameter-tuning.html#cb168-10" tabindex="-1"></a></span>
<span id="cb168-11"><a href="hyperparameter-tuning.html#cb168-11" tabindex="-1"></a><span class="co"># Train - Test Split</span></span>
<span id="cb168-12"><a href="hyperparameter-tuning.html#cb168-12" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">321</span>)</span>
<span id="cb168-13"><a href="hyperparameter-tuning.html#cb168-13" tabindex="-1"></a>shuf <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data), <span class="fu">nrow</span>(data), <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb168-14"><a href="hyperparameter-tuning.html#cb168-14" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">10</span> </span>
<span id="cb168-15"><a href="hyperparameter-tuning.html#cb168-15" tabindex="-1"></a>indx <span class="ot">&lt;-</span> shuf[<span class="dv">1</span><span class="sc">:</span>(<span class="fu">nrow</span>(data)<span class="sc">/</span>k)]</span>
<span id="cb168-16"><a href="hyperparameter-tuning.html#cb168-16" tabindex="-1"></a>testset <span class="ot">&lt;-</span> data[indx, ] <span class="co">#10% of data set a side</span></span>
<span id="cb168-17"><a href="hyperparameter-tuning.html#cb168-17" tabindex="-1"></a>trainset <span class="ot">&lt;-</span> data[<span class="sc">-</span>indx,] </span>
<span id="cb168-18"><a href="hyperparameter-tuning.html#cb168-18" tabindex="-1"></a></span>
<span id="cb168-19"><a href="hyperparameter-tuning.html#cb168-19" tabindex="-1"></a><span class="co"># k-CV, which is the same as before</span></span>
<span id="cb168-20"><a href="hyperparameter-tuning.html#cb168-20" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb168-21"><a href="hyperparameter-tuning.html#cb168-21" tabindex="-1"></a>mysample <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(trainset), <span class="fu">nrow</span>(trainset), <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb168-22"><a href="hyperparameter-tuning.html#cb168-22" tabindex="-1"></a>fold <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb168-23"><a href="hyperparameter-tuning.html#cb168-23" tabindex="-1"></a></span>
<span id="cb168-24"><a href="hyperparameter-tuning.html#cb168-24" tabindex="-1"></a>OPT <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb168-25"><a href="hyperparameter-tuning.html#cb168-25" tabindex="-1"></a></span>
<span id="cb168-26"><a href="hyperparameter-tuning.html#cb168-26" tabindex="-1"></a><span class="co"># CV loop</span></span>
<span id="cb168-27"><a href="hyperparameter-tuning.html#cb168-27" tabindex="-1"></a>nvalid <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">nrow</span>(trainset)<span class="sc">/</span>fold)</span>
<span id="cb168-28"><a href="hyperparameter-tuning.html#cb168-28" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>fold){</span>
<span id="cb168-29"><a href="hyperparameter-tuning.html#cb168-29" tabindex="-1"></a>  <span class="cf">if</span>(i <span class="sc">&lt;</span> fold) valid <span class="ot">&lt;-</span> mysample[((i<span class="dv">-1</span>)<span class="sc">*</span>nvalid<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(i<span class="sc">*</span>nvalid)] <span class="co"># Simpler version</span></span>
<span id="cb168-30"><a href="hyperparameter-tuning.html#cb168-30" tabindex="-1"></a>        </span>
<span id="cb168-31"><a href="hyperparameter-tuning.html#cb168-31" tabindex="-1"></a>  data_valid <span class="ot">&lt;-</span> trainset[valid, ]</span>
<span id="cb168-32"><a href="hyperparameter-tuning.html#cb168-32" tabindex="-1"></a>  data_train <span class="ot">&lt;-</span> trainset[<span class="sc">-</span>valid, ]</span>
<span id="cb168-33"><a href="hyperparameter-tuning.html#cb168-33" tabindex="-1"></a>  </span>
<span id="cb168-34"><a href="hyperparameter-tuning.html#cb168-34" tabindex="-1"></a>  RMSPE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb168-35"><a href="hyperparameter-tuning.html#cb168-35" tabindex="-1"></a>  </span>
<span id="cb168-36"><a href="hyperparameter-tuning.html#cb168-36" tabindex="-1"></a>  <span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(grid)){</span>
<span id="cb168-37"><a href="hyperparameter-tuning.html#cb168-37" tabindex="-1"></a>      model <span class="ot">&lt;-</span> <span class="fu">loess</span>(y <span class="sc">~</span> x, <span class="at">control=</span><span class="fu">loess.control</span>(<span class="at">surface=</span><span class="st">&quot;direct&quot;</span>), </span>
<span id="cb168-38"><a href="hyperparameter-tuning.html#cb168-38" tabindex="-1"></a>      <span class="at">degree=</span>grid[s,<span class="dv">2</span>], <span class="at">span =</span> grid[s,<span class="dv">1</span>], <span class="at">data =</span> data_train)</span>
<span id="cb168-39"><a href="hyperparameter-tuning.html#cb168-39" tabindex="-1"></a>   </span>
<span id="cb168-40"><a href="hyperparameter-tuning.html#cb168-40" tabindex="-1"></a>      fit <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, data_valid<span class="sc">$</span>x)</span>
<span id="cb168-41"><a href="hyperparameter-tuning.html#cb168-41" tabindex="-1"></a>      RMSPE[s] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((data_valid<span class="sc">$</span>y<span class="sc">-</span>fit)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb168-42"><a href="hyperparameter-tuning.html#cb168-42" tabindex="-1"></a>    }</span>
<span id="cb168-43"><a href="hyperparameter-tuning.html#cb168-43" tabindex="-1"></a>  OPT[i] <span class="ot">&lt;-</span> <span class="fu">which</span>(RMSPE <span class="sc">==</span> <span class="fu">min</span>(RMSPE))</span>
<span id="cb168-44"><a href="hyperparameter-tuning.html#cb168-44" tabindex="-1"></a>}</span>
<span id="cb168-45"><a href="hyperparameter-tuning.html#cb168-45" tabindex="-1"></a></span>
<span id="cb168-46"><a href="hyperparameter-tuning.html#cb168-46" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb168-47"><a href="hyperparameter-tuning.html#cb168-47" tabindex="-1"></a>opgrid <span class="ot">&lt;-</span> grid[OPT,]</span>
<span id="cb168-48"><a href="hyperparameter-tuning.html#cb168-48" tabindex="-1"></a><span class="fu">colnames</span>(opgrid) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;span&quot;</span>, <span class="st">&quot;degree&quot;</span>)</span>
<span id="cb168-49"><a href="hyperparameter-tuning.html#cb168-49" tabindex="-1"></a><span class="fu">rownames</span>(opgrid) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span>
<span id="cb168-50"><a href="hyperparameter-tuning.html#cb168-50" tabindex="-1"></a>opt_degree <span class="ot">&lt;-</span> <span class="fu">modal</span>(opgrid[,<span class="dv">2</span>])</span>
<span id="cb168-51"><a href="hyperparameter-tuning.html#cb168-51" tabindex="-1"></a>opt_degree</span></code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="hyperparameter-tuning.html#cb170-1" tabindex="-1"></a>opt_span <span class="ot">&lt;-</span> <span class="fu">mean</span>(opgrid[,<span class="dv">1</span>])</span>
<span id="cb170-2"><a href="hyperparameter-tuning.html#cb170-2" tabindex="-1"></a>opt_span</span></code></pre></div>
<pre><code>## [1] 0.23</code></pre>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="hyperparameter-tuning.html#cb172-1" tabindex="-1"></a><span class="co"># **** Using the test set for final evaluation ******</span></span>
<span id="cb172-2"><a href="hyperparameter-tuning.html#cb172-2" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">loess</span>(y <span class="sc">~</span> x, <span class="at">control=</span><span class="fu">loess.control</span>(<span class="at">surface=</span><span class="st">&quot;direct&quot;</span>), </span>
<span id="cb172-3"><a href="hyperparameter-tuning.html#cb172-3" tabindex="-1"></a>  <span class="at">degree=</span>opt_degree, <span class="at">span =</span> opt_span, <span class="at">data =</span> trainset)</span>
<span id="cb172-4"><a href="hyperparameter-tuning.html#cb172-4" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, testset<span class="sc">$</span>x)</span>
<span id="cb172-5"><a href="hyperparameter-tuning.html#cb172-5" tabindex="-1"></a>RMSPE_test <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((testset<span class="sc">$</span>y<span class="sc">-</span>fit)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb172-6"><a href="hyperparameter-tuning.html#cb172-6" tabindex="-1"></a>RMSPE_test</span></code></pre></div>
<pre><code>## [1] 0.2527365</code></pre>
<p>What we have built is an algorithm that <strong>learns</strong> by trial-and-error. However, we need one more step to finalize this process: instead of doing only one 90%-10% train split, we need to do it multiple times and use the average <code>RMSPE_test</code> and the uncertainty (its variation) associated with it as our final performance metrics. Here again:</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="hyperparameter-tuning.html#cb174-1" tabindex="-1"></a><span class="co">#Using the same data</span></span>
<span id="cb174-2"><a href="hyperparameter-tuning.html#cb174-2" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb174-3"><a href="hyperparameter-tuning.html#cb174-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb174-4"><a href="hyperparameter-tuning.html#cb174-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb174-5"><a href="hyperparameter-tuning.html#cb174-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n)<span class="sc">/</span><span class="dv">4</span></span>
<span id="cb174-6"><a href="hyperparameter-tuning.html#cb174-6" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x)</span>
<span id="cb174-7"><a href="hyperparameter-tuning.html#cb174-7" tabindex="-1"></a></span>
<span id="cb174-8"><a href="hyperparameter-tuning.html#cb174-8" tabindex="-1"></a><span class="co">#Creating a set of possible values for span: GRID</span></span>
<span id="cb174-9"><a href="hyperparameter-tuning.html#cb174-9" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="fu">seq</span>(<span class="at">from =</span> <span class="fl">0.01</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.02</span>), <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb174-10"><a href="hyperparameter-tuning.html#cb174-10" tabindex="-1"></a></span>
<span id="cb174-11"><a href="hyperparameter-tuning.html#cb174-11" tabindex="-1"></a><span class="co"># loop for Train - Test split 100 times</span></span>
<span id="cb174-12"><a href="hyperparameter-tuning.html#cb174-12" tabindex="-1"></a></span>
<span id="cb174-13"><a href="hyperparameter-tuning.html#cb174-13" tabindex="-1"></a>t <span class="ot">=</span> <span class="dv">100</span> <span class="co"># number of times we loop</span></span>
<span id="cb174-14"><a href="hyperparameter-tuning.html#cb174-14" tabindex="-1"></a>RMSPE_test <span class="ot">&lt;-</span> <span class="fu">c</span>() <span class="co"># container for 100 RMSPE&#39;s</span></span>
<span id="cb174-15"><a href="hyperparameter-tuning.html#cb174-15" tabindex="-1"></a></span>
<span id="cb174-16"><a href="hyperparameter-tuning.html#cb174-16" tabindex="-1"></a><span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>t) {</span>
<span id="cb174-17"><a href="hyperparameter-tuning.html#cb174-17" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">10</span><span class="sc">+</span>l)</span>
<span id="cb174-18"><a href="hyperparameter-tuning.html#cb174-18" tabindex="-1"></a>  shuf <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data), <span class="fu">nrow</span>(data), <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb174-19"><a href="hyperparameter-tuning.html#cb174-19" tabindex="-1"></a>  k <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb174-20"><a href="hyperparameter-tuning.html#cb174-20" tabindex="-1"></a>  indx <span class="ot">&lt;-</span> shuf[<span class="dv">1</span><span class="sc">:</span>(<span class="fu">nrow</span>(data)<span class="sc">/</span>k)]</span>
<span id="cb174-21"><a href="hyperparameter-tuning.html#cb174-21" tabindex="-1"></a>  testset <span class="ot">&lt;-</span> data[indx, ] <span class="co">#10% of data set a side</span></span>
<span id="cb174-22"><a href="hyperparameter-tuning.html#cb174-22" tabindex="-1"></a>  trainset <span class="ot">&lt;-</span> data[<span class="sc">-</span>indx,] </span>
<span id="cb174-23"><a href="hyperparameter-tuning.html#cb174-23" tabindex="-1"></a></span>
<span id="cb174-24"><a href="hyperparameter-tuning.html#cb174-24" tabindex="-1"></a>  <span class="co"># k-CV, which is the same as before</span></span>
<span id="cb174-25"><a href="hyperparameter-tuning.html#cb174-25" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">5</span><span class="sc">*</span>l)</span>
<span id="cb174-26"><a href="hyperparameter-tuning.html#cb174-26" tabindex="-1"></a>  mysample <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(trainset), <span class="fu">nrow</span>(trainset), <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb174-27"><a href="hyperparameter-tuning.html#cb174-27" tabindex="-1"></a>  fold <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb174-28"><a href="hyperparameter-tuning.html#cb174-28" tabindex="-1"></a></span>
<span id="cb174-29"><a href="hyperparameter-tuning.html#cb174-29" tabindex="-1"></a>  OPT <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb174-30"><a href="hyperparameter-tuning.html#cb174-30" tabindex="-1"></a></span>
<span id="cb174-31"><a href="hyperparameter-tuning.html#cb174-31" tabindex="-1"></a>  <span class="co">#CV loop</span></span>
<span id="cb174-32"><a href="hyperparameter-tuning.html#cb174-32" tabindex="-1"></a>  nvalid <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">nrow</span>(trainset)<span class="sc">/</span>fold)</span>
<span id="cb174-33"><a href="hyperparameter-tuning.html#cb174-33" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>fold){</span>
<span id="cb174-34"><a href="hyperparameter-tuning.html#cb174-34" tabindex="-1"></a>    <span class="cf">if</span>(i <span class="sc">&lt;</span> fold) valid <span class="ot">&lt;-</span> mysample[((i<span class="dv">-1</span>)<span class="sc">*</span>nvalid<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(i<span class="sc">*</span>nvalid)]</span>
<span id="cb174-35"><a href="hyperparameter-tuning.html#cb174-35" tabindex="-1"></a>        </span>
<span id="cb174-36"><a href="hyperparameter-tuning.html#cb174-36" tabindex="-1"></a>    data_valid <span class="ot">&lt;-</span> trainset[valid, ]</span>
<span id="cb174-37"><a href="hyperparameter-tuning.html#cb174-37" tabindex="-1"></a>    data_train <span class="ot">&lt;-</span> trainset[<span class="sc">-</span>valid, ]</span>
<span id="cb174-38"><a href="hyperparameter-tuning.html#cb174-38" tabindex="-1"></a>  </span>
<span id="cb174-39"><a href="hyperparameter-tuning.html#cb174-39" tabindex="-1"></a>    RMSPE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb174-40"><a href="hyperparameter-tuning.html#cb174-40" tabindex="-1"></a>  </span>
<span id="cb174-41"><a href="hyperparameter-tuning.html#cb174-41" tabindex="-1"></a>    <span class="cf">for</span>(s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(grid)){</span>
<span id="cb174-42"><a href="hyperparameter-tuning.html#cb174-42" tabindex="-1"></a>        model <span class="ot">&lt;-</span> <span class="fu">loess</span>(y <span class="sc">~</span> x, <span class="at">control=</span><span class="fu">loess.control</span>(<span class="at">surface=</span><span class="st">&quot;direct&quot;</span>), </span>
<span id="cb174-43"><a href="hyperparameter-tuning.html#cb174-43" tabindex="-1"></a>        <span class="at">degree=</span>grid[s,<span class="dv">2</span>], <span class="at">span =</span> grid[s,<span class="dv">1</span>], <span class="at">data =</span> data_train)</span>
<span id="cb174-44"><a href="hyperparameter-tuning.html#cb174-44" tabindex="-1"></a>   </span>
<span id="cb174-45"><a href="hyperparameter-tuning.html#cb174-45" tabindex="-1"></a>        fit <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, data_valid<span class="sc">$</span>x)</span>
<span id="cb174-46"><a href="hyperparameter-tuning.html#cb174-46" tabindex="-1"></a>        RMSPE[s] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((data_valid<span class="sc">$</span>y<span class="sc">-</span>fit)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb174-47"><a href="hyperparameter-tuning.html#cb174-47" tabindex="-1"></a>     }</span>
<span id="cb174-48"><a href="hyperparameter-tuning.html#cb174-48" tabindex="-1"></a>   OPT[i] <span class="ot">&lt;-</span> <span class="fu">which</span>(RMSPE <span class="sc">==</span> <span class="fu">min</span>(RMSPE))</span>
<span id="cb174-49"><a href="hyperparameter-tuning.html#cb174-49" tabindex="-1"></a>  }</span>
<span id="cb174-50"><a href="hyperparameter-tuning.html#cb174-50" tabindex="-1"></a></span>
<span id="cb174-51"><a href="hyperparameter-tuning.html#cb174-51" tabindex="-1"></a>  <span class="co"># Hyperparameters</span></span>
<span id="cb174-52"><a href="hyperparameter-tuning.html#cb174-52" tabindex="-1"></a>  opgrid <span class="ot">&lt;-</span> grid[OPT,]</span>
<span id="cb174-53"><a href="hyperparameter-tuning.html#cb174-53" tabindex="-1"></a>  <span class="fu">colnames</span>(opgrid) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;span&quot;</span>, <span class="st">&quot;degree&quot;</span>)</span>
<span id="cb174-54"><a href="hyperparameter-tuning.html#cb174-54" tabindex="-1"></a>  <span class="fu">rownames</span>(opgrid) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span>
<span id="cb174-55"><a href="hyperparameter-tuning.html#cb174-55" tabindex="-1"></a>  opt_degree <span class="ot">&lt;-</span> <span class="fu">modal</span>(opgrid[,<span class="dv">2</span>])</span>
<span id="cb174-56"><a href="hyperparameter-tuning.html#cb174-56" tabindex="-1"></a>  opt_span <span class="ot">&lt;-</span> <span class="fu">mean</span>(opgrid[,<span class="dv">1</span>])</span>
<span id="cb174-57"><a href="hyperparameter-tuning.html#cb174-57" tabindex="-1"></a></span>
<span id="cb174-58"><a href="hyperparameter-tuning.html#cb174-58" tabindex="-1"></a>  <span class="co"># **** Using the test set for final evaluation ******</span></span>
<span id="cb174-59"><a href="hyperparameter-tuning.html#cb174-59" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">loess</span>(y <span class="sc">~</span> x, <span class="at">control=</span><span class="fu">loess.control</span>(<span class="at">surface=</span><span class="st">&quot;direct&quot;</span>), </span>
<span id="cb174-60"><a href="hyperparameter-tuning.html#cb174-60" tabindex="-1"></a>                 <span class="at">degree=</span>opt_degree, <span class="at">span =</span> opt_span, <span class="at">data =</span> trainset)</span>
<span id="cb174-61"><a href="hyperparameter-tuning.html#cb174-61" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, testset<span class="sc">$</span>x)</span>
<span id="cb174-62"><a href="hyperparameter-tuning.html#cb174-62" tabindex="-1"></a>  RMSPE_test[l] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((testset<span class="sc">$</span>y<span class="sc">-</span>fit)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb174-63"><a href="hyperparameter-tuning.html#cb174-63" tabindex="-1"></a>}  </span></code></pre></div>
<p>We can now see the average RMSPE and its variance:</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="hyperparameter-tuning.html#cb175-1" tabindex="-1"></a><span class="fu">plot</span>(RMSPE_test, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb175-2"><a href="hyperparameter-tuning.html#cb175-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a=</span><span class="fu">mean</span>(RMSPE_test), <span class="at">b=</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>)</span></code></pre></div>
<p><img src="11-HyperTuning_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="hyperparameter-tuning.html#cb176-1" tabindex="-1"></a><span class="fu">mean</span>(RMSPE_test)</span></code></pre></div>
<pre><code>## [1] 0.2564112</code></pre>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="hyperparameter-tuning.html#cb178-1" tabindex="-1"></a><span class="fu">var</span>(RMSPE_test)</span></code></pre></div>
<pre><code>## [1] 0.0003384585</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="nonparametric-estimations---basics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="optimization-algorithms---basics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuk/machinemetrics/edit/master/11-HyperTuning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
