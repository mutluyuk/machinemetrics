<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 27 Graphical Network Analysis | MachineMetrics</title>
  <meta name="description" content="Chapter 27 Graphical Network Analysis | MachineMetrics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 27 Graphical Network Analysis | MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 27 Graphical Network Analysis | MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="deep-learning.html"/>
<link rel="next" href="decompositions.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> INTRODUCTION:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html"><i class="fa fa-check"></i><b>2</b> Statistical Models and Simulations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#distinguishing-between-statistical-modeling-and-machine-learning-in-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#goals-and-objectives"><i class="fa fa-check"></i><b>2.1.1</b> Goals and Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#prediction-vs.-inference"><i class="fa fa-check"></i><b>2.1.2</b> Prediction vs. Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#conclusion"><i class="fa fa-check"></i><b>2.1.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#model-selection-and-approaches-in-data-modeling"><i class="fa fa-check"></i><b>2.4</b> Model Selection and Approaches in Data Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#mle"><i class="fa fa-check"></i><b>3.5.3</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error---mse"><i class="fa fa-check"></i><b>5.1</b> Estimation error - MSE</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.2</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#technical-points-about-mse-and-mspe"><i class="fa fa-check"></i><b>5.3</b> Technical points about MSE and MSPE</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.1</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.1</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.2</b> LPM</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
<li class="chapter" data-level="10.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#knn"><i class="fa fa-check"></i><b>10.3</b> Knn</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#adult-dataset"><i class="fa fa-check"></i><b>10.3.1</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="11.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#grid-search"><i class="fa fa-check"></i><b>11.4</b> Grid Search</a></li>
<li class="chapter" data-level="11.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>11.5</b> Cross-validated grid search</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#one-variable"><i class="fa fa-check"></i><b>12.4.1</b> One-variable</a></li>
<li class="chapter" data-level="12.4.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>12.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="12.4.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#multivariable"><i class="fa fa-check"></i><b>12.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>16.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.3</b> Pruning</a></li>
<li class="chapter" data-level="16.4" data-path="regression-trees.html"><a href="regression-trees.html#classification-with-titanic"><i class="fa fa-check"></i><b>16.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="16.5" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>17.2</b> Random Forest</a></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>17.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="17.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="17.3.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.3.3</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.4</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.5</b> Classification</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.6</b> Regression</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.7</b> Exploration</a></li>
<li class="chapter" data-level="17.8" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.8</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression-1"><i class="fa fa-check"></i><b>17.8.1</b> Regression</a></li>
<li class="chapter" data-level="17.8.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.8.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.8.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.8.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.8.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-1"><i class="fa fa-check"></i><b>17.8.4</b> Classification</a></li>
<li class="chapter" data-level="17.8.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost.m1"><i class="fa fa-check"></i><b>17.8.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="17.8.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.8.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#out-sample-prediction-accuracy"><i class="fa fa-check"></i><b>20.3</b> out-sample prediction accuracy</a></li>
<li class="chapter" data-level="20.4" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-2.html"><a href="classification-2.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-2.html"><a href="classification-2.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-2.html"><a href="classification-2.html#mnist-dataset"><i class="fa fa-check"></i><b>21.2</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="21.3" data-path="classification-2.html"><a href="classification-2.html#linear-classifiers-again"><i class="fa fa-check"></i><b>21.3</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="21.4" data-path="classification-2.html"><a href="classification-2.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.4</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.5" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret"><i class="fa fa-check"></i><b>21.5</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="classification-2.html"><a href="classification-2.html#mnist_27"><i class="fa fa-check"></i><b>21.5.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="21.5.2" data-path="classification-2.html"><a href="classification-2.html#adult-dataset-1"><i class="fa fa-check"></i><b>21.5.2</b> Adult dataset</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="classification-2.html"><a href="classification-2.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.6</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.7" data-path="classification-2.html"><a href="classification-2.html#confusion-matrix"><i class="fa fa-check"></i><b>21.7</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.8" data-path="classification-2.html"><a href="classification-2.html#performance-measures"><i class="fa fa-check"></i><b>21.8</b> Performance measures</a></li>
<li class="chapter" data-level="21.9" data-path="classification-2.html"><a href="classification-2.html#roc-curve"><i class="fa fa-check"></i><b>21.9</b> ROC Curve</a></li>
<li class="chapter" data-level="21.10" data-path="classification-2.html"><a href="classification-2.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.10</b> AUC - Area Under the Curve</a></li>
<li class="chapter" data-level="21.11" data-path="classification-2.html"><a href="classification-2.html#classification-example"><i class="fa fa-check"></i><b>21.11</b> Classification Example</a></li>
<li class="chapter" data-level="21.12" data-path="classification-2.html"><a href="classification-2.html#lpm-1"><i class="fa fa-check"></i><b>21.12</b> LPM</a></li>
<li class="chapter" data-level="21.13" data-path="classification-2.html"><a href="classification-2.html#logistic-regression-1"><i class="fa fa-check"></i><b>21.13</b> Logistic Regression</a></li>
<li class="chapter" data-level="21.14" data-path="classification-2.html"><a href="classification-2.html#knn-1"><i class="fa fa-check"></i><b>21.14</b> kNN</a>
<ul>
<li class="chapter" data-level="21.14.1" data-path="classification-2.html"><a href="classification-2.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>21.14.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="21.14.2" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret-1"><i class="fa fa-check"></i><b>21.14.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>22</b> Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="time-series.html"><a href="time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="time-series.html"><a href="time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="time-series.html"><a href="time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="time-series.html"><a href="time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="time-series.html"><a href="time-series.html#stationarity"><i class="fa fa-check"></i><b>22.5</b> Stationarity</a></li>
<li class="chapter" data-level="22.6" data-path="time-series.html"><a href="time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.6</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="time-series.html"><a href="time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.7</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.8" data-path="time-series.html"><a href="time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.8</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.9" data-path="time-series.html"><a href="time-series.html#speed"><i class="fa fa-check"></i><b>22.9</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="forecast.html"><a href="forecast.html"><i class="fa fa-check"></i><b>23</b> Forecast</a>
<ul>
<li class="chapter" data-level="23.1" data-path="forecast.html"><a href="forecast.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="forecast.html"><a href="forecast.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="forecast.html"><a href="forecast.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="forecast.html"><a href="forecast.html#random-forest-1"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="forecast.html"><a href="forecast.html#univariate"><i class="fa fa-check"></i><b>23.5</b> Univariate</a></li>
<li class="chapter" data-level="23.6" data-path="forecast.html"><a href="forecast.html#multivariate"><i class="fa fa-check"></i><b>23.6</b> Multivariate</a></li>
<li class="chapter" data-level="23.7" data-path="forecast.html"><a href="forecast.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.7</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>24.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-margin"><i class="fa fa-check"></i><b>24.1.1</b> The Margin</a></li>
<li class="chapter" data-level="24.1.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-non-separable-case"><i class="fa fa-check"></i><b>24.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>24.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="24.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#application-with-svm"><i class="fa fa-check"></i><b>24.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#precision-matrix"><i class="fa fa-check"></i><b>27.4</b> Precision Matrix</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.5</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.6" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.6</b> Regularized Covariance Matrix</a></li>
<li class="chapter" data-level="27.7" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>27.7</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="27.8" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#high-dimensional-data"><i class="fa fa-check"></i><b>27.8</b> High-dimensional data</a></li>
<li class="chapter" data-level="27.9" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>27.9</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#rankr-approximations"><i class="fa fa-check"></i><b>28.4</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="28.5" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.5</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html"><i class="fa fa-check"></i><b>29</b> PCA (Principle Component Analysis)</a>
<ul>
<li class="chapter" data-level="29.1" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.1</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>30.1</b> Using bins</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>30.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="30.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>30.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="30.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="31.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>31.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="31.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>31.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="text-analysis.html"><a href="text-analysis.html"><i class="fa fa-check"></i><b>32</b> Text Analysis</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#other-nonparametric-estimation-methods-1"><i class="fa fa-check"></i><b>33.1</b> Other Nonparametric Estimation methods</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.2</b> Regression splines</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.3</b> MARS</a></li>
<li class="chapter" data-level="33.4" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.4</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuk/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="graphical-network-analysis" class="section level1 hasAnchor" number="27">
<h1><span class="header-section-number">Chapter 27</span> Graphical Network Analysis<a href="graphical-network-analysis.html#graphical-network-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>A network represents a structure of relationships between objects. Graphical modeling presents a network structure in a graph, which consists of nodes and edges, by expressing conditional (in)dependence between the nodes. If we think of these nodes (objects) as variables and their relationship with each other as edges, a graphical model represents the probabilistic relationships among a set of variables. For example, the absence of edges (partial correlations) corresponds to conditional independence. Graphical models are becoming more popular in statistics because it helps us understand a very complex structure of relationships in networks, such as the dynamic structure of biological systems or social events.</p>
<p>The central idea is that, since any pair of nodes may be joined by an edge, a missing edge represents some form of independency between the pair of variables. The complexity in network analysis comes from the fact that the independency may be either marginal or conditional on some or all of the other variables. Therefore, defining a graphical model requires identification of a type of graph needed for each particular case.</p>
<p>In general, a graphical model could be designed with directed and undirected edges. In a directed graph, an arrow indicates the direction of dependency between nodes. In undirected graphs, however, the edges do not have directions. The field of graphical modeling is vast, hence it is beyond the scope of this book.</p>
<p>Yet, we will look at the precision matrix, which has been shown that its regularization captures the network connections. Hence, the central theme of this section is the estimation of sparse standardized precision matrices, whose results can be illustrated by undirected graphs.</p>
<div id="fundementals" class="section level2 hasAnchor" number="27.1">
<h2><span class="header-section-number">27.1</span> Fundementals<a href="graphical-network-analysis.html#fundementals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we will cover several concepts related to statistical (in)dependence measured by correlations.</p>
</div>
<div id="covariance" class="section level2 hasAnchor" number="27.2">
<h2><span class="header-section-number">27.2</span> Covariance<a href="graphical-network-analysis.html#covariance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We start with a data matrix, which refers to the array of numbers:</p>
<p><span class="math display">\[
\mathbf{X}=\left(\begin{array}{cccc}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 p} \\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2 p} \\
x_{31} &amp; x_{32} &amp; \cdots &amp; x_{3 p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n 1} &amp; x_{n 2} &amp; \cdots &amp; x_{n p}
\end{array}\right)
\]</span></p>
<p>An example would be</p>
<div class="sourceCode" id="cb827"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb827-1"><a href="graphical-network-analysis.html#cb827-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">5</span>)</span>
<span id="cb827-2"><a href="graphical-network-analysis.html#cb827-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">30</span>, <span class="at">sd=</span><span class="fu">runif</span>(<span class="dv">30</span>, <span class="dv">2</span>, <span class="dv">50</span>))</span>
<span id="cb827-3"><a href="graphical-network-analysis.html#cb827-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(x, <span class="dv">10</span>)</span>
<span id="cb827-4"><a href="graphical-network-analysis.html#cb827-4" tabindex="-1"></a>X</span></code></pre></div>
<pre><code>##              [,1]       [,2]       [,3]
##  [1,]   -1.613670  -4.436764  42.563842
##  [2,]  -20.840548  36.237338 -36.942481
##  [3,] -100.484392  25.903897 -24.294407
##  [4,]    3.769073 -18.950442 -22.616651
##  [5,]   -1.821506 -12.454626  -1.243431
##  [6,]   32.103933   3.693050  38.807102
##  [7,]   25.752668  22.861071 -18.452338
##  [8,]   59.864792  98.848864  -3.607105
##  [9,]   33.862342  34.853324  16.704375
## [10,]    5.980194  62.755408 -21.841795</code></pre>
<p>We start with defining the covariance matrix</p>
<p><span class="math display">\[
\mathbf{S}=\left(\begin{array}{ccccc}
s_{1}^{2} &amp; s_{12} &amp; s_{13} &amp; \cdots &amp; s_{1 p} \\
s_{21} &amp; s_{2}^{2} &amp; s_{23} &amp; \cdots &amp; s_{2 p} \\
s_{31} &amp; s_{32} &amp; s_{3}^{2} &amp; \cdots &amp; s_{3 p} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
s_{p 1} &amp; s_{p 2} &amp; s_{p 3} &amp; \cdots &amp; s_{p}^{2}
\end{array}\right)
\]</span>
<span class="math display">\[
s_{j}^{2}=(1 / n) \sum_{i=1}^{n}\left(x_{i j}-\bar{x}_{j}\right)^{2}
\]</span>
is the variance of the <span class="math inline">\(j\)</span>-th variable,</p>
<p><span class="math display">\[
\begin{aligned}
&amp;s_{j k}=(1 / n) \sum_{i=1}^{n}\left(x_{i j}-\bar{x}_{j}\right)\left(x_{i k}-\bar{x}_{k}\right)
\end{aligned}
\]</span>
is the covariance between the <span class="math inline">\(j\)</span>-th and <span class="math inline">\(k\)</span>-th variables; and,</p>
<p><span class="math display">\[
\bar{x}_{j}=(1 / n) \sum_{i=1}^{n} x_{j i}
\]</span>
is the mean of the <span class="math inline">\(j\)</span>-th variable.</p>
<p>We can calculate the covariance matrix such as</p>
<p><span class="math display">\[
\mathbf{S}=\frac{1}{n} \mathbf{X}_{c}^{\prime} \mathbf{X}_{c},
\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}_{c}\)</span> is the centered matrix:</p>
<p><span class="math display">\[
\mathbf{X}_{c}=\left(\begin{array}{cccc}
x_{11}-\bar{x}_{1} &amp; x_{12}-\bar{x}_{2} &amp; \cdots &amp; x_{1 p}-\bar{x}_{p} \\
x_{21}-\bar{x}_{1} &amp; x_{22}-\bar{x}_{2} &amp; \cdots &amp; x_{2 p}-\bar{x}_{p} \\
x_{31}-\bar{x}_{1} &amp; x_{32}-\bar{x}_{2} &amp; \cdots &amp; x_{3 p}-\bar{x}_{p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n 1}-\bar{x}_{1} &amp; x_{n 2}-\bar{x}_{2} &amp; \cdots &amp; x_{n p}-\bar{x}_{p}
\end{array}\right)
\]</span></p>
<p>How?</p>
<div class="sourceCode" id="cb829"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb829-1"><a href="graphical-network-analysis.html#cb829-1" tabindex="-1"></a><span class="co"># More direct</span></span>
<span id="cb829-2"><a href="graphical-network-analysis.html#cb829-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb829-3"><a href="graphical-network-analysis.html#cb829-3" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">1</span>, n, <span class="dv">1</span>)<span class="sc">%*%</span><span class="fu">colMeans</span>(X)</span>
<span id="cb829-4"><a href="graphical-network-analysis.html#cb829-4" tabindex="-1"></a>Xc <span class="ot">&lt;-</span> X<span class="sc">-</span>m</span>
<span id="cb829-5"><a href="graphical-network-analysis.html#cb829-5" tabindex="-1"></a>Xc</span></code></pre></div>
<pre><code>##               [,1]        [,2]        [,3]
##  [1,]   -5.2709585 -29.3678760  45.6561309
##  [2,]  -24.4978367  11.3062262 -33.8501919
##  [3,] -104.1416804   0.9727849 -21.2021184
##  [4,]    0.1117842 -43.8815539 -19.5243622
##  [5,]   -5.4787951 -37.3857380   1.8488577
##  [6,]   28.4466449 -21.2380620  41.8993911
##  [7,]   22.0953790  -2.0700407 -15.3600493
##  [8,]   56.2075038  73.9177518  -0.5148158
##  [9,]   30.2050530   9.9222117  19.7966643
## [10,]    2.3229057  37.8242961 -18.7495065</code></pre>
<div class="sourceCode" id="cb831"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb831-1"><a href="graphical-network-analysis.html#cb831-1" tabindex="-1"></a><span class="co"># Or</span></span>
<span id="cb831-2"><a href="graphical-network-analysis.html#cb831-2" tabindex="-1"></a>C <span class="ot">&lt;-</span> <span class="fu">diag</span>(n) <span class="sc">-</span> <span class="fu">matrix</span>(<span class="dv">1</span><span class="sc">/</span>n, n, n)</span>
<span id="cb831-3"><a href="graphical-network-analysis.html#cb831-3" tabindex="-1"></a>XC <span class="ot">&lt;-</span> C <span class="sc">%*%</span> X</span>
<span id="cb831-4"><a href="graphical-network-analysis.html#cb831-4" tabindex="-1"></a>Xc</span></code></pre></div>
<pre><code>##               [,1]        [,2]        [,3]
##  [1,]   -5.2709585 -29.3678760  45.6561309
##  [2,]  -24.4978367  11.3062262 -33.8501919
##  [3,] -104.1416804   0.9727849 -21.2021184
##  [4,]    0.1117842 -43.8815539 -19.5243622
##  [5,]   -5.4787951 -37.3857380   1.8488577
##  [6,]   28.4466449 -21.2380620  41.8993911
##  [7,]   22.0953790  -2.0700407 -15.3600493
##  [8,]   56.2075038  73.9177518  -0.5148158
##  [9,]   30.2050530   9.9222117  19.7966643
## [10,]    2.3229057  37.8242961 -18.7495065</code></pre>
<div class="sourceCode" id="cb833"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb833-1"><a href="graphical-network-analysis.html#cb833-1" tabindex="-1"></a><span class="co"># We can also use `scale` </span></span>
<span id="cb833-2"><a href="graphical-network-analysis.html#cb833-2" tabindex="-1"></a>Xc <span class="ot">&lt;-</span> <span class="fu">scale</span>(X, <span class="at">center=</span><span class="cn">TRUE</span>, <span class="at">scale=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<p>And, the covariance matrix</p>
<div class="sourceCode" id="cb834"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb834-1"><a href="graphical-network-analysis.html#cb834-1" tabindex="-1"></a><span class="co"># Covariance Matrix</span></span>
<span id="cb834-2"><a href="graphical-network-analysis.html#cb834-2" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">t</span>(Xc) <span class="sc">%*%</span> Xc <span class="sc">/</span> (n<span class="dv">-1</span>)</span>
<span id="cb834-3"><a href="graphical-network-analysis.html#cb834-3" tabindex="-1"></a>S</span></code></pre></div>
<pre><code>##           [,1]      [,2]      [,3]
## [1,] 1875.3209  429.8712  462.4775
## [2,]  429.8712 1306.9817 -262.8231
## [3,]  462.4775 -262.8231  755.5193</code></pre>
<div class="sourceCode" id="cb836"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb836-1"><a href="graphical-network-analysis.html#cb836-1" tabindex="-1"></a><span class="co"># Check it</span></span>
<span id="cb836-2"><a href="graphical-network-analysis.html#cb836-2" tabindex="-1"></a><span class="fu">cov</span>(X)</span></code></pre></div>
<pre><code>##           [,1]      [,2]      [,3]
## [1,] 1875.3209  429.8712  462.4775
## [2,]  429.8712 1306.9817 -262.8231
## [3,]  462.4775 -262.8231  755.5193</code></pre>
</div>
<div id="correlation-1" class="section level2 hasAnchor" number="27.3">
<h2><span class="header-section-number">27.3</span> Correlation<a href="graphical-network-analysis.html#correlation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While covariance is a necessary step, we can capture the size and the direction of relationships between the variables:</p>
<p><span class="math display">\[
\mathbf{R}=\left(\begin{array}{ccccc}
1 &amp; r_{12} &amp; r_{13} &amp; \cdots &amp; r_{1 p} \\
r_{21} &amp; 1 &amp; r_{23} &amp; \cdots &amp; r_{2 p} \\
r_{31} &amp; r_{32} &amp; 1 &amp; \cdots &amp; r_{3 p} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
r_{p 1} &amp; r_{p 2} &amp; r_{p 3} &amp; \cdots &amp; 1
\end{array}\right)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
r_{j k}=\frac{s_{j k}}{s_{j} s_{k}}=\frac{\sum_{i=1}^{n}\left(x_{i j}-\bar{x}_{j}\right)\left(x_{i k}-\bar{x}_{k}\right)}{\sqrt{\sum_{i=1}^{n}\left(x_{i j}-\bar{x}_{j}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(x_{i k}-\bar{x}_{k}\right)^{2}}}
\]</span>
is the Pearson correlation coefficient between variables <span class="math inline">\(\mathbf{X}_{j}\)</span> and <span class="math inline">\(\mathbf{X}_{k}\)</span></p>
<p>We can calculate the correlation matrix</p>
<p><span class="math display">\[
\mathbf{R}=\frac{1}{n} \mathbf{X}_{s}^{\prime} \mathbf{X}_{s}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}_{s}=\mathbf{C X D}^{-1}\)</span> with</p>
<ul>
<li><span class="math inline">\(\mathbf{C}=\mathbf{I}_{n}-n^{-1} \mathbf{1}_{n} \mathbf{1}_{n}^{\prime}\)</span> denoting a centering matrix,</li>
<li><span class="math inline">\(\mathbf{D}=\operatorname{diag}\left(s_{1}, \ldots, s_{p}\right)\)</span> denoting a diagonal scaling matrix.</li>
</ul>
<p>Note that the standardized matrix <span class="math inline">\(\mathbf{X}_{s}\)</span> has the form</p>
<p><span class="math display">\[
\mathbf{X}_{s}=\left(\begin{array}{cccc}
\left(x_{11}-\bar{x}_{1}\right) / s_{1} &amp; \left(x_{12}-\bar{x}_{2}\right) / s_{2} &amp; \cdots &amp; \left(x_{1 p}-\bar{x}_{p}\right) / s_{p} \\
\left(x_{21}-\bar{x}_{1}\right) / s_{1} &amp; \left(x_{22}-\bar{x}_{2}\right) / s_{2} &amp; \cdots &amp; \left(x_{2 p}-\bar{x}_{p}\right) / s_{p} \\
\left(x_{31}-\bar{x}_{1}\right) / s_{1} &amp; \left(x_{32}-\bar{x}_{2}\right) / s_{2} &amp; \cdots &amp; \left(x_{3 p}-\bar{x}_{p}\right) / s_{p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\left(x_{n 1}-\bar{x}_{1}\right) / s_{1} &amp; \left(x_{n 2}-\bar{x}_{2}\right) / s_{2} &amp; \cdots &amp; \left(x_{n p}-\bar{x}_{p}\right) / s_{p}
\end{array}\right)
\]</span></p>
<p>How?</p>
<div class="sourceCode" id="cb838"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb838-1"><a href="graphical-network-analysis.html#cb838-1" tabindex="-1"></a><span class="co"># More direct</span></span>
<span id="cb838-2"><a href="graphical-network-analysis.html#cb838-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(X)</span>
<span id="cb838-3"><a href="graphical-network-analysis.html#cb838-3" tabindex="-1"></a>sdx <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span><span class="fu">matrix</span>(<span class="dv">1</span>, n, <span class="dv">1</span>)<span class="sc">%*%</span><span class="fu">apply</span>(X, <span class="dv">2</span>, sd)</span>
<span id="cb838-4"><a href="graphical-network-analysis.html#cb838-4" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">1</span>, n, <span class="dv">1</span>)<span class="sc">%*%</span><span class="fu">colMeans</span>(X)</span>
<span id="cb838-5"><a href="graphical-network-analysis.html#cb838-5" tabindex="-1"></a>Xs <span class="ot">&lt;-</span> (X<span class="sc">-</span>m)<span class="sc">*</span>sdx</span>
<span id="cb838-6"><a href="graphical-network-analysis.html#cb838-6" tabindex="-1"></a>Xs</span></code></pre></div>
<pre><code>##               [,1]        [,2]        [,3]
##  [1,] -0.121717156 -0.81233989  1.66102560
##  [2,] -0.565704894  0.31273963 -1.23151117
##  [3,] -2.404843294  0.02690804 -0.77135887
##  [4,]  0.002581324 -1.21380031 -0.71032005
##  [5,] -0.126516525 -1.03412063  0.06726369
##  [6,]  0.656890910 -0.58746247  1.52435083
##  [7,]  0.510227259 -0.05725905 -0.55881729
##  [8,]  1.297945627  2.04462654 -0.01872963
##  [9,]  0.697496131  0.27445664  0.72022674
## [10,]  0.053640619  1.04625151 -0.68212986</code></pre>
<div class="sourceCode" id="cb840"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb840-1"><a href="graphical-network-analysis.html#cb840-1" tabindex="-1"></a><span class="co"># Or</span></span>
<span id="cb840-2"><a href="graphical-network-analysis.html#cb840-2" tabindex="-1"></a>C <span class="ot">&lt;-</span> <span class="fu">diag</span>(n) <span class="sc">-</span> <span class="fu">matrix</span>(<span class="dv">1</span><span class="sc">/</span>n, n, n)</span>
<span id="cb840-3"><a href="graphical-network-analysis.html#cb840-3" tabindex="-1"></a>D <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">apply</span>(X, <span class="dv">2</span>, sd))</span>
<span id="cb840-4"><a href="graphical-network-analysis.html#cb840-4" tabindex="-1"></a>Xs <span class="ot">&lt;-</span> C <span class="sc">%*%</span> X <span class="sc">%*%</span> <span class="fu">solve</span>(D)</span>
<span id="cb840-5"><a href="graphical-network-analysis.html#cb840-5" tabindex="-1"></a>Xs</span></code></pre></div>
<pre><code>##               [,1]        [,2]        [,3]
##  [1,] -0.121717156 -0.81233989  1.66102560
##  [2,] -0.565704894  0.31273963 -1.23151117
##  [3,] -2.404843294  0.02690804 -0.77135887
##  [4,]  0.002581324 -1.21380031 -0.71032005
##  [5,] -0.126516525 -1.03412063  0.06726369
##  [6,]  0.656890910 -0.58746247  1.52435083
##  [7,]  0.510227259 -0.05725905 -0.55881729
##  [8,]  1.297945627  2.04462654 -0.01872963
##  [9,]  0.697496131  0.27445664  0.72022674
## [10,]  0.053640619  1.04625151 -0.68212986</code></pre>
<div class="sourceCode" id="cb842"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb842-1"><a href="graphical-network-analysis.html#cb842-1" tabindex="-1"></a><span class="co"># Or </span></span>
<span id="cb842-2"><a href="graphical-network-analysis.html#cb842-2" tabindex="-1"></a>Xs <span class="ot">&lt;-</span> <span class="fu">scale</span>(X, <span class="at">center=</span><span class="cn">TRUE</span>, <span class="at">scale=</span><span class="cn">TRUE</span>)</span>
<span id="cb842-3"><a href="graphical-network-analysis.html#cb842-3" tabindex="-1"></a></span>
<span id="cb842-4"><a href="graphical-network-analysis.html#cb842-4" tabindex="-1"></a><span class="co"># Finally, the correlation Matrix</span></span>
<span id="cb842-5"><a href="graphical-network-analysis.html#cb842-5" tabindex="-1"></a>R <span class="ot">&lt;-</span> <span class="fu">t</span>(Xs) <span class="sc">%*%</span> Xs <span class="sc">/</span> (n<span class="dv">-1</span>)</span>
<span id="cb842-6"><a href="graphical-network-analysis.html#cb842-6" tabindex="-1"></a>R</span></code></pre></div>
<pre><code>##           [,1]       [,2]       [,3]
## [1,] 1.0000000  0.2745780  0.3885349
## [2,] 0.2745780  1.0000000 -0.2644881
## [3,] 0.3885349 -0.2644881  1.0000000</code></pre>
<div class="sourceCode" id="cb844"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb844-1"><a href="graphical-network-analysis.html#cb844-1" tabindex="-1"></a><span class="co"># Check it</span></span>
<span id="cb844-2"><a href="graphical-network-analysis.html#cb844-2" tabindex="-1"></a><span class="fu">cor</span>(X)</span></code></pre></div>
<pre><code>##           [,1]       [,2]       [,3]
## [1,] 1.0000000  0.2745780  0.3885349
## [2,] 0.2745780  1.0000000 -0.2644881
## [3,] 0.3885349 -0.2644881  1.0000000</code></pre>
<p>The correlations above are called “zero-order” or Pearson correlations. They only reflect pairwise correlations without controlling other variables.</p>
</div>
<div id="precision-matrix" class="section level2 hasAnchor" number="27.4">
<h2><span class="header-section-number">27.4</span> Precision Matrix<a href="graphical-network-analysis.html#precision-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The inverse of covariance matrix, if it exists, is called the concentration matrix also knows as the <strong>precision matrix</strong>.</p>
<p>Let us consider a <span class="math inline">\(2 \times 2\)</span> covariance matrix:</p>
<p><span class="math display">\[
\left[\begin{array}{cc}
\sigma^{2}(x) &amp; \rho \sigma(x) \sigma(y) \\
\rho \sigma(x) \sigma(y) &amp; \sigma^{2}(y)
\end{array}\right]
\]</span></p>
<p>And, its inverse:</p>
<p><span class="math display">\[
\frac{1}{\sigma^{2}(x) \sigma^{2}(y)-\rho^{2} \sigma^{2}(x) \sigma^{2}(y)}\left[\begin{array}{cc}
\sigma^{2}(y) &amp; -\rho \sigma(x) \sigma(y) \\
-\rho \sigma(x) \sigma(y) &amp; \sigma^{2}(x)
\end{array}\right]
\]</span></p>
<p>If call the precision matrix <span class="math inline">\(D\)</span>, the correlation coefficient will be</p>
<p><span class="math display">\[
-\frac{d_{i j}}{\sqrt{d_{i i}} \sqrt{d_{j j}}},
\]</span>
Or,</p>
<p><span class="math display">\[
\frac{-\rho \sigma_{x} \sigma_{y}}{\sigma_{x}^{2} \sigma_{y}^{2}\left(1-e^{2}\right)} \times \sqrt{\sigma_{x}^{2}\left(1-\rho^{2}\right)} \sqrt{\sigma_{y}^{2}\left(1-\rho^{2}\right)}=-\rho
\]</span></p>
<p>That was for a <span class="math inline">\(2 \times 2\)</span> variance-covariance matrix. When we have more columns, the correlation coefficient reflects partial correlations. Here is an example:</p>
<div class="sourceCode" id="cb846"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb846-1"><a href="graphical-network-analysis.html#cb846-1" tabindex="-1"></a>pm <span class="ot">&lt;-</span> <span class="fu">solve</span>(S) <span class="co"># precision matrix</span></span>
<span id="cb846-2"><a href="graphical-network-analysis.html#cb846-2" tabindex="-1"></a>pm</span></code></pre></div>
<pre><code>##               [,1]          [,2]          [,3]
## [1,]  0.0007662131 -0.0003723763 -0.0005985624
## [2,] -0.0003723763  0.0010036440  0.0005770819
## [3,] -0.0005985624  0.0005770819  0.0018907421</code></pre>
<div class="sourceCode" id="cb848"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb848-1"><a href="graphical-network-analysis.html#cb848-1" tabindex="-1"></a><span class="co"># Partial correlation of 1,2</span></span>
<span id="cb848-2"><a href="graphical-network-analysis.html#cb848-2" tabindex="-1"></a><span class="sc">-</span>pm[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(<span class="fu">sqrt</span>(pm[<span class="dv">1</span>,<span class="dv">1</span>])<span class="sc">*</span><span class="fu">sqrt</span>(pm[<span class="dv">2</span>,<span class="dv">2</span>])) </span></code></pre></div>
<pre><code>## [1] 0.4246365</code></pre>
<div class="sourceCode" id="cb850"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb850-1"><a href="graphical-network-analysis.html#cb850-1" tabindex="-1"></a><span class="co"># Or</span></span>
<span id="cb850-2"><a href="graphical-network-analysis.html#cb850-2" tabindex="-1"></a><span class="sc">-</span><span class="fu">cov2cor</span>(<span class="fu">solve</span>(S))</span></code></pre></div>
<pre><code>##            [,1]       [,2]       [,3]
## [1,] -1.0000000  0.4246365  0.4973000
## [2,]  0.4246365 -1.0000000 -0.4189204
## [3,]  0.4973000 -0.4189204 -1.0000000</code></pre>
<div class="sourceCode" id="cb852"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb852-1"><a href="graphical-network-analysis.html#cb852-1" tabindex="-1"></a><span class="co"># Or</span></span>
<span id="cb852-2"><a href="graphical-network-analysis.html#cb852-2" tabindex="-1"></a>ppcor<span class="sc">::</span><span class="fu">pcor</span>(X)</span></code></pre></div>
<pre><code>## $estimate
##           [,1]       [,2]       [,3]
## [1,] 1.0000000  0.4246365  0.4973000
## [2,] 0.4246365  1.0000000 -0.4189204
## [3,] 0.4973000 -0.4189204  1.0000000
## 
## $p.value
##           [,1]      [,2]      [,3]
## [1,] 0.0000000 0.2546080 0.1731621
## [2,] 0.2546080 0.0000000 0.2617439
## [3,] 0.1731621 0.2617439 0.0000000
## 
## $statistic
##          [,1]      [,2]      [,3]
## [1,] 0.000000  1.240918  1.516557
## [2,] 1.240918  0.000000 -1.220629
## [3,] 1.516557 -1.220629  0.000000
## 
## $n
## [1] 10
## 
## $gp
## [1] 1
## 
## $method
## [1] &quot;pearson&quot;</code></pre>
</div>
<div id="semi-partial-correlation" class="section level2 hasAnchor" number="27.5">
<h2><span class="header-section-number">27.5</span> Semi-partial Correlation<a href="graphical-network-analysis.html#semi-partial-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>With partial correlation, we find the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> after controlling for the effect of <span class="math inline">\(Z\)</span> on both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. If we want to hold <span class="math inline">\(Z\)</span> constant for just <span class="math inline">\(X\)</span> or just <span class="math inline">\(Y\)</span>, we use a semipartial correlation.</p>
<p>While a partial correlation is computed between two residuals, a semipartial is computed between one residual and another variable. One interpretation of the semipartial is that the influence of a third variable is removed from one of two variables (hence, semipartial). This can be shown with the <span class="math inline">\(R^2\)</span> formulation.</p>
<p>Partial:</p>
<p><span class="math display">\[
r_{12.3}^{2}=\frac{R_{1.23}^{2}-R_{1.3}^{2}}{1-R_{1.3}^{2}}
\]</span></p>
<p>Semi-Partial:</p>
<p><span class="math display">\[
r_{1(2.3)}^{2}=R_{1.23}^{2}-R_{1.3}^{2}
\]</span></p>
<p>Let’s see the difference between a slope coefficient, a semi-partial correlation, and a partial correlation by looking their definitions:</p>
<p><strong>Partial:</strong></p>
<p><span class="math display">\[
r_{12,3}=\frac{r_{12}-r_{13} r_{23}}{\sqrt{1-r_{12}^{2}} \sqrt{1-r_{23}^{2}}}
\]</span></p>
<p><strong>Regression:</strong></p>
<p><span class="math display">\[
X_{1}=b_{1}+b_{2} X_{2}+b_{2} X_{3}
\]</span>
and</p>
<p><span class="math display">\[
b_{2}=\frac{\sum X_{3}^{2} \sum X_{1} X_{2}-\sum X_{1} X_{3} \sum X_{2} X_{3}}{\sum X_{2}^{2} \sum X_{3}^{2}-\left(\sum X_{2} X_{3}\right)^{2}}
\]</span></p>
<p>With standardized variables:</p>
<p><span class="math display">\[
b_{2}=\frac{r_{12}-r_{13} r_{23}}{1-r_{23}^{2}}
\]</span></p>
<p><strong>Semi-partial (or “part”) correlation:</strong></p>
<p><span class="math display">\[
r_{1(2.3)}=\frac{r_{1 2}-r_{1_{3}} r_{23}}{\sqrt{1-r_{23}^{2}}}
\]</span></p>
<p>The difference between the regression coefficient and the semi-partial coefficient is the square root in the denominator. Thus, the regression coefficient can exceed <span class="math inline">\(|1.0|\)</span>; the correlation cannot. In other words, semi-partial normalizes the coefficient between -1 and +1.</p>
<p>The function <code>spcor</code> can calculate the pairwise semi-partial correlations for each pair of variables given others.</p>
<div class="sourceCode" id="cb854"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb854-1"><a href="graphical-network-analysis.html#cb854-1" tabindex="-1"></a>ppcor<span class="sc">::</span><span class="fu">spcor</span>(X)</span></code></pre></div>
<pre><code>## $estimate
##           [,1]       [,2]       [,3]
## [1,] 1.0000000  0.3912745  0.4781862
## [2,] 0.4095148  1.0000000 -0.4028191
## [3,] 0.4795907 -0.3860075  1.0000000
## 
## $p.value
##           [,1]      [,2]      [,3]
## [1,] 0.0000000 0.2977193 0.1929052
## [2,] 0.2737125 0.0000000 0.2824036
## [3,] 0.1914134 0.3048448 0.0000000
## 
## $statistic
##          [,1]      [,2]      [,3]
## [1,] 0.000000  1.124899  1.440535
## [2,] 1.187625  0.000000 -1.164408
## [3,] 1.446027 -1.107084  0.000000
## 
## $n
## [1] 10
## 
## $gp
## [1] 1
## 
## $method
## [1] &quot;pearson&quot;</code></pre>
</div>
<div id="regularized-covariance-matrix" class="section level2 hasAnchor" number="27.6">
<h2><span class="header-section-number">27.6</span> Regularized Covariance Matrix<a href="graphical-network-analysis.html#regularized-covariance-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Due an increasing availability of high-dimensional data sets, graphical models have become powerful tools to discover conditional dependencies over a graph structure.</p>
<p>However, there are two main challenges in identifying the relations in a network: first, the edges (relationships) may not be identified by Pearson or Spearman correlations as they often lead to spurious associations due to missing confounding factors. Second, although, applications with partial correlations might address this issue, traditional precision estimators are not well-defined in case of high-dimensional data.</p>
<p>Why is a covariance matrix <span class="math inline">\(S\)</span> singular when <span class="math inline">\(n&lt;p\)</span> in <span class="math inline">\(\mathbf{X}\)</span>? Consider the <span class="math inline">\(n \times p\)</span> matrix of sample data, <span class="math inline">\(\mathbf{X}\)</span>. Since we know that the rank of <span class="math inline">\(\mathbf{X}\)</span> is at most <span class="math inline">\(\min (n, p)\)</span>. Hence, in</p>
<p><span class="math display">\[
\mathbf{S}=\frac{1}{n} \mathbf{X}_{c}^{\prime} \mathbf{X}_{c},
\]</span></p>
<p><span class="math inline">\(\operatorname{rank}(\mathbf{X}_c)\)</span> will be <span class="math inline">\(n\)</span>. It is clear that the rank of <span class="math inline">\(\mathbf{S}\)</span> won’t be larger than the rank of <span class="math inline">\(\mathbf{X}_c\)</span>. Since <span class="math inline">\(\mathbf{S}\)</span> is <span class="math inline">\(p \times p\)</span> and its rank is <span class="math inline">\(n\)</span>, <span class="math inline">\(\mathbf{S}\)</span> will be singular. That’s, if <span class="math inline">\(n&lt;p\)</span> then <span class="math inline">\(\operatorname{rank}(\mathbf{X})&lt;p\)</span> in which case <span class="math inline">\(\operatorname{rank}(\mathbf{S})&lt;p\)</span>.</p>
<p>This brought several novel precision estimators in applications. Generally, these novel estimators overcome the undersampling by maximization of the log-likelihood augmented with a so-called penalty. A penalty discourages large values among the elements of the precision matrix estimate. This reduces the risk of overfitting but also yields a well-defined penalized precision matrix estimator.</p>
<p>To solve the problem, as we have seen before in Section 6, penalized estimators adds a penalty to the likelihood functions ( <span class="math inline">\(\ell_2\)</span> in Ridge and <span class="math inline">\(\ell_1\)</span> in lasso) that makes the eigenvalues of <span class="math inline">\(\mathbf{S}\)</span> shrink in a particular manner to combat <span class="math inline">\(p \geq n\)</span>. The graphical lasso (gLasso) is the <span class="math inline">\(\ell_1\)</span>-equivalent to graphical ridge. A nice feature of the <span class="math inline">\(\ell_1\)</span> penalty automatically induces sparsity and thus also select the edges in the underlying graph. The <span class="math inline">\(\ell_2\)</span> penalty in Ridge relies on an extra step that selects the edges after the regularized precision matrix with shrunken correlations is estimated.</p>
<p>In this chapter we will see graphical ridge and lasso applications based on Gaussian graphical models that will provide sparse precision matrices in case of <span class="math inline">\(n&lt;p\)</span>.</p>
</div>
<div id="multivariate-gaussian-distribution" class="section level2 hasAnchor" number="27.7">
<h2><span class="header-section-number">27.7</span> Multivariate Gaussian Distribution<a href="graphical-network-analysis.html#multivariate-gaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before understanding <span class="math inline">\(\ell_1\)</span> or <span class="math inline">\(\ell_2\)</span> regularization, we need to see the multivariate Gaussian distribution, its parameterization and maximum likelihood estimation (MLE) solutions.</p>
<p>The multivariate Gaussian distribution of a random vector <span class="math inline">\(\mathbf{X} \in \mathbf{R}^{p}\)</span> is commonly expressed in terms of the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>, where <span class="math inline">\(\mu\)</span> is an <span class="math inline">\(p \times 1\)</span> vector and <span class="math inline">\(\Sigma\)</span> is an <span class="math inline">\(p \times p\)</span>, a nonsingular symmetric covariance matrix. Hence, we have the following form for the density function:</p>
<p><span class="math display">\[
f(x \mid \mu, \Sigma)=\frac{1}{(2 \pi)^{p / 2}|\Sigma|^{1 / 2}} \exp \left\{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right\},
\]</span>
where <span class="math inline">\(|\Sigma|\)</span> is the determinant of the covariance matrix. The likelihood function is:</p>
<p><span class="math display">\[
\mathcal{L}(\mu, \Sigma)=(2 \pi)^{-\frac{n p}{2}} \prod_{i=1}^{n} \operatorname{det}(\Sigma)^{-\frac{1}{2}} \exp \left(-\frac{1}{2}\left(x_{i}-\mu\right)^{\mathrm{T}} \Sigma^{-1}\left(x_{i}-\mu\right)\right)
\]</span>
Since the estimate <span class="math inline">\(\bar{x}\)</span> does not depend on <span class="math inline">\(\Sigma\)</span>, we can just substitute it for <span class="math inline">\(\mu\)</span> in the likelihood function,</p>
<p><span class="math display">\[
\mathcal{L}(\bar{x}, \Sigma) \propto \operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\left(x_{i}-\bar{x}\right)\right)
\]</span></p>
<p>We seek the value of <span class="math inline">\(\Sigma\)</span> that maximizes the likelihood of the data (in practice it is easier to work with <span class="math inline">\(\log \mathcal{L}\)</span> ). With the cyclical nature of trace,</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{L}(\bar{x}, \Sigma) &amp; \propto \operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \sum_{i=1}^{n}\left(\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\left(x_{i}-\bar{x}\right)\right)\right) \\
&amp;=\operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \sum_{i=1}^{n} \operatorname{tr}\left(\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\right)\right) \\
&amp;=\operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \operatorname{tr}\left(\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\right)\right) \\
&amp;=\operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \operatorname{tr}\left(S \Sigma^{-1}\right)\right)
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
S=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \in \mathbf{R}^{p \times p}
\]</span></p>
<p>And finally, we re-write the likelihood in the log form using the trace trick:</p>
<p><span class="math display">\[
\ln \mathcal{L}(\mu, \Sigma)=\text { const }-\frac{n}{2} \ln \operatorname{det}(\Sigma)-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1} \sum_{i=1}^{n}\left(x_{i}-\mu\right)\left(x_{i}-\mu\right)^{\mathrm{T}}\right]
\]</span></p>
<p>or, for a multivariate normal model with mean 0 and covariance <span class="math inline">\(\Sigma\)</span>, the likelihood function in this case is given by</p>
<p><span class="math display">\[
\ell(\Omega ; S)=\ln |\Omega|-\operatorname{tr}(S \Omega)
\]</span></p>
<p>where <span class="math inline">\(\Omega=\Sigma^{-1}\)</span> is the so-called precision matrix (also sometimes called the concentration matrix), which we want to estimate, which we will denote <span class="math inline">\(P\)</span>. Indeed, one can naturally try to use the inverse of <span class="math inline">\(S\)</span> for this.</p>
<p>For an intuitive way to see the whole algebra, let’s start with the general normal density</p>
<p><span class="math display">\[
\frac{1}{\sqrt{2 \pi}} \frac{1}{\sigma} \exp \left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right)
\]</span>
The log-likelihood is
<span class="math display">\[
\mathcal{L}(\mu, \sigma)=\text { A constant }-\frac{n}{2} \log \left(\sigma^{2}\right)-\frac{1}{2} \sum_{i=1}^{n}\left(\frac{x_{i}-\mu}{\sigma}\right)^{2},
\]</span>
maximization of which is equivalent to minimizing</p>
<p><span class="math display">\[
\mathcal{L}(\mu, \sigma)=n \log \left(\sigma^{2}\right)+\sum_{i=1}^{n}\left(\frac{x_{i}-\mu}{\sigma}\right)^{2}
\]</span></p>
<p>We can look at the general multivariate normal (MVN) density</p>
<p><span class="math display">\[
(\sqrt{2 \pi})^{-d}|\boldsymbol{\Sigma}|^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{t} \mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)
\]</span></p>
<p>Note that <span class="math inline">\(|\boldsymbol{\Sigma}|^{-1 / 2}\)</span>, which is the reciprocal of the square root of the determinant of the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, does what <span class="math inline">\(1 / \sigma\)</span> does in the univariate case. Moreover, <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> does what <span class="math inline">\(1 / \sigma^{2}\)</span> does in the univariate case.</p>
<p>The maximization of likelihood would lead to minimizing (analogous to the univariate case)</p>
<p><span class="math display">\[
n \log |\boldsymbol{\Sigma}|+\sum_{i=1}^{n}(\mathbf{x}-\boldsymbol{\mu})^{t} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})
\]</span></p>
<p>Again, <span class="math inline">\(n \log |\mathbf{\Sigma}|\)</span> takes the spot of <span class="math inline">\(n \log \left(\sigma^{2}\right)\)</span> which was there in the univariate case.</p>
<p>If the data is not high-dimensional, the estimations are simple. Let’s start with a data matrix of 10x6, where no need for regularization.</p>
<div class="sourceCode" id="cb856"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb856-1"><a href="graphical-network-analysis.html#cb856-1" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb856-2"><a href="graphical-network-analysis.html#cb856-2" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">6</span></span>
<span id="cb856-3"><a href="graphical-network-analysis.html#cb856-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span> (<span class="fu">rnorm</span>(n<span class="sc">*</span>p), n, p)</span>
<span id="cb856-4"><a href="graphical-network-analysis.html#cb856-4" tabindex="-1"></a></span>
<span id="cb856-5"><a href="graphical-network-analysis.html#cb856-5" tabindex="-1"></a><span class="co"># Cov. &amp; Precision Matrices</span></span>
<span id="cb856-6"><a href="graphical-network-analysis.html#cb856-6" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb856-7"><a href="graphical-network-analysis.html#cb856-7" tabindex="-1"></a>pm <span class="ot">&lt;-</span> <span class="fu">solve</span>(S) <span class="co"># precision</span></span>
<span id="cb856-8"><a href="graphical-network-analysis.html#cb856-8" tabindex="-1"></a></span>
<span id="cb856-9"><a href="graphical-network-analysis.html#cb856-9" tabindex="-1"></a><span class="sc">-</span>pm[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(<span class="fu">sqrt</span>(pm[<span class="dv">1</span>,<span class="dv">1</span>])<span class="sc">*</span><span class="fu">sqrt</span>(pm[<span class="dv">2</span>,<span class="dv">2</span>])) </span></code></pre></div>
<pre><code>## [1] -0.6059634</code></pre>
<div class="sourceCode" id="cb858"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb858-1"><a href="graphical-network-analysis.html#cb858-1" tabindex="-1"></a><span class="sc">-</span><span class="fu">cov2cor</span>(pm)</span></code></pre></div>
<pre><code>##             [,1]       [,2]       [,3]       [,4]       [,5]        [,6]
## [1,] -1.00000000 -0.6059634  0.1947369 -0.4292779  0.3289393  0.06004998
## [2,] -0.60596342 -1.0000000 -0.3477352 -0.7813197  0.6814685  0.50178834
## [3,]  0.19473691 -0.3477352 -1.0000000 -0.1892090  0.0775629  0.40192145
## [4,] -0.42927791 -0.7813197 -0.1892090 -1.0000000  0.8678627  0.27524750
## [5,]  0.32893932  0.6814685  0.0775629  0.8678627 -1.0000000 -0.39719667
## [6,]  0.06004998  0.5017883  0.4019214  0.2752475 -0.3971967 -1.00000000</code></pre>
<div class="sourceCode" id="cb860"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb860-1"><a href="graphical-network-analysis.html#cb860-1" tabindex="-1"></a><span class="co"># ppcor</span></span>
<span id="cb860-2"><a href="graphical-network-analysis.html#cb860-2" tabindex="-1"></a>pc <span class="ot">&lt;-</span> ppcor<span class="sc">::</span><span class="fu">pcor</span>(X)</span>
<span id="cb860-3"><a href="graphical-network-analysis.html#cb860-3" tabindex="-1"></a>pc<span class="sc">$</span>estimate</span></code></pre></div>
<pre><code>##             [,1]       [,2]       [,3]       [,4]       [,5]        [,6]
## [1,]  1.00000000 -0.6059634  0.1947369 -0.4292779  0.3289393  0.06004998
## [2,] -0.60596342  1.0000000 -0.3477352 -0.7813197  0.6814685  0.50178834
## [3,]  0.19473691 -0.3477352  1.0000000 -0.1892090  0.0775629  0.40192145
## [4,] -0.42927791 -0.7813197 -0.1892090  1.0000000  0.8678627  0.27524750
## [5,]  0.32893932  0.6814685  0.0775629  0.8678627  1.0000000 -0.39719667
## [6,]  0.06004998  0.5017883  0.4019214  0.2752475 -0.3971967  1.00000000</code></pre>
<div class="sourceCode" id="cb862"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb862-1"><a href="graphical-network-analysis.html#cb862-1" tabindex="-1"></a><span class="co"># glasso</span></span>
<span id="cb862-2"><a href="graphical-network-analysis.html#cb862-2" tabindex="-1"></a>glassoFast<span class="sc">::</span><span class="fu">glassoFast</span>(S,<span class="at">rho=</span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## $w
##             [,1]        [,2]       [,3]        [,4]        [,5]       [,6]
## [1,]  0.60140347 -0.37332949  0.3171606  0.02177682 -0.13790645 -0.1327159
## [2,] -0.37332949  0.50242861 -0.2405377 -0.25645323  0.01832675  0.2601432
## [3,]  0.31716062 -0.24053766  0.5735265 -0.09239610 -0.28453786  0.1190710
## [4,]  0.02177682 -0.25645323 -0.0923961  0.66021163  0.65364259 -0.3052422
## [5,] -0.13790645  0.01832675 -0.2845379  0.65364259  1.10471906 -0.3465787
## [6,] -0.13271587  0.26014323  0.1190710 -0.30524217 -0.34657873  0.7010335
## 
## $wi
##            [,1]      [,2]       [,3]      [,4]       [,5]       [,6]
## [1,]  4.3912715  4.562946 -0.7541076  2.932966 -1.5229061 -0.2098044
## [2,]  4.5629462 12.914469  2.3082064  9.154998 -5.4111089 -3.0096715
## [3,] -0.7541076  2.308206  3.4125042  1.139186 -0.3163017 -1.2392019
## [4,]  2.9329659  9.154998  1.1391865 10.631981 -6.2528743 -1.4976793
## [5,] -1.5229061 -5.411109 -0.3163017 -6.252874  4.8825940  1.4647642
## [6,] -0.2098044 -3.009672 -1.2392019 -1.497679  1.4647642  2.7860635
## 
## $errflag
## [1] 0
## 
## $niter
## [1] 1</code></pre>
<div class="sourceCode" id="cb864"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb864-1"><a href="graphical-network-analysis.html#cb864-1" tabindex="-1"></a>Rl <span class="ot">&lt;-</span> glassoFast<span class="sc">::</span><span class="fu">glassoFast</span>(S,<span class="at">rho=</span><span class="dv">0</span>)<span class="sc">$</span>wi <span class="co">#</span></span>
<span id="cb864-2"><a href="graphical-network-analysis.html#cb864-2" tabindex="-1"></a><span class="sc">-</span>Rl[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(<span class="fu">sqrt</span>(Rl[<span class="dv">1</span>,<span class="dv">1</span>])<span class="sc">*</span><span class="fu">sqrt</span>(Rl[<span class="dv">2</span>,<span class="dv">2</span>])) </span></code></pre></div>
<pre><code>## [1] -0.6059153</code></pre>
<div class="sourceCode" id="cb866"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb866-1"><a href="graphical-network-analysis.html#cb866-1" tabindex="-1"></a><span class="sc">-</span><span class="fu">cov2cor</span>(Rl)</span></code></pre></div>
<pre><code>##             [,1]       [,2]        [,3]       [,4]        [,5]        [,6]
## [1,] -1.00000000 -0.6059153  0.19480566 -0.4292445  0.32889151  0.05998241
## [2,] -0.60591532 -1.0000000 -0.34769605 -0.7812911  0.68143226  0.50174762
## [3,]  0.19480566 -0.3476961 -1.00000000 -0.1891260  0.07748893  0.40189255
## [4,] -0.42924454 -0.7812911 -0.18912595 -1.0000000  0.86785527  0.27517959
## [5,]  0.32889151  0.6814323  0.07748893  0.8678553 -1.00000000 -0.39714298
## [6,]  0.05998241  0.5017476  0.40189255  0.2751796 -0.39714298 -1.00000000</code></pre>
</div>
<div id="high-dimensional-data" class="section level2 hasAnchor" number="27.8">
<h2><span class="header-section-number">27.8</span> High-dimensional data<a href="graphical-network-analysis.html#high-dimensional-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now with a data matrix of 6x10:</p>
<div class="sourceCode" id="cb868"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb868-1"><a href="graphical-network-analysis.html#cb868-1" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">6</span></span>
<span id="cb868-2"><a href="graphical-network-analysis.html#cb868-2" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb868-3"><a href="graphical-network-analysis.html#cb868-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb868-4"><a href="graphical-network-analysis.html#cb868-4" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span> (<span class="fu">rnorm</span>(n<span class="sc">*</span>p), n, p)</span>
<span id="cb868-5"><a href="graphical-network-analysis.html#cb868-5" tabindex="-1"></a></span>
<span id="cb868-6"><a href="graphical-network-analysis.html#cb868-6" tabindex="-1"></a><span class="co"># Cov. &amp; Precision Matrices</span></span>
<span id="cb868-7"><a href="graphical-network-analysis.html#cb868-7" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb868-8"><a href="graphical-network-analysis.html#cb868-8" tabindex="-1"></a>S</span></code></pre></div>
<pre><code>##               [,1]        [,2]        [,3]        [,4]         [,5]        [,6]
##  [1,]  0.889211221 -0.17223814 -0.36660043  0.35320957 -0.629545741 -0.27978848
##  [2,] -0.172238139  0.34416306 -0.09280183 -0.04282613  0.139236591 -0.26060435
##  [3,] -0.366600426 -0.09280183  1.46701338 -0.50796342 -0.024550727 -0.11504405
##  [4,]  0.353209573 -0.04282613 -0.50796342  1.24117592 -0.292005017  0.42646139
##  [5,] -0.629545741  0.13923659 -0.02455073 -0.29200502  0.553562287  0.26275658
##  [6,] -0.279788479 -0.26060435 -0.11504405  0.42646139  0.262756584  0.81429052
##  [7,]  0.143364328 -0.14895377  0.29598156  0.30839120 -0.275296303  0.04418159
##  [8,] -0.273835576  0.17201439 -0.31052657 -0.39667581  0.376175973 -0.02536104
##  [9,] -0.008919669  0.24390178 -0.50198614  0.52741301  0.008044799 -0.01297542
## [10,] -0.304722895  0.33936685 -1.08854590  0.20441696  0.499437080  0.20218868
##              [,7]        [,8]         [,9]      [,10]
##  [1,]  0.14336433 -0.27383558 -0.008919669 -0.3047229
##  [2,] -0.14895377  0.17201439  0.243901782  0.3393668
##  [3,]  0.29598156 -0.31052657 -0.501986137 -1.0885459
##  [4,]  0.30839120 -0.39667581  0.527413006  0.2044170
##  [5,] -0.27529630  0.37617597  0.008044799  0.4994371
##  [6,]  0.04418159 -0.02536104 -0.012975416  0.2021887
##  [7,]  0.37576405 -0.40476558  0.046294293 -0.4691147
##  [8,] -0.40476558  0.46612332 -0.026813818  0.5588965
##  [9,]  0.04629429 -0.02681382  0.540956259  0.5036908
## [10,] -0.46911465  0.55889647  0.503690786  1.3107637</code></pre>
<div class="sourceCode" id="cb870"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb870-1"><a href="graphical-network-analysis.html#cb870-1" tabindex="-1"></a><span class="fu">try</span>(<span class="fu">solve</span>(S), <span class="at">silent =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## Error in solve.default(S) : 
##   system is computationally singular: reciprocal condition number = 1.04542e-18</code></pre>
<p>The standard definition for the inverse of a matrix fails if the matrix is not square or singular. However, one can generalize the inverse using singular value decomposition. Any rectangular real matrix <span class="math inline">\(\mathbf{M}\)</span> can be decomposed as <span class="math inline">\(\mathbf{M=U \Sigma V^{&#39;}}\)</span>, where <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are orthogonal and <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix containing only the positive singular values. The pseudoinverse, also known as <strong>Moore-Penrose</strong> or generalized inverse is then obtained as</p>
<p><span class="math display">\[
\mathbf{M^+} = \mathbf{V \Sigma^{-1} U&#39;}
\]</span></p>
<p>Don’t be confused due to notation: <span class="math inline">\(\Sigma\)</span> is not the covariance matrix here</p>
<p>With using the method of generalized inverse by <code>ppcor</code> and <code>corpcor</code>:</p>
<div class="sourceCode" id="cb872"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb872-1"><a href="graphical-network-analysis.html#cb872-1" tabindex="-1"></a>Si <span class="ot">&lt;-</span> corpcor<span class="sc">::</span><span class="fu">pseudoinverse</span>(S)</span>
<span id="cb872-2"><a href="graphical-network-analysis.html#cb872-2" tabindex="-1"></a><span class="sc">-</span>Si[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(<span class="fu">sqrt</span>(Si[<span class="dv">1</span>,<span class="dv">1</span>])<span class="sc">*</span><span class="fu">sqrt</span>(Si[<span class="dv">2</span>,<span class="dv">2</span>])) </span></code></pre></div>
<pre><code>## [1] -0.4823509</code></pre>
<div class="sourceCode" id="cb874"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb874-1"><a href="graphical-network-analysis.html#cb874-1" tabindex="-1"></a><span class="co"># ppcor</span></span>
<span id="cb874-2"><a href="graphical-network-analysis.html#cb874-2" tabindex="-1"></a>pc <span class="ot">&lt;-</span> ppcor<span class="sc">::</span><span class="fu">pcor</span>(X)</span></code></pre></div>
<pre><code>## Warning in ppcor::pcor(X): The inverse of variance-covariance matrix is
## calculated using Moore-Penrose generalized matrix invers due to its determinant
## of zero.</code></pre>
<pre><code>## Warning in sqrt((n - 2 - gp)/(1 - pcor^2)): NaNs produced</code></pre>
<div class="sourceCode" id="cb877"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb877-1"><a href="graphical-network-analysis.html#cb877-1" tabindex="-1"></a>pc<span class="sc">$</span>estimate</span></code></pre></div>
<pre><code>##              [,1]        [,2]        [,3]       [,4]        [,5]          [,6]
##  [1,]  1.00000000 -0.48235089 -0.43471080 -0.6132218  0.59239395 -0.1515785108
##  [2,] -0.48235089  1.00000000 -0.85835176 -0.7984656  0.08341783  0.1922476120
##  [3,] -0.43471080 -0.85835176  1.00000000 -0.8107355 -0.06073205 -0.1395456329
##  [4,] -0.61322177 -0.79846556 -0.81073546  1.0000000  0.11814582 -0.3271223659
##  [5,]  0.59239395  0.08341783 -0.06073205  0.1181458  1.00000000 -0.4056046405
##  [6,] -0.15157851  0.19224761 -0.13954563 -0.3271224 -0.40560464  1.0000000000
##  [7,]  0.81227748  0.76456650  0.76563183  0.7861380 -0.07927500  0.2753626258
##  [8,] -0.74807903 -0.67387820 -0.64812735 -0.6321303 -0.04063566 -0.2660628754
##  [9,]  0.79435763  0.32542381  0.52481792  0.5106454 -0.08284875  0.5458020595
## [10,]  0.01484899 -0.34289348  0.01425498 -0.2181704 -0.41275254  0.0006582396
##             [,7]        [,8]        [,9]         [,10]
##  [1,]  0.8122775 -0.74807903  0.79435763  0.0148489929
##  [2,]  0.7645665 -0.67387820  0.32542381 -0.3428934821
##  [3,]  0.7656318 -0.64812735  0.52481792  0.0142549759
##  [4,]  0.7861380 -0.63213032  0.51064540 -0.2181703890
##  [5,] -0.0792750 -0.04063566 -0.08284875 -0.4127525424
##  [6,]  0.2753626 -0.26606288  0.54580206  0.0006582396
##  [7,]  1.0000000  0.96888026 -0.84167300  0.2703213517
##  [8,]  0.9688803  1.00000000  0.84455999 -0.3746342510
##  [9,] -0.8416730  0.84455999  1.00000000 -0.0701428715
## [10,]  0.2703214 -0.37463425 -0.07014287  1.0000000000</code></pre>
<div class="sourceCode" id="cb879"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb879-1"><a href="graphical-network-analysis.html#cb879-1" tabindex="-1"></a><span class="co"># corpcor with pseudo inverse</span></span>
<span id="cb879-2"><a href="graphical-network-analysis.html#cb879-2" tabindex="-1"></a>corpcor<span class="sc">::</span><span class="fu">cor2pcor</span>(S)</span></code></pre></div>
<pre><code>##              [,1]        [,2]        [,3]       [,4]        [,5]          [,6]
##  [1,]  1.00000000 -0.48235089 -0.43471080 -0.6132218  0.59239395 -0.1515785108
##  [2,] -0.48235089  1.00000000 -0.85835176 -0.7984656  0.08341783  0.1922476120
##  [3,] -0.43471080 -0.85835176  1.00000000 -0.8107355 -0.06073205 -0.1395456329
##  [4,] -0.61322177 -0.79846556 -0.81073546  1.0000000  0.11814582 -0.3271223659
##  [5,]  0.59239395  0.08341783 -0.06073205  0.1181458  1.00000000 -0.4056046405
##  [6,] -0.15157851  0.19224761 -0.13954563 -0.3271224 -0.40560464  1.0000000000
##  [7,]  0.81227748  0.76456650  0.76563183  0.7861380 -0.07927500  0.2753626258
##  [8,] -0.74807903 -0.67387820 -0.64812735 -0.6321303 -0.04063566 -0.2660628754
##  [9,]  0.79435763  0.32542381  0.52481792  0.5106454 -0.08284875  0.5458020595
## [10,]  0.01484899 -0.34289348  0.01425498 -0.2181704 -0.41275254  0.0006582396
##             [,7]        [,8]        [,9]         [,10]
##  [1,]  0.8122775 -0.74807903  0.79435763  0.0148489929
##  [2,]  0.7645665 -0.67387820  0.32542381 -0.3428934821
##  [3,]  0.7656318 -0.64812735  0.52481792  0.0142549759
##  [4,]  0.7861380 -0.63213032  0.51064540 -0.2181703890
##  [5,] -0.0792750 -0.04063566 -0.08284875 -0.4127525424
##  [6,]  0.2753626 -0.26606288  0.54580206  0.0006582396
##  [7,]  1.0000000  0.96888026 -0.84167300  0.2703213517
##  [8,]  0.9688803  1.00000000  0.84455999 -0.3746342510
##  [9,] -0.8416730  0.84455999  1.00000000 -0.0701428715
## [10,]  0.2703214 -0.37463425 -0.07014287  1.0000000000</code></pre>
<p>However, we know from Chapter 29 that these solutions are not stable. Further, we also want to identify the sparsity in the precision matrix that differentiates the significant edges from insignificant ones for a network analysis .</p>
</div>
<div id="ridge-ell_2-and-glasso-ell_1" class="section level2 hasAnchor" number="27.9">
<h2><span class="header-section-number">27.9</span> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)<a href="graphical-network-analysis.html#ridge-ell_2-and-glasso-ell_1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A contemporary use for precision matrices is found in network reconstruction through graphical modeling (Network Analysis).</p>
<p>In a multivariate normal model, <span class="math inline">\(p_{i j}=p_{j i}=0\)</span> (the entries in the precision matrix) if and only if <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(X_{j}\)</span> are independent when condition ong all other variables. In real world applications, <span class="math inline">\(P\)</span> (the precision matrix) is often relatively sparse with lots of zeros. With the close relationship between <span class="math inline">\(P\)</span> and the partial correlations, <strong>the non-zero entries of the precision matrix can be interpreted the edges of a graph where nodes correspond to the variables.</strong></p>
<p>Regularization helps us find the sparsified partial correlation matrix. We first start with Ridge and <code>rags2ridges</code> (see, <a href="https://cran.r-project.org/web/packages/rags2ridges/vignettes/rags2ridges.html">Introduction to rags2ridges</a>), which is an R-package for fast and proper <span class="math inline">\(\ell_{2}\)</span>-penalized estimation of precision (and covariance) matrices also called ridge estimation.</p>
<p>Their algorithm solves the following:</p>
<p><span class="math display">\[
\ell(\Omega ; S)=\ln |\Omega|-\operatorname{tr}(S \Omega)-\frac{\lambda}{2}\|\Omega-T\|_{2}^{2}
\]</span></p>
<p>where <span class="math inline">\(\lambda&gt;0\)</span> is the ridge penalty parameter, <span class="math inline">\(T\)</span> is a <span class="math inline">\(p \times p\)</span> known target matrix and <span class="math inline">\(\|\cdot\|_{2}\)</span> is the <span class="math inline">\(\ell_{2}\)</span>-norm. Assume for now the target matrix is an all zero matrix and thus out of the equation. The core function of <code>rags2ridges</code> is <code>ridgeP</code> which computes this estimate in a fast manner.</p>
<p>Let’s try some simulations:</p>
<div class="sourceCode" id="cb881"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb881-1"><a href="graphical-network-analysis.html#cb881-1" tabindex="-1"></a><span class="fu">library</span>(rags2ridges)</span>
<span id="cb881-2"><a href="graphical-network-analysis.html#cb881-2" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb881-3"><a href="graphical-network-analysis.html#cb881-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb881-4"><a href="graphical-network-analysis.html#cb881-4" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">createS</span>(<span class="at">n =</span> n, <span class="at">p =</span> p, <span class="at">dataset =</span> <span class="cn">TRUE</span>)</span>
<span id="cb881-5"><a href="graphical-network-analysis.html#cb881-5" tabindex="-1"></a></span>
<span id="cb881-6"><a href="graphical-network-analysis.html#cb881-6" tabindex="-1"></a><span class="co"># Cov. &amp; Precision Matrices</span></span>
<span id="cb881-7"><a href="graphical-network-analysis.html#cb881-7" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb881-8"><a href="graphical-network-analysis.html#cb881-8" tabindex="-1"></a>S</span></code></pre></div>
<pre><code>##             A           B           C           D           E          F
## A  0.45682789 -0.11564467  0.13200583 -0.01595920  0.09809975 0.01702341
## B -0.11564467  0.55871526 -0.06301115  0.12714447  0.16007573 0.01767518
## C  0.13200583 -0.06301115  0.85789870 -0.03128875 -0.05379863 0.13134788
## D -0.01595920  0.12714447 -0.03128875  0.99469250  0.03927349 0.10959642
## E  0.09809975  0.16007573 -0.05379863  0.03927349  0.91136419 0.02529372
## F  0.01702341  0.01767518  0.13134788  0.10959642  0.02529372 1.27483389</code></pre>
<div class="sourceCode" id="cb883"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb883-1"><a href="graphical-network-analysis.html#cb883-1" tabindex="-1"></a><span class="fu">try</span>(<span class="fu">solve</span>(S), <span class="at">silent =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>##              A           B           C           D           E            F
## A  2.534157787  0.60434887 -0.37289623 -0.03330977 -0.39969959  0.006995153
## B  0.604348874  2.09324877  0.02734562 -0.23919383 -0.42049199 -0.011003651
## C -0.372896230  0.02734562  1.25338509  0.03989939  0.11121753 -0.130174434
## D -0.033309770 -0.23919383  0.03989939  1.04638061  0.00537128 -0.090412788
## E -0.399699586 -0.42049199  0.11121753  0.00537128  1.22116416 -0.024982169
## F  0.006995153 -0.01100365 -0.13017443 -0.09041279 -0.02498217  0.806155504</code></pre>
<div class="sourceCode" id="cb885"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb885-1"><a href="graphical-network-analysis.html#cb885-1" tabindex="-1"></a>P <span class="ot">&lt;-</span> rags2ridges<span class="sc">::</span><span class="fu">ridgeP</span>(S, <span class="at">lambda =</span> <span class="fl">0.0001</span>)</span>
<span id="cb885-2"><a href="graphical-network-analysis.html#cb885-2" tabindex="-1"></a>P</span></code></pre></div>
<pre><code>## A 6 x 6 ridge precision matrix estimate with lambda = 0.000100
##              A           B           C            D            E            F
## A  2.533115451  0.60366542 -0.37265274 -0.033220588 -0.399324682  0.006973044
## B  0.603665423  2.09268463  0.02745898 -0.239097683 -0.420206304 -0.011013671
## C -0.372652744  0.02745898  1.25336484  0.039885330  0.111143219 -0.130167968
## D -0.033220588 -0.23909768  0.03988533  1.046411061  0.005328985 -0.090412858
## E -0.399324682 -0.42020630  0.11114322  0.005328985  1.221068947 -0.024975879
## F  0.006973044 -0.01101367 -0.13016797 -0.090412858 -0.024975879  0.806196516</code></pre>
<div class="sourceCode" id="cb887"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb887-1"><a href="graphical-network-analysis.html#cb887-1" tabindex="-1"></a><span class="fu">library</span>(rags2ridges)</span>
<span id="cb887-2"><a href="graphical-network-analysis.html#cb887-2" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb887-3"><a href="graphical-network-analysis.html#cb887-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb887-4"><a href="graphical-network-analysis.html#cb887-4" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">createS</span>(<span class="at">n =</span> n, <span class="at">p =</span> p, <span class="at">dataset =</span> <span class="cn">TRUE</span>)</span>
<span id="cb887-5"><a href="graphical-network-analysis.html#cb887-5" tabindex="-1"></a></span>
<span id="cb887-6"><a href="graphical-network-analysis.html#cb887-6" tabindex="-1"></a><span class="co"># Cov. &amp; Precision Matrices</span></span>
<span id="cb887-7"><a href="graphical-network-analysis.html#cb887-7" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb887-8"><a href="graphical-network-analysis.html#cb887-8" tabindex="-1"></a><span class="fu">try</span>(<span class="fu">solve</span>(S), <span class="at">silent =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## Error in solve.default(S) : 
##   system is computationally singular: reciprocal condition number = 1.2598e-18</code></pre>
<div class="sourceCode" id="cb889"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb889-1"><a href="graphical-network-analysis.html#cb889-1" tabindex="-1"></a>P <span class="ot">&lt;-</span> rags2ridges<span class="sc">::</span><span class="fu">ridgeP</span>(S, <span class="at">lambda =</span> <span class="fl">1.17</span>)</span>
<span id="cb889-2"><a href="graphical-network-analysis.html#cb889-2" tabindex="-1"></a>P[<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>]</span></code></pre></div>
<pre><code>##              A           B           C            D          E           F
## A  2.743879476 -0.03541676 -0.01830371  0.008774811 -0.1056438  0.01539484
## B -0.035416755  2.63060175  0.23945569  0.088696164 -0.2786984 -0.29657059
## C -0.018303709  0.23945569  2.55818158 -0.092298329  0.1512445  0.08314785
## D  0.008774811  0.08869616 -0.09229833  2.373307290  0.3717918  0.01829917
## E -0.105643841 -0.27869839  0.15124449  0.371791841  2.3048669 -0.32627382
## F  0.015394836 -0.29657059  0.08314785  0.018299166 -0.3262738  2.79070578
## G -0.059760460 -0.18022734 -0.08924614 -0.149071791 -0.1574611  0.06178467
##             G
## A -0.05976046
## B -0.18022734
## C -0.08924614
## D -0.14907179
## E -0.15746109
## F  0.06178467
## G  2.61837378</code></pre>
<p>What Lambda should we choose? One strategy for choosing <span class="math inline">\(\lambda\)</span> is selecting it to be stable yet precise (a bias-variance trade-off). Automatic k-fold cross-validation can be done with <code>optPenalty.kCVauto()</code> is well suited for this.</p>
<div class="sourceCode" id="cb891"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb891-1"><a href="graphical-network-analysis.html#cb891-1" tabindex="-1"></a>opt <span class="ot">&lt;-</span> <span class="fu">optPenalty.kCVauto</span>(X, <span class="at">lambdaMin =</span> <span class="fl">0.001</span>, <span class="at">lambdaMax =</span> <span class="dv">100</span>)</span>
<span id="cb891-2"><a href="graphical-network-analysis.html#cb891-2" tabindex="-1"></a><span class="fu">str</span>(opt)</span></code></pre></div>
<pre><code>## List of 2
##  $ optLambda: num 0.721
##  $ optPrec  : &#39;ridgeP&#39; num [1:25, 1:25] 2.7894 -0.0521 -0.0324 0.0211 -0.1459 ...
##   ..- attr(*, &quot;lambda&quot;)= num 0.721
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:25] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ...
##   .. ..$ : chr [1:25] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ...</code></pre>
<div class="sourceCode" id="cb893"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb893-1"><a href="graphical-network-analysis.html#cb893-1" tabindex="-1"></a>op <span class="ot">&lt;-</span> opt<span class="sc">$</span>optLambda</span></code></pre></div>
<p>We know that Ridge will not provide a sparse solution. Yet, we need a sparse precision matrix for network analysis. The <span class="math inline">\(\ell_{2}\)</span> penalty of <code>rags2ridges</code> relies on an extra step that selects the edges after the precision matrix is estimated. The extra step is explained in their <a href="https://www.sciencedirect.com/science/article/pii/S0167947316301141">paper</a> (van Wieringen, W.N. and Peeters, C.F.W., 2016):</p>
<blockquote>
<p>While some may argue this as a drawback (typically due to a lack of perceived simplicity), it is often beneficial to separate the “variable selection” and estimation.</p>
</blockquote>
<blockquote>
<p>First, a separate post-hoc selection step allows for greater flexibility. Secondly, when co-linearity is present the L1 penalty is “unstable” in the selection between the items, i.e, if 2 covariances are co-linear only one of them will typically be selected in a unpredictable way whereas the L2 will put equal weight on both and “average” their effect. Ultimately, this means that the L2 estimate is typically more stable than the L1.</p>
</blockquote>
<blockquote>
<p>At last point to mention here is also that the true underlying graph might not always be very sparse (or sparse at all).</p>
</blockquote>
<p>The function <code>spasify()</code> handles the the spasification by applying the FDR (False Discovery Rate) method:</p>
<div class="sourceCode" id="cb894"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb894-1"><a href="graphical-network-analysis.html#cb894-1" tabindex="-1"></a>P <span class="ot">&lt;-</span> <span class="fu">ridgeP</span>(S, <span class="at">lambda =</span> op)</span>
<span id="cb894-2"><a href="graphical-network-analysis.html#cb894-2" tabindex="-1"></a>spar <span class="ot">&lt;-</span> <span class="fu">sparsify</span>(P, <span class="at">threshold =</span> <span class="st">&quot;localFDR&quot;</span>)</span></code></pre></div>
<pre><code>## Step 1... determine cutoff point
## Step 2... estimate parameters of null distribution and eta0
## Step 3... compute p-values and estimate empirical PDF/CDF
## Step 4... compute q-values and local fdr
## Step 5... prepare for plotting</code></pre>
<p><img src="27-GraphicalNetworkAnalysis_files/figure-html/rn7-1.png" width="672" /></p>
<pre><code>## 
## - Retained elements:  0 
## - Corresponding to 0 % of possible edges 
## </code></pre>
<div class="sourceCode" id="cb897"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb897-1"><a href="graphical-network-analysis.html#cb897-1" tabindex="-1"></a>spar</span></code></pre></div>
<pre><code>## $sparseParCor
## A 25 x 25 ridge precision matrix estimate with lambda = 0.721308
##   A B C D E F …
## A 1 0 0 0 0 0 …
## B 0 1 0 0 0 0 …
## C 0 0 1 0 0 0 …
## D 0 0 0 1 0 0 …
## E 0 0 0 0 1 0 …
## F 0 0 0 0 0 1 …
## … 19 more rows and 19 more columns
## 
## $sparsePrecision
## A 25 x 25 ridge precision matrix estimate with lambda = 0.721308
##          A        B        C        D        E        F …
## A 2.626295 0.000000 0.000000 0.000000 0.000000 0.000000 …
## B 0.000000 2.528829 0.000000 0.000000 0.000000 0.000000 …
## C 0.000000 0.000000 2.409569 0.000000 0.000000 0.000000 …
## D 0.000000 0.000000 0.000000 2.179168 0.000000 0.000000 …
## E 0.000000 0.000000 0.000000 0.000000 2.145443 0.000000 …
## F 0.000000 0.000000 0.000000 0.000000 0.000000 2.724853 …
## … 19 more rows and 19 more columns</code></pre>
<p>The steps are explained in their paper. After edge selections, <code>GGMnetworkStats()</code> can be utilized to get summary statistics of the resulting graph topology:</p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb899-1"><a href="graphical-network-analysis.html#cb899-1" tabindex="-1"></a>fc <span class="ot">&lt;-</span> <span class="fu">GGMnetworkStats</span>(P)</span>
<span id="cb899-2"><a href="graphical-network-analysis.html#cb899-2" tabindex="-1"></a>fc</span></code></pre></div>
<pre><code>## $degree
##  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y 
## 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 
## 
## $betweenness
## A B C D E F G H I J K L M N O P Q R S T U V W X Y 
## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
## 
## $closeness
##          A          B          C          D          E          F          G 
## 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 
##          H          I          J          K          L          M          N 
## 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 
##          O          P          Q          R          S          T          U 
## 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 
##          V          W          X          Y 
## 0.04166667 0.04166667 0.04166667 0.04166667 
## 
## $eigenCentrality
##   A   B   C   D   E   F   G   H   I   J   K   L   M   N   O   P   Q   R   S   T 
## 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 
##   U   V   W   X   Y 
## 0.2 0.2 0.2 0.2 0.2 
## 
## $nNeg
##  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y 
## 13 14 10 13 13 12 17 13 14 15  9  9 16  9 13 11 10 16  6 12  9  9 13 14 16 
## 
## $nPos
##  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y 
## 11 10 14 11 11 12  7 11 10  9 15 15  8 15 11 13 14  8 18 12 15 15 11 10  8 
## 
## $chordal
## [1] TRUE
## 
## $mutualInfo
##         A         B         C         D         E         F         G         H 
## 0.1807197 0.4154956 0.3812980 0.4185954 0.5532431 0.3059883 0.4113305 0.3120867 
##         I         J         K         L         M         N         O         P 
## 0.2574861 0.5072752 0.7451307 0.3550588 0.3779470 0.4719414 0.3452659 0.3017873 
##         Q         R         S         T         U         V         W         X 
## 0.2619416 0.4744925 0.1594554 0.1324863 0.2547769 0.2546357 0.2225756 0.2803463 
##         Y 
## 0.1775594 
## 
## $variance
##         A         B         C         D         E         F         G         H 
## 0.4561861 0.5991397 0.6076539 0.6974337 0.8105009 0.4983632 0.6001093 0.4855818 
##         I         J         K         L         M         N         O         P 
## 0.4370395 0.6095917 0.9474860 0.6172182 0.7700104 0.7586031 0.6285586 0.4753553 
##         Q         R         S         T         U         V         W         X 
## 0.5676784 0.7166316 0.4281188 0.4160550 0.5350846 0.5846058 0.5149757 0.4866827 
##         Y 
## 0.4732289 
## 
## $partialVar
##         A         B         C         D         E         F         G         H 
## 0.3807646 0.3954400 0.4150120 0.4588907 0.4661042 0.3669923 0.3977332 0.3554061 
##         I         J         K         L         M         N         O         P 
## 0.3378282 0.3670559 0.4497453 0.4327515 0.5276626 0.4732091 0.4450397 0.3515231 
##         Q         R         S         T         U         V         W         X 
## 0.4368603 0.4458887 0.3650175 0.3644288 0.4147384 0.4531858 0.4122146 0.3676995 
##         Y 
## 0.3962399</code></pre>
<p>While the <span class="math inline">\(\ell_{2}\)</span> penalty of graphical ridge relies on an extra step to select the edges after <span class="math inline">\(P\)</span> is estimated, the graphical lasso (<code>gLasso</code>) is the <span class="math inline">\(\ell_{1}\)</span>-equivalent to graphical ridge, where the <span class="math inline">\(\ell_{1}\)</span> penalty automatically induces sparsity and select the edges in the underlying graph.</p>
<p>The graphical lasso aims to solve the following regularized maximum likelihood problem:</p>
<p><span class="math display">\[
\mathcal{L}(\Omega)=\operatorname{tr}(\Omega S)-\log |\Omega|+\lambda\|\Omega\|_1
\]</span></p>
<div class="sourceCode" id="cb901"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb901-1"><a href="graphical-network-analysis.html#cb901-1" tabindex="-1"></a>gl <span class="ot">&lt;-</span> glasso<span class="sc">::</span><span class="fu">glasso</span>(S, <span class="at">rho =</span> <span class="fl">0.2641</span>, <span class="at">approx =</span> <span class="cn">FALSE</span>)[<span class="fu">c</span>(<span class="st">&#39;w&#39;</span>, <span class="st">&#39;wi&#39;</span>)]</span>
<span id="cb901-2"><a href="graphical-network-analysis.html#cb901-2" tabindex="-1"></a><span class="sc">-</span> <span class="fu">cov2cor</span>(gl<span class="sc">$</span>wi)[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span></code></pre></div>
<pre><code>##       [,1]        [,2]        [,3]       [,4]        [,5]       [,6]
##  [1,]   -1  0.00000000  0.00000000  0.0000000  0.00000000  0.0000000
##  [2,]    0 -1.00000000 -0.07439378  0.0000000  0.09011239  0.1280014
##  [3,]    0 -0.07439164 -1.00000000  0.0000000  0.00000000  0.0000000
##  [4,]    0  0.00000000  0.00000000 -1.0000000 -0.15354175  0.0000000
##  [5,]    0  0.09011181  0.00000000 -0.1535415 -1.00000000  0.1502234
##  [6,]    0  0.12800152  0.00000000  0.0000000  0.15022332 -1.0000000
##  [7,]    0  0.00000000  0.00000000  0.0000000  0.00000000  0.0000000
##  [8,]    0  0.00000000  0.00000000  0.0000000  0.10986140  0.0000000
##  [9,]    0  0.00000000 -0.09170730  0.0000000  0.26299733  0.0000000
## [10,]    0  0.00000000  0.20156741  0.0000000  0.00000000  0.0000000
##               [,7]         [,8]        [,9]       [,10]
##  [1,]  0.000000000  0.000000000  0.00000000  0.00000000
##  [2,]  0.000000000  0.000000000  0.00000000  0.00000000
##  [3,]  0.000000000  0.000000000 -0.09170577  0.20156678
##  [4,]  0.000000000  0.000000000  0.00000000  0.00000000
##  [5,]  0.000000000  0.109861399  0.26299730  0.00000000
##  [6,]  0.000000000  0.000000000  0.00000000  0.00000000
##  [7,] -1.000000000 -0.007621682  0.00000000  0.05018338
##  [8,] -0.007621648 -1.000000000  0.00000000  0.00000000
##  [9,]  0.000000000  0.000000000 -1.00000000  0.00000000
## [10,]  0.050183134  0.000000000  0.00000000 -1.00000000</code></pre>
<p>The <code>glasso</code> package does not provide an option for tuning parameter selection. In practice, users apply can be done by cross-validation and eBIC. There are also multiple packages and function to plot the networks for a visual inspection.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="deep-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="decompositions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuk/machinemetrics/edit/master/27-GraphicalNetworkAnalysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
