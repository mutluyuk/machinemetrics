<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 17 Ensemble Methods | MachineMetrics</title>
  <meta name="description" content="Chapter 17 Ensemble Methods | MachineMetrics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 17 Ensemble Methods | MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 17 Ensemble Methods | MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression-trees.html"/>
<link rel="next" href="causal-effect-1.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html"><i class="fa fa-check"></i><b>2</b> Statistical Models and Simulations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#distinguishing-between-statistical-modeling-and-machine-learning-in-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#goals-and-objectives"><i class="fa fa-check"></i><b>2.1.1</b> Goals and Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#prediction-vs.-inference"><i class="fa fa-check"></i><b>2.1.2</b> Prediction vs. Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#conclusion"><i class="fa fa-check"></i><b>2.1.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#model-selection-and-approaches-in-data-modeling"><i class="fa fa-check"></i><b>2.4</b> Model Selection and Approaches in Data Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#mle"><i class="fa fa-check"></i><b>3.5.3</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error---mse"><i class="fa fa-check"></i><b>5.1</b> Estimation error - MSE</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.2</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#technical-points-about-mse-and-mspe"><i class="fa fa-check"></i><b>5.3</b> Technical points about MSE and MSPE</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.1</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.1</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.2</b> LPM</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
<li class="chapter" data-level="10.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#knn"><i class="fa fa-check"></i><b>10.3</b> Knn</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#adult-dataset"><i class="fa fa-check"></i><b>10.3.1</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="11.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#grid-search"><i class="fa fa-check"></i><b>11.4</b> Grid Search</a></li>
<li class="chapter" data-level="11.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>11.5</b> Cross-validated grid search</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#one-variable"><i class="fa fa-check"></i><b>12.4.1</b> One-variable</a></li>
<li class="chapter" data-level="12.4.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>12.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="12.4.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#multivariable"><i class="fa fa-check"></i><b>12.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>16.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.3</b> Pruning</a></li>
<li class="chapter" data-level="16.4" data-path="regression-trees.html"><a href="regression-trees.html#classification-with-titanic"><i class="fa fa-check"></i><b>16.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="16.5" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>17.2</b> Random Forest</a></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>17.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="17.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="17.3.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.3.3</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.4</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.5</b> Classification</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.6</b> Regression</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.7</b> Exploration</a></li>
<li class="chapter" data-level="17.8" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.8</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression-1"><i class="fa fa-check"></i><b>17.8.1</b> Regression</a></li>
<li class="chapter" data-level="17.8.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.8.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.8.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.8.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.8.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-1"><i class="fa fa-check"></i><b>17.8.4</b> Classification</a></li>
<li class="chapter" data-level="17.8.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost.m1"><i class="fa fa-check"></i><b>17.8.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="17.8.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.8.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#out-sample-prediction-accuracy"><i class="fa fa-check"></i><b>20.3</b> out-sample prediction accuracy</a></li>
<li class="chapter" data-level="20.4" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-2.html"><a href="classification-2.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-2.html"><a href="classification-2.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-2.html"><a href="classification-2.html#mnist-dataset"><i class="fa fa-check"></i><b>21.2</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="21.3" data-path="classification-2.html"><a href="classification-2.html#linear-classifiers-again"><i class="fa fa-check"></i><b>21.3</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="21.4" data-path="classification-2.html"><a href="classification-2.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.4</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.5" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret"><i class="fa fa-check"></i><b>21.5</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="classification-2.html"><a href="classification-2.html#mnist_27"><i class="fa fa-check"></i><b>21.5.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="21.5.2" data-path="classification-2.html"><a href="classification-2.html#adult-dataset-1"><i class="fa fa-check"></i><b>21.5.2</b> Adult dataset</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="classification-2.html"><a href="classification-2.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.6</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.7" data-path="classification-2.html"><a href="classification-2.html#confusion-matrix"><i class="fa fa-check"></i><b>21.7</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.8" data-path="classification-2.html"><a href="classification-2.html#performance-measures"><i class="fa fa-check"></i><b>21.8</b> Performance measures</a></li>
<li class="chapter" data-level="21.9" data-path="classification-2.html"><a href="classification-2.html#roc-curve"><i class="fa fa-check"></i><b>21.9</b> ROC Curve</a></li>
<li class="chapter" data-level="21.10" data-path="classification-2.html"><a href="classification-2.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.10</b> AUC - Area Under the Curve</a></li>
<li class="chapter" data-level="21.11" data-path="classification-2.html"><a href="classification-2.html#classification-example"><i class="fa fa-check"></i><b>21.11</b> Classification Example</a></li>
<li class="chapter" data-level="21.12" data-path="classification-2.html"><a href="classification-2.html#lpm-1"><i class="fa fa-check"></i><b>21.12</b> LPM</a></li>
<li class="chapter" data-level="21.13" data-path="classification-2.html"><a href="classification-2.html#logistic-regression-1"><i class="fa fa-check"></i><b>21.13</b> Logistic Regression</a></li>
<li class="chapter" data-level="21.14" data-path="classification-2.html"><a href="classification-2.html#knn-1"><i class="fa fa-check"></i><b>21.14</b> kNN</a>
<ul>
<li class="chapter" data-level="21.14.1" data-path="classification-2.html"><a href="classification-2.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>21.14.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="21.14.2" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret-1"><i class="fa fa-check"></i><b>21.14.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>22</b> Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="time-series.html"><a href="time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="time-series.html"><a href="time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="time-series.html"><a href="time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="time-series.html"><a href="time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="time-series.html"><a href="time-series.html#stationarity"><i class="fa fa-check"></i><b>22.5</b> Stationarity</a></li>
<li class="chapter" data-level="22.6" data-path="time-series.html"><a href="time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.6</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="time-series.html"><a href="time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.7</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.8" data-path="time-series.html"><a href="time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.8</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.9" data-path="time-series.html"><a href="time-series.html#speed"><i class="fa fa-check"></i><b>22.9</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="forecast.html"><a href="forecast.html"><i class="fa fa-check"></i><b>23</b> Forecast</a>
<ul>
<li class="chapter" data-level="23.1" data-path="forecast.html"><a href="forecast.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="forecast.html"><a href="forecast.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="forecast.html"><a href="forecast.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="forecast.html"><a href="forecast.html#random-forest-1"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="forecast.html"><a href="forecast.html#univariate"><i class="fa fa-check"></i><b>23.5</b> Univariate</a></li>
<li class="chapter" data-level="23.6" data-path="forecast.html"><a href="forecast.html#multivariate"><i class="fa fa-check"></i><b>23.6</b> Multivariate</a></li>
<li class="chapter" data-level="23.7" data-path="forecast.html"><a href="forecast.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.7</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>24.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-margin"><i class="fa fa-check"></i><b>24.1.1</b> The Margin</a></li>
<li class="chapter" data-level="24.1.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-non-separable-case"><i class="fa fa-check"></i><b>24.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>24.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="24.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#application-with-svm"><i class="fa fa-check"></i><b>24.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#precision-matrix"><i class="fa fa-check"></i><b>27.4</b> Precision Matrix</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.5</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.6" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.6</b> Regularized Covariance Matrix</a></li>
<li class="chapter" data-level="27.7" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>27.7</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="27.8" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#high-dimensional-data"><i class="fa fa-check"></i><b>27.8</b> High-dimensional data</a></li>
<li class="chapter" data-level="27.9" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>27.9</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#rankr-approximations"><i class="fa fa-check"></i><b>28.4</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="28.5" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.5</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html"><i class="fa fa-check"></i><b>29</b> PCA (Principle Component Analysis)</a>
<ul>
<li class="chapter" data-level="29.1" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.1</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>30.1</b> Using bins</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>30.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="30.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>30.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="30.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="31.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>31.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="31.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>31.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="text-analysis.html"><a href="text-analysis.html"><i class="fa fa-check"></i><b>32</b> Text Analysis</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#other-nonparametric-estimation-methods-1"><i class="fa fa-check"></i><b>33.1</b> Other Nonparametric Estimation methods</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.2</b> Regression splines</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.3</b> MARS</a></li>
<li class="chapter" data-level="33.4" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.4</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuk/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ensemble-methods" class="section level1 hasAnchor" number="17">
<h1><span class="header-section-number">Chapter 17</span> Ensemble Methods<a href="ensemble-methods.html#ensemble-methods" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Bagging, random forests and, boosting methods are the main methods of ensemble learning - a machine learning method where multiple models are trained to solve the same problem. The main idea is that, instead of using all features (predictors) in one complex base model running on the whole data, we combine multiple models each using selected number of features and subsections of the data. With this, we can have a more robust learning system.</p>
<p>What inspires ensemble learning is the idea of the “wisdom of crowds”. It suggests that “many are smarter than the few” so that collective decision-making of a diverse and larger group of individuals is better than that of a single expert. When we use a single robust model, poor predictors would be eliminated in the training procedure. Although each poor predictor has a very small contribution in training, their combination would be significant. Ensemble learning systems help these poor predictors have their “voice” in the training process by <strong>keeping them in the system rather than eliminating them</strong>. That is the main reason why ensemble methods represent robust learning algorithms in machine learning.</p>
<div id="bagging" class="section level2 hasAnchor" number="17.1">
<h2><span class="header-section-number">17.1</span> Bagging<a href="ensemble-methods.html#bagging" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bagging gets its name from <strong>B</strong>ootstrap <strong>agg</strong>regat<strong>ing</strong> of trees. The idea is simple: we train many trees each of which use a separate bootstrapped sample then aggregate them to one tree for the final decision. It works with few steps:</p>
<ol style="list-style-type: decimal">
<li>Select number of trees (B), and the tree depth (D),</li>
<li>Create a loop (B) times,</li>
<li>In each loop, (a) generate a bootstrap sample from the original data; (b) estimate a tree of depth D on that sample.</li>
</ol>
<p>Let’s see an example with the titanic dataset:</p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="ensemble-methods.html#cb395-1" tabindex="-1"></a><span class="fu">library</span>(PASWR)</span>
<span id="cb395-2"><a href="ensemble-methods.html#cb395-2" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb395-3"><a href="ensemble-methods.html#cb395-3" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb395-4"><a href="ensemble-methods.html#cb395-4" tabindex="-1"></a><span class="fu">data</span>(titanic3)</span>
<span id="cb395-5"><a href="ensemble-methods.html#cb395-5" tabindex="-1"></a></span>
<span id="cb395-6"><a href="ensemble-methods.html#cb395-6" tabindex="-1"></a><span class="co"># This is for a set of colors in each tree</span></span>
<span id="cb395-7"><a href="ensemble-methods.html#cb395-7" tabindex="-1"></a>clr <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;pink&quot;</span>,<span class="st">&quot;red&quot;</span>,<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;yellow&quot;</span>,<span class="st">&quot;darkgreen&quot;</span>,</span>
<span id="cb395-8"><a href="ensemble-methods.html#cb395-8" tabindex="-1"></a>        <span class="st">&quot;orange&quot;</span>,<span class="st">&quot;brown&quot;</span>,<span class="st">&quot;purple&quot;</span>,<span class="st">&quot;darkblue&quot;</span>)</span>
<span id="cb395-9"><a href="ensemble-methods.html#cb395-9" tabindex="-1"></a></span>
<span id="cb395-10"><a href="ensemble-methods.html#cb395-10" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(titanic3)</span>
<span id="cb395-11"><a href="ensemble-methods.html#cb395-11" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">3</span>))</span>
<span id="cb395-12"><a href="ensemble-methods.html#cb395-12" tabindex="-1"></a></span>
<span id="cb395-13"><a href="ensemble-methods.html#cb395-13" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">9</span>){  <span class="co"># Here B = 9</span></span>
<span id="cb395-14"><a href="ensemble-methods.html#cb395-14" tabindex="-1"></a>  <span class="fu">set.seed</span>(i<span class="sc">*</span><span class="dv">2</span>)</span>
<span id="cb395-15"><a href="ensemble-methods.html#cb395-15" tabindex="-1"></a>  idx <span class="ot">=</span> <span class="fu">sample</span>(n, n, <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="co">#Bootstrap sampling with replacement</span></span>
<span id="cb395-16"><a href="ensemble-methods.html#cb395-16" tabindex="-1"></a>  tr <span class="ot">&lt;-</span> titanic3[idx,]</span>
<span id="cb395-17"><a href="ensemble-methods.html#cb395-17" tabindex="-1"></a>  cart <span class="ot">=</span>  <span class="fu">rpart</span>(survived<span class="sc">~</span>sex<span class="sc">+</span>age<span class="sc">+</span>pclass<span class="sc">+</span>sibsp<span class="sc">+</span>parch,</span>
<span id="cb395-18"><a href="ensemble-methods.html#cb395-18" tabindex="-1"></a>                <span class="at">cp =</span> <span class="dv">0</span>, <span class="at">data =</span> tr, <span class="at">method =</span> <span class="st">&quot;class&quot;</span>) <span class="co">#unpruned</span></span>
<span id="cb395-19"><a href="ensemble-methods.html#cb395-19" tabindex="-1"></a>  <span class="fu">prp</span>(cart, <span class="at">type=</span><span class="dv">1</span>, <span class="at">extra=</span><span class="dv">1</span>, <span class="at">box.col=</span>clr[i])</span>
<span id="cb395-20"><a href="ensemble-methods.html#cb395-20" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en1-1.png" width="672" /></p>
<p>What are we going to do with these 9 trees?</p>
<p>In regression trees, the prediction will be the <strong>average of the resulting predictions</strong>. In classification trees, we <strong>take a majority vote</strong>.</p>
<p>Since averaging a set of observations by bootstrapping reduces the variance, the prediction accuracy increases. More importantly, compared to CART, the results would be much less sensitive to the original sample. As a result, they show impressive improvement in accuracy.</p>
<p>Below, we have an algorithm that follows the steps for bagging in classification. Let’s start with a single tree and see how we can improve it with bagging:</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="ensemble-methods.html#cb396-1" tabindex="-1"></a><span class="fu">library</span>(ROCR)</span>
<span id="cb396-2"><a href="ensemble-methods.html#cb396-2" tabindex="-1"></a></span>
<span id="cb396-3"><a href="ensemble-methods.html#cb396-3" tabindex="-1"></a><span class="co">#test/train split</span></span>
<span id="cb396-4"><a href="ensemble-methods.html#cb396-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb396-5"><a href="ensemble-methods.html#cb396-5" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(titanic3), <span class="fu">nrow</span>(titanic3) <span class="sc">*</span> <span class="fl">0.8</span>)</span>
<span id="cb396-6"><a href="ensemble-methods.html#cb396-6" tabindex="-1"></a>train <span class="ot">&lt;-</span> titanic3[ind, ]</span>
<span id="cb396-7"><a href="ensemble-methods.html#cb396-7" tabindex="-1"></a>test <span class="ot">&lt;-</span> titanic3[<span class="sc">-</span>ind, ]</span>
<span id="cb396-8"><a href="ensemble-methods.html#cb396-8" tabindex="-1"></a></span>
<span id="cb396-9"><a href="ensemble-methods.html#cb396-9" tabindex="-1"></a><span class="co">#Single tree</span></span>
<span id="cb396-10"><a href="ensemble-methods.html#cb396-10" tabindex="-1"></a>cart <span class="ot">&lt;-</span>  <span class="fu">rpart</span>(survived <span class="sc">~</span> sex <span class="sc">+</span> age <span class="sc">+</span> pclass <span class="sc">+</span> sibsp <span class="sc">+</span> parch,</span>
<span id="cb396-11"><a href="ensemble-methods.html#cb396-11" tabindex="-1"></a>                <span class="at">data =</span> train, <span class="at">method =</span> <span class="st">&quot;class&quot;</span>) <span class="co">#Pruned</span></span>
<span id="cb396-12"><a href="ensemble-methods.html#cb396-12" tabindex="-1"></a>phat1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(cart, test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>) </span>
<span id="cb396-13"><a href="ensemble-methods.html#cb396-13" tabindex="-1"></a></span>
<span id="cb396-14"><a href="ensemble-methods.html#cb396-14" tabindex="-1"></a><span class="co">#AUC</span></span>
<span id="cb396-15"><a href="ensemble-methods.html#cb396-15" tabindex="-1"></a>pred_rocr <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat1[,<span class="dv">2</span>], test<span class="sc">$</span>survived)</span>
<span id="cb396-16"><a href="ensemble-methods.html#cb396-16" tabindex="-1"></a>auc_ROCR <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb396-17"><a href="ensemble-methods.html#cb396-17" tabindex="-1"></a>auc_ROCR<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0.8352739</code></pre>
<p>Now, we apply bagging:</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb398-1"><a href="ensemble-methods.html#cb398-1" tabindex="-1"></a>B <span class="ot">=</span> <span class="dv">100</span> <span class="co"># number of trees</span></span>
<span id="cb398-2"><a href="ensemble-methods.html#cb398-2" tabindex="-1"></a></span>
<span id="cb398-3"><a href="ensemble-methods.html#cb398-3" tabindex="-1"></a>phat2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, B, <span class="fu">nrow</span>(test))</span>
<span id="cb398-4"><a href="ensemble-methods.html#cb398-4" tabindex="-1"></a></span>
<span id="cb398-5"><a href="ensemble-methods.html#cb398-5" tabindex="-1"></a><span class="co"># Loops</span></span>
<span id="cb398-6"><a href="ensemble-methods.html#cb398-6" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B){  </span>
<span id="cb398-7"><a href="ensemble-methods.html#cb398-7" tabindex="-1"></a>  <span class="fu">set.seed</span>(i) <span class="co"># to make it reproducible</span></span>
<span id="cb398-8"><a href="ensemble-methods.html#cb398-8" tabindex="-1"></a>  idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(train), <span class="fu">nrow</span>(train), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb398-9"><a href="ensemble-methods.html#cb398-9" tabindex="-1"></a>  dt <span class="ot">&lt;-</span> train[idx, ]</span>
<span id="cb398-10"><a href="ensemble-methods.html#cb398-10" tabindex="-1"></a> </span>
<span id="cb398-11"><a href="ensemble-methods.html#cb398-11" tabindex="-1"></a>  cart_B <span class="ot">&lt;-</span> <span class="fu">rpart</span>(survived <span class="sc">~</span> sex <span class="sc">+</span> age <span class="sc">+</span> pclass <span class="sc">+</span> sibsp <span class="sc">+</span> parch,</span>
<span id="cb398-12"><a href="ensemble-methods.html#cb398-12" tabindex="-1"></a>                <span class="at">cp =</span> <span class="dv">0</span>, <span class="at">data =</span> dt, <span class="at">method =</span> <span class="st">&quot;class&quot;</span>) <span class="co"># unpruned</span></span>
<span id="cb398-13"><a href="ensemble-methods.html#cb398-13" tabindex="-1"></a>  phat2[i,] <span class="ot">&lt;-</span> <span class="fu">predict</span>(cart_B, test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)[, <span class="dv">2</span>]</span>
<span id="cb398-14"><a href="ensemble-methods.html#cb398-14" tabindex="-1"></a>}</span>
<span id="cb398-15"><a href="ensemble-methods.html#cb398-15" tabindex="-1"></a></span>
<span id="cb398-16"><a href="ensemble-methods.html#cb398-16" tabindex="-1"></a><span class="fu">dim</span>(phat2)</span></code></pre></div>
<pre><code>## [1] 100 262</code></pre>
<p>You can see in that <code>phat2</code> is a <span class="math inline">\(100 \times 262\)</span> matrix. Each column is representing the predicted probability that <code>survived</code> = 1. We have 100 trees (rows in <code>phat2</code>) and 100 predicted probabilities for each the observation in the test data. The only job we will have now to take the average of 100 predicted probabilities for each column.</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb400-1"><a href="ensemble-methods.html#cb400-1" tabindex="-1"></a><span class="co"># Take the average</span></span>
<span id="cb400-2"><a href="ensemble-methods.html#cb400-2" tabindex="-1"></a>phat_f <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(phat2)</span>
<span id="cb400-3"><a href="ensemble-methods.html#cb400-3" tabindex="-1"></a></span>
<span id="cb400-4"><a href="ensemble-methods.html#cb400-4" tabindex="-1"></a><span class="co">#AUC</span></span>
<span id="cb400-5"><a href="ensemble-methods.html#cb400-5" tabindex="-1"></a>pred_rocr <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat_f, test<span class="sc">$</span>survived)</span>
<span id="cb400-6"><a href="ensemble-methods.html#cb400-6" tabindex="-1"></a>auc_ROCR <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb400-7"><a href="ensemble-methods.html#cb400-7" tabindex="-1"></a>auc_ROCR<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0.8765668</code></pre>
<p>Hence, we have a slight improvement over a single tree. We can see how the number of trees (B) would cumulatively increases AUC (reduces MSPE in regressions).</p>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="ensemble-methods.html#cb402-1" tabindex="-1"></a>B <span class="ot">=</span> <span class="dv">300</span> </span>
<span id="cb402-2"><a href="ensemble-methods.html#cb402-2" tabindex="-1"></a></span>
<span id="cb402-3"><a href="ensemble-methods.html#cb402-3" tabindex="-1"></a>phat3 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, B, <span class="fu">nrow</span>(test))</span>
<span id="cb402-4"><a href="ensemble-methods.html#cb402-4" tabindex="-1"></a>AUC <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb402-5"><a href="ensemble-methods.html#cb402-5" tabindex="-1"></a></span>
<span id="cb402-6"><a href="ensemble-methods.html#cb402-6" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B) {</span>
<span id="cb402-7"><a href="ensemble-methods.html#cb402-7" tabindex="-1"></a>  <span class="fu">set.seed</span>(i) </span>
<span id="cb402-8"><a href="ensemble-methods.html#cb402-8" tabindex="-1"></a>  idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(train), <span class="fu">nrow</span>(train), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb402-9"><a href="ensemble-methods.html#cb402-9" tabindex="-1"></a>  dt <span class="ot">&lt;-</span> train[idx,]</span>
<span id="cb402-10"><a href="ensemble-methods.html#cb402-10" tabindex="-1"></a>  </span>
<span id="cb402-11"><a href="ensemble-methods.html#cb402-11" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(</span>
<span id="cb402-12"><a href="ensemble-methods.html#cb402-12" tabindex="-1"></a>    survived <span class="sc">~</span> sex <span class="sc">+</span> age <span class="sc">+</span> pclass <span class="sc">+</span> sibsp <span class="sc">+</span> parch,</span>
<span id="cb402-13"><a href="ensemble-methods.html#cb402-13" tabindex="-1"></a>    <span class="at">cp =</span> <span class="dv">0</span>,</span>
<span id="cb402-14"><a href="ensemble-methods.html#cb402-14" tabindex="-1"></a>    <span class="at">data =</span> dt,</span>
<span id="cb402-15"><a href="ensemble-methods.html#cb402-15" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;class&quot;</span></span>
<span id="cb402-16"><a href="ensemble-methods.html#cb402-16" tabindex="-1"></a>  )</span>
<span id="cb402-17"><a href="ensemble-methods.html#cb402-17" tabindex="-1"></a>  </span>
<span id="cb402-18"><a href="ensemble-methods.html#cb402-18" tabindex="-1"></a>  phat3[i, ] <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)[, <span class="dv">2</span>]</span>
<span id="cb402-19"><a href="ensemble-methods.html#cb402-19" tabindex="-1"></a>  </span>
<span id="cb402-20"><a href="ensemble-methods.html#cb402-20" tabindex="-1"></a>  phat_f <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(phat3)</span>
<span id="cb402-21"><a href="ensemble-methods.html#cb402-21" tabindex="-1"></a>  </span>
<span id="cb402-22"><a href="ensemble-methods.html#cb402-22" tabindex="-1"></a>  <span class="co">#AUC</span></span>
<span id="cb402-23"><a href="ensemble-methods.html#cb402-23" tabindex="-1"></a>  pred_rocr <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat_f, test<span class="sc">$</span>survived)</span>
<span id="cb402-24"><a href="ensemble-methods.html#cb402-24" tabindex="-1"></a>  auc_ROCR <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb402-25"><a href="ensemble-methods.html#cb402-25" tabindex="-1"></a>  AUC[i] <span class="ot">&lt;-</span> auc_ROCR<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb402-26"><a href="ensemble-methods.html#cb402-26" tabindex="-1"></a>}</span>
<span id="cb402-27"><a href="ensemble-methods.html#cb402-27" tabindex="-1"></a></span>
<span id="cb402-28"><a href="ensemble-methods.html#cb402-28" tabindex="-1"></a><span class="fu">plot</span>(AUC, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb402-29"><a href="ensemble-methods.html#cb402-29" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;B - Number of trees&quot;</span>,</span>
<span id="cb402-30"><a href="ensemble-methods.html#cb402-30" tabindex="-1"></a>     <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en5-1.png" width="672" /></p>
<p>As it is clear from the plot that, when we use a large value of B, the AUC (or error in regression) becomes stable. Therefore, we do not need to tune the number of trees with bagging. Using a value of B sufficiently large would suffice.</p>
<p>The main idea behind bagging is to reduce the variance in prediction. The reason for this reduction is simple: we take the mean prediction of all bootstrapped samples. Remember, when we use a simple tree and make 500 bootstrapping validations, each one of them gives a different MSPE (in regressions) or AUC (in classification). The difference now is that we average <code>yhat</code> in regressions or <code>phat</code> in classifications (or <code>yhat</code> with majority voting). This reduces the prediction uncertainty drastically.</p>
<p>Bagging works very well for high-variance base learners, such as decision trees or kNN. If the learner is stable, bagging adds little to the model’s performance. This brings us to another and improved ensemble method, random forests.</p>
</div>
<div id="random-forest" class="section level2 hasAnchor" number="17.2">
<h2><span class="header-section-number">17.2</span> Random Forest<a href="ensemble-methods.html#random-forest" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Random Forest = Bagging + subsample of covariates at each node</strong>. We have done the first part before. Random forests algorithm produces many single trees based on randomly selected a subset of observations and features. Since the algorithm leads to many single trees (like a forest) with a sufficient variation, the averaging them provides relatively a smooth and, more importantly, better predictive power than a single tree. Random forests and regression trees are particularly effective in settings with a large number of features that are correlated each other. The splits will generally ignore those covariates, and as a result, the performance will remain strong even in settings with many features.</p>
<p>We will use the Breiman and Cutler’s <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">Random Forests for Classification and Regression</a>: <code>randomForest()</code><span class="citation">(<a href="#ref-Brei_2004"><strong>Brei_2004?</strong></a>)</span>.</p>
<p>Here are the steps and the loop structure:</p>
<ol style="list-style-type: decimal">
<li>Select number of trees (<code>ntree</code>), subsampling parameter (<code>mtry</code>), and the tree depth <code>maxnodes</code>,</li>
<li>Create a loop <code>ntree</code> times,</li>
<li>In each loop, (a) generate a bootstrap sample from the original data; (b) estimate a tree of depth <code>maxnodes</code> on that sample,</li>
<li>But, for each split in the tree (this is our second loop), randomly select <code>mtry</code> original covariates and do the split among those.</li>
</ol>
<p>Hence, bagging is a special case of random forest, with <code>mtry</code> = number of features (<span class="math inline">\(P\)</span>).</p>
<p>As we think on the idea of “subsampling covariates at each node” little bit more, we can see the rationale: suppose there is one very strong covariate in the sample. Almost all trees will use this covariate in the top split. All of the trees will look quite similar to each other. Hence the predictions will be highly correlated. <strong>Averaging many highly correlated quantities does not lead to a large reduction in variance</strong>. Random forests <strong>decorrelate</strong> the trees and, thus, further reduces the sensitivity of trees to the data points that are not in the original dataset.</p>
<p>How are we going to pick <code>mtry</code>? In practice, default values are <code>mtry</code> = <span class="math inline">\(P/3\)</span> in regression and <code>mtry</code> = <span class="math inline">\(\sqrt{P}\)</span> classification. (See <code>mtry</code> in <code>?randomForest</code>). Note that, with this parameter (<code>mtry</code>), we can run a pure bagging model with <code>randomForest()</code>, instead of <code>rpart()</code>, if we set <code>mtry</code> = <span class="math inline">\(P\)</span>.</p>
<p>With the bootstrap resampling process for each tree, random forests have an efficient and reasonable approximation of the test error calculated from out-of-bag (OOB) sets. When bootstrap aggregating is performed, two independent sets are created. One set, the bootstrap sample, is the data chosen to be “in-the-bag” by sampling with replacement. The out-of-bag set is all data not chosen in the sampling process. Hence, there is no need for cross-validation or a separate test set to obtain an unbiased estimate of the prediction error. It is estimated internally as each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases (observations) are left out of the bootstrap sample and not used in the construction of the <span class="math inline">\(k^{th}\)</span> tree. In this way, a test set classification is obtained for each case in about one-third of the trees. In each run, the class is selected when it
gets most of the votes among the OOB cases. The proportion of times that the selected class for the observation is not equal to the true class over all observations in OOB set is called the OOB error estimate. This has proven to be unbiased in many tests. Note that the forest’s variance decreases as the number of trees grows. Thus, more accurate predictions are likely to be obtained by choosing a large number of trees.</p>
<p>You can think of a random forest model as a robust version of CART models. There are some default parameters that can be tuned in <code>randomForest()</code>. It is argued that, however, the problem of overfitting is minor in random forests.</p>
<blockquote>
<p>Segal (2004) demonstrates small gains in performance by controlling the depths of the individual trees grown in random forests. Our experience is that using full-grown trees seldom costs much, and results in one less tuning parameter. Figure 15.8 shows the modest effect of depth control in a simple regression example. (Hastie et al., 2009, p.596)</p>
</blockquote>
<p>Let’s start with a simulation to show random forest and CART models:</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="ensemble-methods.html#cb403-1" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb403-2"><a href="ensemble-methods.html#cb403-2" tabindex="-1"></a><span class="co"># Note that this is actually Bagging since we have only 1 variable</span></span>
<span id="cb403-3"><a href="ensemble-methods.html#cb403-3" tabindex="-1"></a></span>
<span id="cb403-4"><a href="ensemble-methods.html#cb403-4" tabindex="-1"></a><span class="co"># Our simulated data</span></span>
<span id="cb403-5"><a href="ensemble-methods.html#cb403-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb403-6"><a href="ensemble-methods.html#cb403-6" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">500</span></span>
<span id="cb403-7"><a href="ensemble-methods.html#cb403-7" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)</span>
<span id="cb403-8"><a href="ensemble-methods.html#cb403-8" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(<span class="dv">12</span> <span class="sc">*</span> (x <span class="sc">+</span> .<span class="dv">2</span>)) <span class="sc">/</span> (x <span class="sc">+</span> .<span class="dv">2</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(n) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb403-9"><a href="ensemble-methods.html#cb403-9" tabindex="-1"></a></span>
<span id="cb403-10"><a href="ensemble-methods.html#cb403-10" tabindex="-1"></a><span class="co"># Fitting the models</span></span>
<span id="cb403-11"><a href="ensemble-methods.html#cb403-11" tabindex="-1"></a>fit.tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(y <span class="sc">~</span> x) <span class="co">#CART</span></span>
<span id="cb403-12"><a href="ensemble-methods.html#cb403-12" tabindex="-1"></a>fit.rf1 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(y <span class="sc">~</span> x) <span class="co">#No depth control</span></span>
<span id="cb403-13"><a href="ensemble-methods.html#cb403-13" tabindex="-1"></a>fit.rf2 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(y <span class="sc">~</span> x, <span class="at">maxnodes =</span> <span class="dv">20</span>) <span class="co"># Control it</span></span>
<span id="cb403-14"><a href="ensemble-methods.html#cb403-14" tabindex="-1"></a></span>
<span id="cb403-15"><a href="ensemble-methods.html#cb403-15" tabindex="-1"></a><span class="co"># Plot observations and predicted values</span></span>
<span id="cb403-16"><a href="ensemble-methods.html#cb403-16" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb403-17"><a href="ensemble-methods.html#cb403-17" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb403-18"><a href="ensemble-methods.html#cb403-18" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb403-19"><a href="ensemble-methods.html#cb403-19" tabindex="-1"></a><span class="fu">lines</span>(z,</span>
<span id="cb403-20"><a href="ensemble-methods.html#cb403-20" tabindex="-1"></a>      <span class="fu">predict</span>(fit.rf1, <span class="fu">data.frame</span>(<span class="at">x =</span> z)),</span>
<span id="cb403-21"><a href="ensemble-methods.html#cb403-21" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;green&quot;</span>,</span>
<span id="cb403-22"><a href="ensemble-methods.html#cb403-22" tabindex="-1"></a>      <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb403-23"><a href="ensemble-methods.html#cb403-23" tabindex="-1"></a><span class="fu">lines</span>(z,</span>
<span id="cb403-24"><a href="ensemble-methods.html#cb403-24" tabindex="-1"></a>      <span class="fu">predict</span>(fit.rf2, <span class="fu">data.frame</span>(<span class="at">x =</span> z)),</span>
<span id="cb403-25"><a href="ensemble-methods.html#cb403-25" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb403-26"><a href="ensemble-methods.html#cb403-26" tabindex="-1"></a>      <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb403-27"><a href="ensemble-methods.html#cb403-27" tabindex="-1"></a><span class="fu">lines</span>(z,</span>
<span id="cb403-28"><a href="ensemble-methods.html#cb403-28" tabindex="-1"></a>      <span class="fu">predict</span>(fit.tree, <span class="fu">data.frame</span>(<span class="at">x =</span> z)),</span>
<span id="cb403-29"><a href="ensemble-methods.html#cb403-29" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb403-30"><a href="ensemble-methods.html#cb403-30" tabindex="-1"></a>      <span class="at">lwd =</span> <span class="fl">1.5</span>)</span>
<span id="cb403-31"><a href="ensemble-methods.html#cb403-31" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>,</span>
<span id="cb403-32"><a href="ensemble-methods.html#cb403-32" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">&quot;Random Forest: maxnodes=max&quot;</span>,</span>
<span id="cb403-33"><a href="ensemble-methods.html#cb403-33" tabindex="-1"></a>    <span class="st">&quot;Random Forest: maxnodes=20&quot;</span>,</span>
<span id="cb403-34"><a href="ensemble-methods.html#cb403-34" tabindex="-1"></a>    <span class="st">&quot;CART: single regression tree&quot;</span>),</span>
<span id="cb403-35"><a href="ensemble-methods.html#cb403-35" tabindex="-1"></a>  <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>),</span>
<span id="cb403-36"><a href="ensemble-methods.html#cb403-36" tabindex="-1"></a>  <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb403-37"><a href="ensemble-methods.html#cb403-37" tabindex="-1"></a>  <span class="at">bty =</span> <span class="st">&quot;n&quot;</span></span>
<span id="cb403-38"><a href="ensemble-methods.html#cb403-38" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en6-1.png" width="672" /></p>
<p>The random forest models are definitely improvements over CART, but which one is better? Although random forest models should not overfit when increasing the number of trees (<code>ntree</code>) in the forest, in practice <code>maxnodes</code> and <code>mtry</code> are used as hyperparameters in the field.</p>
<p>Let’s use out-of-bag (OOB) MSE to tune Random Forest parameters in our case to see if there is any improvement.</p>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="ensemble-methods.html#cb404-1" tabindex="-1"></a>maxnode <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>)</span>
<span id="cb404-2"><a href="ensemble-methods.html#cb404-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(maxnode)) {</span>
<span id="cb404-3"><a href="ensemble-methods.html#cb404-3" tabindex="-1"></a>  a <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(y <span class="sc">~</span> x, <span class="at">maxnodes =</span> maxnode[i])</span>
<span id="cb404-4"><a href="ensemble-methods.html#cb404-4" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">c</span>(maxnode[i], a<span class="sc">$</span>mse[<span class="dv">500</span>]))</span>
<span id="cb404-5"><a href="ensemble-methods.html#cb404-5" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## [1] 10.0000000  0.3878058
## [1] 50.000000  0.335875
## [1] 100.0000000   0.3592119
## [1] 500.0000000   0.3905135</code></pre>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="ensemble-methods.html#cb406-1" tabindex="-1"></a><span class="co"># Increase ntree = 1500</span></span>
<span id="cb406-2"><a href="ensemble-methods.html#cb406-2" tabindex="-1"></a>maxnode <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>)</span>
<span id="cb406-3"><a href="ensemble-methods.html#cb406-3" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(maxnode)) {</span>
<span id="cb406-4"><a href="ensemble-methods.html#cb406-4" tabindex="-1"></a>  a <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(y <span class="sc">~</span> x, <span class="at">maxnodes =</span> maxnode[i], <span class="at">ntree =</span> <span class="dv">2500</span>)</span>
<span id="cb406-5"><a href="ensemble-methods.html#cb406-5" tabindex="-1"></a>  <span class="fu">print</span>(<span class="fu">c</span>(maxnode[i], a<span class="sc">$</span>mse[<span class="dv">500</span>]))</span>
<span id="cb406-6"><a href="ensemble-methods.html#cb406-6" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## [1] 10.000000  0.391755
## [1] 50.0000000  0.3353198
## [1] 100.0000000   0.3616621
## [1] 500.0000000   0.3900982</code></pre>
<p>We can see that OOB-MSE is smaller with <code>maxnodes</code> = 50 even when we increase <code>ntree</code> = 1500 . Of course we can have a finer sequence of <code>maxnodes</code> series to test. Similarly, we can select parameter <code>mtry</code> with a grid search. In a bagged model we set <code>mtry</code> = <span class="math inline">\(P\)</span>. If we don’t set it, the default values for <code>mtry</code> are square-root of <span class="math inline">\(p\)</span> for classification and <span class="math inline">\(p/3\)</span> in regression, where <span class="math inline">\(p\)</span> is number of features. If we want, we can tune both parameters with cross-validation. The effectiveness of tuning random forest models in improving their prediction accuracy is an open question in practice.</p>
<p>Bagging and random forest models tend to work well for problems where there are important nonlinearities and interactions. More importantly, <strong>they are robust to the original sample</strong> and more efficient than single trees. However, <strong>the results would be less intuitive and difficult to interpret</strong>. Nevertheless, we can obtain an overall summary of the importance of each covariates using SSR (for regression) or Gini index (for classification). The index records the total amount that the SSR or Gini is decreased due to splits over a given covariate, averaged over all <code>ntree</code> trees.</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="ensemble-methods.html#cb408-1" tabindex="-1"></a>rf <span class="ot">&lt;-</span></span>
<span id="cb408-2"><a href="ensemble-methods.html#cb408-2" tabindex="-1"></a>  <span class="fu">randomForest</span>(</span>
<span id="cb408-3"><a href="ensemble-methods.html#cb408-3" tabindex="-1"></a>    <span class="fu">as.factor</span>(survived) <span class="sc">~</span> sex <span class="sc">+</span> age <span class="sc">+</span> pclass <span class="sc">+</span> sibsp <span class="sc">+</span> parch,</span>
<span id="cb408-4"><a href="ensemble-methods.html#cb408-4" tabindex="-1"></a>    <span class="at">data =</span> titanic3,</span>
<span id="cb408-5"><a href="ensemble-methods.html#cb408-5" tabindex="-1"></a>    <span class="at">na.action =</span> na.omit,</span>
<span id="cb408-6"><a href="ensemble-methods.html#cb408-6" tabindex="-1"></a>    <span class="at">localImp =</span> <span class="cn">TRUE</span>,</span>
<span id="cb408-7"><a href="ensemble-methods.html#cb408-7" tabindex="-1"></a>  )</span>
<span id="cb408-8"><a href="ensemble-methods.html#cb408-8" tabindex="-1"></a></span>
<span id="cb408-9"><a href="ensemble-methods.html#cb408-9" tabindex="-1"></a><span class="fu">plot</span>(rf, <span class="at">main =</span> <span class="st">&quot;Learning curve of the forest&quot;</span>)</span>
<span id="cb408-10"><a href="ensemble-methods.html#cb408-10" tabindex="-1"></a><span class="fu">legend</span>(</span>
<span id="cb408-11"><a href="ensemble-methods.html#cb408-11" tabindex="-1"></a>  <span class="st">&quot;topright&quot;</span>,</span>
<span id="cb408-12"><a href="ensemble-methods.html#cb408-12" tabindex="-1"></a>  <span class="fu">c</span>(</span>
<span id="cb408-13"><a href="ensemble-methods.html#cb408-13" tabindex="-1"></a>    <span class="st">&quot;Error for &#39;Survived&#39;&quot;</span>,</span>
<span id="cb408-14"><a href="ensemble-methods.html#cb408-14" tabindex="-1"></a>    <span class="st">&quot;Misclassification error&quot;</span>,</span>
<span id="cb408-15"><a href="ensemble-methods.html#cb408-15" tabindex="-1"></a>    <span class="st">&quot;Error for &#39;Dead&#39;&quot;</span></span>
<span id="cb408-16"><a href="ensemble-methods.html#cb408-16" tabindex="-1"></a>  ),</span>
<span id="cb408-17"><a href="ensemble-methods.html#cb408-17" tabindex="-1"></a>  <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb408-18"><a href="ensemble-methods.html#cb408-18" tabindex="-1"></a>  <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>)</span>
<span id="cb408-19"><a href="ensemble-methods.html#cb408-19" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en8-1.png" width="672" /></p>
<p>The plot shows the evolution of out-of-bag errors when the number of trees increases. The learning curves reach to a stable section right after a couple of trees. With 500 trees, which is the default number, the OOB estimate of our error rate is around 0.2, which can be seen in the basic information concerning our model below</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="ensemble-methods.html#cb409-1" tabindex="-1"></a>rf</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = as.factor(survived) ~ sex + age + pclass +      sibsp + parch, data = titanic3, localImp = TRUE, , na.action = na.omit) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 19.69%
## Confusion matrix:
##     0   1 class.error
## 0 562  57  0.09208401
## 1 149 278  0.34894614</code></pre>
<p>We will see a several applications on CART, bagging and Random Forest later in Chapter 14.</p>
</div>
<div id="boosting" class="section level2 hasAnchor" number="17.3">
<h2><span class="header-section-number">17.3</span> Boosting<a href="ensemble-methods.html#boosting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Boosting is an ensemble method that combines a set of “weak learners” into a strong learner to improve the prediction accuracy in self-learning algorithms. In boosting, the constructed iteration selects a random sample of data, fits a model, and then train sequentially. In each sequential model, the algorithm learns from the weaknesses of its predecessor (predictions errors) and tries to compensate for the weaknesses by “boosting” the weak rules from each individual classifier. The first original boosted application was offered in 1990 by Robert Schapire (1990) <span class="citation">(<a href="#ref-Schapire"><strong>Schapire?</strong></a>)</span>.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>Today, there are many boosting algorithms that are mainly grouped in the following three types:</p>
<ul>
<li>Gradient descent algorithm,</li>
<li>AdaBoost algorithm,</li>
<li>Xtreme gradient descent algorithm,</li>
</ul>
<p>We will start with a general application to show the idea behind the algorithm using the package <code>gbm</code>, <a href="https://cran.r-project.org/web/packages/gbm/gbm.pdf">Generalized Boosted Regression Models</a></p>
<div id="sequential-ensemble-with-gbm" class="section level3 hasAnchor" number="17.3.1">
<h3><span class="header-section-number">17.3.1</span> Sequential ensemble with <code>gbm</code><a href="ensemble-methods.html#sequential-ensemble-with-gbm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to bagging, boosting also combines a large number of decision trees. However, the trees are grown <strong>sequentially</strong> without bootstrap sampling. Instead each tree is fit on a modified version of the original dataset, the <strong>error</strong>.</p>
<p>In regression trees, for example, each tree is fit to the residuals from the previous tree model so that each iteration is focused on <strong>improving previous errors</strong>. This process would be very weird for an econometrician. The accepted model building practice in econometrics is that you should have a model that the errors (residuals) should be orthogonal (independent from) to covariates. Here, what we suggest is the opposite of this practice: start with a very low-depth (shallow) model that omits many relevant variables, run a linear regression, get the residuals (prediction errors), and run another regression that explains the residuals with covariates. This is called <strong>learning from mistakes</strong>.</p>
<p>Since there is no bootstrapping, this process is open to overfitting problem as it aims to minimize the in-sample prediction error. Hence, we introduce a <strong>hyperparameter</strong> that we can tune the learning process with cross-validation to stop the overfitting and get the best predictive model.</p>
<p>This hyperparameter (shrinkage parameter, also known as the <strong>learning rate</strong> or <strong>step-size reduction</strong>) limits the size of the errors.</p>
<p>Let’s see the whole process in a simple example inspired by <a href="https://freakonometrics.hypotheses.org/52782">Freakonometrics</a> <span class="citation">(<a href="#ref-Hyp_2018"><strong>Hyp_2018?</strong></a>)</span>:</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="ensemble-methods.html#cb411-1" tabindex="-1"></a><span class="co"># First we will simulate our data</span></span>
<span id="cb411-2"><a href="ensemble-methods.html#cb411-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">300</span></span>
<span id="cb411-3"><a href="ensemble-methods.html#cb411-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb411-4"><a href="ensemble-methods.html#cb411-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n) <span class="sc">*</span> <span class="dv">2</span> <span class="sc">*</span> pi)</span>
<span id="cb411-5"><a href="ensemble-methods.html#cb411-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n) <span class="sc">/</span> <span class="dv">4</span></span>
<span id="cb411-6"><a href="ensemble-methods.html#cb411-6" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;x&quot;</span> <span class="ot">=</span> x, <span class="st">&quot;y&quot;</span> <span class="ot">=</span> y)</span>
<span id="cb411-7"><a href="ensemble-methods.html#cb411-7" tabindex="-1"></a><span class="fu">plot</span>(df<span class="sc">$</span>x, df<span class="sc">$</span>y, <span class="at">ylab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="at">col =</span> <span class="st">&quot;grey&quot;</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en10-1.png" width="672" /></p>
<p>We will “boost” a single regression tree:</p>
<p><strong>Step 1</strong>: Fit the model by using in-sample data</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="ensemble-methods.html#cb412-1" tabindex="-1"></a><span class="co"># Regression tree with rpart()</span></span>
<span id="cb412-2"><a href="ensemble-methods.html#cb412-2" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(y <span class="sc">~</span> x, <span class="at">data =</span> df) <span class="co"># First fit: y~x</span></span>
<span id="cb412-3"><a href="ensemble-methods.html#cb412-3" tabindex="-1"></a>yp <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit) <span class="co"># using in-sample data</span></span>
<span id="cb412-4"><a href="ensemble-methods.html#cb412-4" tabindex="-1"></a></span>
<span id="cb412-5"><a href="ensemble-methods.html#cb412-5" tabindex="-1"></a><span class="co"># Plot for single regression tree</span></span>
<span id="cb412-6"><a href="ensemble-methods.html#cb412-6" tabindex="-1"></a><span class="fu">plot</span>(df<span class="sc">$</span>x, df<span class="sc">$</span>y, <span class="at">ylab =</span> <span class="st">&quot;y&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="at">col =</span> <span class="st">&quot;grey&quot;</span>)</span>
<span id="cb412-7"><a href="ensemble-methods.html#cb412-7" tabindex="-1"></a><span class="fu">lines</span>(df<span class="sc">$</span>x, yp, <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en11-1.png" width="672" /></p>
<p>Now, we will have a loop that will “boost” the model. What we mean by boosting is that we seek to improve <code>yhat</code>, i.e. <span class="math inline">\(\hat{f}(x_i)\)</span>, in areas where it does not perform well by fitting trees to the residuals.</p>
<p><strong>Step 2</strong>: Find the “error” and introduce a hyperparameter <code>h</code>.</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="ensemble-methods.html#cb413-1" tabindex="-1"></a>h <span class="ot">&lt;-</span> <span class="fl">0.1</span> <span class="co"># shrinkage parameter</span></span>
<span id="cb413-2"><a href="ensemble-methods.html#cb413-2" tabindex="-1"></a></span>
<span id="cb413-3"><a href="ensemble-methods.html#cb413-3" tabindex="-1"></a><span class="co"># Calculate the prediction error adjusted by h</span></span>
<span id="cb413-4"><a href="ensemble-methods.html#cb413-4" tabindex="-1"></a>yr <span class="ot">&lt;-</span> df<span class="sc">$</span>y <span class="sc">-</span> h <span class="sc">*</span> yp </span>
<span id="cb413-5"><a href="ensemble-methods.html#cb413-5" tabindex="-1"></a></span>
<span id="cb413-6"><a href="ensemble-methods.html#cb413-6" tabindex="-1"></a><span class="co"># Add this adjusted prediction error, `yr` to our main data frame,</span></span>
<span id="cb413-7"><a href="ensemble-methods.html#cb413-7" tabindex="-1"></a><span class="co"># which will be our target variable to predict later</span></span>
<span id="cb413-8"><a href="ensemble-methods.html#cb413-8" tabindex="-1"></a>df<span class="sc">$</span>yr <span class="ot">&lt;-</span> yr</span>
<span id="cb413-9"><a href="ensemble-methods.html#cb413-9" tabindex="-1"></a></span>
<span id="cb413-10"><a href="ensemble-methods.html#cb413-10" tabindex="-1"></a><span class="co"># Store the &quot;first&quot; predictions in YP</span></span>
<span id="cb413-11"><a href="ensemble-methods.html#cb413-11" tabindex="-1"></a>YP <span class="ot">&lt;-</span> h <span class="sc">*</span> yp </span></code></pre></div>
<p>Note that if <code>h</code>= 1, it would give us usual “residuals”. Hence, <code>h</code> controls for “how much error” we would like to reduce.</p>
<p><strong>Step 3</strong>: Now, we will predict the “error” in a loop that repeats itself many times.</p>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="ensemble-methods.html#cb414-1" tabindex="-1"></a><span class="co"># Boosting loop for t times (trees)</span></span>
<span id="cb414-2"><a href="ensemble-methods.html#cb414-2" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb414-3"><a href="ensemble-methods.html#cb414-3" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(yr <span class="sc">~</span> x, <span class="at">data =</span> df) <span class="co"># here it&#39;s yr~x.</span></span>
<span id="cb414-4"><a href="ensemble-methods.html#cb414-4" tabindex="-1"></a>  <span class="co"># We try to understand the prediction error by x&#39;s</span></span>
<span id="cb414-5"><a href="ensemble-methods.html#cb414-5" tabindex="-1"></a>  </span>
<span id="cb414-6"><a href="ensemble-methods.html#cb414-6" tabindex="-1"></a>  yp <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">newdata =</span> df)</span>
<span id="cb414-7"><a href="ensemble-methods.html#cb414-7" tabindex="-1"></a>  </span>
<span id="cb414-8"><a href="ensemble-methods.html#cb414-8" tabindex="-1"></a>  <span class="co"># This is your main prediction added to YP</span></span>
<span id="cb414-9"><a href="ensemble-methods.html#cb414-9" tabindex="-1"></a>  YP <span class="ot">&lt;-</span> <span class="fu">cbind</span>(YP, h <span class="sc">*</span> yp) </span>
<span id="cb414-10"><a href="ensemble-methods.html#cb414-10" tabindex="-1"></a>  </span>
<span id="cb414-11"><a href="ensemble-methods.html#cb414-11" tabindex="-1"></a>  df<span class="sc">$</span>yr <span class="ot">&lt;-</span> df<span class="sc">$</span>yr <span class="sc">-</span> h <span class="sc">*</span> yp <span class="co"># errors for the next iteration</span></span>
<span id="cb414-12"><a href="ensemble-methods.html#cb414-12" tabindex="-1"></a>  <span class="co"># i.e. the next target to predict!</span></span>
<span id="cb414-13"><a href="ensemble-methods.html#cb414-13" tabindex="-1"></a>}</span>
<span id="cb414-14"><a href="ensemble-methods.html#cb414-14" tabindex="-1"></a></span>
<span id="cb414-15"><a href="ensemble-methods.html#cb414-15" tabindex="-1"></a><span class="fu">str</span>(YP)</span></code></pre></div>
<pre><code>##  num [1:300, 1:101] 0.00966 0.00966 0.00966 0.00966 0.00966 ...
##  - attr(*, &quot;dimnames&quot;)=List of 2
##   ..$ : chr [1:300] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##   ..$ : chr [1:101] &quot;YP&quot; &quot;&quot; &quot;&quot; &quot;&quot; ...</code></pre>
<p>Look at <code>YP</code> now. We have a matrix 300 by 101. This is a matrix of <strong>predicted errors</strong>, except for the first column. So what?</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="ensemble-methods.html#cb416-1" tabindex="-1"></a><span class="co"># Function to plot a single tree and boosted trees for different t</span></span>
<span id="cb416-2"><a href="ensemble-methods.html#cb416-2" tabindex="-1"></a>viz <span class="ot">&lt;-</span> <span class="cf">function</span>(M) {</span>
<span id="cb416-3"><a href="ensemble-methods.html#cb416-3" tabindex="-1"></a>  <span class="co"># Boosting</span></span>
<span id="cb416-4"><a href="ensemble-methods.html#cb416-4" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> <span class="fu">apply</span>(YP[, <span class="dv">1</span><span class="sc">:</span>M], <span class="dv">1</span>, sum) <span class="co"># This is predicted y for depth M</span></span>
<span id="cb416-5"><a href="ensemble-methods.html#cb416-5" tabindex="-1"></a>  <span class="fu">plot</span>(df<span class="sc">$</span>x, df<span class="sc">$</span>y, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>) <span class="co"># Data points</span></span>
<span id="cb416-6"><a href="ensemble-methods.html#cb416-6" tabindex="-1"></a>  <span class="fu">lines</span>(df<span class="sc">$</span>x, yhat, <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>) <span class="co"># line for boosting</span></span>
<span id="cb416-7"><a href="ensemble-methods.html#cb416-7" tabindex="-1"></a>  </span>
<span id="cb416-8"><a href="ensemble-methods.html#cb416-8" tabindex="-1"></a>  <span class="co"># Single Tree</span></span>
<span id="cb416-9"><a href="ensemble-methods.html#cb416-9" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(y <span class="sc">~</span> x, <span class="at">data =</span> df) <span class="co"># Single regression tree</span></span>
<span id="cb416-10"><a href="ensemble-methods.html#cb416-10" tabindex="-1"></a>  yp <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">newdata =</span> df) <span class="co"># prediction for the single tree</span></span>
<span id="cb416-11"><a href="ensemble-methods.html#cb416-11" tabindex="-1"></a>  <span class="fu">lines</span>(df<span class="sc">$</span>x, yp, <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>) <span class="co"># line for single tree</span></span>
<span id="cb416-12"><a href="ensemble-methods.html#cb416-12" tabindex="-1"></a>  <span class="fu">lines</span>(df<span class="sc">$</span>x, <span class="fu">sin</span>(df<span class="sc">$</span>x), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>) <span class="co"># Line for DGM</span></span>
<span id="cb416-13"><a href="ensemble-methods.html#cb416-13" tabindex="-1"></a>}</span>
<span id="cb416-14"><a href="ensemble-methods.html#cb416-14" tabindex="-1"></a></span>
<span id="cb416-15"><a href="ensemble-methods.html#cb416-15" tabindex="-1"></a><span class="co"># Run each</span></span>
<span id="cb416-16"><a href="ensemble-methods.html#cb416-16" tabindex="-1"></a><span class="fu">viz</span>(<span class="dv">5</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en14-1.png" width="672" /></p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="ensemble-methods.html#cb417-1" tabindex="-1"></a><span class="fu">viz</span>(<span class="dv">101</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en14-2.png" width="672" /></p>
<p>Each of 100 trees is given in the <code>YP</code> matrix. <strong>Boosting combines the outputs of many “weak” learners (each tree) to produce a powerful “committee”.</strong> What if we change the shrinkage parameter? Let’s increase it to 1.8.</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="ensemble-methods.html#cb418-1" tabindex="-1"></a>h <span class="ot">&lt;-</span> <span class="fl">1.8</span> <span class="co"># shrinkage parameter </span></span>
<span id="cb418-2"><a href="ensemble-methods.html#cb418-2" tabindex="-1"></a>df<span class="sc">$</span>yr <span class="ot">&lt;-</span> df<span class="sc">$</span>y <span class="sc">-</span> h<span class="sc">*</span>yp <span class="co"># Prediction errors with &quot;h&quot; after rpart</span></span>
<span id="cb418-3"><a href="ensemble-methods.html#cb418-3" tabindex="-1"></a>YP <span class="ot">&lt;-</span> h<span class="sc">*</span>yp  <span class="co">#Store the &quot;first&quot; prediction errors in YP</span></span>
<span id="cb418-4"><a href="ensemble-methods.html#cb418-4" tabindex="-1"></a></span>
<span id="cb418-5"><a href="ensemble-methods.html#cb418-5" tabindex="-1"></a><span class="co"># Boosting Loop for t (number of trees) times</span></span>
<span id="cb418-6"><a href="ensemble-methods.html#cb418-6" tabindex="-1"></a><span class="cf">for</span>(t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>){</span>
<span id="cb418-7"><a href="ensemble-methods.html#cb418-7" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">rpart</span>(yr<span class="sc">~</span>x, <span class="at">data=</span>df) <span class="co"># here it&#39;s yr~x.  </span></span>
<span id="cb418-8"><a href="ensemble-methods.html#cb418-8" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">newdata=</span>df)</span>
<span id="cb418-9"><a href="ensemble-methods.html#cb418-9" tabindex="-1"></a>  df<span class="sc">$</span>yr <span class="ot">&lt;-</span> df<span class="sc">$</span>yr <span class="sc">-</span> h<span class="sc">*</span>yhat <span class="co"># errors for the next iteration</span></span>
<span id="cb418-10"><a href="ensemble-methods.html#cb418-10" tabindex="-1"></a>  YP <span class="ot">&lt;-</span> <span class="fu">cbind</span>(YP, h<span class="sc">*</span>yhat) <span class="co"># This is your main prediction added to YP </span></span>
<span id="cb418-11"><a href="ensemble-methods.html#cb418-11" tabindex="-1"></a>}</span>
<span id="cb418-12"><a href="ensemble-methods.html#cb418-12" tabindex="-1"></a></span>
<span id="cb418-13"><a href="ensemble-methods.html#cb418-13" tabindex="-1"></a><span class="fu">viz</span>(<span class="dv">101</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en15-1.png" width="672" /></p>
<p>It overfits. Unlike random forests, boosting can overfit if the number of trees (B) and depth of each tree (D) are too large. By averaging over a large number of trees, bagging and random forests reduces variability. Boosting does not average over the trees.</p>
<p>This shows that <span class="math inline">\(h\)</span> should be tuned by a proper process. The generalized <a href="https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf">boosted regression modeling (GBM)</a> <span class="citation">(<a href="#ref-Ridgeway_2020"><strong>Ridgeway_2020?</strong></a>)</span> can also be used for boosting regressions. Note that there are many arguments with their specific default values in the function. For example, <code>n.tree</code> (B) is 100 and <code>shrinkage</code> (<span class="math inline">\(h\)</span>) is 0.1. The <code>gbm()</code> function also has <code>interaction.depth</code> (D) specifying the maximum depth of each tree. When it is 1, the model is just an additive model, while 2 implies a model with up to 2-way interactions. A smaller <span class="math inline">\(h\)</span> typically requires more trees <span class="math inline">\(B\)</span>. It allows more and different shaped trees to attack the residuals.</p>
<p>Here is the application of <code>gbm</code> to our simulated data:</p>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb419-1"><a href="ensemble-methods.html#cb419-1" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb419-2"><a href="ensemble-methods.html#cb419-2" tabindex="-1"></a></span>
<span id="cb419-3"><a href="ensemble-methods.html#cb419-3" tabindex="-1"></a><span class="co"># Note bag.fraction = 1 (no CV).  The default is 0.5</span></span>
<span id="cb419-4"><a href="ensemble-methods.html#cb419-4" tabindex="-1"></a>bo1 <span class="ot">&lt;-</span> <span class="fu">gbm</span>(y <span class="sc">~</span> x, <span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="at">n.tree =</span> <span class="dv">100</span>, <span class="at">data =</span> df,</span>
<span id="cb419-5"><a href="ensemble-methods.html#cb419-5" tabindex="-1"></a>              <span class="at">shrinkage =</span> <span class="fl">0.1</span>, <span class="at">bag.fraction =</span> <span class="dv">1</span>)</span>
<span id="cb419-6"><a href="ensemble-methods.html#cb419-6" tabindex="-1"></a></span>
<span id="cb419-7"><a href="ensemble-methods.html#cb419-7" tabindex="-1"></a>bo2 <span class="ot">&lt;-</span> <span class="fu">gbm</span>(y <span class="sc">~</span> x, <span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="at">data =</span> df) <span class="co"># All default</span></span>
<span id="cb419-8"><a href="ensemble-methods.html#cb419-8" tabindex="-1"></a></span>
<span id="cb419-9"><a href="ensemble-methods.html#cb419-9" tabindex="-1"></a><span class="fu">plot</span>(df<span class="sc">$</span>x, df<span class="sc">$</span>y, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;&quot;</span>) <span class="co">#Data points</span></span>
<span id="cb419-10"><a href="ensemble-methods.html#cb419-10" tabindex="-1"></a><span class="fu">lines</span>(df<span class="sc">$</span>x, <span class="fu">predict</span>(bo1, <span class="at">data =</span> df, <span class="at">n.trees =</span> t), <span class="at">type =</span> <span class="st">&quot;s&quot;</span>,</span>
<span id="cb419-11"><a href="ensemble-methods.html#cb419-11" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>) <span class="co">#line for without CV</span></span>
<span id="cb419-12"><a href="ensemble-methods.html#cb419-12" tabindex="-1"></a><span class="fu">lines</span>(df<span class="sc">$</span>x, <span class="fu">predict</span>(bo2, <span class="at">n.trees =</span> t, <span class="at">data =</span> df), <span class="at">type =</span> <span class="st">&quot;s&quot;</span>,</span>
<span id="cb419-13"><a href="ensemble-methods.html#cb419-13" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>) <span class="co">#line with default parameters with CV</span></span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en16-1.png" width="672" /></p>
<p>We will have more applications with <code>gbm</code> in the next chapter. We can also have a boosting application for classification problems. While the <code>gbm</code> function can be used for classification that requires a different distribution (“bernoulli” - logistic regression for 0-1 outcomes), there is a special boosting method for classification problems, AdaBoost.M1, which is what we will look at next.</p>
</div>
<div id="adaboost" class="section level3 hasAnchor" number="17.3.2">
<h3><span class="header-section-number">17.3.2</span> AdaBoost<a href="ensemble-methods.html#adaboost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the most popular boosting algorithm is <strong>AdaBost.M1</strong> due to Freund and Schpire (1997). We consider two-class problem where <span class="math inline">\(y \in\{-1,1\}\)</span>, which is a categorical outcome. With a set of predictor variables <span class="math inline">\(X\)</span>, a classifier <span class="math inline">\(\hat{m}_{b}(x)\)</span> at tree <span class="math inline">\(b\)</span> among B trees, produces a prediction taking the two values <span class="math inline">\(\{-1,1\}\)</span>.</p>
<p>To understand how AdaBoost works, let’s look at the algorithm step by step:</p>
<ol style="list-style-type: decimal">
<li>Select the number of trees B, and the tree depth D;</li>
<li>Set initial weights, <span class="math inline">\(w_i=1/n\)</span>, for each observation.</li>
<li>Fit a classification tree <span class="math inline">\(\hat{m}_{b}(x)\)</span> at <span class="math inline">\(b=1\)</span>, the first tree.</li>
<li>Calculate the following misclassification error for <span class="math inline">\(b=1\)</span>:</li>
</ol>
<p><span class="math display">\[
\mathbf{err}_{b=1}=\frac{\sum_{i=1}^{n} \mathbf{I}\left(y_{i} \neq \hat{m}_{b}\left(x_{i}\right)\right)}{n}
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>By using this error, calculate</li>
</ol>
<p><span class="math display">\[
\alpha_{b}=0.5\log \left(\frac{1-e r r_{b}}{e r r_{b}}\right)
\]</span></p>
<p>For example, suppose <span class="math inline">\(err_b = 0.3\)</span>, then <span class="math inline">\(\alpha_{b}=0.5\text{log}(0.7/0.3)\)</span>, which is a log odds or <span class="math inline">\(\log (\text{success}/\text{failure})\)</span>.</p>
<ol start="5" style="list-style-type: decimal">
<li>If the observation <span class="math inline">\(i\)</span> is misclassified, update its weights, if not, use <span class="math inline">\(w_i\)</span> which is <span class="math inline">\(1/n\)</span>:</li>
</ol>
<p><span class="math display">\[
w_{i} \leftarrow w_{i} e^{\alpha_b}
\]</span></p>
<p>Let’s try some numbers:</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="ensemble-methods.html#cb420-1" tabindex="-1"></a><span class="co">#Suppose err = 0.2, n=100</span></span>
<span id="cb420-2"><a href="ensemble-methods.html#cb420-2" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb420-3"><a href="ensemble-methods.html#cb420-3" tabindex="-1"></a>err <span class="ot">=</span> <span class="fl">0.2</span></span>
<span id="cb420-4"><a href="ensemble-methods.html#cb420-4" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">log</span>((<span class="dv">1</span> <span class="sc">-</span> err) <span class="sc">/</span> err)</span>
<span id="cb420-5"><a href="ensemble-methods.html#cb420-5" tabindex="-1"></a>alpha</span></code></pre></div>
<pre><code>## [1] 0.6931472</code></pre>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="ensemble-methods.html#cb422-1" tabindex="-1"></a><span class="fu">exp</span>(alpha)</span></code></pre></div>
<pre><code>## [1] 2</code></pre>
<p>So, the new weight for the misclassified <span class="math inline">\(i\)</span> in the second tree (i.e., <span class="math inline">\(b=2\)</span> stump) will be</p>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="ensemble-methods.html#cb424-1" tabindex="-1"></a><span class="co"># For misclassified observations</span></span>
<span id="cb424-2"><a href="ensemble-methods.html#cb424-2" tabindex="-1"></a>weight_miss <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">/</span> n) <span class="sc">*</span> (<span class="fu">exp</span>(alpha))</span>
<span id="cb424-3"><a href="ensemble-methods.html#cb424-3" tabindex="-1"></a>weight_miss</span></code></pre></div>
<pre><code>## [1] 0.02</code></pre>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="ensemble-methods.html#cb426-1" tabindex="-1"></a><span class="co"># For correctly classified observations</span></span>
<span id="cb426-2"><a href="ensemble-methods.html#cb426-2" tabindex="-1"></a>weight_corr <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">/</span> n) <span class="sc">*</span> (<span class="fu">exp</span>(<span class="sc">-</span>alpha))</span>
<span id="cb426-3"><a href="ensemble-methods.html#cb426-3" tabindex="-1"></a>weight_corr</span></code></pre></div>
<pre><code>## [1] 0.005</code></pre>
<p>This shows that as the misclassification error goes up, it increases the weights for each misclassified observation and reduces the weights for correctly classified observations.</p>
<ol start="6" style="list-style-type: decimal">
<li>With this procedure, in each loop from <span class="math inline">\(b\)</span> to B, it applies <span class="math inline">\(\hat{m}_{b}(x)\)</span> to the data using updated weights <span class="math inline">\(w_i\)</span> in each <span class="math inline">\(b\)</span>:</li>
</ol>
<p><span class="math display">\[
\mathbf{err}_{b}=\frac{\sum_{i=1}^{n} w_{i} \mathbf{I}\left(y_{i} \neq \hat{m}_{b}\left(x_{i}\right)\right)}{\sum_{i=1}^{n} w_{i}}
\]</span></p>
<p>We normalize all weights between 0 and 1 so that sum of the weights would be one in each iteration. Hence, the algorithm works in a way that it randomly replicates the observations as new data points by using the weights as their probabilities. This process also resembles to under- and oversampling at the same time so that the number of observations stays the same. The new dataset now is used again for the next tree (<span class="math inline">\(b=2\)</span>) and this iteration continues until B. We can use <code>rpart</code> in each tree with <code>weights</code> option as we will shown momentarily.</p>
<p>Here is an example with the <code>myocarde</code> data that we only use the first 6 observations:</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="ensemble-methods.html#cb428-1" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb428-2"><a href="ensemble-methods.html#cb428-2" tabindex="-1"></a></span>
<span id="cb428-3"><a href="ensemble-methods.html#cb428-3" tabindex="-1"></a>myocarde <span class="ot">&lt;-</span> <span class="fu">read_delim</span>(<span class="st">&quot;myocarde.csv&quot;</span>, <span class="at">delim =</span> <span class="st">&quot;;&quot;</span> ,</span>
<span id="cb428-4"><a href="ensemble-methods.html#cb428-4" tabindex="-1"></a>                       <span class="at">escape_double =</span> <span class="cn">FALSE</span>, <span class="at">trim_ws =</span> <span class="cn">TRUE</span>,</span>
<span id="cb428-5"><a href="ensemble-methods.html#cb428-5" tabindex="-1"></a>                       <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb428-6"><a href="ensemble-methods.html#cb428-6" tabindex="-1"></a>myocarde <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(myocarde)</span>
<span id="cb428-7"><a href="ensemble-methods.html#cb428-7" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">head</span>(myocarde)</span>
<span id="cb428-8"><a href="ensemble-methods.html#cb428-8" tabindex="-1"></a>df<span class="sc">$</span>Weights <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">nrow</span>(df)</span>
<span id="cb428-9"><a href="ensemble-methods.html#cb428-9" tabindex="-1"></a>df</span></code></pre></div>
<pre><code>##   FRCAR INCAR INSYS PRDIA PAPUL PVENT REPUL  PRONO   Weights
## 1    90  1.71  19.0    16  19.5  16.0   912 SURVIE 0.1666667
## 2    90  1.68  18.7    24  31.0  14.0  1476  DECES 0.1666667
## 3   120  1.40  11.7    23  29.0   8.0  1657  DECES 0.1666667
## 4    82  1.79  21.8    14  17.5  10.0   782 SURVIE 0.1666667
## 5    80  1.58  19.7    21  28.0  18.5  1418  DECES 0.1666667
## 6    80  1.13  14.1    18  23.5   9.0  1664  DECES 0.1666667</code></pre>
<p>Suppose that our first stump misclassifies the first observation. So the error rate</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="ensemble-methods.html#cb430-1" tabindex="-1"></a><span class="co"># Alpha</span></span>
<span id="cb430-2"><a href="ensemble-methods.html#cb430-2" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(df)</span>
<span id="cb430-3"><a href="ensemble-methods.html#cb430-3" tabindex="-1"></a>err <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> n</span>
<span id="cb430-4"><a href="ensemble-methods.html#cb430-4" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">log</span>((<span class="dv">1</span> <span class="sc">-</span> err) <span class="sc">/</span> err)</span>
<span id="cb430-5"><a href="ensemble-methods.html#cb430-5" tabindex="-1"></a>alpha</span></code></pre></div>
<pre><code>## [1] 0.804719</code></pre>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="ensemble-methods.html#cb432-1" tabindex="-1"></a><span class="fu">exp</span>(alpha)</span></code></pre></div>
<pre><code>## [1] 2.236068</code></pre>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="ensemble-methods.html#cb434-1" tabindex="-1"></a><span class="co"># Weights for misclassified observations</span></span>
<span id="cb434-2"><a href="ensemble-methods.html#cb434-2" tabindex="-1"></a>weight_miss <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">/</span> n) <span class="sc">*</span> (<span class="fu">exp</span>(alpha))</span>
<span id="cb434-3"><a href="ensemble-methods.html#cb434-3" tabindex="-1"></a>weight_miss</span></code></pre></div>
<pre><code>## [1] 0.372678</code></pre>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="ensemble-methods.html#cb436-1" tabindex="-1"></a><span class="co"># Weights for correctly classified observations</span></span>
<span id="cb436-2"><a href="ensemble-methods.html#cb436-2" tabindex="-1"></a>weight_corr <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">/</span> n) <span class="sc">*</span> (<span class="fu">exp</span>(<span class="sc">-</span>alpha))</span>
<span id="cb436-3"><a href="ensemble-methods.html#cb436-3" tabindex="-1"></a>weight_corr</span></code></pre></div>
<pre><code>## [1] 0.0745356</code></pre>
<p>Hence, our new sample weights</p>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="ensemble-methods.html#cb438-1" tabindex="-1"></a>df<span class="sc">$</span>New_weights <span class="ot">&lt;-</span> <span class="fu">c</span>(weight_miss, <span class="fu">rep</span>(weight_corr, <span class="dv">5</span>))</span>
<span id="cb438-2"><a href="ensemble-methods.html#cb438-2" tabindex="-1"></a>df<span class="sc">$</span>Norm_weights <span class="ot">&lt;-</span> df<span class="sc">$</span>New_weight <span class="sc">/</span> <span class="fu">sum</span>(df<span class="sc">$</span>New_weight) <span class="co"># normalizing</span></span>
<span id="cb438-3"><a href="ensemble-methods.html#cb438-3" tabindex="-1"></a><span class="co"># Not reporting X&#39;s for now</span></span>
<span id="cb438-4"><a href="ensemble-methods.html#cb438-4" tabindex="-1"></a>df[, <span class="dv">8</span><span class="sc">:</span><span class="dv">11</span>]</span></code></pre></div>
<pre><code>##    PRONO   Weights New_weights Norm_weights
## 1 SURVIE 0.1666667   0.3726780          0.5
## 2  DECES 0.1666667   0.0745356          0.1
## 3  DECES 0.1666667   0.0745356          0.1
## 4 SURVIE 0.1666667   0.0745356          0.1
## 5  DECES 0.1666667   0.0745356          0.1
## 6  DECES 0.1666667   0.0745356          0.1</code></pre>
<p>We can see that the misclassified observation (the first one) has 5 times more likelihood than the other correctly classified observations. We now need to incorporate these weights and resample these six observations. Since incorrectly classified records have higher sample weights, the probability to select those records is very high.</p>
<p>If we use a simple tree as our base classifier, we can directly incorporate these weights into <code>rpart</code>. We can use other base classifier. In that case, we can do resampling with these probability weights:</p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="ensemble-methods.html#cb440-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb440-2"><a href="ensemble-methods.html#cb440-2" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">6</span>, <span class="dv">6</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> df<span class="sc">$</span>Norm_weights)</span>
<span id="cb440-3"><a href="ensemble-methods.html#cb440-3" tabindex="-1"></a>df[ind, <span class="sc">-</span><span class="fu">c</span>(<span class="dv">9</span><span class="sc">:</span><span class="dv">12</span>)] <span class="co"># After</span></span></code></pre></div>
<pre><code>##     FRCAR INCAR INSYS PRDIA PAPUL PVENT REPUL  PRONO
## 1      90  1.71  19.0    16  19.5  16.0   912 SURVIE
## 5      80  1.58  19.7    21  28.0  18.5  1418  DECES
## 1.1    90  1.71  19.0    16  19.5  16.0   912 SURVIE
## 6      80  1.13  14.1    18  23.5   9.0  1664  DECES
## 2      90  1.68  18.7    24  31.0  14.0  1476  DECES
## 1.2    90  1.71  19.0    16  19.5  16.0   912 SURVIE</code></pre>
<p>As we can see, the misclassified observation is repeated three times in the new sample. Hence, observations that are misclassified will have more influence in the next classifier. <strong>This is an incredible boost that forces the classification tree to adjust its prediction to do better job for misclassified observations.</strong></p>
<ol start="7" style="list-style-type: decimal">
<li>Finally, in the output, the contributions from classifiers that fit the data better are given more weight (a larger <span class="math inline">\(\alpha_b\)</span> means a better fit). Unlike a random forest algorithm where each tree gets an equal weight in final decision, here some stumps get more say in final classification. Moreover, “forest of stumps” the order of trees is important.</li>
</ol>
<p>Hence, the final prediction on <span class="math inline">\(y_i\)</span> will be combined from all trees, <span class="math inline">\(b\)</span> to B, through a weighted majority vote:</p>
<p><span class="math display">\[
\hat{y}_{i}=\operatorname{sign}\left(\sum_{b=1}^{B} \alpha_{b} \hat{m}_{b}(x)\right),
\]</span></p>
<p>which is a signum function defined as follows:</p>
<p><span class="math display">\[
\operatorname{sign}(x):=\left\{\begin{array}{ll}
{-1} &amp; {\text { if } x&lt;0} \\
{0} &amp; {\text { if } x=0} \\
{1} &amp; {\text { if } x&gt;0}
\end{array}\right.
\]</span></p>
<p>Here is a simple simulation to show how <span class="math inline">\(\alpha_b\)</span> will make the importance of each tree (<span class="math inline">\(\hat{m}_{b}(x)\)</span>) different:</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a href="ensemble-methods.html#cb442-1" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">1000</span></span>
<span id="cb442-2"><a href="ensemble-methods.html#cb442-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb442-3"><a href="ensemble-methods.html#cb442-3" tabindex="-1"></a>err <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb442-4"><a href="ensemble-methods.html#cb442-4" tabindex="-1"></a>alpha <span class="ot">=</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">log</span>((<span class="dv">1</span> <span class="sc">-</span> err) <span class="sc">/</span> err)</span>
<span id="cb442-5"><a href="ensemble-methods.html#cb442-5" tabindex="-1"></a>ind <span class="ot">=</span> <span class="fu">order</span>(err)</span>
<span id="cb442-6"><a href="ensemble-methods.html#cb442-6" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb442-7"><a href="ensemble-methods.html#cb442-7" tabindex="-1"></a>  err[ind],</span>
<span id="cb442-8"><a href="ensemble-methods.html#cb442-8" tabindex="-1"></a>  alpha[ind],</span>
<span id="cb442-9"><a href="ensemble-methods.html#cb442-9" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;error (err)&quot;</span>,</span>
<span id="cb442-10"><a href="ensemble-methods.html#cb442-10" tabindex="-1"></a>  <span class="at">ylab =</span> <span class="st">&quot;alpha&quot;</span>,</span>
<span id="cb442-11"><a href="ensemble-methods.html#cb442-11" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&quot;o&quot;</span>,</span>
<span id="cb442-12"><a href="ensemble-methods.html#cb442-12" tabindex="-1"></a>  <span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb442-13"><a href="ensemble-methods.html#cb442-13" tabindex="-1"></a>  <span class="at">lwd =</span> <span class="dv">2</span></span>
<span id="cb442-14"><a href="ensemble-methods.html#cb442-14" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en24-1.png" width="672" /></p>
<p>We can see that when there is no misclassification error (<code>err</code> = 0), “alpha” will be a large positive number. When the classifier very weak and predicts as good as a random guess (<code>err</code> = 0.5), the importance of the classifier will be 0. If all the observations are incorrectly classified (<code>err</code> = 1), our alpha value will be a negative integer.</p>
<p>The AdaBoost.M1 is known as a “discrete classifier” because it directly calculates discrete class labels <span class="math inline">\(\hat{y}_i\)</span>, rather than predicted probabilities, <span class="math inline">\(\hat{p}_i\)</span>.</p>
<p>What type of classifier, <span class="math inline">\(\hat{m}_{b}(x)\)</span>, would we choose? Usually a “weak classifier” like a “stump” (a two terminal-node classification tree, i.e one split) would be enough. The <span class="math inline">\(\hat{m}_{b}(x)\)</span> choose one variable to form a stump that gives the lowest Gini index.</p>
<p>Here is our simple example with the <code>myocarde</code> data to show how we can boost a simple weak learner (stump) by using AdaBoost algorithm:</p>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb443-1"><a href="ensemble-methods.html#cb443-1" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb443-2"><a href="ensemble-methods.html#cb443-2" tabindex="-1"></a></span>
<span id="cb443-3"><a href="ensemble-methods.html#cb443-3" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb443-4"><a href="ensemble-methods.html#cb443-4" tabindex="-1"></a>myocarde <span class="ot">&lt;-</span> <span class="fu">read_delim</span>(<span class="st">&quot;myocarde.csv&quot;</span>, <span class="at">delim =</span> <span class="st">&quot;;&quot;</span> ,</span>
<span id="cb443-5"><a href="ensemble-methods.html#cb443-5" tabindex="-1"></a>                       <span class="at">escape_double =</span> <span class="cn">FALSE</span>, <span class="at">trim_ws =</span> <span class="cn">TRUE</span>,</span>
<span id="cb443-6"><a href="ensemble-methods.html#cb443-6" tabindex="-1"></a>                       <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb443-7"><a href="ensemble-methods.html#cb443-7" tabindex="-1"></a></span>
<span id="cb443-8"><a href="ensemble-methods.html#cb443-8" tabindex="-1"></a>myocarde <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(myocarde)</span>
<span id="cb443-9"><a href="ensemble-methods.html#cb443-9" tabindex="-1"></a>y <span class="ot">&lt;-</span> (myocarde[ , <span class="st">&quot;PRONO&quot;</span>] <span class="sc">==</span> <span class="st">&quot;SURVIE&quot;</span>) <span class="sc">*</span> <span class="dv">2</span> <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb443-10"><a href="ensemble-methods.html#cb443-10" tabindex="-1"></a>x <span class="ot">&lt;-</span> myocarde[ , <span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>]</span>
<span id="cb443-11"><a href="ensemble-methods.html#cb443-11" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y)</span>
<span id="cb443-12"><a href="ensemble-methods.html#cb443-12" tabindex="-1"></a></span>
<span id="cb443-13"><a href="ensemble-methods.html#cb443-13" tabindex="-1"></a><span class="co"># Setting</span></span>
<span id="cb443-14"><a href="ensemble-methods.html#cb443-14" tabindex="-1"></a>rnd <span class="ot">=</span> <span class="dv">100</span> <span class="co"># number of rounds</span></span>
<span id="cb443-15"><a href="ensemble-methods.html#cb443-15" tabindex="-1"></a>m <span class="ot">=</span> <span class="fu">nrow</span>(x)</span>
<span id="cb443-16"><a href="ensemble-methods.html#cb443-16" tabindex="-1"></a>whts <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span> <span class="sc">/</span> m, m) <span class="co"># initial weights</span></span>
<span id="cb443-17"><a href="ensemble-methods.html#cb443-17" tabindex="-1"></a>st <span class="ot">&lt;-</span> <span class="fu">list</span>() <span class="co"># container to save all stumps</span></span>
<span id="cb443-18"><a href="ensemble-methods.html#cb443-18" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">&quot;numeric&quot;</span>, rnd) <span class="co"># container for alpha</span></span>
<span id="cb443-19"><a href="ensemble-methods.html#cb443-19" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">vector</span>(<span class="at">mode =</span> <span class="st">&quot;numeric&quot;</span>, m) <span class="co"># container for final predictions</span></span>
<span id="cb443-20"><a href="ensemble-methods.html#cb443-20" tabindex="-1"></a></span>
<span id="cb443-21"><a href="ensemble-methods.html#cb443-21" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb443-22"><a href="ensemble-methods.html#cb443-22" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>rnd) {</span>
<span id="cb443-23"><a href="ensemble-methods.html#cb443-23" tabindex="-1"></a>  st[[i]] <span class="ot">&lt;-</span> <span class="fu">rpart</span>(y <span class="sc">~</span>., <span class="at">data =</span> df, <span class="at">weights =</span> whts, <span class="at">maxdepth =</span> <span class="dv">1</span>, <span class="at">method =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb443-24"><a href="ensemble-methods.html#cb443-24" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(st[[i]], x, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb443-25"><a href="ensemble-methods.html#cb443-25" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(yhat))</span>
<span id="cb443-26"><a href="ensemble-methods.html#cb443-26" tabindex="-1"></a>  e <span class="ot">&lt;-</span> <span class="fu">sum</span>((yhat <span class="sc">!=</span> y) <span class="sc">*</span> whts)</span>
<span id="cb443-27"><a href="ensemble-methods.html#cb443-27" tabindex="-1"></a>  <span class="co"># alpha</span></span>
<span id="cb443-28"><a href="ensemble-methods.html#cb443-28" tabindex="-1"></a>  alpha[i] <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">log</span>((<span class="dv">1</span> <span class="sc">-</span> e) <span class="sc">/</span> e)</span>
<span id="cb443-29"><a href="ensemble-methods.html#cb443-29" tabindex="-1"></a>  <span class="co"># Updating weights </span></span>
<span id="cb443-30"><a href="ensemble-methods.html#cb443-30" tabindex="-1"></a>  <span class="co"># Note that, for true predictions, (y * yhat) will be +, otherwise -</span></span>
<span id="cb443-31"><a href="ensemble-methods.html#cb443-31" tabindex="-1"></a>  whts <span class="ot">&lt;-</span> whts <span class="sc">*</span> <span class="fu">exp</span>(<span class="sc">-</span>alpha[i] <span class="sc">*</span> y <span class="sc">*</span> yhat)</span>
<span id="cb443-32"><a href="ensemble-methods.html#cb443-32" tabindex="-1"></a>  <span class="co"># Normalizing weights</span></span>
<span id="cb443-33"><a href="ensemble-methods.html#cb443-33" tabindex="-1"></a>  whts <span class="ot">&lt;-</span> whts <span class="sc">/</span> <span class="fu">sum</span>(whts)</span>
<span id="cb443-34"><a href="ensemble-methods.html#cb443-34" tabindex="-1"></a>}</span>
<span id="cb443-35"><a href="ensemble-methods.html#cb443-35" tabindex="-1"></a> </span>
<span id="cb443-36"><a href="ensemble-methods.html#cb443-36" tabindex="-1"></a><span class="co"># Using each stump for final predictions</span></span>
<span id="cb443-37"><a href="ensemble-methods.html#cb443-37" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>rnd) {</span>
<span id="cb443-38"><a href="ensemble-methods.html#cb443-38" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">predict</span>(st[[i]], df, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb443-39"><a href="ensemble-methods.html#cb443-39" tabindex="-1"></a>  pred <span class="ot">=</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(pred))</span>
<span id="cb443-40"><a href="ensemble-methods.html#cb443-40" tabindex="-1"></a>  y_hat <span class="ot">=</span> y_hat <span class="sc">+</span> (alpha[i] <span class="sc">*</span> pred)</span>
<span id="cb443-41"><a href="ensemble-methods.html#cb443-41" tabindex="-1"></a>}</span>
<span id="cb443-42"><a href="ensemble-methods.html#cb443-42" tabindex="-1"></a></span>
<span id="cb443-43"><a href="ensemble-methods.html#cb443-43" tabindex="-1"></a><span class="co"># Let&#39;s see what y_hat is</span></span>
<span id="cb443-44"><a href="ensemble-methods.html#cb443-44" tabindex="-1"></a>y_hat</span></code></pre></div>
<pre><code>##  [1]   3.132649  -4.135656  -4.290437   7.547707  -3.118702  -6.946686
##  [7]   2.551433   1.960603   9.363346   6.221990   3.012195   6.982287
## [13]   9.765139   8.053999   8.494254   7.454104   4.112493   5.838279
## [19]   4.918513   9.514860   9.765139  -3.519537  -3.172093  -7.134057
## [25]  -3.066699  -4.539863  -2.532759  -2.490742   5.412605   2.903552
## [31]   2.263095  -6.718090  -2.790474   6.813963  -5.131830   3.680202
## [37]   3.495350   3.014052  -7.435835   6.594157  -7.435835  -6.838387
## [43]   3.951168   5.091548  -3.594420   8.237515  -6.718090  -9.582674
## [49]   2.658501 -10.282682   4.490239   9.765139  -5.891116  -5.593352
## [55]   6.802687  -2.059754   2.832103   7.655197  10.635851   9.312842
## [61]  -5.804151   2.464149  -5.634676   1.938855   9.765139   7.023157
## [67]  -6.078756  -7.031840   5.651634  -1.867942   9.472835</code></pre>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb445-1"><a href="ensemble-methods.html#cb445-1" tabindex="-1"></a><span class="co"># sign() function</span></span>
<span id="cb445-2"><a href="ensemble-methods.html#cb445-2" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">sign</span>(y_hat)</span>
<span id="cb445-3"><a href="ensemble-methods.html#cb445-3" tabindex="-1"></a></span>
<span id="cb445-4"><a href="ensemble-methods.html#cb445-4" tabindex="-1"></a><span class="co"># Confusion matrix</span></span>
<span id="cb445-5"><a href="ensemble-methods.html#cb445-5" tabindex="-1"></a><span class="fu">table</span>(pred, y)</span></code></pre></div>
<pre><code>##     y
## pred -1  1
##   -1 29  0
##   1   0 42</code></pre>
<p>This is our in-sample confusion table. We can also see several stumps:</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="ensemble-methods.html#cb447-1" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb447-2"><a href="ensemble-methods.html#cb447-2" tabindex="-1"></a></span>
<span id="cb447-3"><a href="ensemble-methods.html#cb447-3" tabindex="-1"></a>plt <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">30</span>, <span class="dv">60</span>, <span class="dv">90</span>)</span>
<span id="cb447-4"><a href="ensemble-methods.html#cb447-4" tabindex="-1"></a></span>
<span id="cb447-5"><a href="ensemble-methods.html#cb447-5" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb447-6"><a href="ensemble-methods.html#cb447-6" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(plt)){</span>
<span id="cb447-7"><a href="ensemble-methods.html#cb447-7" tabindex="-1"></a><span class="fu">prp</span>(st[[i]], <span class="at">type =</span> <span class="dv">2</span>, <span class="at">extra =</span> <span class="dv">1</span>, <span class="at">split.col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb447-8"><a href="ensemble-methods.html#cb447-8" tabindex="-1"></a>    <span class="at">split.border.col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">box.col =</span> <span class="st">&quot;pink&quot;</span>)</span>
<span id="cb447-9"><a href="ensemble-methods.html#cb447-9" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en26-1.png" width="672" /></p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="ensemble-methods.html#cb448-1" tabindex="-1"></a><span class="fu">par</span>(p)</span></code></pre></div>
<p>Let’s see it with the <code>JOUSBoost</code> <a href="https://cran.r-project.org/web/packages/JOUSBoost/vignettes/JOUS.pdf">package</a>:</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="ensemble-methods.html#cb449-1" tabindex="-1"></a><span class="fu">library</span>(JOUSBoost)</span>
<span id="cb449-2"><a href="ensemble-methods.html#cb449-2" tabindex="-1"></a>ada <span class="ot">&lt;-</span> <span class="fu">adaboost</span>(<span class="fu">as.matrix</span>(x), y, <span class="at">tree_depth =</span> <span class="dv">1</span>, <span class="at">n_rounds =</span> rnd)</span>
<span id="cb449-3"><a href="ensemble-methods.html#cb449-3" tabindex="-1"></a><span class="fu">summary</span>(ada)</span></code></pre></div>
<pre><code>##                  Length Class  Mode   
## alphas           100    -none- numeric
## trees            100    -none- list   
## tree_depth         1    -none- numeric
## terms              3    terms  call   
## confusion_matrix   4    table  numeric</code></pre>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="ensemble-methods.html#cb451-1" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ada, x)</span>
<span id="cb451-2"><a href="ensemble-methods.html#cb451-2" tabindex="-1"></a><span class="fu">table</span>(pred, y)</span></code></pre></div>
<pre><code>##     y
## pred -1  1
##   -1 29  0
##   1   0 42</code></pre>
<p>These results provide in-sample predictions. When we use it in a real example, we can train AdaBoost.M1 by the tree depths (1 in our example) and the number of iterations (100 trees in our example).</p>
<p>An application is provided in the next chapter.</p>
</div>
<div id="xgboost" class="section level3 hasAnchor" number="17.3.3">
<h3><span class="header-section-number">17.3.3</span> XGBoost<a href="ensemble-methods.html#xgboost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Extreme Gradient Boosting (XGBoost) the most efficient version of the gradient boosting framework by its capacity to implement parallel computation on a single machine. It can be used for regression and classification problems with two modes: linear models and tree learning algorithm. That means XGBoost can also be used for regularization in linear models (Section VIII). As decision trees are much better to catch a nonlinear link between predictors and outcome, comparison between two modes can provide quick information to the practitioner, specially in causal analyses, about the structure of alternative models.</p>
<p>The XGBoost has several unique advantages: its speed is measured as “10 times faster than the <code>gbm</code>” (see its <a href="https://xgboost.readthedocs.io/en/stable/R-package/xgboostPresentation.html">vignette</a>) and it accepts very efficient input data structures, such as a <em>sparse</em> matrix<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>. This special input structure in <code>xgboost</code> requires some additional data preparation: a matrix input for the features and a vector for the response. Therefore, a matrix input of the features requires to encode our categorical variables. The matrix can also be selected several possible choices: a regular R matrix, a sparse matrix from the <code>Matrix</code> package, and its own class, <code>xgb.Matrix</code>.</p>
<p>We start with a regression example here and leave the classification example to the next chapter in boosting applications. We will use the <a href="https://www.tmwr.org/ames.html">Ames housing data</a> to see the best “predictors” of the sale price.</p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a href="ensemble-methods.html#cb453-1" tabindex="-1"></a><span class="fu">library</span>(xgboost)</span>
<span id="cb453-2"><a href="ensemble-methods.html#cb453-2" tabindex="-1"></a><span class="fu">library</span>(mltools)</span>
<span id="cb453-3"><a href="ensemble-methods.html#cb453-3" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb453-4"><a href="ensemble-methods.html#cb453-4" tabindex="-1"></a><span class="fu">library</span>(modeldata) <span class="co"># This can also be loaded by the tidymodels package</span></span>
<span id="cb453-5"><a href="ensemble-methods.html#cb453-5" tabindex="-1"></a><span class="fu">data</span>(ames)</span>
<span id="cb453-6"><a href="ensemble-methods.html#cb453-6" tabindex="-1"></a><span class="fu">dim</span>(ames)</span></code></pre></div>
<pre><code>## [1] 2930   74</code></pre>
<p>Since, the <code>xgboost</code> algorithm accepts its input data as a matrix, all categorical variables have be one-hot coded, which creates a large matrix even with a small size data. That’s why using more memory efficient matrix types (sparse matrix etc.) speeds up the process. We ignore it here and use a regular R matrix, for now.</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a href="ensemble-methods.html#cb455-1" tabindex="-1"></a>ames_new <span class="ot">&lt;-</span> <span class="fu">one_hot</span>(<span class="fu">as.data.table</span>(ames))</span>
<span id="cb455-2"><a href="ensemble-methods.html#cb455-2" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(ames_new)</span>
<span id="cb455-3"><a href="ensemble-methods.html#cb455-3" tabindex="-1"></a></span>
<span id="cb455-4"><a href="ensemble-methods.html#cb455-4" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb455-5"><a href="ensemble-methods.html#cb455-5" tabindex="-1"></a>train <span class="ot">&lt;-</span> df[ind,]</span>
<span id="cb455-6"><a href="ensemble-methods.html#cb455-6" tabindex="-1"></a>test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind,]</span>
<span id="cb455-7"><a href="ensemble-methods.html#cb455-7" tabindex="-1"></a></span>
<span id="cb455-8"><a href="ensemble-methods.html#cb455-8" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(train[,<span class="sc">-</span><span class="fu">which</span>(<span class="fu">names</span>(train) <span class="sc">==</span> <span class="st">&quot;Sale_Price&quot;</span>)])</span>
<span id="cb455-9"><a href="ensemble-methods.html#cb455-9" tabindex="-1"></a>Y <span class="ot">&lt;-</span> train<span class="sc">$</span>Sale_Price</span></code></pre></div>
<p>Now we are ready for finding the optimal tuning parameters. One strategy in tuning is to see if there is a substantial difference between train and CV errors. We first start with the number of trees and the learning rate. If the difference still persists, we introduce regularization parameters. There are three regularization parameters: <code>gamma</code>, <code>lambda</code>, and <code>alpha</code>. The last two are similar to what we will see in regularization in Section VIII.</p>
<p>Here is our first run without a grid search. We will have a regression tree. The default booster is <code>gbtree</code> for tree-based models. For linear models it should be set to <code>gblinear</code>. The number of parameters and their combinations are very extensive in XGBoost. Please see them here: <a href="https://xgboost.readthedocs.io/en/latest/parameter.html#global-configuration" class="uri">https://xgboost.readthedocs.io/en/latest/parameter.html#global-configuration</a>. The combination of parameters we picked below is just an example.</p>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="ensemble-methods.html#cb456-1" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb456-2"><a href="ensemble-methods.html#cb456-2" tabindex="-1"></a>params <span class="ot">=</span> <span class="fu">list</span>(</span>
<span id="cb456-3"><a href="ensemble-methods.html#cb456-3" tabindex="-1"></a>  <span class="at">eta =</span> <span class="fl">0.1</span>, <span class="co"># Step size in boosting (default is 0.3)</span></span>
<span id="cb456-4"><a href="ensemble-methods.html#cb456-4" tabindex="-1"></a>  <span class="at">max_depth =</span> <span class="dv">3</span>, <span class="co"># maximum depth of the tree (default is 6)</span></span>
<span id="cb456-5"><a href="ensemble-methods.html#cb456-5" tabindex="-1"></a>  <span class="at">min_child_weight =</span> <span class="dv">3</span>, <span class="co"># minimum number of instances in each node</span></span>
<span id="cb456-6"><a href="ensemble-methods.html#cb456-6" tabindex="-1"></a>  <span class="at">subsample =</span> <span class="fl">0.8</span>, <span class="co"># Subsample ratio of the training instances</span></span>
<span id="cb456-7"><a href="ensemble-methods.html#cb456-7" tabindex="-1"></a>  <span class="at">colsample_bytree =</span> <span class="fl">1.0</span> <span class="co"># the fraction of columns to be subsampled</span></span>
<span id="cb456-8"><a href="ensemble-methods.html#cb456-8" tabindex="-1"></a>)</span>
<span id="cb456-9"><a href="ensemble-methods.html#cb456-9" tabindex="-1"></a></span>
<span id="cb456-10"><a href="ensemble-methods.html#cb456-10" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb456-11"><a href="ensemble-methods.html#cb456-11" tabindex="-1"></a>boost <span class="ot">&lt;-</span> <span class="fu">xgb.cv</span>(</span>
<span id="cb456-12"><a href="ensemble-methods.html#cb456-12" tabindex="-1"></a>  <span class="at">data =</span> X,</span>
<span id="cb456-13"><a href="ensemble-methods.html#cb456-13" tabindex="-1"></a>  <span class="at">label =</span> Y,</span>
<span id="cb456-14"><a href="ensemble-methods.html#cb456-14" tabindex="-1"></a>  <span class="at">nrounds =</span> <span class="dv">3000</span>, <span class="co"># the max number of iterations</span></span>
<span id="cb456-15"><a href="ensemble-methods.html#cb456-15" tabindex="-1"></a>  <span class="at">nthread =</span> <span class="dv">4</span>, <span class="co"># the number of CPU cores</span></span>
<span id="cb456-16"><a href="ensemble-methods.html#cb456-16" tabindex="-1"></a>  <span class="at">objective =</span> <span class="st">&quot;reg:squarederror&quot;</span>, <span class="co"># regression tree</span></span>
<span id="cb456-17"><a href="ensemble-methods.html#cb456-17" tabindex="-1"></a>  <span class="at">early_stopping_rounds =</span> <span class="dv">50</span>, <span class="co"># Stop if doesn&#39;t improve after 50 rounds </span></span>
<span id="cb456-18"><a href="ensemble-methods.html#cb456-18" tabindex="-1"></a>  <span class="at">nfold =</span> <span class="dv">10</span>, <span class="co"># 10-fold-CV</span></span>
<span id="cb456-19"><a href="ensemble-methods.html#cb456-19" tabindex="-1"></a>  <span class="at">params =</span> params,</span>
<span id="cb456-20"><a href="ensemble-methods.html#cb456-20" tabindex="-1"></a>  <span class="at">verbose =</span> <span class="dv">0</span> <span class="co">#silent</span></span>
<span id="cb456-21"><a href="ensemble-methods.html#cb456-21" tabindex="-1"></a>)  </span></code></pre></div>
<p>Let’s see the RMSE and the best iteration:</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="ensemble-methods.html#cb457-1" tabindex="-1"></a>best_it <span class="ot">&lt;-</span> boost<span class="sc">$</span>best_iteration</span>
<span id="cb457-2"><a href="ensemble-methods.html#cb457-2" tabindex="-1"></a>best_it</span></code></pre></div>
<pre><code>## [1] 1781</code></pre>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb459-1"><a href="ensemble-methods.html#cb459-1" tabindex="-1"></a><span class="fu">min</span>(boost<span class="sc">$</span>evaluation_log<span class="sc">$</span>test_rmse_mean)</span></code></pre></div>
<pre><code>## [1] 16807.16</code></pre>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb461-1"><a href="ensemble-methods.html#cb461-1" tabindex="-1"></a><span class="co"># One possible grid would be:</span></span>
<span id="cb461-2"><a href="ensemble-methods.html#cb461-2" tabindex="-1"></a><span class="co"># param_grid &lt;- expand.grid(</span></span>
<span id="cb461-3"><a href="ensemble-methods.html#cb461-3" tabindex="-1"></a><span class="co">#   eta = 0.01,</span></span>
<span id="cb461-4"><a href="ensemble-methods.html#cb461-4" tabindex="-1"></a><span class="co">#   max_depth = 3,</span></span>
<span id="cb461-5"><a href="ensemble-methods.html#cb461-5" tabindex="-1"></a><span class="co">#   min_child_weight = 3,</span></span>
<span id="cb461-6"><a href="ensemble-methods.html#cb461-6" tabindex="-1"></a><span class="co">#   subsample = 0.5,</span></span>
<span id="cb461-7"><a href="ensemble-methods.html#cb461-7" tabindex="-1"></a><span class="co">#   colsample_bytree = 0.5,</span></span>
<span id="cb461-8"><a href="ensemble-methods.html#cb461-8" tabindex="-1"></a><span class="co">#   gamma = c(0, 1, 10, 100, 1000),</span></span>
<span id="cb461-9"><a href="ensemble-methods.html#cb461-9" tabindex="-1"></a><span class="co">#   lambda = seq(0, 0.01, 0.1, 1, 100, 1000),</span></span>
<span id="cb461-10"><a href="ensemble-methods.html#cb461-10" tabindex="-1"></a><span class="co">#   alpha = c(0, 0.01, 0.1, 1, 100, 1000)</span></span>
<span id="cb461-11"><a href="ensemble-methods.html#cb461-11" tabindex="-1"></a><span class="co">#   )</span></span>
<span id="cb461-12"><a href="ensemble-methods.html#cb461-12" tabindex="-1"></a></span>
<span id="cb461-13"><a href="ensemble-methods.html#cb461-13" tabindex="-1"></a><span class="co"># After going through the grid in a loop with `xgb.cv`</span></span>
<span id="cb461-14"><a href="ensemble-methods.html#cb461-14" tabindex="-1"></a><span class="co"># we save multiple `test_rmse_mean` and `best_iteration`</span></span>
<span id="cb461-15"><a href="ensemble-methods.html#cb461-15" tabindex="-1"></a><span class="co"># and find the parameters that gives the minimum rmse</span></span></code></pre></div>
<p>Now after identifying the tuning parameters, we build the best model:</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb462-1"><a href="ensemble-methods.html#cb462-1" tabindex="-1"></a>tr_model <span class="ot">&lt;-</span> <span class="fu">xgboost</span>(</span>
<span id="cb462-2"><a href="ensemble-methods.html#cb462-2" tabindex="-1"></a>  <span class="at">params =</span> params,</span>
<span id="cb462-3"><a href="ensemble-methods.html#cb462-3" tabindex="-1"></a>  <span class="at">data =</span> X,</span>
<span id="cb462-4"><a href="ensemble-methods.html#cb462-4" tabindex="-1"></a>  <span class="at">label =</span> Y,</span>
<span id="cb462-5"><a href="ensemble-methods.html#cb462-5" tabindex="-1"></a>  <span class="at">nrounds =</span> best_it,</span>
<span id="cb462-6"><a href="ensemble-methods.html#cb462-6" tabindex="-1"></a>  <span class="at">objective =</span> <span class="st">&quot;reg:squarederror&quot;</span>,</span>
<span id="cb462-7"><a href="ensemble-methods.html#cb462-7" tabindex="-1"></a>  <span class="at">verbose =</span> <span class="dv">0</span></span>
<span id="cb462-8"><a href="ensemble-methods.html#cb462-8" tabindex="-1"></a>)</span></code></pre></div>
<p>We can obtain the top 10 influential features in our final model using the impurity (gain) metric:</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="ensemble-methods.html#cb463-1" tabindex="-1"></a><span class="fu">library</span>(vip)</span>
<span id="cb463-2"><a href="ensemble-methods.html#cb463-2" tabindex="-1"></a><span class="fu">vip</span>(tr_model,</span>
<span id="cb463-3"><a href="ensemble-methods.html#cb463-3" tabindex="-1"></a>    <span class="at">aesthetics =</span> <span class="fu">list</span>(<span class="at">color =</span> <span class="st">&quot;green&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;orange&quot;</span>)) </span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/en33-1.png" width="672" /></p>
<p>Now, we can use our trained model for predictions using our test set. Note that, again, <code>xgboost</code> would only accept matrix inputs.</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb464-1"><a href="ensemble-methods.html#cb464-1" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(tr_model,</span>
<span id="cb464-2"><a href="ensemble-methods.html#cb464-2" tabindex="-1"></a>                <span class="fu">as.matrix</span>(test[, <span class="sc">-</span><span class="fu">which</span>(<span class="fu">names</span>(test) <span class="sc">==</span> <span class="st">&quot;Sale_Price&quot;</span>)]))</span>
<span id="cb464-3"><a href="ensemble-methods.html#cb464-3" tabindex="-1"></a>rmse_test <span class="ot">&lt;-</span></span>
<span id="cb464-4"><a href="ensemble-methods.html#cb464-4" tabindex="-1"></a>  <span class="fu">sqrt</span>(<span class="fu">mean</span>((test[, <span class="fu">which</span>(<span class="fu">names</span>(train) <span class="sc">==</span> <span class="st">&quot;Sale_Price&quot;</span>)] <span class="sc">-</span> yhat) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb464-5"><a href="ensemble-methods.html#cb464-5" tabindex="-1"></a>rmse_test</span></code></pre></div>
<pre><code>## [1] 23364.86</code></pre>
<p>Note the big difference between training and test RMSPE’s. This is an indication that our “example grid” is not doing a good job. We should include regularization tuning parameters and run a full scale grid search. We will look at a classification example in the next chapter (Chapter 13).</p>
</div>
</div>
<div id="ensemble-applications" class="section level2 hasAnchor" number="17.4">
<h2><span class="header-section-number">17.4</span> Ensemble Applications<a href="ensemble-methods.html#ensemble-applications" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To conclude this section we will cover classification and regression applications using bagging, random forest and, boosting. First we will start with a classification problem. In comparing different ensemble methods, we must look not only at their accuracy, but evaluate their stability as well.</p>
</div>
<div id="classification" class="section level2 hasAnchor" number="17.5">
<h2><span class="header-section-number">17.5</span> Classification<a href="ensemble-methods.html#classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will again predict survival on the Titanic, using CART, bagging and random forest. We will use the following variables:</p>
<p><code>survived</code> - 1 if true, 0 otherwise;<br />
<code>sex</code> - the gender of the passenger;<br />
<code>age</code> - age of the passenger in years;<br />
<code>pclass</code> - the passengers class of passage;<br />
<code>sibsp</code> - the number of siblings/spouses aboard;<br />
<code>parch</code> - the number of parents/children aboard.</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="ensemble-methods.html#cb466-1" tabindex="-1"></a><span class="fu">library</span>(PASWR)</span>
<span id="cb466-2"><a href="ensemble-methods.html#cb466-2" tabindex="-1"></a><span class="fu">library</span>(ROCR)</span>
<span id="cb466-3"><a href="ensemble-methods.html#cb466-3" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb466-4"><a href="ensemble-methods.html#cb466-4" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb466-5"><a href="ensemble-methods.html#cb466-5" tabindex="-1"></a></span>
<span id="cb466-6"><a href="ensemble-methods.html#cb466-6" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb466-7"><a href="ensemble-methods.html#cb466-7" tabindex="-1"></a><span class="fu">data</span>(titanic3)</span>
<span id="cb466-8"><a href="ensemble-methods.html#cb466-8" tabindex="-1"></a>nam <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;survived&quot;</span>, <span class="st">&quot;sex&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;pclass&quot;</span>, <span class="st">&quot;sibsp&quot;</span>, <span class="st">&quot;parch&quot;</span>)</span>
<span id="cb466-9"><a href="ensemble-methods.html#cb466-9" tabindex="-1"></a>df <span class="ot">&lt;-</span> titanic3[, nam]</span>
<span id="cb466-10"><a href="ensemble-methods.html#cb466-10" tabindex="-1"></a>dfc <span class="ot">&lt;-</span> df[<span class="fu">complete.cases</span>(df), ]</span>
<span id="cb466-11"><a href="ensemble-methods.html#cb466-11" tabindex="-1"></a>dfc<span class="sc">$</span>survived <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(dfc<span class="sc">$</span>survived)</span>
<span id="cb466-12"><a href="ensemble-methods.html#cb466-12" tabindex="-1"></a></span>
<span id="cb466-13"><a href="ensemble-methods.html#cb466-13" tabindex="-1"></a>AUC1 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb466-14"><a href="ensemble-methods.html#cb466-14" tabindex="-1"></a>AUC2 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb466-15"><a href="ensemble-methods.html#cb466-15" tabindex="-1"></a>AUC3 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb466-16"><a href="ensemble-methods.html#cb466-16" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb466-17"><a href="ensemble-methods.html#cb466-17" tabindex="-1"></a>B <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb466-18"><a href="ensemble-methods.html#cb466-18" tabindex="-1"></a></span>
<span id="cb466-19"><a href="ensemble-methods.html#cb466-19" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb466-20"><a href="ensemble-methods.html#cb466-20" tabindex="-1"></a>  <span class="fu">set.seed</span>(i<span class="sc">+</span>i<span class="sc">*</span><span class="dv">100</span>)</span>
<span id="cb466-21"><a href="ensemble-methods.html#cb466-21" tabindex="-1"></a>  ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(dfc), <span class="fu">nrow</span>(dfc), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb466-22"><a href="ensemble-methods.html#cb466-22" tabindex="-1"></a>  train <span class="ot">&lt;-</span> dfc[ind, ]</span>
<span id="cb466-23"><a href="ensemble-methods.html#cb466-23" tabindex="-1"></a>  test <span class="ot">&lt;-</span> dfc[<span class="sc">-</span>ind, ]</span>
<span id="cb466-24"><a href="ensemble-methods.html#cb466-24" tabindex="-1"></a>  </span>
<span id="cb466-25"><a href="ensemble-methods.html#cb466-25" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">ncol</span>(train)<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb466-26"><a href="ensemble-methods.html#cb466-26" tabindex="-1"></a></span>
<span id="cb466-27"><a href="ensemble-methods.html#cb466-27" tabindex="-1"></a>  <span class="co">#3 Methods</span></span>
<span id="cb466-28"><a href="ensemble-methods.html#cb466-28" tabindex="-1"></a>  model1 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(survived<span class="sc">~</span>sex<span class="sc">+</span>age<span class="sc">+</span>pclass<span class="sc">+</span>sibsp<span class="sc">+</span>parch,</span>
<span id="cb466-29"><a href="ensemble-methods.html#cb466-29" tabindex="-1"></a>                <span class="at">data=</span>train, <span class="at">method=</span><span class="st">&quot;class&quot;</span>) <span class="co">#Single tree, pruned</span></span>
<span id="cb466-30"><a href="ensemble-methods.html#cb466-30" tabindex="-1"></a>  model2 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(survived<span class="sc">~</span>sex<span class="sc">+</span>age<span class="sc">+</span>pclass<span class="sc">+</span>sibsp<span class="sc">+</span>parch,</span>
<span id="cb466-31"><a href="ensemble-methods.html#cb466-31" tabindex="-1"></a>                         <span class="at">ntree =</span> B, <span class="at">mtry =</span> p, <span class="at">data =</span> train) <span class="co">#Bagged</span></span>
<span id="cb466-32"><a href="ensemble-methods.html#cb466-32" tabindex="-1"></a>  model3 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(survived<span class="sc">~</span>sex<span class="sc">+</span>age<span class="sc">+</span>pclass<span class="sc">+</span>sibsp<span class="sc">+</span>parch,</span>
<span id="cb466-33"><a href="ensemble-methods.html#cb466-33" tabindex="-1"></a>                         <span class="at">ntree =</span> B, <span class="at">data =</span> train, <span class="at">localImp =</span> <span class="cn">TRUE</span>) <span class="co"># RF    </span></span>
<span id="cb466-34"><a href="ensemble-methods.html#cb466-34" tabindex="-1"></a>  </span>
<span id="cb466-35"><a href="ensemble-methods.html#cb466-35" tabindex="-1"></a>  phat1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb466-36"><a href="ensemble-methods.html#cb466-36" tabindex="-1"></a>  phat2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model2, test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb466-37"><a href="ensemble-methods.html#cb466-37" tabindex="-1"></a>  phat3 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model3, test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb466-38"><a href="ensemble-methods.html#cb466-38" tabindex="-1"></a>  </span>
<span id="cb466-39"><a href="ensemble-methods.html#cb466-39" tabindex="-1"></a>  <span class="co">#AUC1</span></span>
<span id="cb466-40"><a href="ensemble-methods.html#cb466-40" tabindex="-1"></a>  pred_rocr1 <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat1[,<span class="dv">2</span>], test<span class="sc">$</span>survived)</span>
<span id="cb466-41"><a href="ensemble-methods.html#cb466-41" tabindex="-1"></a>  auc_ROCR1 <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr1, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb466-42"><a href="ensemble-methods.html#cb466-42" tabindex="-1"></a>  AUC1[i] <span class="ot">&lt;-</span> auc_ROCR1<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb466-43"><a href="ensemble-methods.html#cb466-43" tabindex="-1"></a>  </span>
<span id="cb466-44"><a href="ensemble-methods.html#cb466-44" tabindex="-1"></a>  <span class="co">#AUC2</span></span>
<span id="cb466-45"><a href="ensemble-methods.html#cb466-45" tabindex="-1"></a>  pred_rocr2 <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat2[,<span class="dv">2</span>], test<span class="sc">$</span>survived)</span>
<span id="cb466-46"><a href="ensemble-methods.html#cb466-46" tabindex="-1"></a>  auc_ROCR2 <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr2, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb466-47"><a href="ensemble-methods.html#cb466-47" tabindex="-1"></a>  AUC2[i] <span class="ot">&lt;-</span> auc_ROCR2<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb466-48"><a href="ensemble-methods.html#cb466-48" tabindex="-1"></a>  </span>
<span id="cb466-49"><a href="ensemble-methods.html#cb466-49" tabindex="-1"></a>  <span class="co">#AUC3</span></span>
<span id="cb466-50"><a href="ensemble-methods.html#cb466-50" tabindex="-1"></a>  pred_rocr3 <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat3[,<span class="dv">2</span>], test<span class="sc">$</span>survived)</span>
<span id="cb466-51"><a href="ensemble-methods.html#cb466-51" tabindex="-1"></a>  auc_ROCR3 <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr3, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb466-52"><a href="ensemble-methods.html#cb466-52" tabindex="-1"></a>  AUC3[i] <span class="ot">&lt;-</span> auc_ROCR3<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb466-53"><a href="ensemble-methods.html#cb466-53" tabindex="-1"></a>}</span>
<span id="cb466-54"><a href="ensemble-methods.html#cb466-54" tabindex="-1"></a></span>
<span id="cb466-55"><a href="ensemble-methods.html#cb466-55" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Single-Tree&quot;</span>, <span class="st">&quot;Bagging&quot;</span>, <span class="st">&quot;RF&quot;</span>)</span>
<span id="cb466-56"><a href="ensemble-methods.html#cb466-56" tabindex="-1"></a>AUCs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">mean</span>(AUC1), <span class="fu">mean</span>(AUC2), <span class="fu">mean</span>(AUC3))</span>
<span id="cb466-57"><a href="ensemble-methods.html#cb466-57" tabindex="-1"></a>sd <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">sqrt</span>(<span class="fu">var</span>(AUC1)), <span class="fu">sqrt</span>(<span class="fu">var</span>(AUC2)), <span class="fu">sqrt</span>(<span class="fu">var</span>(AUC3)))</span>
<span id="cb466-58"><a href="ensemble-methods.html#cb466-58" tabindex="-1"></a><span class="fu">data.frame</span>(model, AUCs, sd) </span></code></pre></div>
<pre><code>##         model      AUCs         sd
## 1 Single-Tree 0.8129740 0.02585391
## 2     Bagging 0.8129736 0.01713075
## 3          RF 0.8413922 0.01684504</code></pre>
<p>There is a consensus that we can determine a bagged model’s test error without using cross-validation. We used <code>randomForest</code> for bagging in the previous application. By default, bagging grows classification trees to their maximal size. If we want to prune each tree, however, it is not clear whether or not this may decrease prediction error. Let’s see if we can obtain a similar result with our manual bagging using <code>rpart</code> pruned and unpruned:</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="ensemble-methods.html#cb468-1" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb468-2"><a href="ensemble-methods.html#cb468-2" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb468-3"><a href="ensemble-methods.html#cb468-3" tabindex="-1"></a>AUCp <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb468-4"><a href="ensemble-methods.html#cb468-4" tabindex="-1"></a>AUCup <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb468-5"><a href="ensemble-methods.html#cb468-5" tabindex="-1"></a></span>
<span id="cb468-6"><a href="ensemble-methods.html#cb468-6" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb468-7"><a href="ensemble-methods.html#cb468-7" tabindex="-1"></a>  </span>
<span id="cb468-8"><a href="ensemble-methods.html#cb468-8" tabindex="-1"></a>  <span class="fu">set.seed</span>(i<span class="sc">+</span>i<span class="sc">*</span><span class="dv">100</span>)</span>
<span id="cb468-9"><a href="ensemble-methods.html#cb468-9" tabindex="-1"></a>  ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(dfc), <span class="fu">nrow</span>(dfc), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb468-10"><a href="ensemble-methods.html#cb468-10" tabindex="-1"></a>  train <span class="ot">&lt;-</span> dfc[ind, ]</span>
<span id="cb468-11"><a href="ensemble-methods.html#cb468-11" tabindex="-1"></a>  test <span class="ot">&lt;-</span> dfc[<span class="sc">-</span>ind, ]</span>
<span id="cb468-12"><a href="ensemble-methods.html#cb468-12" tabindex="-1"></a>  </span>
<span id="cb468-13"><a href="ensemble-methods.html#cb468-13" tabindex="-1"></a>  phatp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, B, <span class="fu">nrow</span>(test))</span>
<span id="cb468-14"><a href="ensemble-methods.html#cb468-14" tabindex="-1"></a>  phatup <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, B, <span class="fu">nrow</span>(test))</span>
<span id="cb468-15"><a href="ensemble-methods.html#cb468-15" tabindex="-1"></a></span>
<span id="cb468-16"><a href="ensemble-methods.html#cb468-16" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B) {</span>
<span id="cb468-17"><a href="ensemble-methods.html#cb468-17" tabindex="-1"></a>    <span class="fu">set.seed</span>(j<span class="sc">+</span>j<span class="sc">*</span><span class="dv">2</span>)</span>
<span id="cb468-18"><a href="ensemble-methods.html#cb468-18" tabindex="-1"></a>    ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(train), <span class="fu">nrow</span>(train), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb468-19"><a href="ensemble-methods.html#cb468-19" tabindex="-1"></a>    tr <span class="ot">&lt;-</span> train[ind, ]</span>
<span id="cb468-20"><a href="ensemble-methods.html#cb468-20" tabindex="-1"></a></span>
<span id="cb468-21"><a href="ensemble-methods.html#cb468-21" tabindex="-1"></a>    modelp <span class="ot">&lt;-</span> <span class="fu">rpart</span>(survived <span class="sc">~</span> sex <span class="sc">+</span> age <span class="sc">+</span> pclass <span class="sc">+</span> sibsp <span class="sc">+</span> parch,</span>
<span id="cb468-22"><a href="ensemble-methods.html#cb468-22" tabindex="-1"></a>                  <span class="at">data =</span> tr, <span class="at">method =</span> <span class="st">&quot;class&quot;</span>) <span class="co"># Pruned</span></span>
<span id="cb468-23"><a href="ensemble-methods.html#cb468-23" tabindex="-1"></a>    modelup <span class="ot">&lt;-</span> <span class="fu">rpart</span>(survived <span class="sc">~</span> sex <span class="sc">+</span> age <span class="sc">+</span> pclass <span class="sc">+</span> sibsp <span class="sc">+</span> parch,</span>
<span id="cb468-24"><a href="ensemble-methods.html#cb468-24" tabindex="-1"></a>                  <span class="at">data =</span> tr,</span>
<span id="cb468-25"><a href="ensemble-methods.html#cb468-25" tabindex="-1"></a>                  <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">2</span>, <span class="at">minbucket =</span> <span class="dv">1</span></span>
<span id="cb468-26"><a href="ensemble-methods.html#cb468-26" tabindex="-1"></a>                                          , <span class="at">cp =</span> <span class="dv">0</span>),</span>
<span id="cb468-27"><a href="ensemble-methods.html#cb468-27" tabindex="-1"></a>                  <span class="at">method =</span> <span class="st">&quot;class&quot;</span>) <span class="co"># Unpruned</span></span>
<span id="cb468-28"><a href="ensemble-methods.html#cb468-28" tabindex="-1"></a>    phatp[j, ] <span class="ot">&lt;-</span> <span class="fu">predict</span>(modelp, test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)[, <span class="dv">2</span>]</span>
<span id="cb468-29"><a href="ensemble-methods.html#cb468-29" tabindex="-1"></a>    phatup[j, ] <span class="ot">&lt;-</span> <span class="fu">predict</span>(modelup, test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)[, <span class="dv">2</span>]</span>
<span id="cb468-30"><a href="ensemble-methods.html#cb468-30" tabindex="-1"></a>  }</span>
<span id="cb468-31"><a href="ensemble-methods.html#cb468-31" tabindex="-1"></a>  <span class="co"># Averaging for B Trees</span></span>
<span id="cb468-32"><a href="ensemble-methods.html#cb468-32" tabindex="-1"></a>  phatpr <span class="ot">&lt;-</span> <span class="fu">apply</span>(phatp, <span class="dv">2</span>, mean)</span>
<span id="cb468-33"><a href="ensemble-methods.html#cb468-33" tabindex="-1"></a>  phatupr <span class="ot">&lt;-</span> <span class="fu">apply</span>(phatup, <span class="dv">2</span>, mean)</span>
<span id="cb468-34"><a href="ensemble-methods.html#cb468-34" tabindex="-1"></a>  </span>
<span id="cb468-35"><a href="ensemble-methods.html#cb468-35" tabindex="-1"></a>  <span class="co"># AUC pruned</span></span>
<span id="cb468-36"><a href="ensemble-methods.html#cb468-36" tabindex="-1"></a>  pred_rocr <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phatpr, test<span class="sc">$</span>survived)</span>
<span id="cb468-37"><a href="ensemble-methods.html#cb468-37" tabindex="-1"></a>  auc_ROCR <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb468-38"><a href="ensemble-methods.html#cb468-38" tabindex="-1"></a>  AUCp[i] <span class="ot">&lt;-</span> auc_ROCR<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb468-39"><a href="ensemble-methods.html#cb468-39" tabindex="-1"></a>  </span>
<span id="cb468-40"><a href="ensemble-methods.html#cb468-40" tabindex="-1"></a>  <span class="co"># AUC unpruned</span></span>
<span id="cb468-41"><a href="ensemble-methods.html#cb468-41" tabindex="-1"></a>  pred_rocr <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phatupr, test<span class="sc">$</span>survived)</span>
<span id="cb468-42"><a href="ensemble-methods.html#cb468-42" tabindex="-1"></a>  auc_ROCR <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb468-43"><a href="ensemble-methods.html#cb468-43" tabindex="-1"></a>  AUCup[i] <span class="ot">&lt;-</span> auc_ROCR<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb468-44"><a href="ensemble-methods.html#cb468-44" tabindex="-1"></a>}</span>
<span id="cb468-45"><a href="ensemble-methods.html#cb468-45" tabindex="-1"></a></span>
<span id="cb468-46"><a href="ensemble-methods.html#cb468-46" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Pruned&quot;</span>, <span class="st">&quot;Unpruned&quot;</span>)</span>
<span id="cb468-47"><a href="ensemble-methods.html#cb468-47" tabindex="-1"></a>AUCs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">mean</span>(AUCp), <span class="fu">mean</span>(AUCup))</span>
<span id="cb468-48"><a href="ensemble-methods.html#cb468-48" tabindex="-1"></a>sd <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">sqrt</span>(<span class="fu">var</span>(AUCp)), <span class="fu">sqrt</span>(<span class="fu">var</span>(AUCup)))</span>
<span id="cb468-49"><a href="ensemble-methods.html#cb468-49" tabindex="-1"></a><span class="fu">data.frame</span>(model, AUCs, sd) </span></code></pre></div>
<pre><code>##      model      AUCs         sd
## 1   Pruned 0.8523158 0.01626892
## 2 Unpruned 0.8180811 0.01692990</code></pre>
<p>We can see a significant reduction in uncertainty and improvement in accuracy relative to a single tree. When we use “unpruned” single-tree using <code>rpart()</code> for bagging, the result becomes very similar to one that we obtain with random forest. Using pruned trees for bagging improves the accuracy in our case.</p>
<p>This would also be the case in regression trees, where we would be averaging <code>yhat</code>’s and calculating RMSPE and its standard deviations instead of AUC.</p>
</div>
<div id="regression" class="section level2 hasAnchor" number="17.6">
<h2><span class="header-section-number">17.6</span> Regression<a href="ensemble-methods.html#regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider the data we used earlier chapters to predict baseball player’s salary:</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="ensemble-methods.html#cb470-1" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb470-2"><a href="ensemble-methods.html#cb470-2" tabindex="-1"></a></span>
<span id="cb470-3"><a href="ensemble-methods.html#cb470-3" tabindex="-1"></a><span class="fu">remove</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span>
<span id="cb470-4"><a href="ensemble-methods.html#cb470-4" tabindex="-1"></a></span>
<span id="cb470-5"><a href="ensemble-methods.html#cb470-5" tabindex="-1"></a><span class="fu">data</span>(Hitters)</span>
<span id="cb470-6"><a href="ensemble-methods.html#cb470-6" tabindex="-1"></a>df <span class="ot">&lt;-</span> Hitters[<span class="fu">complete.cases</span>(Hitters<span class="sc">$</span>Salary), ]</span></code></pre></div>
<p>Let’s use only a single tree with bagging:</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="ensemble-methods.html#cb471-1" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb471-2"><a href="ensemble-methods.html#cb471-2" tabindex="-1"></a></span>
<span id="cb471-3"><a href="ensemble-methods.html#cb471-3" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb471-4"><a href="ensemble-methods.html#cb471-4" tabindex="-1"></a>df<span class="sc">$</span>logsal <span class="ot">&lt;-</span> <span class="fu">log</span>(df<span class="sc">$</span>Salary)</span>
<span id="cb471-5"><a href="ensemble-methods.html#cb471-5" tabindex="-1"></a>df <span class="ot">&lt;-</span> df[, <span class="sc">-</span><span class="dv">19</span>]</span>
<span id="cb471-6"><a href="ensemble-methods.html#cb471-6" tabindex="-1"></a></span>
<span id="cb471-7"><a href="ensemble-methods.html#cb471-7" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb471-8"><a href="ensemble-methods.html#cb471-8" tabindex="-1"></a>B <span class="ot">=</span> <span class="dv">500</span></span>
<span id="cb471-9"><a href="ensemble-methods.html#cb471-9" tabindex="-1"></a>RMSPEp <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb471-10"><a href="ensemble-methods.html#cb471-10" tabindex="-1"></a>RMSPEup <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb471-11"><a href="ensemble-methods.html#cb471-11" tabindex="-1"></a></span>
<span id="cb471-12"><a href="ensemble-methods.html#cb471-12" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb471-13"><a href="ensemble-methods.html#cb471-13" tabindex="-1"></a>  <span class="fu">set.seed</span>(i<span class="sc">+</span>i<span class="sc">*</span><span class="dv">8</span>)</span>
<span id="cb471-14"><a href="ensemble-methods.html#cb471-14" tabindex="-1"></a>  ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb471-15"><a href="ensemble-methods.html#cb471-15" tabindex="-1"></a>  train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb471-16"><a href="ensemble-methods.html#cb471-16" tabindex="-1"></a>  test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb471-17"><a href="ensemble-methods.html#cb471-17" tabindex="-1"></a></span>
<span id="cb471-18"><a href="ensemble-methods.html#cb471-18" tabindex="-1"></a>  yhatp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, B, <span class="fu">nrow</span>(test))</span>
<span id="cb471-19"><a href="ensemble-methods.html#cb471-19" tabindex="-1"></a>  yhatup <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, B, <span class="fu">nrow</span>(test))</span>
<span id="cb471-20"><a href="ensemble-methods.html#cb471-20" tabindex="-1"></a>    </span>
<span id="cb471-21"><a href="ensemble-methods.html#cb471-21" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B) {</span>
<span id="cb471-22"><a href="ensemble-methods.html#cb471-22" tabindex="-1"></a>    <span class="fu">set.seed</span>(j<span class="sc">+</span>j<span class="sc">*</span><span class="dv">2</span>)</span>
<span id="cb471-23"><a href="ensemble-methods.html#cb471-23" tabindex="-1"></a>    ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(train), <span class="fu">nrow</span>(train), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb471-24"><a href="ensemble-methods.html#cb471-24" tabindex="-1"></a>    tr <span class="ot">&lt;-</span> train[ind, ]</span>
<span id="cb471-25"><a href="ensemble-methods.html#cb471-25" tabindex="-1"></a>    </span>
<span id="cb471-26"><a href="ensemble-methods.html#cb471-26" tabindex="-1"></a>    modelp <span class="ot">&lt;-</span> <span class="fu">rpart</span>(logsal <span class="sc">~</span> ., <span class="at">data =</span> tr, <span class="at">method =</span> <span class="st">&quot;anova&quot;</span>) <span class="co"># Pruned</span></span>
<span id="cb471-27"><a href="ensemble-methods.html#cb471-27" tabindex="-1"></a>    modelup <span class="ot">&lt;-</span> <span class="fu">rpart</span>(logsal <span class="sc">~</span> ., <span class="at">data =</span> tr,</span>
<span id="cb471-28"><a href="ensemble-methods.html#cb471-28" tabindex="-1"></a>                    <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">2</span>, <span class="at">minbucket =</span> <span class="dv">1</span></span>
<span id="cb471-29"><a href="ensemble-methods.html#cb471-29" tabindex="-1"></a>                                            ,<span class="at">cp =</span> <span class="dv">0</span>),</span>
<span id="cb471-30"><a href="ensemble-methods.html#cb471-30" tabindex="-1"></a>                    <span class="at">method =</span> <span class="st">&quot;anova&quot;</span>) <span class="co"># unpruned</span></span>
<span id="cb471-31"><a href="ensemble-methods.html#cb471-31" tabindex="-1"></a>    yhatp[j,] <span class="ot">&lt;-</span> <span class="fu">predict</span>(modelp, test)</span>
<span id="cb471-32"><a href="ensemble-methods.html#cb471-32" tabindex="-1"></a>    yhatup[j,] <span class="ot">&lt;-</span> <span class="fu">predict</span>(modelup, test)</span>
<span id="cb471-33"><a href="ensemble-methods.html#cb471-33" tabindex="-1"></a>  }</span>
<span id="cb471-34"><a href="ensemble-methods.html#cb471-34" tabindex="-1"></a>  <span class="co"># Averaging for B Trees</span></span>
<span id="cb471-35"><a href="ensemble-methods.html#cb471-35" tabindex="-1"></a>  yhatpr <span class="ot">&lt;-</span> <span class="fu">apply</span>(yhatp, <span class="dv">2</span>, mean)</span>
<span id="cb471-36"><a href="ensemble-methods.html#cb471-36" tabindex="-1"></a>  yhatupr <span class="ot">&lt;-</span> <span class="fu">apply</span>(yhatup, <span class="dv">2</span>, mean)</span>
<span id="cb471-37"><a href="ensemble-methods.html#cb471-37" tabindex="-1"></a>  </span>
<span id="cb471-38"><a href="ensemble-methods.html#cb471-38" tabindex="-1"></a>  RMSPEp[i] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((test<span class="sc">$</span>logsal <span class="sc">-</span> yhatpr)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb471-39"><a href="ensemble-methods.html#cb471-39" tabindex="-1"></a>  RMSPEup[i] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((test<span class="sc">$</span>logsal <span class="sc">-</span> yhatupr)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb471-40"><a href="ensemble-methods.html#cb471-40" tabindex="-1"></a>}</span>
<span id="cb471-41"><a href="ensemble-methods.html#cb471-41" tabindex="-1"></a></span>
<span id="cb471-42"><a href="ensemble-methods.html#cb471-42" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Pruned&quot;</span>, <span class="st">&quot;Unpruned&quot;</span>)</span>
<span id="cb471-43"><a href="ensemble-methods.html#cb471-43" tabindex="-1"></a>RMSPEs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">mean</span>(RMSPEp), <span class="fu">mean</span>(RMSPEup))</span>
<span id="cb471-44"><a href="ensemble-methods.html#cb471-44" tabindex="-1"></a>sd <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">sqrt</span>(<span class="fu">var</span>(RMSPEp)), <span class="fu">sqrt</span>(<span class="fu">var</span>(RMSPEup)))</span>
<span id="cb471-45"><a href="ensemble-methods.html#cb471-45" tabindex="-1"></a><span class="fu">data.frame</span>(model, RMSPEs, sd) </span></code></pre></div>
<pre><code>##      model    RMSPEs         sd
## 1   Pruned 0.5019840 0.05817388
## 2 Unpruned 0.4808079 0.06223845</code></pre>
<p>With and without pruning, the results are very similar. Let’s put all these together and do it with Random Forest:</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="ensemble-methods.html#cb473-1" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb473-2"><a href="ensemble-methods.html#cb473-2" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb473-3"><a href="ensemble-methods.html#cb473-3" tabindex="-1"></a></span>
<span id="cb473-4"><a href="ensemble-methods.html#cb473-4" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb473-5"><a href="ensemble-methods.html#cb473-5" tabindex="-1"></a><span class="fu">remove</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span>
<span id="cb473-6"><a href="ensemble-methods.html#cb473-6" tabindex="-1"></a><span class="fu">data</span>(Hitters)</span>
<span id="cb473-7"><a href="ensemble-methods.html#cb473-7" tabindex="-1"></a>df <span class="ot">&lt;-</span> Hitters[<span class="fu">complete.cases</span>(Hitters<span class="sc">$</span>Salary), ]</span>
<span id="cb473-8"><a href="ensemble-methods.html#cb473-8" tabindex="-1"></a>df<span class="sc">$</span>logsal <span class="ot">&lt;-</span> <span class="fu">log</span>(df<span class="sc">$</span>Salary)</span>
<span id="cb473-9"><a href="ensemble-methods.html#cb473-9" tabindex="-1"></a>df <span class="ot">&lt;-</span> df[, <span class="sc">-</span><span class="dv">19</span>]</span>
<span id="cb473-10"><a href="ensemble-methods.html#cb473-10" tabindex="-1"></a></span>
<span id="cb473-11"><a href="ensemble-methods.html#cb473-11" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb473-12"><a href="ensemble-methods.html#cb473-12" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb473-13"><a href="ensemble-methods.html#cb473-13" tabindex="-1"></a>RMSPE1 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb473-14"><a href="ensemble-methods.html#cb473-14" tabindex="-1"></a>RMSPE2 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb473-15"><a href="ensemble-methods.html#cb473-15" tabindex="-1"></a>RMSPE3 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb473-16"><a href="ensemble-methods.html#cb473-16" tabindex="-1"></a></span>
<span id="cb473-17"><a href="ensemble-methods.html#cb473-17" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb473-18"><a href="ensemble-methods.html#cb473-18" tabindex="-1"></a>  <span class="fu">set.seed</span>(i<span class="sc">+</span>i<span class="sc">*</span><span class="dv">8</span>)</span>
<span id="cb473-19"><a href="ensemble-methods.html#cb473-19" tabindex="-1"></a>  ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb473-20"><a href="ensemble-methods.html#cb473-20" tabindex="-1"></a>  train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb473-21"><a href="ensemble-methods.html#cb473-21" tabindex="-1"></a>  test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb473-22"><a href="ensemble-methods.html#cb473-22" tabindex="-1"></a>  </span>
<span id="cb473-23"><a href="ensemble-methods.html#cb473-23" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">ncol</span>(train)<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb473-24"><a href="ensemble-methods.html#cb473-24" tabindex="-1"></a></span>
<span id="cb473-25"><a href="ensemble-methods.html#cb473-25" tabindex="-1"></a>  model1 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(logsal<span class="sc">~</span>., <span class="at">data =</span>train) <span class="co"># Single Tree</span></span>
<span id="cb473-26"><a href="ensemble-methods.html#cb473-26" tabindex="-1"></a>  model2 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(logsal<span class="sc">~</span>., <span class="at">ntree =</span> B, <span class="at">mtry =</span> p, <span class="at">data =</span> train) <span class="co">#Bagged</span></span>
<span id="cb473-27"><a href="ensemble-methods.html#cb473-27" tabindex="-1"></a>  model3 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(logsal<span class="sc">~</span>., <span class="at">ntree =</span> B, <span class="at">localImp =</span> <span class="cn">TRUE</span>, <span class="at">data =</span> train) <span class="co"># RF</span></span>
<span id="cb473-28"><a href="ensemble-methods.html#cb473-28" tabindex="-1"></a></span>
<span id="cb473-29"><a href="ensemble-methods.html#cb473-29" tabindex="-1"></a>  yhat1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, test)</span>
<span id="cb473-30"><a href="ensemble-methods.html#cb473-30" tabindex="-1"></a>  yhat2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model2, test)</span>
<span id="cb473-31"><a href="ensemble-methods.html#cb473-31" tabindex="-1"></a>  yhat3 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model3, test)</span>
<span id="cb473-32"><a href="ensemble-methods.html#cb473-32" tabindex="-1"></a>  </span>
<span id="cb473-33"><a href="ensemble-methods.html#cb473-33" tabindex="-1"></a>  RMSPE1[i] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((test<span class="sc">$</span>logsal <span class="sc">-</span> yhat1)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb473-34"><a href="ensemble-methods.html#cb473-34" tabindex="-1"></a>  RMSPE2[i] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((test<span class="sc">$</span>logsal <span class="sc">-</span> yhat2)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb473-35"><a href="ensemble-methods.html#cb473-35" tabindex="-1"></a>  RMSPE3[i] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((test<span class="sc">$</span>logsal <span class="sc">-</span> yhat3)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb473-36"><a href="ensemble-methods.html#cb473-36" tabindex="-1"></a>}</span>
<span id="cb473-37"><a href="ensemble-methods.html#cb473-37" tabindex="-1"></a></span>
<span id="cb473-38"><a href="ensemble-methods.html#cb473-38" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Single-Tree&quot;</span>, <span class="st">&quot;Bagging&quot;</span>, <span class="st">&quot;RF&quot;</span>)</span>
<span id="cb473-39"><a href="ensemble-methods.html#cb473-39" tabindex="-1"></a>RMSPEs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">mean</span>(RMSPE1), <span class="fu">mean</span>(RMSPE2), <span class="fu">mean</span>(RMSPE3))</span>
<span id="cb473-40"><a href="ensemble-methods.html#cb473-40" tabindex="-1"></a>sd <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">sqrt</span>(<span class="fu">var</span>(RMSPE1)), <span class="fu">sqrt</span>(<span class="fu">var</span>(RMSPE2)), <span class="fu">sqrt</span>(<span class="fu">var</span>(RMSPE3)))</span>
<span id="cb473-41"><a href="ensemble-methods.html#cb473-41" tabindex="-1"></a><span class="fu">data.frame</span>(model, RMSPEs, sd) </span></code></pre></div>
<pre><code>##         model    RMSPEs         sd
## 1 Single-Tree 0.5739631 0.05360920
## 2     Bagging 0.4807307 0.06122742
## 3          RF 0.4631913 0.06038268</code></pre>
<p>Random forest has the lowest RMSPE.</p>
</div>
<div id="exploration" class="section level2 hasAnchor" number="17.7">
<h2><span class="header-section-number">17.7</span> Exploration<a href="ensemble-methods.html#exploration" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While the task in machine learning is to achieve the best predictive capacity, for many applications identifying the major predictors could be the major objective. Of course, finding the most important predictors is contingent on the model’s predictive performance. As we discussed earlier, however, there is a trade-off between prediction accuracy and interpretability. Although there are many different aspects of interpretability, it refer to understanding the relationship between the predicted outcome and the predictors.</p>
<p>The interpretability in predictive modeling is an active research area. Two excellent books on the subject provide much needed comprehensive information about the interpretability and explanatory analysis in machine learning: <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a> by Christoph Molnar and <a href="https://ema.drwhy.ai">Explanatory Model Analysis</a> by Biecek and Burzykowski (2020).</p>
<p>Explorations of predictive models are classified in two major groups. The first one is the instance-level exploration, or example-based explanation methods, which present methods for exploration of a model’s predictions for a single observation. For example, for a particular subject (person, firm, patient), we may want to know contribution of the different features to the predicted outcome for the subject. The main idea is to understand marginal effect of a predictor on the prediction for a specific subject. There are two important methods in this level: Shapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME). We will not explain and apply them here in this book. These two methods are easily accessible with multiple examples in both books we cited ealrier.</p>
<p>The second group of explanation methods focuses on dataset-level explainers, which help understand the average behavior of a machine learning model for an entire set of observations. Here, we will focus on several variable-importance measures. They are permutation-based variable importance metrics offering a model-agnostic approach to the assessment of the influence of an explanatory variable on a model’s performance.</p>
<p>There are several options to evaluate how important is the variable <span class="math inline">\(x\)</span> in predictions. One major method is the permutation-based variable-importance in which the effect of a variable is removed through a random reshuffling of the data in <span class="math inline">\(x\)</span>. This method takes the original data under <span class="math inline">\(x\)</span>, permutates (mixes) its values, and gets “new” data, on which computes the weighted decrease of accuracy corresponding to splits along the variable <span class="math inline">\(x\)</span> and averages this quantity over all trees. If a variable is an important predictor in the model, after its permutation, the mean decrease accuracy (MDA) rises. It stems from the idea that if the variable is not important, rearranging its values should not degrade prediction accuracy. The MDA relies on a different principle and uses the out-of-bag error estimate. Every tree in the forest has its own out-of-bag sample, on which the prediction accuracy is measured. To calculate MDA, the values of the variable in the out-of-bag-sample are randomly shuffled and the decrease in prediction accuracy on the shuffled data is measured. This process is repeated for all variables and trees. The MDA averaged over all trees is ranked. If a variable has insignificant predictive power, shuffling may not lead to substantial decrease in accuracy. It is shown that building a tree with additional irrelevant variables does not alter the importance of relevant variables in an infinite sample setting.</p>
<p>Another measure of significance is Mean Decrease Impurity (MDI). It is not permutation-based; instead, it is based on the impurity decrease attributable to a particular feature during the construction of the decision trees that make up the random forest. In a Random Forest model, multiple decision trees are built using a random subset of features and a random subset of the training dataset. Each decision tree is constructed through a process called recursive binary splitting, where the best split for a particular node is determined by maximizing the impurity decrease. Impurity is a measure of how well the samples at a node in the decision tree are classified. Common impurity measures include Gini impurity and entropy. The impurity decrease is calculated by comparing the impurity of the parent node with the weighted average impurity of the child nodes. For each feature, the impurity decrease is calculated at every split where the feature is used. The impurity decreases are then summed up across all the trees in the random forest for that feature. The sum of the impurity decreases is then normalized by the total sum of the impurity decreases across all the features to calculate the MDI value for each feature.</p>
<p>The MDI values represent the average contribution of a feature to the decrease in impurity across all the trees in the random forest. A higher MDI value for a feature indicates that it is more important for making accurate predictions, while a lower value indicates a less important feature.</p>
<p>In contrast, permutation-based feature importance, such as Mean Decrease in Accuracy (MDA), measures the impact of a feature on model performance by randomly permuting the feature’s values and evaluating the change in model accuracy. This approach provides an estimate of the importance of a feature by assessing the performance drop when the feature’s information is removed or disrupted.</p>
<p>For a numeric outcome (regression problem) there are two similar measures. The percentage increase in mean square error (<code>%IncMSE</code>), which is calculated by shuffling the values of the out-of-bag samples, is analogous to MDA. Increase in node purity (<code>IncNodePurity</code>), which is calculated based on the reduction in sum of squared errors whenever a variable is chosen to split is, analogous to MDI.</p>
<p>Here are the variable importance measures for our random forest application (<code>model3</code>):</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="ensemble-methods.html#cb475-1" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb475-2"><a href="ensemble-methods.html#cb475-2" tabindex="-1"></a><span class="fu">varImpPlot</span>(model3)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/ea6-1.png" width="672" /></p>
<p>And, the partial dependence plot gives a graphical representation of the marginal effect of a variable on the class probability (classification) or response (regression). The intuition behind it is simple: change the value of a predictor and see how much the prediction will change (log wage in our example).</p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb476-1"><a href="ensemble-methods.html#cb476-1" tabindex="-1"></a><span class="fu">partialPlot</span>(model3, test, CRuns, <span class="at">xlab=</span><span class="st">&quot;CRuns&quot;</span>,</span>
<span id="cb476-2"><a href="ensemble-methods.html#cb476-2" tabindex="-1"></a>                <span class="at">main=</span><span class="st">&quot;Effects of CRuns&quot;</span>,</span>
<span id="cb476-3"><a href="ensemble-methods.html#cb476-3" tabindex="-1"></a>                <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/ea9-1.png" width="672" /></p>
<p>Partial dependence plots (PDPs) are a graphical tool used to visualize the relationship between a single feature and the predicted outcome in a machine learning model, while averaging out the effects of all other features. For each unique value of the chosen feature, the algorithm fixes the value and keeps all other feature values unchanged. Then, the modified dataset with the fixed feature value is used by the Random Forest model to obtain predictions for each instance. We compute the average prediction across all instances for the fixed feature value. This represents the partial dependence of the outcome on the chosen feature value. We perform these steps for all unique values of the chosen feature, and obtain the partial dependence values for each feature value. A plot with the chosen feature values on the x-axis and the corresponding partial dependence values on the y-axis is the Partial dependence plot.</p>
<p>The resulting partial dependence plot illustrates the relationship between the chosen feature and the model’s predictions, while accounting for the average effect of all other features. The plot helps to identify the direction (positive or negative) and strength of the relationship, as well as any non-linear patterns or interactions with other features. Keep in mind that partial dependence plots are most useful for understanding the effects of individual features in isolation, and they may not capture the full complexity of the model if there are strong interactions between features.</p>
<p>There are several libraries that we can use to improve presentation of permutation-based variable importance metrics: the <code>randomForestExplainer</code> package (see its <a href="https://htmlpreview.github.io/?https://github.com/geneticsMiNIng/BlackBoxOpener/master/randomForestExplainer/inst/doc/randomForestExplainer.html">vignette</a>) <span class="citation">(<a href="#ref-Palu_2012"><strong>Palu_2012?</strong></a>)</span> and.the <code>DALEX</code> packages.</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="ensemble-methods.html#cb477-1" tabindex="-1"></a><span class="fu">library</span>(randomForestExplainer)</span>
<span id="cb477-2"><a href="ensemble-methods.html#cb477-2" tabindex="-1"></a></span>
<span id="cb477-3"><a href="ensemble-methods.html#cb477-3" tabindex="-1"></a>importance_frame <span class="ot">&lt;-</span> <span class="fu">measure_importance</span>(model3)</span>
<span id="cb477-4"><a href="ensemble-methods.html#cb477-4" tabindex="-1"></a>importance_frame</span></code></pre></div>
<pre><code>##     variable mean_min_depth no_of_nodes mse_increase node_purity_increase
## 1    Assists       4.385264        2351 0.0111643040            2.0354183
## 2      AtBat       2.880632        2691 0.0823060539            9.8976694
## 3     CAtBat       2.378316        2598 0.2180919045           38.3175006
## 4      CHits       2.254316        2711 0.2219603757           34.6913645
## 5     CHmRun       3.444948        2556 0.0465389503            6.5334618
## 6       CRBI       2.826000        2752 0.1037441042           19.5413640
## 7      CRuns       2.076316        2731 0.2415297175           35.0893626
## 8     CWalks       3.090316        2579 0.0842675407           18.0455320
## 9   Division       7.025920         691 0.0009003443            0.2610306
## 10    Errors       4.626844        2080 0.0091803849            1.2750433
## 11      Hits       3.086316        2582 0.0891232078            9.3889994
## 12     HmRun       4.019580        2229 0.0229235515            3.5544146
## 13    League       7.723940         442 0.0007442309            0.1574101
## 14 NewLeague       7.097292         627 0.0012483369            0.2430058
## 15   PutOuts       3.654632        2593 0.0174281111            3.9026093
## 16       RBI       3.486948        2620 0.0406771125            6.9162313
## 17      Runs       3.518948        2543 0.0515670394            5.8962241
## 18     Walks       3.532316        2576 0.0397964535            5.9405180
## 19     Years       4.597688        1716 0.0246697278            5.5647402
##    no_of_trees times_a_root      p_value
## 1          496            0 3.136068e-04
## 2          498            5 2.277643e-26
## 3          499          133 2.885642e-18
## 4          499          110 2.632589e-28
## 5          497            7 4.203385e-15
## 6          500           55 1.727502e-32
## 7          499          101 2.602255e-30
## 8          499           52 8.510193e-17
## 9          380            0 1.000000e+00
## 10         491            0 9.939409e-01
## 11         499            7 5.036363e-17
## 12         495            0 2.179972e-01
## 13         285            0 1.000000e+00
## 14         363            0 1.000000e+00
## 15         498            0 7.131388e-18
## 16         497            7 4.777556e-20
## 17         497            1 3.461522e-14
## 18         499            0 1.432750e-16
## 19         482           22 1.000000e+00</code></pre>
<p>This table shows few more metrics in addition to <code>mse_increase</code> and <code>node_purity_increase</code>. The first column, <code>mean_min_depth</code>, the average of the first time this variable is used to split the tree. Therefore, more important variables have lower minimum depth values. The metric <code>no_of_nodes</code> shows the total number of nodes that use for splitting. Finally, <code>times_a_root</code> shows how many times the split occurs at the root. The last column, <code>p_value</code> for the one-sided binomial test, which tells us whether the observed number of of nodes in which the variable was used for splitting exceeds the theoretical number of successes if they were random.</p>
<p>We can take advantage of several multidimensional plots from the <code>randomForestExplainer</code> package:</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="ensemble-methods.html#cb479-1" tabindex="-1"></a><span class="fu">plot_multi_way_importance</span>(importance_frame, <span class="at">x_measure =</span> <span class="st">&quot;mean_min_depth&quot;</span>,</span>
<span id="cb479-2"><a href="ensemble-methods.html#cb479-2" tabindex="-1"></a>                          <span class="at">y_measure =</span> <span class="st">&quot;mse_increase&quot;</span>,</span>
<span id="cb479-3"><a href="ensemble-methods.html#cb479-3" tabindex="-1"></a>                          <span class="at">size_measure =</span> <span class="st">&quot;p_value&quot;</span>, <span class="at">no_of_labels =</span> <span class="dv">6</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/ea8-1.png" width="672" /></p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="ensemble-methods.html#cb480-1" tabindex="-1"></a>min_depth_frame <span class="ot">&lt;-</span> <span class="fu">min_depth_distribution</span>(model3)</span>
<span id="cb480-2"><a href="ensemble-methods.html#cb480-2" tabindex="-1"></a><span class="fu">plot_min_depth_distribution</span>(min_depth_frame, <span class="at">mean_sample =</span> <span class="st">&quot;all_trees&quot;</span>, <span class="at">k =</span><span class="dv">20</span>,</span>
<span id="cb480-3"><a href="ensemble-methods.html#cb480-3" tabindex="-1"></a>                            <span class="at">main =</span> <span class="st">&quot;Distribution of minimal depth and its mean&quot;</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
</div>
<div id="boosting-applications" class="section level2 hasAnchor" number="17.8">
<h2><span class="header-section-number">17.8</span> Boosting Applications<a href="ensemble-methods.html#boosting-applications" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We need to tune the boosting applications with <code>gbm()</code>. There are two groups of tuning parameters: boosting parameters and tree parameters.</p>
<ul>
<li><strong>Boosting parameters:</strong> The number iterations (<code>n.trees</code> = 100) and learning rate (<code>shrinkage</code> = 0.1).<br />
</li>
<li><strong>Tree parameters:</strong> The maximum depth of each tree (<code>interaction.depth</code> = 1) and the minimum number of observations in the terminal nodes of the trees (<code>n.minobsinnode</code> = 10)</li>
</ul>
<p>The <code>gbm</code> algorithm offers three tuning options internally to select the best iteration: <code>OOB</code>, <code>test</code>, and <code>cv.fold</code>. The <code>test</code> uses a single holdout test set to select the optimal number of iterations. It’s regulated by <code>train.fraction</code>, which creates a test set by <code>train.fraction</code> × <code>nrow(data)</code>. This is not a cross validation but could be used with multiple loops running externally.</p>
<p>The k-fold cross validation is regulated by <code>cv.fold</code> that canbe used to find the optimal number of iterations. For example, if <code>cv.folds</code>=5 then <code>gbm</code> fits five <code>gbm</code> models to compute the cross validation error. The using the best (average iterations) it fits a sixth and final gbm model using all of the data. The <code>cv.error</code> reported this final model will determined the the best iteration.</p>
<p>Finally, there is one parameter, <code>bag.fraction</code>, the fraction of the training set observations randomly selected to propose the next tree in the expansion. This introduces randomnesses into the model fit, hence, reduces overfitting possibilities. The “improvements” the error (prediected errors) in each iterations is reported by <code>oobag.improve</code>.</p>
<p>Below, we show these three methods to identify the best iteration</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="ensemble-methods.html#cb481-1" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb481-2"><a href="ensemble-methods.html#cb481-2" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb481-3"><a href="ensemble-methods.html#cb481-3" tabindex="-1"></a></span>
<span id="cb481-4"><a href="ensemble-methods.html#cb481-4" tabindex="-1"></a><span class="fu">data</span>(Hitters)</span>
<span id="cb481-5"><a href="ensemble-methods.html#cb481-5" tabindex="-1"></a>df <span class="ot">&lt;-</span> Hitters[<span class="fu">complete.cases</span>(Hitters<span class="sc">$</span>Salary), ]</span>
<span id="cb481-6"><a href="ensemble-methods.html#cb481-6" tabindex="-1"></a>df<span class="sc">$</span>Salary <span class="ot">&lt;-</span> <span class="fu">log</span>(df<span class="sc">$</span>Salary)</span>
<span id="cb481-7"><a href="ensemble-methods.html#cb481-7" tabindex="-1"></a></span>
<span id="cb481-8"><a href="ensemble-methods.html#cb481-8" tabindex="-1"></a>model_cv <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Salary<span class="sc">~</span>., <span class="at">distribution =</span><span class="st">&quot;gaussian&quot;</span>, <span class="at">n.trees=</span><span class="dv">1000</span>,</span>
<span id="cb481-9"><a href="ensemble-methods.html#cb481-9" tabindex="-1"></a>            <span class="at">interaction.depth =</span> <span class="dv">3</span>, <span class="at">shrinkage =</span> <span class="fl">0.01</span>, <span class="at">data =</span> df,</span>
<span id="cb481-10"><a href="ensemble-methods.html#cb481-10" tabindex="-1"></a>            <span class="at">bag.fraction =</span> <span class="fl">0.5</span>,</span>
<span id="cb481-11"><a href="ensemble-methods.html#cb481-11" tabindex="-1"></a>            <span class="at">cv.folds =</span> <span class="dv">5</span>)</span>
<span id="cb481-12"><a href="ensemble-methods.html#cb481-12" tabindex="-1"></a>best <span class="ot">&lt;-</span> <span class="fu">which.min</span>(model_cv<span class="sc">$</span>cv.error)</span>
<span id="cb481-13"><a href="ensemble-methods.html#cb481-13" tabindex="-1"></a><span class="fu">sqrt</span>(model_cv<span class="sc">$</span>cv.error[best])</span></code></pre></div>
<pre><code>## [1] 0.4666599</code></pre>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="ensemble-methods.html#cb483-1" tabindex="-1"></a><span class="co"># or this can be obtained</span></span>
<span id="cb483-2"><a href="ensemble-methods.html#cb483-2" tabindex="-1"></a><span class="fu">gbm.perf</span>(model_cv, <span class="at">method=</span><span class="st">&quot;cv&quot;</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/ea100-1.png" width="672" /></p>
<pre><code>## [1] 988</code></pre>
<p>The following method can be combined with an external loops that runs several times, for example.</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="ensemble-methods.html#cb485-1" tabindex="-1"></a>model_test <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Salary<span class="sc">~</span>., <span class="at">distribution =</span><span class="st">&quot;gaussian&quot;</span>, <span class="at">n.trees=</span><span class="dv">1000</span>,</span>
<span id="cb485-2"><a href="ensemble-methods.html#cb485-2" tabindex="-1"></a>            <span class="at">interaction.depth =</span> <span class="dv">3</span>, <span class="at">shrinkage =</span> <span class="fl">0.01</span>, <span class="at">data =</span> df,</span>
<span id="cb485-3"><a href="ensemble-methods.html#cb485-3" tabindex="-1"></a>            <span class="at">bag.fraction =</span> <span class="fl">0.5</span>,</span>
<span id="cb485-4"><a href="ensemble-methods.html#cb485-4" tabindex="-1"></a>            <span class="at">train.fraction =</span> <span class="fl">0.8</span>)</span>
<span id="cb485-5"><a href="ensemble-methods.html#cb485-5" tabindex="-1"></a><span class="fu">gbm.perf</span>(model_test, <span class="at">method=</span><span class="st">&quot;test&quot;</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/ea101-1.png" width="672" /></p>
<pre><code>## [1] 999</code></pre>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="ensemble-methods.html#cb487-1" tabindex="-1"></a><span class="fu">which.min</span>(model_test<span class="sc">$</span>valid.error)</span></code></pre></div>
<pre><code>## [1] 999</code></pre>
<div class="sourceCode" id="cb489"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb489-1"><a href="ensemble-methods.html#cb489-1" tabindex="-1"></a><span class="fu">min</span>(model_test<span class="sc">$</span>valid.error)</span></code></pre></div>
<pre><code>## [1] 0.303144</code></pre>
<p>The OOB is option is not suggested for model selection (see <a href="http://127.0.0.1:17306/library/gbm/doc/gbm.pdf">A guide to the gbm package</a>. The <code>bag.fraction</code>, however, can be used to reduce overfitting as the fraction of the training set observations randomly selected to propose the next tree in the expansion. This introduces randomnesses into the model fit, hence, reduces overfitting.</p>
<p>We can also override all the internal process and apply our own grid search. Below, we show several examples. We should also note that the <code>gbm</code> function uses parallel processing in iterations.</p>
<div id="regression-1" class="section level3 hasAnchor" number="17.8.1">
<h3><span class="header-section-number">17.8.1</span> Regression<a href="ensemble-methods.html#regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This will give you an idea how tuning the boosting by using <code>h</code> would be done:</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="ensemble-methods.html#cb491-1" tabindex="-1"></a><span class="co"># Test/Train Split</span></span>
<span id="cb491-2"><a href="ensemble-methods.html#cb491-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb491-3"><a href="ensemble-methods.html#cb491-3" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb491-4"><a href="ensemble-methods.html#cb491-4" tabindex="-1"></a>train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb491-5"><a href="ensemble-methods.html#cb491-5" tabindex="-1"></a>test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb491-6"><a href="ensemble-methods.html#cb491-6" tabindex="-1"></a></span>
<span id="cb491-7"><a href="ensemble-methods.html#cb491-7" tabindex="-1"></a>h <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">1.8</span>, <span class="fl">0.01</span>)</span>
<span id="cb491-8"><a href="ensemble-methods.html#cb491-8" tabindex="-1"></a></span>
<span id="cb491-9"><a href="ensemble-methods.html#cb491-9" tabindex="-1"></a>test_mse <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb491-10"><a href="ensemble-methods.html#cb491-10" tabindex="-1"></a></span>
<span id="cb491-11"><a href="ensemble-methods.html#cb491-11" tabindex="-1"></a><span class="co"># D = 1 and B = 1000</span></span>
<span id="cb491-12"><a href="ensemble-methods.html#cb491-12" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(h)){</span>
<span id="cb491-13"><a href="ensemble-methods.html#cb491-13" tabindex="-1"></a>    boos <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Salary<span class="sc">~</span>., <span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="at">n.trees =</span> <span class="dv">1000</span>,</span>
<span id="cb491-14"><a href="ensemble-methods.html#cb491-14" tabindex="-1"></a>            <span class="at">interaction.depth =</span> <span class="dv">1</span>, <span class="at">shrinkage =</span> h[i], <span class="at">data =</span> train)</span>
<span id="cb491-15"><a href="ensemble-methods.html#cb491-15" tabindex="-1"></a>    prboos <span class="ot">&lt;-</span> <span class="fu">predict</span>(boos, test, <span class="at">n.trees =</span> <span class="dv">100</span>)</span>
<span id="cb491-16"><a href="ensemble-methods.html#cb491-16" tabindex="-1"></a>    test_mse[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test<span class="sc">$</span>Salary <span class="sc">-</span> prboos) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb491-17"><a href="ensemble-methods.html#cb491-17" tabindex="-1"></a>}</span>
<span id="cb491-18"><a href="ensemble-methods.html#cb491-18" tabindex="-1"></a><span class="fu">plot</span>(h, test_mse, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">main =</span> <span class="st">&quot;MSE - Prediction&quot;</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/ea10-1.png" width="672" /></p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="ensemble-methods.html#cb492-1" tabindex="-1"></a>h[<span class="fu">which.min</span>(test_mse)]</span></code></pre></div>
<pre><code>## [1] 0.08</code></pre>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="ensemble-methods.html#cb494-1" tabindex="-1"></a><span class="fu">min</span>(test_mse)</span></code></pre></div>
<pre><code>## [1] 0.181286</code></pre>
<p>A complete but limited grid search is here:</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="ensemble-methods.html#cb496-1" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb496-2"><a href="ensemble-methods.html#cb496-2" tabindex="-1"></a></span>
<span id="cb496-3"><a href="ensemble-methods.html#cb496-3" tabindex="-1"></a>h <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.3</span>, <span class="fl">0.01</span>)</span>
<span id="cb496-4"><a href="ensemble-methods.html#cb496-4" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">300</span>, <span class="dv">500</span>, <span class="dv">750</span>, <span class="dv">900</span>)</span>
<span id="cb496-5"><a href="ensemble-methods.html#cb496-5" tabindex="-1"></a>D <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span></span>
<span id="cb496-6"><a href="ensemble-methods.html#cb496-6" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">expand.grid</span>(D, B, h))</span>
<span id="cb496-7"><a href="ensemble-methods.html#cb496-7" tabindex="-1"></a></span>
<span id="cb496-8"><a href="ensemble-methods.html#cb496-8" tabindex="-1"></a>mse <span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb496-9"><a href="ensemble-methods.html#cb496-9" tabindex="-1"></a>sdmse <span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb496-10"><a href="ensemble-methods.html#cb496-10" tabindex="-1"></a></span>
<span id="cb496-11"><a href="ensemble-methods.html#cb496-11" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(grid)){</span>
<span id="cb496-12"><a href="ensemble-methods.html#cb496-12" tabindex="-1"></a>  test_mse <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb496-13"><a href="ensemble-methods.html#cb496-13" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>) {</span>
<span id="cb496-14"><a href="ensemble-methods.html#cb496-14" tabindex="-1"></a>    <span class="fu">try</span>({</span>
<span id="cb496-15"><a href="ensemble-methods.html#cb496-15" tabindex="-1"></a>      <span class="fu">set.seed</span>(j)</span>
<span id="cb496-16"><a href="ensemble-methods.html#cb496-16" tabindex="-1"></a>      ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb496-17"><a href="ensemble-methods.html#cb496-17" tabindex="-1"></a>      train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb496-18"><a href="ensemble-methods.html#cb496-18" tabindex="-1"></a>      test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb496-19"><a href="ensemble-methods.html#cb496-19" tabindex="-1"></a>      boos <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Salary<span class="sc">~</span>., <span class="at">distribution =</span><span class="st">&quot;gaussian&quot;</span>, <span class="at">n.trees =</span> <span class="dv">1000</span>,</span>
<span id="cb496-20"><a href="ensemble-methods.html#cb496-20" tabindex="-1"></a>              <span class="at">interaction.depth =</span> grid[i,<span class="dv">1</span>], <span class="at">shrinkage =</span> grid[i,<span class="dv">3</span>], <span class="at">data =</span> train)</span>
<span id="cb496-21"><a href="ensemble-methods.html#cb496-21" tabindex="-1"></a>      prboos <span class="ot">&lt;-</span> <span class="fu">predict</span>(boos, test, <span class="at">n.trees =</span> grid[i,<span class="dv">2</span>])</span>
<span id="cb496-22"><a href="ensemble-methods.html#cb496-22" tabindex="-1"></a>      test_mse[j] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test<span class="sc">$</span>Salary <span class="sc">-</span> prboos) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb496-23"><a href="ensemble-methods.html#cb496-23" tabindex="-1"></a>      },</span>
<span id="cb496-24"><a href="ensemble-methods.html#cb496-24" tabindex="-1"></a>    <span class="at">silent =</span> <span class="cn">TRUE</span>)</span>
<span id="cb496-25"><a href="ensemble-methods.html#cb496-25" tabindex="-1"></a>  }</span>
<span id="cb496-26"><a href="ensemble-methods.html#cb496-26" tabindex="-1"></a>mse[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(test_mse)</span>
<span id="cb496-27"><a href="ensemble-methods.html#cb496-27" tabindex="-1"></a>sdmse[i] <span class="ot">&lt;-</span> <span class="fu">sd</span>(test_mse)</span>
<span id="cb496-28"><a href="ensemble-methods.html#cb496-28" tabindex="-1"></a>}</span>
<span id="cb496-29"><a href="ensemble-methods.html#cb496-29" tabindex="-1"></a></span>
<span id="cb496-30"><a href="ensemble-methods.html#cb496-30" tabindex="-1"></a><span class="fu">min</span>(mse)</span></code></pre></div>
<pre><code>## [1] 0.2108654</code></pre>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="ensemble-methods.html#cb498-1" tabindex="-1"></a>grid[<span class="fu">as.numeric</span>(<span class="fu">which.min</span>(mse)), ]</span></code></pre></div>
<pre><code>##  Var1  Var2  Var3 
## 2e+00 9e+02 1e-02</code></pre>
</div>
<div id="random-search-with-parallel-processing" class="section level3 hasAnchor" number="17.8.2">
<h3><span class="header-section-number">17.8.2</span> Random search with parallel processing<a href="ensemble-methods.html#random-search-with-parallel-processing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, we will apply a random grid search introduced by Bergstra and Bengio in <a href="https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf">Random Search for Hyper-Parameter Optimization</a>) <span class="citation">(<a href="#ref-Bergs_2012"><strong>Bergs_2012?</strong></a>)</span>. This paper shows that randomly chosen trials are more efficient for hyperparameter optimization than trials on a grid. Random search is a slight variation on grid search. Instead of searching over the entire grid, random search evaluates randomly selected parts on the grid.</p>
<p>To characterize the performance of random search, the authors use the analytic form of the expectation. The expected probability of finding the target is <span class="math inline">\(1.0\)</span> minus the probability of missing the target with every single one of <span class="math inline">\(T\)</span> trials in the experiment. If the volume of the target relative to the unit hypercube is <span class="math inline">\((v / V=0.01)\)</span> and there are <span class="math inline">\(T\)</span> trials, then this probability of finding the target is</p>
<p><span class="math display">\[
1-\left(1-\frac{v}{V}\right)^T=1-0.99^T .
\]</span>
In more practical terms, for any distribution over a sample space with a maximum, we can find the number of randomly selected points from the grid. First, we define the confidence level, say 95%. Then we decide how many points we wish to have around the maximum. We can decide as a number or directly as a percentage. Let’s say we decide 0.01% interval around the maximum. Then the formula will be</p>
<p><span class="math display">\[
1-(1-0.01)^T&gt;0.95,
\]</span>
which can be solved as</p>
<p><span class="math display">\[
\text{T} = \log (1-0.95)/\log (1-0.01)
\]</span></p>
<p>We also apply a parallel multicore processing using <code>doParallel</code> and <code>foreach()</code> to accelerate the grid search. More details can be found at <a href="https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf">Getting Started with doParallel and foreach</a>.</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="ensemble-methods.html#cb500-1" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb500-2"><a href="ensemble-methods.html#cb500-2" tabindex="-1"></a><span class="fu">library</span>(doParallel)</span>
<span id="cb500-3"><a href="ensemble-methods.html#cb500-3" tabindex="-1"></a><span class="fu">library</span>(foreach)</span>
<span id="cb500-4"><a href="ensemble-methods.html#cb500-4" tabindex="-1"></a></span>
<span id="cb500-5"><a href="ensemble-methods.html#cb500-5" tabindex="-1"></a>h <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.001</span>, <span class="fl">0.25</span>, <span class="fl">0.001</span>)</span>
<span id="cb500-6"><a href="ensemble-methods.html#cb500-6" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">100</span>, <span class="dv">800</span>, <span class="dv">20</span>)</span>
<span id="cb500-7"><a href="ensemble-methods.html#cb500-7" tabindex="-1"></a>D <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span></span>
<span id="cb500-8"><a href="ensemble-methods.html#cb500-8" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">expand.grid</span>(D, B, h))</span>
<span id="cb500-9"><a href="ensemble-methods.html#cb500-9" tabindex="-1"></a></span>
<span id="cb500-10"><a href="ensemble-methods.html#cb500-10" tabindex="-1"></a><span class="co">#Random grid-search</span></span>
<span id="cb500-11"><a href="ensemble-methods.html#cb500-11" tabindex="-1"></a>conf_lev <span class="ot">&lt;-</span> <span class="fl">0.95</span></span>
<span id="cb500-12"><a href="ensemble-methods.html#cb500-12" tabindex="-1"></a>num_max <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="co"># we define it by numbers</span></span>
<span id="cb500-13"><a href="ensemble-methods.html#cb500-13" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">log</span>(<span class="dv">1</span><span class="sc">-</span>conf_lev)<span class="sc">/</span><span class="fu">log</span>(<span class="dv">1</span><span class="sc">-</span>num_max<span class="sc">/</span><span class="fu">nrow</span>(grid))</span>
<span id="cb500-14"><a href="ensemble-methods.html#cb500-14" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb500-15"><a href="ensemble-methods.html#cb500-15" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(grid), <span class="fu">nrow</span>(grid)<span class="sc">*</span>(n<span class="sc">/</span><span class="fu">nrow</span>(grid)), <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb500-16"><a href="ensemble-methods.html#cb500-16" tabindex="-1"></a>comb <span class="ot">&lt;-</span> grid[ind, ]</span>
<span id="cb500-17"><a href="ensemble-methods.html#cb500-17" tabindex="-1"></a></span>
<span id="cb500-18"><a href="ensemble-methods.html#cb500-18" tabindex="-1"></a><span class="co"># Set-up for multicore loops</span></span>
<span id="cb500-19"><a href="ensemble-methods.html#cb500-19" tabindex="-1"></a>trials <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(comb)</span>
<span id="cb500-20"><a href="ensemble-methods.html#cb500-20" tabindex="-1"></a>numCores <span class="ot">&lt;-</span> <span class="fu">detectCores</span>()</span>
<span id="cb500-21"><a href="ensemble-methods.html#cb500-21" tabindex="-1"></a><span class="fu">registerDoParallel</span>(numCores)</span>
<span id="cb500-22"><a href="ensemble-methods.html#cb500-22" tabindex="-1"></a></span>
<span id="cb500-23"><a href="ensemble-methods.html#cb500-23" tabindex="-1"></a><span class="co"># Bootstrapping with parallel process </span></span>
<span id="cb500-24"><a href="ensemble-methods.html#cb500-24" tabindex="-1"></a>lst <span class="ot">&lt;-</span> <span class="fu">foreach</span>(<span class="at">k=</span>trials, <span class="at">.combine=</span>c, <span class="at">.errorhandling =</span> <span class="st">&#39;remove&#39;</span>) <span class="sc">%dopar%</span> {</span>
<span id="cb500-25"><a href="ensemble-methods.html#cb500-25" tabindex="-1"></a>  test_mse <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb500-26"><a href="ensemble-methods.html#cb500-26" tabindex="-1"></a>  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb500-27"><a href="ensemble-methods.html#cb500-27" tabindex="-1"></a>    <span class="fu">try</span>({</span>
<span id="cb500-28"><a href="ensemble-methods.html#cb500-28" tabindex="-1"></a>      <span class="fu">set.seed</span>(j)</span>
<span id="cb500-29"><a href="ensemble-methods.html#cb500-29" tabindex="-1"></a>      ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb500-30"><a href="ensemble-methods.html#cb500-30" tabindex="-1"></a>      train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb500-31"><a href="ensemble-methods.html#cb500-31" tabindex="-1"></a>      test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb500-32"><a href="ensemble-methods.html#cb500-32" tabindex="-1"></a>      boos <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Salary<span class="sc">~</span>., <span class="at">distribution =</span><span class="st">&quot;gaussian&quot;</span>, <span class="at">n.trees=</span><span class="dv">1000</span>,</span>
<span id="cb500-33"><a href="ensemble-methods.html#cb500-33" tabindex="-1"></a>             <span class="at">interaction.depth =</span>comb[k,<span class="dv">1</span>], <span class="at">shrinkage =</span> comb[k,<span class="dv">3</span>], <span class="at">data =</span> train)</span>
<span id="cb500-34"><a href="ensemble-methods.html#cb500-34" tabindex="-1"></a>      prboos <span class="ot">&lt;-</span> <span class="fu">predict</span>(boos, test, <span class="at">n.trees =</span> comb[k,<span class="dv">2</span>])</span>
<span id="cb500-35"><a href="ensemble-methods.html#cb500-35" tabindex="-1"></a>      test_mse[j] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test<span class="sc">$</span>Salary <span class="sc">-</span> prboos)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb500-36"><a href="ensemble-methods.html#cb500-36" tabindex="-1"></a>      },</span>
<span id="cb500-37"><a href="ensemble-methods.html#cb500-37" tabindex="-1"></a>    <span class="at">silent =</span> <span class="cn">TRUE</span>)</span>
<span id="cb500-38"><a href="ensemble-methods.html#cb500-38" tabindex="-1"></a>  }</span>
<span id="cb500-39"><a href="ensemble-methods.html#cb500-39" tabindex="-1"></a><span class="fu">list</span>(<span class="fu">c</span>(k, <span class="fu">mean</span>(test_mse), <span class="fu">sd</span>(test_mse)))</span>
<span id="cb500-40"><a href="ensemble-methods.html#cb500-40" tabindex="-1"></a>}</span>
<span id="cb500-41"><a href="ensemble-methods.html#cb500-41" tabindex="-1"></a></span>
<span id="cb500-42"><a href="ensemble-methods.html#cb500-42" tabindex="-1"></a><span class="fu">stopImplicitCluster</span>()</span>
<span id="cb500-43"><a href="ensemble-methods.html#cb500-43" tabindex="-1"></a></span>
<span id="cb500-44"><a href="ensemble-methods.html#cb500-44" tabindex="-1"></a>unlst  <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, lst)</span>
<span id="cb500-45"><a href="ensemble-methods.html#cb500-45" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">cbind</span>(comb[unlst[,<span class="dv">1</span>],], unlst)</span>
<span id="cb500-46"><a href="ensemble-methods.html#cb500-46" tabindex="-1"></a>sorted  <span class="ot">&lt;-</span> result[<span class="fu">order</span>(result[,<span class="dv">5</span>]), <span class="sc">-</span><span class="dv">4</span>]</span>
<span id="cb500-47"><a href="ensemble-methods.html#cb500-47" tabindex="-1"></a><span class="fu">colnames</span>(sorted) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;D&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;h&quot;</span>, <span class="st">&quot;MSPE&quot;</span>, <span class="st">&quot;sd&quot;</span>)</span>
<span id="cb500-48"><a href="ensemble-methods.html#cb500-48" tabindex="-1"></a><span class="fu">head</span>(sorted)</span></code></pre></div>
<pre><code>##      D   B     h      MSPE         sd
## [1,] 2 360 0.024 0.2057671 0.05657079
## [2,] 2 300 0.024 0.2060013 0.05807494
## [3,] 2 340 0.022 0.2061847 0.05827857
## [4,] 2 340 0.023 0.2061895 0.05823719
## [5,] 2 320 0.023 0.2062056 0.05874694
## [6,] 2 360 0.021 0.2062124 0.05785775</code></pre>
<p>You can increase <code>for (j in 1:10)</code> to <code>for (j in 1:50)</code> depending on your computer’s capacity.</p>
</div>
<div id="boosting-vs.-others" class="section level3 hasAnchor" number="17.8.3">
<h3><span class="header-section-number">17.8.3</span> Boosting vs. Others<a href="ensemble-methods.html#boosting-vs.-others" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s add OLS to this competition just for curiosity. Here is a one possible script:</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="ensemble-methods.html#cb502-1" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb502-2"><a href="ensemble-methods.html#cb502-2" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb502-3"><a href="ensemble-methods.html#cb502-3" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb502-4"><a href="ensemble-methods.html#cb502-4" tabindex="-1"></a></span>
<span id="cb502-5"><a href="ensemble-methods.html#cb502-5" tabindex="-1"></a>df <span class="ot">&lt;-</span> Hitters[<span class="fu">complete.cases</span>(Hitters<span class="sc">$</span>Salary), ]</span>
<span id="cb502-6"><a href="ensemble-methods.html#cb502-6" tabindex="-1"></a>df<span class="sc">$</span>Salary <span class="ot">&lt;-</span> <span class="fu">log</span>(df<span class="sc">$</span>Salary)</span>
<span id="cb502-7"><a href="ensemble-methods.html#cb502-7" tabindex="-1"></a></span>
<span id="cb502-8"><a href="ensemble-methods.html#cb502-8" tabindex="-1"></a><span class="co"># Containers</span></span>
<span id="cb502-9"><a href="ensemble-methods.html#cb502-9" tabindex="-1"></a>mse_cart <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>)</span>
<span id="cb502-10"><a href="ensemble-methods.html#cb502-10" tabindex="-1"></a>mse_bag <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>)</span>
<span id="cb502-11"><a href="ensemble-methods.html#cb502-11" tabindex="-1"></a>mse_rf <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>)</span>
<span id="cb502-12"><a href="ensemble-methods.html#cb502-12" tabindex="-1"></a>mse_boost <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>)</span>
<span id="cb502-13"><a href="ensemble-methods.html#cb502-13" tabindex="-1"></a>mse_ols <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>)</span>
<span id="cb502-14"><a href="ensemble-methods.html#cb502-14" tabindex="-1"></a></span>
<span id="cb502-15"><a href="ensemble-methods.html#cb502-15" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">200</span>){</span>
<span id="cb502-16"><a href="ensemble-methods.html#cb502-16" tabindex="-1"></a>  <span class="fu">set.seed</span>(i)</span>
<span id="cb502-17"><a href="ensemble-methods.html#cb502-17" tabindex="-1"></a>      ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb502-18"><a href="ensemble-methods.html#cb502-18" tabindex="-1"></a>      train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb502-19"><a href="ensemble-methods.html#cb502-19" tabindex="-1"></a>      test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb502-20"><a href="ensemble-methods.html#cb502-20" tabindex="-1"></a>      </span>
<span id="cb502-21"><a href="ensemble-methods.html#cb502-21" tabindex="-1"></a>      OLS <span class="ot">&lt;-</span> <span class="fu">lm</span>(Salary<span class="sc">~</span>., <span class="at">data =</span> train)</span>
<span id="cb502-22"><a href="ensemble-methods.html#cb502-22" tabindex="-1"></a>      pols <span class="ot">&lt;-</span> <span class="fu">predict</span>(OLS, test)</span>
<span id="cb502-23"><a href="ensemble-methods.html#cb502-23" tabindex="-1"></a>      </span>
<span id="cb502-24"><a href="ensemble-methods.html#cb502-24" tabindex="-1"></a>      cart <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Salary<span class="sc">~</span>., <span class="at">data =</span> train)</span>
<span id="cb502-25"><a href="ensemble-methods.html#cb502-25" tabindex="-1"></a>      pcart <span class="ot">&lt;-</span> <span class="fu">predict</span>(cart, test)</span>
<span id="cb502-26"><a href="ensemble-methods.html#cb502-26" tabindex="-1"></a>      </span>
<span id="cb502-27"><a href="ensemble-methods.html#cb502-27" tabindex="-1"></a>      bags <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Salary <span class="sc">~</span>., <span class="at">mtry =</span> <span class="dv">19</span>, <span class="at">data =</span> train)</span>
<span id="cb502-28"><a href="ensemble-methods.html#cb502-28" tabindex="-1"></a>      pbag <span class="ot">&lt;-</span> <span class="fu">predict</span>(bags, test)</span>
<span id="cb502-29"><a href="ensemble-methods.html#cb502-29" tabindex="-1"></a>      </span>
<span id="cb502-30"><a href="ensemble-methods.html#cb502-30" tabindex="-1"></a>      rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(Salary <span class="sc">~</span>., <span class="at">data =</span> train)</span>
<span id="cb502-31"><a href="ensemble-methods.html#cb502-31" tabindex="-1"></a>      prf <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, test)</span>
<span id="cb502-32"><a href="ensemble-methods.html#cb502-32" tabindex="-1"></a>      </span>
<span id="cb502-33"><a href="ensemble-methods.html#cb502-33" tabindex="-1"></a>      boost <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Salary<span class="sc">~</span>., <span class="at">distribution =</span><span class="st">&quot;gaussian&quot;</span>, <span class="at">n.trees =</span> <span class="dv">1000</span>,</span>
<span id="cb502-34"><a href="ensemble-methods.html#cb502-34" tabindex="-1"></a>                 <span class="at">data =</span> train) <span class="co"># without a grid search</span></span>
<span id="cb502-35"><a href="ensemble-methods.html#cb502-35" tabindex="-1"></a>      pboost <span class="ot">&lt;-</span> <span class="fu">predict</span>(boost, test, <span class="at">n.trees =</span> <span class="dv">100</span>)</span>
<span id="cb502-36"><a href="ensemble-methods.html#cb502-36" tabindex="-1"></a>      </span>
<span id="cb502-37"><a href="ensemble-methods.html#cb502-37" tabindex="-1"></a>      mse_ols[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test<span class="sc">$</span>Salary <span class="sc">-</span> pols)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb502-38"><a href="ensemble-methods.html#cb502-38" tabindex="-1"></a>      mse_cart[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test<span class="sc">$</span>Salary <span class="sc">-</span> pcart)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb502-39"><a href="ensemble-methods.html#cb502-39" tabindex="-1"></a>      mse_bag[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test<span class="sc">$</span>Salary <span class="sc">-</span> pbag)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb502-40"><a href="ensemble-methods.html#cb502-40" tabindex="-1"></a>      mse_rf[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test<span class="sc">$</span>Salary <span class="sc">-</span> prf)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb502-41"><a href="ensemble-methods.html#cb502-41" tabindex="-1"></a>      mse_boost[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((test<span class="sc">$</span>Salary <span class="sc">-</span> pboost)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb502-42"><a href="ensemble-methods.html#cb502-42" tabindex="-1"></a>}</span>
<span id="cb502-43"><a href="ensemble-methods.html#cb502-43" tabindex="-1"></a></span>
<span id="cb502-44"><a href="ensemble-methods.html#cb502-44" tabindex="-1"></a><span class="co"># Bootstrapping Results</span></span>
<span id="cb502-45"><a href="ensemble-methods.html#cb502-45" tabindex="-1"></a></span>
<span id="cb502-46"><a href="ensemble-methods.html#cb502-46" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fu">mean</span>(mse_cart), <span class="fu">mean</span>(mse_bag), <span class="fu">mean</span>(mse_rf), <span class="fu">mean</span>(mse_boost), <span class="fu">mean</span>(mse_ols)), <span class="dv">5</span>, <span class="dv">1</span>)</span>
<span id="cb502-47"><a href="ensemble-methods.html#cb502-47" tabindex="-1"></a><span class="fu">row.names</span>(a) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;mse_cart&quot;</span>, <span class="st">&quot;mse_bag&quot;</span>, <span class="st">&quot;mse_rf&quot;</span>, <span class="st">&quot;mse_boost&quot;</span>, <span class="st">&quot;mse_ols&quot;</span>)</span>
<span id="cb502-48"><a href="ensemble-methods.html#cb502-48" tabindex="-1"></a>a</span></code></pre></div>
<pre><code>##                [,1]
## mse_cart  0.3172687
## mse_bag   0.2206152
## mse_rf    0.2057663
## mse_boost 0.2451800
## mse_ols   0.4584240</code></pre>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="ensemble-methods.html#cb504-1" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fu">sqrt</span>(<span class="fu">var</span>(mse_cart)), <span class="fu">sqrt</span>(<span class="fu">var</span>(mse_bag)), <span class="fu">sqrt</span>(<span class="fu">var</span>(mse_rf)), <span class="fu">sqrt</span>(<span class="fu">var</span>(mse_boost)), <span class="fu">sqrt</span>(<span class="fu">var</span>(mse_ols))), <span class="dv">5</span>, <span class="dv">1</span>)</span>
<span id="cb504-2"><a href="ensemble-methods.html#cb504-2" tabindex="-1"></a><span class="fu">row.names</span>(b) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;mse_cart&quot;</span>, <span class="st">&quot;mse_bag&quot;</span>, <span class="st">&quot;mse_rf&quot;</span>, <span class="st">&quot;mse_boost&quot;</span>, <span class="st">&quot;mse_ols&quot;</span>)</span>
<span id="cb504-3"><a href="ensemble-methods.html#cb504-3" tabindex="-1"></a>b</span></code></pre></div>
<pre><code>##                 [,1]
## mse_cart  0.07308726
## mse_bag   0.06279604
## mse_rf    0.05981292
## mse_boost 0.05929530
## mse_ols   0.06907506</code></pre>
<p>The random forest and boosting have similar performances. However, boosting and is not tuned in the algorithm. With the full grid search in the previous algorithm, boosting would be a better choice.</p>
<p>Let’s have a classification example.</p>
</div>
<div id="classification-1" class="section level3 hasAnchor" number="17.8.4">
<h3><span class="header-section-number">17.8.4</span> Classification<a href="ensemble-methods.html#classification-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A simulated data set containing sales of child car seats at 400 different stores from. We will predict the sale, a binary variable that will be 1 if the sale is higher than 8. See <a href="https://rdrr.io/cran/ISLR/man/Carseats.html">ISLR</a> <span class="citation">(<a href="#ref-ISLR_Car"><strong>ISLR_Car?</strong></a>)</span> for the details.</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="ensemble-methods.html#cb506-1" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb506-2"><a href="ensemble-methods.html#cb506-2" tabindex="-1"></a>df <span class="ot">&lt;-</span> Carseats</span>
<span id="cb506-3"><a href="ensemble-methods.html#cb506-3" tabindex="-1"></a><span class="fu">str</span>(df)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  11 variables:
##  $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...
##  $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...
##  $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...
##  $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...
##  $ Population : num  276 260 269 466 340 501 45 425 108 131 ...
##  $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...
##  $ ShelveLoc  : Factor w/ 3 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Medium&quot;: 1 2 3 3 1 1 3 2 3 3 ...
##  $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...
##  $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...
##  $ Urban      : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 2 2 1 1 ...
##  $ US         : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 1 2 1 2 1 2 ...</code></pre>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="ensemble-methods.html#cb508-1" tabindex="-1"></a><span class="co">#Change SALES to a factor variable</span></span>
<span id="cb508-2"><a href="ensemble-methods.html#cb508-2" tabindex="-1"></a>df<span class="sc">$</span>Sales <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(Carseats<span class="sc">$</span>Sales <span class="sc">&lt;=</span> <span class="dv">8</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb508-3"><a href="ensemble-methods.html#cb508-3" tabindex="-1"></a><span class="fu">str</span>(df<span class="sc">$</span>Sales)</span></code></pre></div>
<pre><code>##  num [1:400] 1 1 1 0 0 1 0 1 0 0 ...</code></pre>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="ensemble-methods.html#cb510-1" tabindex="-1"></a><span class="fu">library</span>(PASWR)</span>
<span id="cb510-2"><a href="ensemble-methods.html#cb510-2" tabindex="-1"></a><span class="fu">library</span>(ROCR)</span>
<span id="cb510-3"><a href="ensemble-methods.html#cb510-3" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb510-4"><a href="ensemble-methods.html#cb510-4" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb510-5"><a href="ensemble-methods.html#cb510-5" tabindex="-1"></a></span>
<span id="cb510-6"><a href="ensemble-methods.html#cb510-6" tabindex="-1"></a>df <span class="ot">&lt;-</span> df[<span class="fu">complete.cases</span>(df), ]</span>
<span id="cb510-7"><a href="ensemble-methods.html#cb510-7" tabindex="-1"></a>df<span class="sc">$</span>d <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(df<span class="sc">$</span>Sales)</span>
<span id="cb510-8"><a href="ensemble-methods.html#cb510-8" tabindex="-1"></a></span>
<span id="cb510-9"><a href="ensemble-methods.html#cb510-9" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb510-10"><a href="ensemble-methods.html#cb510-10" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb510-11"><a href="ensemble-methods.html#cb510-11" tabindex="-1"></a>AUC1 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb510-12"><a href="ensemble-methods.html#cb510-12" tabindex="-1"></a>AUC2 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb510-13"><a href="ensemble-methods.html#cb510-13" tabindex="-1"></a>AUC3 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb510-14"><a href="ensemble-methods.html#cb510-14" tabindex="-1"></a>AUC4 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb510-15"><a href="ensemble-methods.html#cb510-15" tabindex="-1"></a></span>
<span id="cb510-16"><a href="ensemble-methods.html#cb510-16" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb510-17"><a href="ensemble-methods.html#cb510-17" tabindex="-1"></a>  <span class="fu">set.seed</span>(i)</span>
<span id="cb510-18"><a href="ensemble-methods.html#cb510-18" tabindex="-1"></a>  ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb510-19"><a href="ensemble-methods.html#cb510-19" tabindex="-1"></a>  train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb510-20"><a href="ensemble-methods.html#cb510-20" tabindex="-1"></a>  test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb510-21"><a href="ensemble-methods.html#cb510-21" tabindex="-1"></a>  </span>
<span id="cb510-22"><a href="ensemble-methods.html#cb510-22" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">ncol</span>(train)<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb510-23"><a href="ensemble-methods.html#cb510-23" tabindex="-1"></a></span>
<span id="cb510-24"><a href="ensemble-methods.html#cb510-24" tabindex="-1"></a>  <span class="co"># We used two different outcome structure: &quot;Sales&quot; and &quot;d&quot;</span></span>
<span id="cb510-25"><a href="ensemble-methods.html#cb510-25" tabindex="-1"></a>  <span class="co"># &quot;d&quot; is a factor and &quot;Sales&quot; is numeric</span></span>
<span id="cb510-26"><a href="ensemble-methods.html#cb510-26" tabindex="-1"></a>  <span class="co"># Factor variable is necessary for RF but GBM needs a numeric variable</span></span>
<span id="cb510-27"><a href="ensemble-methods.html#cb510-27" tabindex="-1"></a>  <span class="co"># That&#39;s sometimes annoying but wee need to be careful about the data</span></span>
<span id="cb510-28"><a href="ensemble-methods.html#cb510-28" tabindex="-1"></a>  model1 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(Sales<span class="sc">~</span>., <span class="at">data=</span>train[,<span class="sc">-</span><span class="dv">12</span>], <span class="at">method =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb510-29"><a href="ensemble-methods.html#cb510-29" tabindex="-1"></a>  model2 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(d<span class="sc">~</span>., <span class="at">ntree =</span> B, <span class="at">mtry =</span> p, <span class="at">data =</span> train[, <span class="sc">-</span><span class="dv">1</span>]) <span class="co">#Bagged</span></span>
<span id="cb510-30"><a href="ensemble-methods.html#cb510-30" tabindex="-1"></a>  model3 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(d<span class="sc">~</span>., <span class="at">ntree =</span> B, <span class="at">data =</span> train[, <span class="sc">-</span><span class="dv">1</span>]) <span class="co"># RF    </span></span>
<span id="cb510-31"><a href="ensemble-methods.html#cb510-31" tabindex="-1"></a>  model4 <span class="ot">&lt;-</span> <span class="fu">gbm</span>(Sales<span class="sc">~</span>., <span class="at">data=</span>train[,<span class="sc">-</span><span class="dv">12</span>], <span class="at">n.trees =</span> B,</span>
<span id="cb510-32"><a href="ensemble-methods.html#cb510-32" tabindex="-1"></a>                <span class="at">distribution =</span> <span class="st">&quot;bernoulli&quot;</span>) <span class="co"># Boosting without grid search    </span></span>
<span id="cb510-33"><a href="ensemble-methods.html#cb510-33" tabindex="-1"></a>  </span>
<span id="cb510-34"><a href="ensemble-methods.html#cb510-34" tabindex="-1"></a>  phat1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, test[,<span class="sc">-</span><span class="dv">12</span>], <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb510-35"><a href="ensemble-methods.html#cb510-35" tabindex="-1"></a>  phat2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model2, test[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb510-36"><a href="ensemble-methods.html#cb510-36" tabindex="-1"></a>  phat3 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model3, test[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb510-37"><a href="ensemble-methods.html#cb510-37" tabindex="-1"></a>  phat4 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model4, <span class="at">n.trees =</span> B, test[,<span class="sc">-</span><span class="dv">12</span>], <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb510-38"><a href="ensemble-methods.html#cb510-38" tabindex="-1"></a>  </span>
<span id="cb510-39"><a href="ensemble-methods.html#cb510-39" tabindex="-1"></a>  <span class="co">#AUC1</span></span>
<span id="cb510-40"><a href="ensemble-methods.html#cb510-40" tabindex="-1"></a>  pred_rocr1 <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat1[,<span class="dv">2</span>], test<span class="sc">$</span>Sales)</span>
<span id="cb510-41"><a href="ensemble-methods.html#cb510-41" tabindex="-1"></a>  auc_ROCR1 <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr1, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb510-42"><a href="ensemble-methods.html#cb510-42" tabindex="-1"></a>  AUC1[i] <span class="ot">&lt;-</span> auc_ROCR1<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb510-43"><a href="ensemble-methods.html#cb510-43" tabindex="-1"></a>  </span>
<span id="cb510-44"><a href="ensemble-methods.html#cb510-44" tabindex="-1"></a>  <span class="co">#AUC2</span></span>
<span id="cb510-45"><a href="ensemble-methods.html#cb510-45" tabindex="-1"></a>  pred_rocr2 <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat2[,<span class="dv">2</span>], test<span class="sc">$</span>d)</span>
<span id="cb510-46"><a href="ensemble-methods.html#cb510-46" tabindex="-1"></a>  auc_ROCR2 <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr2, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb510-47"><a href="ensemble-methods.html#cb510-47" tabindex="-1"></a>  AUC2[i] <span class="ot">&lt;-</span> auc_ROCR2<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb510-48"><a href="ensemble-methods.html#cb510-48" tabindex="-1"></a>  </span>
<span id="cb510-49"><a href="ensemble-methods.html#cb510-49" tabindex="-1"></a>  <span class="co">#AUC3</span></span>
<span id="cb510-50"><a href="ensemble-methods.html#cb510-50" tabindex="-1"></a>  pred_rocr3 <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat3[,<span class="dv">2</span>], test<span class="sc">$</span>d)</span>
<span id="cb510-51"><a href="ensemble-methods.html#cb510-51" tabindex="-1"></a>  auc_ROCR3 <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr3, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb510-52"><a href="ensemble-methods.html#cb510-52" tabindex="-1"></a>  AUC3[i] <span class="ot">&lt;-</span> auc_ROCR3<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb510-53"><a href="ensemble-methods.html#cb510-53" tabindex="-1"></a>  </span>
<span id="cb510-54"><a href="ensemble-methods.html#cb510-54" tabindex="-1"></a>  <span class="co">#AUC4</span></span>
<span id="cb510-55"><a href="ensemble-methods.html#cb510-55" tabindex="-1"></a>  pred_rocr4 <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat4, test<span class="sc">$</span>Sales)</span>
<span id="cb510-56"><a href="ensemble-methods.html#cb510-56" tabindex="-1"></a>  auc_ROCR4 <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr4, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb510-57"><a href="ensemble-methods.html#cb510-57" tabindex="-1"></a>  AUC4[i] <span class="ot">&lt;-</span> auc_ROCR4<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb510-58"><a href="ensemble-methods.html#cb510-58" tabindex="-1"></a>}</span>
<span id="cb510-59"><a href="ensemble-methods.html#cb510-59" tabindex="-1"></a></span>
<span id="cb510-60"><a href="ensemble-methods.html#cb510-60" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Single-Tree&quot;</span>, <span class="st">&quot;Bagging&quot;</span>, <span class="st">&quot;RF&quot;</span>, <span class="st">&quot;Boosting&quot;</span>)</span>
<span id="cb510-61"><a href="ensemble-methods.html#cb510-61" tabindex="-1"></a>AUCs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">mean</span>(AUC1), <span class="fu">mean</span>(AUC2), <span class="fu">mean</span>(AUC3), <span class="fu">mean</span>(AUC4))</span>
<span id="cb510-62"><a href="ensemble-methods.html#cb510-62" tabindex="-1"></a>sd <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">sqrt</span>(<span class="fu">var</span>(AUC1)), <span class="fu">sqrt</span>(<span class="fu">var</span>(AUC2)), <span class="fu">sqrt</span>(<span class="fu">var</span>(AUC3)), <span class="fu">sqrt</span>(<span class="fu">var</span>(AUC4)))</span>
<span id="cb510-63"><a href="ensemble-methods.html#cb510-63" tabindex="-1"></a><span class="fu">data.frame</span>(model, AUCs, sd) </span></code></pre></div>
<pre><code>##         model      AUCs         sd
## 1 Single-Tree 0.7607756 0.03203628
## 2     Bagging 0.8642527 0.02674844
## 3          RF 0.8777902 0.02393084
## 4    Boosting 0.9176397 0.01742109</code></pre>
<p>The results are very telling: booster is a clear winner for prediction accuracy and stability. When we have these machine learning applications, one should always show the “baseline” prediction that we can judge the winner performance: A simple LPM would be a good baseline model:</p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="ensemble-methods.html#cb512-1" tabindex="-1"></a>AUC5 <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb512-2"><a href="ensemble-methods.html#cb512-2" tabindex="-1"></a></span>
<span id="cb512-3"><a href="ensemble-methods.html#cb512-3" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb512-4"><a href="ensemble-methods.html#cb512-4" tabindex="-1"></a>  <span class="fu">set.seed</span>(i)</span>
<span id="cb512-5"><a href="ensemble-methods.html#cb512-5" tabindex="-1"></a>  ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb512-6"><a href="ensemble-methods.html#cb512-6" tabindex="-1"></a>  train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb512-7"><a href="ensemble-methods.html#cb512-7" tabindex="-1"></a>  test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb512-8"><a href="ensemble-methods.html#cb512-8" tabindex="-1"></a>  </span>
<span id="cb512-9"><a href="ensemble-methods.html#cb512-9" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sales <span class="sc">~</span> ., <span class="at">data=</span> train[,<span class="sc">-</span><span class="dv">12</span>])</span>
<span id="cb512-10"><a href="ensemble-methods.html#cb512-10" tabindex="-1"></a>  phat5 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, test[, <span class="sc">-</span><span class="dv">12</span>])</span>
<span id="cb512-11"><a href="ensemble-methods.html#cb512-11" tabindex="-1"></a>  </span>
<span id="cb512-12"><a href="ensemble-methods.html#cb512-12" tabindex="-1"></a>  pred_rocr5 <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat5, test<span class="sc">$</span>Sales)</span>
<span id="cb512-13"><a href="ensemble-methods.html#cb512-13" tabindex="-1"></a>  auc_ROCR5 <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr5, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb512-14"><a href="ensemble-methods.html#cb512-14" tabindex="-1"></a>  AUC5[i] <span class="ot">&lt;-</span> auc_ROCR5<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb512-15"><a href="ensemble-methods.html#cb512-15" tabindex="-1"></a>} </span>
<span id="cb512-16"><a href="ensemble-methods.html#cb512-16" tabindex="-1"></a></span>
<span id="cb512-17"><a href="ensemble-methods.html#cb512-17" tabindex="-1"></a><span class="fu">mean</span>(AUC5)</span></code></pre></div>
<pre><code>## [1] 0.9546986</code></pre>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="ensemble-methods.html#cb514-1" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">var</span>(AUC5)) </span></code></pre></div>
<pre><code>## [1] 0.0117673</code></pre>
<p>I choose this example to show that we cannot assume that our complex algorithms will always be better than a simple OLS. We judge the success of prediction not only its own AUC and stability, but also how much it improves over a benchmark.</p>
</div>
<div id="adaboost.m1" class="section level3 hasAnchor" number="17.8.5">
<h3><span class="header-section-number">17.8.5</span> AdaBoost.M1<a href="ensemble-methods.html#adaboost.m1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s apply AdaBoost to our example to see if we can have any improvements</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="ensemble-methods.html#cb516-1" tabindex="-1"></a><span class="fu">library</span>(JOUSBoost)</span>
<span id="cb516-2"><a href="ensemble-methods.html#cb516-2" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb516-3"><a href="ensemble-methods.html#cb516-3" tabindex="-1"></a>df <span class="ot">&lt;-</span> Carseats</span>
<span id="cb516-4"><a href="ensemble-methods.html#cb516-4" tabindex="-1"></a></span>
<span id="cb516-5"><a href="ensemble-methods.html#cb516-5" tabindex="-1"></a><span class="co">#Change SALES to a factor variable</span></span>
<span id="cb516-6"><a href="ensemble-methods.html#cb516-6" tabindex="-1"></a>df<span class="sc">$</span>Sales <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(Carseats<span class="sc">$</span>Sales <span class="sc">&lt;=</span> <span class="dv">8</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="co">#adaboost requires -1,1 coding</span></span>
<span id="cb516-7"><a href="ensemble-methods.html#cb516-7" tabindex="-1"></a><span class="fu">str</span>(df<span class="sc">$</span>Sales)</span></code></pre></div>
<pre><code>##  num [1:400] 1 1 1 -1 -1 1 -1 1 -1 -1 ...</code></pre>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="ensemble-methods.html#cb518-1" tabindex="-1"></a><span class="co"># adaboost requires X as a matrix</span></span>
<span id="cb518-2"><a href="ensemble-methods.html#cb518-2" tabindex="-1"></a><span class="co"># so factor variables must be coded as numerical</span></span>
<span id="cb518-3"><a href="ensemble-methods.html#cb518-3" tabindex="-1"></a></span>
<span id="cb518-4"><a href="ensemble-methods.html#cb518-4" tabindex="-1"></a><span class="co"># With `one-hot()`</span></span>
<span id="cb518-5"><a href="ensemble-methods.html#cb518-5" tabindex="-1"></a><span class="fu">library</span>(mltools)</span>
<span id="cb518-6"><a href="ensemble-methods.html#cb518-6" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb518-7"><a href="ensemble-methods.html#cb518-7" tabindex="-1"></a>df_new <span class="ot">&lt;-</span> <span class="fu">one_hot</span>(<span class="fu">as.data.table</span>(df))</span></code></pre></div>
<p>Now, we are ready:</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="ensemble-methods.html#cb519-1" tabindex="-1"></a>rnd <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">50</span>)</span>
<span id="cb519-2"><a href="ensemble-methods.html#cb519-2" tabindex="-1"></a>MAUC <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb519-3"><a href="ensemble-methods.html#cb519-3" tabindex="-1"></a></span>
<span id="cb519-4"><a href="ensemble-methods.html#cb519-4" tabindex="-1"></a><span class="cf">for</span> (r <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(rnd)) {</span>
<span id="cb519-5"><a href="ensemble-methods.html#cb519-5" tabindex="-1"></a>  AUC <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb519-6"><a href="ensemble-methods.html#cb519-6" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>) {</span>
<span id="cb519-7"><a href="ensemble-methods.html#cb519-7" tabindex="-1"></a>    <span class="fu">set.seed</span>(i)</span>
<span id="cb519-8"><a href="ensemble-methods.html#cb519-8" tabindex="-1"></a>    ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df_new), <span class="fu">nrow</span>(df_new), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb519-9"><a href="ensemble-methods.html#cb519-9" tabindex="-1"></a>    train <span class="ot">&lt;-</span> df_new[ind,]</span>
<span id="cb519-10"><a href="ensemble-methods.html#cb519-10" tabindex="-1"></a>    test <span class="ot">&lt;-</span> df_new[<span class="sc">-</span>ind,]</span>
<span id="cb519-11"><a href="ensemble-methods.html#cb519-11" tabindex="-1"></a>    </span>
<span id="cb519-12"><a href="ensemble-methods.html#cb519-12" tabindex="-1"></a>    ada <span class="ot">&lt;-</span> <span class="fu">adaboost</span>(<span class="fu">as.matrix</span>(train[, <span class="sc">-</span><span class="st">&quot;Sales&quot;</span>]),</span>
<span id="cb519-13"><a href="ensemble-methods.html#cb519-13" tabindex="-1"></a>                    train<span class="sc">$</span>Sales,</span>
<span id="cb519-14"><a href="ensemble-methods.html#cb519-14" tabindex="-1"></a>                    <span class="at">tree_depth =</span> <span class="dv">1</span>,</span>
<span id="cb519-15"><a href="ensemble-methods.html#cb519-15" tabindex="-1"></a>                    <span class="at">n_rounds =</span> rnd[r])</span>
<span id="cb519-16"><a href="ensemble-methods.html#cb519-16" tabindex="-1"></a>    phat <span class="ot">&lt;-</span> <span class="fu">predict</span>(ada, test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb519-17"><a href="ensemble-methods.html#cb519-17" tabindex="-1"></a>    </span>
<span id="cb519-18"><a href="ensemble-methods.html#cb519-18" tabindex="-1"></a>    pred_rocr <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat, test<span class="sc">$</span>Sales)</span>
<span id="cb519-19"><a href="ensemble-methods.html#cb519-19" tabindex="-1"></a>    auc_ROCR <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb519-20"><a href="ensemble-methods.html#cb519-20" tabindex="-1"></a>    AUC[i] <span class="ot">&lt;-</span> auc_ROCR<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb519-21"><a href="ensemble-methods.html#cb519-21" tabindex="-1"></a>  }</span>
<span id="cb519-22"><a href="ensemble-methods.html#cb519-22" tabindex="-1"></a>  MAUC[r] <span class="ot">&lt;-</span> <span class="fu">mean</span>(AUC)</span>
<span id="cb519-23"><a href="ensemble-methods.html#cb519-23" tabindex="-1"></a>}</span>
<span id="cb519-24"><a href="ensemble-methods.html#cb519-24" tabindex="-1"></a></span>
<span id="cb519-25"><a href="ensemble-methods.html#cb519-25" tabindex="-1"></a><span class="fu">mean</span>(MAUC)</span></code></pre></div>
<pre><code>## [1] 0.9218254</code></pre>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="ensemble-methods.html#cb521-1" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">var</span>(MAUC)) </span></code></pre></div>
<pre><code>## [1] 0.001441398</code></pre>
<p>It’s slightly better than the gradient boosting (<code>gbm</code>) but not much from LPM. We should apply a better grid for the rounds of iterations.</p>
</div>
<div id="classification-with-xgboost" class="section level3 hasAnchor" number="17.8.6">
<h3><span class="header-section-number">17.8.6</span> Classification with XGBoost<a href="ensemble-methods.html#classification-with-xgboost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before jumping into an example, let’s first understand about the most frequently used hyperparameters in <code>xgboost</code>. You can refer to its <a href="https://xgboost.readthedocs.io/en/latest/parameter.html">official documentation</a> for more details.</p>
<p>We will classify them in three groups:</p>
<ol style="list-style-type: decimal">
<li>Booster type: <code>Booster = gbtree</code> is the default. It could be set to <code>gblinear</code> or <code>dart</code>. The first one uses a linear model and the second one refers to <em>Dropout Additive Regression Trees</em>. When constructing a gradient boosting machine, the first few trees at the beginning dominate the model performance relative to trees added later. Thus, the idea of “dropout” is to build an ensemble by randomly dropping trees in the boosting sequence.</li>
<li>Tuning parameters (note that when <code>gblinear</code> is used, only <code>nround</code>, <code>lambda</code>, and <code>alpha</code> are used):<br />
</li>
</ol>
<ul>
<li><code>nrounds</code> = 100 (default). It controls the maximum number of iterations (or trees for classification).<br />
</li>
<li><code>eta</code> = 0.3. It controls the learning rate. Typically, it lies between 0.01 - 0.3.<br />
</li>
<li><code>gamma</code> = 0. It controls regularization (or prevents overfitting - a higher difference between the train and test prediction performance). It can be used as it brings improvements when shallow (low <code>max_depth</code>) trees are employed.</li>
<li><code>max_depth</code> = 6. It controls the depth of the tree.</li>
<li><code>min_child_weight</code> = 1. It blocks the potential feature interactions to prevent overfitting. (The minimum number of instances required in a child node.)</li>
<li><code>subsample</code> = 1. It controls the number of observations supplied to a tree. Generally, it lies between 0.01 - 0.3. (remember bagging).</li>
<li><code>colsample_bytree</code> = 1. It control the number of features (variables) supplied to a tree. Both <code>subsample</code> and <code>colsample_bytree</code> can be use to build a “random forest” type learner.</li>
<li><code>lambda</code> = 0, equivalent to Ridge regression</li>
<li><code>alpha</code> = 1, equivalent to Lasso regression (more useful on high dimensional data sets). When both are set different than zero, it becomes an “Elastic Net”, which we will see later.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Evaluation parameters:<br />
</li>
</ol>
<ul>
<li><code>objective</code> = “reg:squarederror” for linear regression, “binary:logistic” binary classification (it returns class probabilities). See the official guide for more options.</li>
<li><code>eval_metric</code> = no default. Depending on objective selected, it could be one of those: <code>mae</code>, <code>Logloss</code>, <code>AUC</code>, <code>RMSE</code>, <code>error</code> - (#wrong cases/#all cases), <code>mlogloss</code> - multiclass.</li>
</ul>
<p>Before executing a full-scale grid search, see what default parameters provide you. That’s your “base” model’s prediction accuracy, which can improve from. If the result is not giving you a desired accuracy, as we did in Chapter 13.3.3, set <code>eta</code> = 0.1 and the other parameters at their default values. Using <code>xgb.cv</code> function get best <code>n_rounds</code> and build a model with these parameters. See how much improvement you will get in its accuracy. Then apply the full-scale grid search.</p>
<p>We will use the same data (“Adult”) as we used in Chapter 11.</p>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb523-1"><a href="ensemble-methods.html#cb523-1" tabindex="-1"></a><span class="fu">library</span>(xgboost)</span>
<span id="cb523-2"><a href="ensemble-methods.html#cb523-2" tabindex="-1"></a><span class="fu">library</span>(mltools)</span>
<span id="cb523-3"><a href="ensemble-methods.html#cb523-3" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb523-4"><a href="ensemble-methods.html#cb523-4" tabindex="-1"></a></span>
<span id="cb523-5"><a href="ensemble-methods.html#cb523-5" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;adult_train.csv&quot;</span>, <span class="at">header =</span> <span class="cn">FALSE</span>)</span>
<span id="cb523-6"><a href="ensemble-methods.html#cb523-6" tabindex="-1"></a></span>
<span id="cb523-7"><a href="ensemble-methods.html#cb523-7" tabindex="-1"></a>varNames <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Age&quot;</span>, </span>
<span id="cb523-8"><a href="ensemble-methods.html#cb523-8" tabindex="-1"></a>              <span class="st">&quot;WorkClass&quot;</span>,</span>
<span id="cb523-9"><a href="ensemble-methods.html#cb523-9" tabindex="-1"></a>              <span class="st">&quot;fnlwgt&quot;</span>,</span>
<span id="cb523-10"><a href="ensemble-methods.html#cb523-10" tabindex="-1"></a>              <span class="st">&quot;Education&quot;</span>,</span>
<span id="cb523-11"><a href="ensemble-methods.html#cb523-11" tabindex="-1"></a>              <span class="st">&quot;EducationNum&quot;</span>,</span>
<span id="cb523-12"><a href="ensemble-methods.html#cb523-12" tabindex="-1"></a>              <span class="st">&quot;MaritalStatus&quot;</span>,</span>
<span id="cb523-13"><a href="ensemble-methods.html#cb523-13" tabindex="-1"></a>              <span class="st">&quot;Occupation&quot;</span>,</span>
<span id="cb523-14"><a href="ensemble-methods.html#cb523-14" tabindex="-1"></a>              <span class="st">&quot;Relationship&quot;</span>,</span>
<span id="cb523-15"><a href="ensemble-methods.html#cb523-15" tabindex="-1"></a>              <span class="st">&quot;Race&quot;</span>,</span>
<span id="cb523-16"><a href="ensemble-methods.html#cb523-16" tabindex="-1"></a>              <span class="st">&quot;Sex&quot;</span>,</span>
<span id="cb523-17"><a href="ensemble-methods.html#cb523-17" tabindex="-1"></a>              <span class="st">&quot;CapitalGain&quot;</span>,</span>
<span id="cb523-18"><a href="ensemble-methods.html#cb523-18" tabindex="-1"></a>              <span class="st">&quot;CapitalLoss&quot;</span>,</span>
<span id="cb523-19"><a href="ensemble-methods.html#cb523-19" tabindex="-1"></a>              <span class="st">&quot;HoursPerWeek&quot;</span>,</span>
<span id="cb523-20"><a href="ensemble-methods.html#cb523-20" tabindex="-1"></a>              <span class="st">&quot;NativeCountry&quot;</span>,</span>
<span id="cb523-21"><a href="ensemble-methods.html#cb523-21" tabindex="-1"></a>              <span class="st">&quot;IncomeLevel&quot;</span>)</span>
<span id="cb523-22"><a href="ensemble-methods.html#cb523-22" tabindex="-1"></a></span>
<span id="cb523-23"><a href="ensemble-methods.html#cb523-23" tabindex="-1"></a><span class="fu">names</span>(train) <span class="ot">&lt;-</span> varNames</span>
<span id="cb523-24"><a href="ensemble-methods.html#cb523-24" tabindex="-1"></a>data <span class="ot">&lt;-</span> train</span>
<span id="cb523-25"><a href="ensemble-methods.html#cb523-25" tabindex="-1"></a></span>
<span id="cb523-26"><a href="ensemble-methods.html#cb523-26" tabindex="-1"></a>tbl <span class="ot">&lt;-</span> <span class="fu">table</span>(data<span class="sc">$</span>IncomeLevel)</span>
<span id="cb523-27"><a href="ensemble-methods.html#cb523-27" tabindex="-1"></a>tbl</span></code></pre></div>
<pre><code>## 
##  &lt;=50K   &gt;50K 
##  24720   7841</code></pre>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="ensemble-methods.html#cb525-1" tabindex="-1"></a><span class="co"># we remove some outliers - See Ch.11</span></span>
<span id="cb525-2"><a href="ensemble-methods.html#cb525-2" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(data<span class="sc">$</span>NativeCountry<span class="sc">==</span><span class="st">&quot; Holand-Netherlands&quot;</span>)</span>
<span id="cb525-3"><a href="ensemble-methods.html#cb525-3" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[<span class="sc">-</span>ind, ]</span>
<span id="cb525-4"><a href="ensemble-methods.html#cb525-4" tabindex="-1"></a></span>
<span id="cb525-5"><a href="ensemble-methods.html#cb525-5" tabindex="-1"></a><span class="co">#Converting chr to factor with `apply()` family</span></span>
<span id="cb525-6"><a href="ensemble-methods.html#cb525-6" tabindex="-1"></a>df <span class="ot">&lt;-</span> data</span>
<span id="cb525-7"><a href="ensemble-methods.html#cb525-7" tabindex="-1"></a>df[<span class="fu">sapply</span>(df, is.character)] <span class="ot">&lt;-</span> <span class="fu">lapply</span>(df[<span class="fu">sapply</span>(df, is.character)],</span>
<span id="cb525-8"><a href="ensemble-methods.html#cb525-8" tabindex="-1"></a>                                       as.factor)</span>
<span id="cb525-9"><a href="ensemble-methods.html#cb525-9" tabindex="-1"></a></span>
<span id="cb525-10"><a href="ensemble-methods.html#cb525-10" tabindex="-1"></a><span class="fu">str</span>(df)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32560 obs. of  15 variables:
##  $ Age          : int  39 50 38 53 28 37 49 52 31 42 ...
##  $ WorkClass    : Factor w/ 9 levels &quot; ?&quot;,&quot; Federal-gov&quot;,..: 8 7 5 5 5 5 5 7 5 5 ...
##  $ fnlwgt       : int  77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ...
##  $ Education    : Factor w/ 16 levels &quot; 10th&quot;,&quot; 11th&quot;,..: 10 10 12 2 10 13 7 12 13 10 ...
##  $ EducationNum : int  13 13 9 7 13 14 5 9 14 13 ...
##  $ MaritalStatus: Factor w/ 7 levels &quot; Divorced&quot;,&quot; Married-AF-spouse&quot;,..: 5 3 1 3 3 3 4 3 5 3 ...
##  $ Occupation   : Factor w/ 15 levels &quot; ?&quot;,&quot; Adm-clerical&quot;,..: 2 5 7 7 11 5 9 5 11 5 ...
##  $ Relationship : Factor w/ 6 levels &quot; Husband&quot;,&quot; Not-in-family&quot;,..: 2 1 2 1 6 6 2 1 2 1 ...
##  $ Race         : Factor w/ 5 levels &quot; Amer-Indian-Eskimo&quot;,..: 5 5 5 3 3 5 3 5 5 5 ...
##  $ Sex          : Factor w/ 2 levels &quot; Female&quot;,&quot; Male&quot;: 2 2 2 2 1 1 1 2 1 2 ...
##  $ CapitalGain  : int  2174 0 0 0 0 0 0 0 14084 5178 ...
##  $ CapitalLoss  : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ HoursPerWeek : int  40 13 40 40 40 40 16 45 50 40 ...
##  $ NativeCountry: Factor w/ 41 levels &quot; ?&quot;,&quot; Cambodia&quot;,..: 39 39 39 39 6 39 23 39 39 39 ...
##  $ IncomeLevel  : Factor w/ 2 levels &quot; &lt;=50K&quot;,&quot; &gt;50K&quot;: 1 1 1 1 1 1 1 2 2 2 ...</code></pre>
<p>As required by the <code>xgboost</code> package, we need a numeric <span class="math inline">\(Y\)</span> and all the factor variables have to be one-hot coded</p>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="ensemble-methods.html#cb527-1" tabindex="-1"></a>df<span class="sc">$</span>Y <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(data<span class="sc">$</span>IncomeLevel<span class="sc">==</span><span class="st">&quot; &lt;=50K&quot;</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb527-2"><a href="ensemble-methods.html#cb527-2" tabindex="-1"></a></span>
<span id="cb527-3"><a href="ensemble-methods.html#cb527-3" tabindex="-1"></a><span class="co">#Remove `IncomeLevel`</span></span>
<span id="cb527-4"><a href="ensemble-methods.html#cb527-4" tabindex="-1"></a>df <span class="ot">&lt;-</span> df[, <span class="sc">-</span><span class="dv">15</span>]</span>
<span id="cb527-5"><a href="ensemble-methods.html#cb527-5" tabindex="-1"></a></span>
<span id="cb527-6"><a href="ensemble-methods.html#cb527-6" tabindex="-1"></a><span class="fu">anyNA</span>(df) <span class="co"># no NA&#39;s</span></span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="ensemble-methods.html#cb529-1" tabindex="-1"></a><span class="co"># Initial Split 90-10% split</span></span>
<span id="cb529-2"><a href="ensemble-methods.html#cb529-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">321</span>)</span>
<span id="cb529-3"><a href="ensemble-methods.html#cb529-3" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df)<span class="sc">*</span><span class="fl">0.90</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb529-4"><a href="ensemble-methods.html#cb529-4" tabindex="-1"></a>train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb529-5"><a href="ensemble-methods.html#cb529-5" tabindex="-1"></a>test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb529-6"><a href="ensemble-methods.html#cb529-6" tabindex="-1"></a></span>
<span id="cb529-7"><a href="ensemble-methods.html#cb529-7" tabindex="-1"></a><span class="co"># One-hot coding using R&#39;s `model.matrix`</span></span>
<span id="cb529-8"><a href="ensemble-methods.html#cb529-8" tabindex="-1"></a>ty <span class="ot">&lt;-</span> train<span class="sc">$</span>Y</span>
<span id="cb529-9"><a href="ensemble-methods.html#cb529-9" tabindex="-1"></a>tsy <span class="ot">&lt;-</span> test<span class="sc">$</span>Y</span>
<span id="cb529-10"><a href="ensemble-methods.html#cb529-10" tabindex="-1"></a>hot_tr <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>.<span class="sc">+</span><span class="dv">0</span>, <span class="at">data =</span> train[,<span class="sc">-</span><span class="fu">which</span>(<span class="fu">names</span>(train) <span class="sc">==</span> <span class="st">&quot;Y&quot;</span>)]) </span>
<span id="cb529-11"><a href="ensemble-methods.html#cb529-11" tabindex="-1"></a>hot_ts <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span>.<span class="sc">+</span><span class="dv">0</span>, <span class="at">data =</span> test[,<span class="sc">-</span><span class="fu">which</span>(<span class="fu">names</span>(train) <span class="sc">==</span> <span class="st">&quot;Y&quot;</span>)])</span>
<span id="cb529-12"><a href="ensemble-methods.html#cb529-12" tabindex="-1"></a></span>
<span id="cb529-13"><a href="ensemble-methods.html#cb529-13" tabindex="-1"></a><span class="co"># Preparing efficient matrix</span></span>
<span id="cb529-14"><a href="ensemble-methods.html#cb529-14" tabindex="-1"></a>ttrain <span class="ot">&lt;-</span> <span class="fu">xgb.DMatrix</span>(<span class="at">data =</span> hot_tr, <span class="at">label =</span> ty) </span>
<span id="cb529-15"><a href="ensemble-methods.html#cb529-15" tabindex="-1"></a>ttest <span class="ot">&lt;-</span> <span class="fu">xgb.DMatrix</span>(<span class="at">data =</span> hot_ts, <span class="at">label =</span> tsy)</span></code></pre></div>
<p>Now we are ready to set our first <code>xgb.sv</code> with default parameters</p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="ensemble-methods.html#cb530-1" tabindex="-1"></a>params <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">booster =</span> <span class="st">&quot;gbtree&quot;</span>,</span>
<span id="cb530-2"><a href="ensemble-methods.html#cb530-2" tabindex="-1"></a>               <span class="at">objective =</span> <span class="st">&quot;binary:logistic&quot;</span></span>
<span id="cb530-3"><a href="ensemble-methods.html#cb530-3" tabindex="-1"></a>               )</span>
<span id="cb530-4"><a href="ensemble-methods.html#cb530-4" tabindex="-1"></a></span>
<span id="cb530-5"><a href="ensemble-methods.html#cb530-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">112</span>)</span>
<span id="cb530-6"><a href="ensemble-methods.html#cb530-6" tabindex="-1"></a>cvb <span class="ot">&lt;-</span> <span class="fu">xgb.cv</span>( <span class="at">params =</span> params,</span>
<span id="cb530-7"><a href="ensemble-methods.html#cb530-7" tabindex="-1"></a>               <span class="at">nrounds =</span> <span class="dv">100</span>,</span>
<span id="cb530-8"><a href="ensemble-methods.html#cb530-8" tabindex="-1"></a>               <span class="at">data =</span> ttrain,</span>
<span id="cb530-9"><a href="ensemble-methods.html#cb530-9" tabindex="-1"></a>               <span class="at">nfold =</span> <span class="dv">5</span>,</span>
<span id="cb530-10"><a href="ensemble-methods.html#cb530-10" tabindex="-1"></a>               <span class="at">showsd =</span> T,</span>
<span id="cb530-11"><a href="ensemble-methods.html#cb530-11" tabindex="-1"></a>               <span class="at">stratified =</span> T,</span>
<span id="cb530-12"><a href="ensemble-methods.html#cb530-12" tabindex="-1"></a>               <span class="at">print.every.n =</span> <span class="dv">10</span>,</span>
<span id="cb530-13"><a href="ensemble-methods.html#cb530-13" tabindex="-1"></a>               <span class="at">early.stop.round =</span> <span class="dv">20</span>,</span>
<span id="cb530-14"><a href="ensemble-methods.html#cb530-14" tabindex="-1"></a>               <span class="at">maximize =</span> F</span>
<span id="cb530-15"><a href="ensemble-methods.html#cb530-15" tabindex="-1"></a>               )</span></code></pre></div>
<pre><code>## [1]  train-logloss:0.541285+0.000640 test-logloss:0.542411+0.001768 
## Multiple eval metrics are present. Will use test_logloss for early stopping.
## Will train until test_logloss hasn&#39;t improved in 20 rounds.
## 
## [11] train-logloss:0.290701+0.000486 test-logloss:0.302696+0.003658 
## [21] train-logloss:0.264326+0.000814 test-logloss:0.285655+0.004132 
## [31] train-logloss:0.251203+0.001082 test-logloss:0.280880+0.004269 
## [41] train-logloss:0.243382+0.001291 test-logloss:0.279297+0.004772 
## [51] train-logloss:0.237065+0.001390 test-logloss:0.278460+0.004780 
## [61] train-logloss:0.230541+0.001288 test-logloss:0.278528+0.004913 
## [71] train-logloss:0.225721+0.001117 test-logloss:0.279118+0.005197 
## Stopping. Best iteration:
## [59] train-logloss:0.231852+0.000732 test-logloss:0.278273+0.004699</code></pre>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb532-1"><a href="ensemble-methods.html#cb532-1" tabindex="-1"></a>theb <span class="ot">&lt;-</span> cvb<span class="sc">$</span>best_iteration</span>
<span id="cb532-2"><a href="ensemble-methods.html#cb532-2" tabindex="-1"></a>theb</span></code></pre></div>
<pre><code>## [1] 59</code></pre>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="ensemble-methods.html#cb534-1" tabindex="-1"></a>model_default <span class="ot">&lt;-</span> <span class="fu">xgb.train</span> (<span class="at">params =</span> params,</span>
<span id="cb534-2"><a href="ensemble-methods.html#cb534-2" tabindex="-1"></a>                            <span class="at">data =</span> ttrain,</span>
<span id="cb534-3"><a href="ensemble-methods.html#cb534-3" tabindex="-1"></a>                            <span class="at">nrounds =</span> theb,</span>
<span id="cb534-4"><a href="ensemble-methods.html#cb534-4" tabindex="-1"></a>                            <span class="at">watchlist =</span> <span class="fu">list</span>(<span class="at">val=</span>ttest,<span class="at">train=</span>ttrain),</span>
<span id="cb534-5"><a href="ensemble-methods.html#cb534-5" tabindex="-1"></a>                            <span class="at">print_every_n =</span> <span class="dv">10</span>,</span>
<span id="cb534-6"><a href="ensemble-methods.html#cb534-6" tabindex="-1"></a>                            <span class="at">maximize =</span> F ,</span>
<span id="cb534-7"><a href="ensemble-methods.html#cb534-7" tabindex="-1"></a>                            <span class="at">eval_metric =</span> <span class="st">&quot;auc&quot;</span>)</span></code></pre></div>
<pre><code>## [1]  val-auc:0.898067    train-auc:0.895080 
## [11] val-auc:0.922919    train-auc:0.925884 
## [21] val-auc:0.927905    train-auc:0.936823 
## [31] val-auc:0.928464    train-auc:0.942277 
## [41] val-auc:0.929252    train-auc:0.946379 
## [51] val-auc:0.928459    train-auc:0.949633 
## [59] val-auc:0.928224    train-auc:0.951403</code></pre>
<p>And the prediction:</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="ensemble-methods.html#cb536-1" tabindex="-1"></a>phat <span class="ot">&lt;-</span> <span class="fu">predict</span> (model_default, ttest)</span>
<span id="cb536-2"><a href="ensemble-methods.html#cb536-2" tabindex="-1"></a></span>
<span id="cb536-3"><a href="ensemble-methods.html#cb536-3" tabindex="-1"></a><span class="co"># AUC</span></span>
<span id="cb536-4"><a href="ensemble-methods.html#cb536-4" tabindex="-1"></a><span class="fu">library</span>(ROCR)</span>
<span id="cb536-5"><a href="ensemble-methods.html#cb536-5" tabindex="-1"></a>pred_rocr <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat, tsy)</span>
<span id="cb536-6"><a href="ensemble-methods.html#cb536-6" tabindex="-1"></a>auc_ROCR <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb536-7"><a href="ensemble-methods.html#cb536-7" tabindex="-1"></a>auc_ROCR<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0.9282243</code></pre>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="ensemble-methods.html#cb538-1" tabindex="-1"></a><span class="co"># ROCR</span></span>
<span id="cb538-2"><a href="ensemble-methods.html#cb538-2" tabindex="-1"></a>perf <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr,<span class="st">&quot;tpr&quot;</span>,<span class="st">&quot;fpr&quot;</span>)</span>
<span id="cb538-3"><a href="ensemble-methods.html#cb538-3" tabindex="-1"></a><span class="fu">plot</span>(perf, <span class="at">colorize=</span><span class="cn">TRUE</span>)</span>
<span id="cb538-4"><a href="ensemble-methods.html#cb538-4" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="17-EnsembleMethods_files/figure-html/ea25-1.png" width="672" /></p>
<p>You can go back to Chapter 11.3.2 and see that XGBoost is better than kNN in this example without even a proper grid search.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p><a href="https://web.archive.org/web/20121010030839/http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf" class="uri">https://web.archive.org/web/20121010030839/http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf</a><a href="ensemble-methods.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>In a sparse matrix, cells containing 0 are not stored in memory. Therefore, in a dataset mainly made of 0, the memory size is reduced.<a href="ensemble-methods.html#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="causal-effect-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuk/machinemetrics/edit/master/17-EnsembleMethods.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
