<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 21 Classification | MachineMetrics</title>
  <meta name="description" content="Chapter 21 Classification | MachineMetrics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 21 Classification | MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 21 Classification | MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="model-selection-and-sparsity.html"/>
<link rel="next" href="time-series.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> INTRODUCTION:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html"><i class="fa fa-check"></i><b>2</b> Statistical Models and Simulations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#distinguishing-between-statistical-modeling-and-machine-learning-in-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#goals-and-objectives"><i class="fa fa-check"></i><b>2.1.1</b> Goals and Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#prediction-vs.-inference"><i class="fa fa-check"></i><b>2.1.2</b> Prediction vs. Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#conclusion"><i class="fa fa-check"></i><b>2.1.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#model-selection-and-approaches-in-data-modeling"><i class="fa fa-check"></i><b>2.4</b> Model Selection and Approaches in Data Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#mle"><i class="fa fa-check"></i><b>3.5.3</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error---mse"><i class="fa fa-check"></i><b>5.1</b> Estimation error - MSE</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.2</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#technical-points-about-mse-and-mspe"><i class="fa fa-check"></i><b>5.3</b> Technical points about MSE and MSPE</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.1</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.1</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.2</b> LPM</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
<li class="chapter" data-level="10.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#knn"><i class="fa fa-check"></i><b>10.3</b> Knn</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#adult-dataset"><i class="fa fa-check"></i><b>10.3.1</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="11.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#grid-search"><i class="fa fa-check"></i><b>11.4</b> Grid Search</a></li>
<li class="chapter" data-level="11.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>11.5</b> Cross-validated grid search</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#one-variable"><i class="fa fa-check"></i><b>12.4.1</b> One-variable</a></li>
<li class="chapter" data-level="12.4.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>12.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="12.4.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#multivariable"><i class="fa fa-check"></i><b>12.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>16.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.3</b> Pruning</a></li>
<li class="chapter" data-level="16.4" data-path="regression-trees.html"><a href="regression-trees.html#classification-with-titanic"><i class="fa fa-check"></i><b>16.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="16.5" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>17.2</b> Random Forest</a></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>17.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="17.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="17.3.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.3.3</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.4</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.5</b> Classification</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.6</b> Regression</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.7</b> Exploration</a></li>
<li class="chapter" data-level="17.8" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.8</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression-1"><i class="fa fa-check"></i><b>17.8.1</b> Regression</a></li>
<li class="chapter" data-level="17.8.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.8.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.8.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.8.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.8.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-1"><i class="fa fa-check"></i><b>17.8.4</b> Classification</a></li>
<li class="chapter" data-level="17.8.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost.m1"><i class="fa fa-check"></i><b>17.8.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="17.8.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.8.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#out-sample-prediction-accuracy"><i class="fa fa-check"></i><b>20.3</b> out-sample prediction accuracy</a></li>
<li class="chapter" data-level="20.4" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-2.html"><a href="classification-2.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-2.html"><a href="classification-2.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-2.html"><a href="classification-2.html#mnist-dataset"><i class="fa fa-check"></i><b>21.2</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="21.3" data-path="classification-2.html"><a href="classification-2.html#linear-classifiers-again"><i class="fa fa-check"></i><b>21.3</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="21.4" data-path="classification-2.html"><a href="classification-2.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.4</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.5" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret"><i class="fa fa-check"></i><b>21.5</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="classification-2.html"><a href="classification-2.html#mnist_27"><i class="fa fa-check"></i><b>21.5.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="21.5.2" data-path="classification-2.html"><a href="classification-2.html#adult-dataset-1"><i class="fa fa-check"></i><b>21.5.2</b> Adult dataset</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="classification-2.html"><a href="classification-2.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.6</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.7" data-path="classification-2.html"><a href="classification-2.html#confusion-matrix"><i class="fa fa-check"></i><b>21.7</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.8" data-path="classification-2.html"><a href="classification-2.html#performance-measures"><i class="fa fa-check"></i><b>21.8</b> Performance measures</a></li>
<li class="chapter" data-level="21.9" data-path="classification-2.html"><a href="classification-2.html#roc-curve"><i class="fa fa-check"></i><b>21.9</b> ROC Curve</a></li>
<li class="chapter" data-level="21.10" data-path="classification-2.html"><a href="classification-2.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.10</b> AUC - Area Under the Curve</a></li>
<li class="chapter" data-level="21.11" data-path="classification-2.html"><a href="classification-2.html#classification-example"><i class="fa fa-check"></i><b>21.11</b> Classification Example</a></li>
<li class="chapter" data-level="21.12" data-path="classification-2.html"><a href="classification-2.html#lpm-1"><i class="fa fa-check"></i><b>21.12</b> LPM</a></li>
<li class="chapter" data-level="21.13" data-path="classification-2.html"><a href="classification-2.html#logistic-regression-1"><i class="fa fa-check"></i><b>21.13</b> Logistic Regression</a></li>
<li class="chapter" data-level="21.14" data-path="classification-2.html"><a href="classification-2.html#knn-1"><i class="fa fa-check"></i><b>21.14</b> kNN</a>
<ul>
<li class="chapter" data-level="21.14.1" data-path="classification-2.html"><a href="classification-2.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>21.14.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="21.14.2" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret-1"><i class="fa fa-check"></i><b>21.14.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>22</b> Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="time-series.html"><a href="time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="time-series.html"><a href="time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="time-series.html"><a href="time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="time-series.html"><a href="time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="time-series.html"><a href="time-series.html#stationarity"><i class="fa fa-check"></i><b>22.5</b> Stationarity</a></li>
<li class="chapter" data-level="22.6" data-path="time-series.html"><a href="time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.6</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="time-series.html"><a href="time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.7</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.8" data-path="time-series.html"><a href="time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.8</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.9" data-path="time-series.html"><a href="time-series.html#speed"><i class="fa fa-check"></i><b>22.9</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="forecast.html"><a href="forecast.html"><i class="fa fa-check"></i><b>23</b> Forecast</a>
<ul>
<li class="chapter" data-level="23.1" data-path="forecast.html"><a href="forecast.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="forecast.html"><a href="forecast.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="forecast.html"><a href="forecast.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="forecast.html"><a href="forecast.html#random-forest-1"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="forecast.html"><a href="forecast.html#univariate"><i class="fa fa-check"></i><b>23.5</b> Univariate</a></li>
<li class="chapter" data-level="23.6" data-path="forecast.html"><a href="forecast.html#multivariate"><i class="fa fa-check"></i><b>23.6</b> Multivariate</a></li>
<li class="chapter" data-level="23.7" data-path="forecast.html"><a href="forecast.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.7</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>24.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-margin"><i class="fa fa-check"></i><b>24.1.1</b> The Margin</a></li>
<li class="chapter" data-level="24.1.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-non-separable-case"><i class="fa fa-check"></i><b>24.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>24.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="24.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#application-with-svm"><i class="fa fa-check"></i><b>24.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#precision-matrix"><i class="fa fa-check"></i><b>27.4</b> Precision Matrix</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.5</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.6" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.6</b> Regularized Covariance Matrix</a></li>
<li class="chapter" data-level="27.7" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>27.7</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="27.8" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#high-dimensional-data"><i class="fa fa-check"></i><b>27.8</b> High-dimensional data</a></li>
<li class="chapter" data-level="27.9" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>27.9</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#rankr-approximations"><i class="fa fa-check"></i><b>28.4</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="28.5" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.5</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html"><i class="fa fa-check"></i><b>29</b> PCA (Principle Component Analysis)</a>
<ul>
<li class="chapter" data-level="29.1" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.1</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>30.1</b> Using bins</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>30.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="30.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>30.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="30.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="31.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>31.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="31.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>31.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="text-analysis.html"><a href="text-analysis.html"><i class="fa fa-check"></i><b>32</b> Text Analysis</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#other-nonparametric-estimation-methods-1"><i class="fa fa-check"></i><b>33.1</b> Other Nonparametric Estimation methods</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.2</b> Regression splines</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.3</b> MARS</a></li>
<li class="chapter" data-level="33.4" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.4</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuk/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-2" class="section level1 hasAnchor" number="21">
<h1><span class="header-section-number">Chapter 21</span> Classification<a href="classification-2.html#classification-2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="nonparametric-classifier---knn" class="section level2 hasAnchor" number="21.1">
<h2><span class="header-section-number">21.1</span> Nonparametric Classifier - kNN<a href="classification-2.html#nonparametric-classifier---knn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We complete this section, Nonparametric Estimations, with a nonparametric classifier and compare its performance with parametric classifiers, LPM and Logistic.</p>
</div>
<div id="mnist-dataset" class="section level2 hasAnchor" number="21.2">
<h2><span class="header-section-number">21.2</span> <code>mnist</code> Dataset<a href="classification-2.html#mnist-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Reading hand-written letters and numbers is not a big deal nowadays. For example, In Canada Post, computers read postal codes and robots sorts them for each postal code groups. This application is mainly achieved by machine learning algorithms. In order to understand how, let’s use a real dataset, Mnist. Here is the description of the dataset by Wikipedia:</p>
<blockquote>
<p>The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST’s training dataset, while the other half of the training set and the other half of the test set were taken from NIST’s testing dataset. There have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.23%.</p>
</blockquote>
<p><img src="png/digits.png" width="130%" height="130%" /></p>
<p>These images are converted into <span class="math inline">\(28 \times 28 = 784\)</span> pixels and, for each pixel, there is a measure that scales the darkness in that pixel between 0 (white) and 255 (black). Hence, for each digitized image, we have an indicator variable <span class="math inline">\(Y\)</span> between 0 and 9, and we have 784 variables that identifies each pixel in the digitized image. Let’s download the data. (<a href="http://yann.lecun.com/exdb/mnist/">More details about the data</a>).</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="classification-2.html#cb547-1" tabindex="-1"></a><span class="co">#loading the data</span></span>
<span id="cb547-2"><a href="classification-2.html#cb547-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb547-3"><a href="classification-2.html#cb547-3" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb547-4"><a href="classification-2.html#cb547-4" tabindex="-1"></a><span class="co">#Download the data to your directory.  It&#39;s big!</span></span>
<span id="cb547-5"><a href="classification-2.html#cb547-5" tabindex="-1"></a><span class="co">#mnist &lt;- read_mnist() </span></span>
<span id="cb547-6"><a href="classification-2.html#cb547-6" tabindex="-1"></a><span class="co">#save(mnist, file = &quot;mnist.Rdata&quot;)</span></span>
<span id="cb547-7"><a href="classification-2.html#cb547-7" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;mnist.Rdata&quot;</span>)</span>
<span id="cb547-8"><a href="classification-2.html#cb547-8" tabindex="-1"></a><span class="fu">str</span>(mnist)</span></code></pre></div>
<pre><code>## List of 2
##  $ train:List of 2
##   ..$ images: int [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...
##   ..$ labels: int [1:60000] 5 0 4 1 9 2 1 3 1 4 ...
##  $ test :List of 2
##   ..$ images: int [1:10000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...
##   ..$ labels: int [1:10000] 7 2 1 0 4 1 4 9 5 9 ...</code></pre>
<p>The data is given as a list and already divided into train and test sets. We have 60,000 images in the train set and 10,000 images in the test set. For the train set, we have two nested sets: <code>images</code>, which contains all 784 features for 60,000 images. Hence, it’s a <span class="math inline">\(60000 \times 784\)</span> matrix. And, <code>labels</code> contains the labes (from 0 to 9) for each image.</p>
<p>The digitizing can be understood from this image better:</p>
<p><img src="png/mnist.png" width="130%" height="130%" /></p>
<p>Each image has <span class="math inline">\(28 \times 28\)</span> = 784 pixels. For each image, the pixels are features with a label that shows the true number between 0 and 9. This methods is called as “flattening”, which is a technique that is used to convert multi-dimensional image into a one-dimension array (vector).</p>
<p>For now, we will use a smaller version of this data set given in the <code>dslabs</code> package, which is a random sample of 1,000 images (only for 2 and 7 digits), 800 in the training set and 200 in the test set, with only two features: the proportion of dark pixels that are in the upper left quadrant, <code>x_1</code>, and the lower right quadrant, <code>x_2</code>.</p>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb549-1"><a href="classification-2.html#cb549-1" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;mnist_27&quot;</span>)</span>
<span id="cb549-2"><a href="classification-2.html#cb549-2" tabindex="-1"></a><span class="fu">str</span>(mnist_27)</span></code></pre></div>
<pre><code>## List of 5
##  $ train      :&#39;data.frame&#39;: 800 obs. of  3 variables:
##   ..$ y  : Factor w/ 2 levels &quot;2&quot;,&quot;7&quot;: 1 2 1 1 2 1 2 2 2 1 ...
##   ..$ x_1: num [1:800] 0.0395 0.1607 0.0213 0.1358 0.3902 ...
##   ..$ x_2: num [1:800] 0.1842 0.0893 0.2766 0.2222 0.3659 ...
##  $ test       :&#39;data.frame&#39;: 200 obs. of  3 variables:
##   ..$ y  : Factor w/ 2 levels &quot;2&quot;,&quot;7&quot;: 1 2 2 2 2 1 1 1 1 2 ...
##   ..$ x_1: num [1:200] 0.148 0.283 0.29 0.195 0.218 ...
##   ..$ x_2: num [1:200] 0.261 0.348 0.435 0.115 0.397 ...
##  $ index_train: int [1:800] 40334 33996 3200 38360 36239 38816 8085 9098 15470 5096 ...
##  $ index_test : int [1:200] 46218 35939 23443 30466 2677 54248 5909 13402 11031 47308 ...
##  $ true_p     :&#39;data.frame&#39;: 22500 obs. of  3 variables:
##   ..$ x_1: num [1:22500] 0 0.00352 0.00703 0.01055 0.01406 ...
##   ..$ x_2: num [1:22500] 0 0 0 0 0 0 0 0 0 0 ...
##   ..$ p  : num [1:22500] 0.703 0.711 0.719 0.727 0.734 ...
##   ..- attr(*, &quot;out.attrs&quot;)=List of 2
##   .. ..$ dim     : Named int [1:2] 150 150
##   .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;x_1&quot; &quot;x_2&quot;
##   .. ..$ dimnames:List of 2
##   .. .. ..$ x_1: chr [1:150] &quot;x_1=0.0000000&quot; &quot;x_1=0.0035155&quot; &quot;x_1=0.0070310&quot; &quot;x_1=0.0105465&quot; ...
##   .. .. ..$ x_2: chr [1:150] &quot;x_2=0.000000000&quot; &quot;x_2=0.004101417&quot; &quot;x_2=0.008202834&quot; &quot;x_2=0.012304251&quot; ...</code></pre>
</div>
<div id="linear-classifiers-again" class="section level2 hasAnchor" number="21.3">
<h2><span class="header-section-number">21.3</span> Linear classifiers (again)<a href="classification-2.html#linear-classifiers-again" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A linear classifier (like LPM and Logistic) is one where a “hyperplane” is formed by taking a linear combination of the features. Hyperplane represents a decision boundary chosen by our classifier to separate the data points in different class labels. let’s start with LPM:</p>
<p><span class="math display" id="eq:8-1">\[\begin{equation}
\operatorname{Pr}\left(Y=1 | X_{1}=x_{1}, X_{2}=x_{2}\right)=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}
  \tag{21.1}
\end{equation}\]</span></p>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="classification-2.html#cb551-1" tabindex="-1"></a><span class="co"># LPM requires numerical 1 and 0</span></span>
<span id="cb551-2"><a href="classification-2.html#cb551-2" tabindex="-1"></a>y10 <span class="ot">=</span> <span class="fu">ifelse</span>(mnist_27<span class="sc">$</span>train<span class="sc">$</span>y <span class="sc">==</span> <span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb551-3"><a href="classification-2.html#cb551-3" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(mnist_27<span class="sc">$</span>train, y10)</span>
<span id="cb551-4"><a href="classification-2.html#cb551-4" tabindex="-1"></a><span class="fu">plot</span>(train<span class="sc">$</span>x_1, train<span class="sc">$</span>x_2, <span class="at">col =</span> train<span class="sc">$</span>y10 <span class="sc">+</span> <span class="dv">1</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/knn5-1.png" width="672" /></p>
<p>Here, the black dots are 2 and red dots are 7. Note that if we use 0.5 as a decision rule such that it separates pairs (<span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>) for which <span class="math inline">\(\operatorname{Pr}\left(Y=1 | X_{1}=x_{1}, X_{2}=x_{2}\right) &lt; 0.5\)</span> then we can have a hyperplane as</p>
<p><span class="math display">\[
\hat{\beta}_{0}+\hat{\beta}_{1} x_{1}+\hat{\beta}_{2} x_{2}=0.5 \Longrightarrow x_{2}=\left(0.5-\hat{\beta}_{0}\right) / \hat{\beta}_{2}-\hat{\beta}_{1} / \hat{\beta}_{2} x_{1}.
\]</span></p>
<p>If we incorporate this into our plot for the train data:</p>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="classification-2.html#cb552-1" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y10 <span class="sc">~</span> x_1 <span class="sc">+</span> x_2, train)</span>
<span id="cb552-2"><a href="classification-2.html#cb552-2" tabindex="-1"></a></span>
<span id="cb552-3"><a href="classification-2.html#cb552-3" tabindex="-1"></a>tr <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb552-4"><a href="classification-2.html#cb552-4" tabindex="-1"></a>a <span class="ot">&lt;-</span> tr <span class="sc">-</span> model<span class="sc">$</span>coefficients[<span class="dv">1</span>]</span>
<span id="cb552-5"><a href="classification-2.html#cb552-5" tabindex="-1"></a>a <span class="ot">&lt;-</span> a <span class="sc">/</span> model<span class="sc">$</span>coefficients[<span class="dv">3</span>]</span>
<span id="cb552-6"><a href="classification-2.html#cb552-6" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="sc">-</span>model<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">/</span> model<span class="sc">$</span>coefficients[<span class="dv">3</span>]</span>
<span id="cb552-7"><a href="classification-2.html#cb552-7" tabindex="-1"></a><span class="fu">plot</span>(train<span class="sc">$</span>x_1, train<span class="sc">$</span>x_2, <span class="at">col =</span> train<span class="sc">$</span>y10 <span class="sc">+</span> <span class="dv">1</span>, <span class="at">cex =</span> <span class="fl">0.72</span>)</span>
<span id="cb552-8"><a href="classification-2.html#cb552-8" tabindex="-1"></a><span class="fu">abline</span>(a, b, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="fl">2.8</span>)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/knn6-1.png" width="672" /></p>
<p>Play with the (discriminating) threshold and see how the hyperplane moves. When we change it to different numbers between 0 and 1, the number of correct and wrong predictions, a separation of red and black dots located in different sides, changes as well. Moreover <strong>the decision boundary is linear</strong>. That’s why LPM is called a linear classifier.</p>
<p>Would including interactions and polynomials (nonlinear parts) would place the line such a way that separation of these dots (2s and 7s) would be better?</p>
<p>Let’s see if adding a polynomial to our LPM improves this.</p>
<div class="sourceCode" id="cb553"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb553-1"><a href="classification-2.html#cb553-1" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y10 <span class="sc">~</span> x_1 <span class="sc">+</span> <span class="fu">I</span>(x_1 <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> x_2, train)</span>
<span id="cb553-2"><a href="classification-2.html#cb553-2" tabindex="-1"></a><span class="fu">summary</span>(model2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y10 ~ x_1 + I(x_1^2) + x_2, data = train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.14744 -0.28816  0.03999  0.28431  1.06759 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.09328    0.06571   1.419   0.1562    
## x_1          4.81884    0.55310   8.712  &lt; 2e-16 ***
## I(x_1^2)    -2.75520    1.40760  -1.957   0.0507 .  
## x_2         -1.18864    0.17252  -6.890 1.14e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3891 on 796 degrees of freedom
## Multiple R-squared:  0.3956, Adjusted R-squared:  0.3933 
## F-statistic: 173.7 on 3 and 796 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb555"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb555-1"><a href="classification-2.html#cb555-1" tabindex="-1"></a>tr <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb555-2"><a href="classification-2.html#cb555-2" tabindex="-1"></a>s <span class="ot">&lt;-</span> model2<span class="sc">$</span>coefficients</span>
<span id="cb555-3"><a href="classification-2.html#cb555-3" tabindex="-1"></a>a <span class="ot">=</span> tr <span class="sc">/</span> s[<span class="dv">3</span>]</span>
<span id="cb555-4"><a href="classification-2.html#cb555-4" tabindex="-1"></a>b <span class="ot">=</span> s[<span class="dv">1</span>] <span class="sc">/</span> s[<span class="dv">3</span>]</span>
<span id="cb555-5"><a href="classification-2.html#cb555-5" tabindex="-1"></a>d <span class="ot">=</span> s[<span class="dv">2</span>] <span class="sc">/</span> s[<span class="dv">3</span>]</span>
<span id="cb555-6"><a href="classification-2.html#cb555-6" tabindex="-1"></a>e <span class="ot">=</span> s[<span class="dv">4</span>] <span class="sc">/</span> s[<span class="dv">3</span>]</span>
<span id="cb555-7"><a href="classification-2.html#cb555-7" tabindex="-1"></a>x22 <span class="ot">=</span> a <span class="sc">-</span> b <span class="sc">-</span> d <span class="sc">*</span> train<span class="sc">$</span>x_1 <span class="sc">-</span> e <span class="sc">*</span> (train<span class="sc">$</span>x_1 <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb555-8"><a href="classification-2.html#cb555-8" tabindex="-1"></a><span class="fu">plot</span>(train<span class="sc">$</span>x_1, train<span class="sc">$</span>x_2, <span class="at">col =</span> train<span class="sc">$</span>y10 <span class="sc">+</span> <span class="dv">1</span>, <span class="at">cex =</span> <span class="fl">0.72</span>)</span>
<span id="cb555-9"><a href="classification-2.html#cb555-9" tabindex="-1"></a><span class="fu">lines</span>(train<span class="sc">$</span>x_1[<span class="fu">order</span>(x22)], x22[<span class="fu">order</span>(x22)], <span class="at">lwd =</span> <span class="fl">2.8</span>)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/knn7-1.png" width="672" /></p>
<p>The coefficient of the polynomial is barely significant and very negligible in magnitude. And in fact the classification seems worse than the previous one.</p>
<p>Would a logistic regression give us a better line? We don’t need to estimate it, but we can obtain the decision boundary for the logistic regression. Remember,</p>
<p><span class="math display">\[
P(Y=1 | x)=\frac{\exp \left(w_{0}+\sum_{i} w_{i} x_{i}\right)}{1+\exp \left(w_{0}+\sum_{i} w_{i} x_{i}\right)}
\]</span></p>
<p>And,</p>
<p><span class="math display">\[
P(Y=0 | x)=1-P(Y=1 | x)= \frac{1}{1+\exp \left(w_{0}+\sum_{i} w_{i} x_{i}\right)}
\]</span></p>
<p>if we take the ratio of success over failure, <span class="math inline">\(P/1-P\)</span>,</p>
<p><span class="math display">\[
\frac{P}{1-P}=\exp \left(w_{0}+\sum_{i} w_{i} x_{i}\right)
\]</span></p>
<p>If this ratio is higher than 1, we think that the probability for <span class="math inline">\(Y=1\)</span> is higher than the probability for <span class="math inline">\(Y=0\)</span>. And this only happens when <span class="math inline">\(P&gt;0.5\)</span>. Hence, the condition to classify the observation as <span class="math inline">\(Y=1\)</span> is:</p>
<p><span class="math display">\[
\frac{P}{1-P}=\exp \left(w_{0}+\sum_{i} w_{i} x_{i}\right) &gt; 1
\]</span></p>
<p>If we take the log of both sides,</p>
<p><span class="math display">\[
w_{0}+\sum_{i} w_{i} X_{i}&gt;0
\]</span></p>
<p>From here, the hyperplane function in our case becomes,</p>
<p><span class="math display">\[
\hat{\beta}_{0}+\hat{\beta}_{1} x_{1}+\hat{\beta}_{2} x_{2}=0 \Longrightarrow x_{2}=-\hat{\beta}_{0} / \hat{\beta}_{2}-\hat{\beta}_{1} / \hat{\beta}_{2} x_{1}.
\]</span></p>
<p>We see that the decision boundary is again linear. Therefore, LPM and logistic regressions are called as <strong>linear classifiers</strong>, which are good <strong>only if the problem on hand is linearly separable</strong>.</p>
<p>Would it be possible to have a nonlinear boundary condition so that we can get a better classification for our predicted probabilities?</p>
</div>
<div id="k-nearest-neighbors" class="section level2 hasAnchor" number="21.4">
<h2><span class="header-section-number">21.4</span> k-Nearest Neighbors<a href="classification-2.html#k-nearest-neighbors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>k-nearest neighbors (kNN) is a nonparametric method used for classification (or regression), which estimate <span class="math inline">\(p(x_1, x_2)\)</span> by using a method similar to <em>bin smoothing</em>. In <em>kNN classification</em>, the output is a class membership. An object is assigned to the class most common among its k-nearest neighbors. In <em>kNN regressions</em>, the output is the average of the values of k-nearest neighbors, which we’ve seen in bin smoothing applications.</p>
<p><img src="png/kNN1.png" /><!-- --></p>
<p>Suppose we have to classify (identify) the red dot as 7 or 2. Since it’s a nonparametric approach, we have to define bins. If the number of observations in bins set to 1 (<span class="math inline">\(k = 1\)</span>), then we need to find one observation that is nearest to the red dot. How? Since we know to coordinates (<span class="math inline">\(x_1, x_2\)</span>) of that red dot, we can find its nearest neighbors by some distance functions among all points (observations) in the data. A popular choice is the Euclidean distance given by</p>
<p><span class="math display">\[
d\left(x, x^{\prime}\right)=\sqrt{\left(x_{1}-x_{1}^{\prime}\right)^{2}+\ldots+\left(x_{n}-x_{n}^{\prime}\right)^{2}}.
\]</span></p>
<p>Other measures are also available and can be more suitable in different settings including the Manhattan, Chebyshev and Hamming distance. The last one is used if the features are binary. In our case the features are continuous so we can use the Euclidean distance. We now have to calculate this measure for every point (observation) in our data. In our graph we have 10 points, and we have to have 10 distance measures from the red dot. Usually, in practice, we calculate all distance measures between each point, which becomes a symmetric matrix with <span class="math inline">\(n\)</span>x<span class="math inline">\(n\)</span> dimensions.</p>
<p>For example, for two dimensional space, we can calculate the distances as follows</p>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="classification-2.html#cb556-1" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="fl">2.1</span>, <span class="dv">4</span>, <span class="fl">4.3</span>)</span>
<span id="cb556-2"><a href="classification-2.html#cb556-2" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="fl">3.3</span>, <span class="dv">5</span>, <span class="fl">5.1</span>)</span>
<span id="cb556-3"><a href="classification-2.html#cb556-3" tabindex="-1"></a></span>
<span id="cb556-4"><a href="classification-2.html#cb556-4" tabindex="-1"></a>EDistance <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y){</span>
<span id="cb556-5"><a href="classification-2.html#cb556-5" tabindex="-1"></a>  dx <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="fu">length</span>(x), <span class="fu">length</span>(x))</span>
<span id="cb556-6"><a href="classification-2.html#cb556-6" tabindex="-1"></a>  dy <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="fu">length</span>(x), <span class="fu">length</span>(x))</span>
<span id="cb556-7"><a href="classification-2.html#cb556-7" tabindex="-1"></a>  </span>
<span id="cb556-8"><a href="classification-2.html#cb556-8" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(x)) {</span>
<span id="cb556-9"><a href="classification-2.html#cb556-9" tabindex="-1"></a>    dx[i,] <span class="ot">&lt;-</span> (x[i] <span class="sc">-</span> x)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb556-10"><a href="classification-2.html#cb556-10" tabindex="-1"></a>    dy[i,] <span class="ot">&lt;-</span> (y[i] <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb556-11"><a href="classification-2.html#cb556-11" tabindex="-1"></a>    dd <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(dx<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> dy<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb556-12"><a href="classification-2.html#cb556-12" tabindex="-1"></a>  }</span>
<span id="cb556-13"><a href="classification-2.html#cb556-13" tabindex="-1"></a>  <span class="fu">return</span>(dd)</span>
<span id="cb556-14"><a href="classification-2.html#cb556-14" tabindex="-1"></a>}</span>
<span id="cb556-15"><a href="classification-2.html#cb556-15" tabindex="-1"></a>  </span>
<span id="cb556-16"><a href="classification-2.html#cb556-16" tabindex="-1"></a><span class="fu">EDistance</span>(x1, x2)</span></code></pre></div>
<pre><code>##            [,1]       [,2]       [,3]       [,4]
## [1,] 0.00000000 0.09055385 5.65685425 6.88710389
## [2,] 0.09055385 0.00000000 4.62430535 5.82436263
## [3,] 5.65685425 4.62430535 0.00000000 0.09055385
## [4,] 6.88710389 5.82436263 0.09055385 0.00000000</code></pre>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb558-1"><a href="classification-2.html#cb558-1" tabindex="-1"></a><span class="fu">plot</span>(x1, x2, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb558-2"><a href="classification-2.html#cb558-2" tabindex="-1"></a><span class="co">#segments(x1[1], x2[1], x1[2:4], x2[2:4], col = &quot;blue&quot; )</span></span>
<span id="cb558-3"><a href="classification-2.html#cb558-3" tabindex="-1"></a><span class="co">#segments(x1[2], x2[2], x1[c(1, 3:4)], x2[c(1, 3:4)], col = &quot;green&quot; )</span></span>
<span id="cb558-4"><a href="classification-2.html#cb558-4" tabindex="-1"></a><span class="co">#segments(x1[3], x2[3], x1[c(1:2, 4)], x2[c(1:2, 4)], col = &quot;orange&quot; )</span></span>
<span id="cb558-5"><a href="classification-2.html#cb558-5" tabindex="-1"></a><span class="fu">segments</span>(x1[<span class="dv">4</span>], x2[<span class="dv">4</span>], x1[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>], x2[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>], <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span> )</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/knn9-1.png" width="672" /></p>
<p>The matrix shows all distances for four points and, as we expect, it is symmetric. The green lines show the distance from the last point (<span class="math inline">\(x = 4.3,~ y = 5.1\)</span>) to all other points. Using this matrix, we can easily find the k-nearest neighbors for any point.</p>
<p>When <span class="math inline">\(k=1\)</span>, the observation that has the shortest distance is going to be the one to predict what the red dot could be. This is shown in the figure below:</p>
<p><img src="png/kNN2.png" /><!-- --></p>
<p>If we define the bin as <span class="math inline">\(k=3\)</span>, we look for the 3 nearest points to the red dot and then take an average of the 1s (7s) and 0s (2s) associated with these points. Here is an example:</p>
<p><img src="png/kNN3.png" /><!-- --></p>
<p>Using <span class="math inline">\(k\)</span> neighbors to estimate the probability of <span class="math inline">\(Y=1\)</span> (the dot is 7), that is</p>
<p><span class="math display" id="eq:8-2">\[\begin{equation}
\hat{P}_{k}(Y=1 | X=x)=\frac{1}{k} \sum_{i \in \mathcal{N}_{k}(x, D)} I\left(y_{i}=1\right)
  \tag{10.11}
\end{equation}\]</span></p>
<p>With this predicted probability, we classify the red dot to the class with the most observations in the <span class="math inline">\(k\)</span> nearest neighbors (we assign a class at random to one of the classes tied for highest). Here is the rule in our case:</p>
<p><span class="math display">\[
\hat{C}_{k}(x)=\left\{\begin{array}{ll}{1} &amp; {\hat{p}_{k 0}(x)&gt;0.5} \\ {0} &amp; {\hat{p}_{k 1}(x)&lt;0.5}\end{array}\right.
\]</span></p>
<p>Suppose our red dot has <span class="math inline">\(x=(x_1,x_2)=(4,3)\)</span></p>
<p><span class="math display">\[
\begin{aligned} \hat{P}\left(Y=\text { Seven } | X_{1}=4, X_{2}=3\right)=\frac{2}{3} \\ \hat{P}\left(Y=\text { Two} | X_{1}=4, X_{2}=3\right)=\frac{1}{3} \end{aligned}
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\hat{C}_{k=4}\left(x_{1}=4, x_{2}=3\right)=\text { Seven }
\]</span></p>
<p>As it’s clear from this application, <span class="math inline">\(k\)</span> is our hyperparameter and we need to tune it as to have the best predictive kNN algorithm. The following section will show its application. But before that, we need to understand how decision boundaries can be found in kNN</p>
<div class="sourceCode" id="cb559"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb559-1"><a href="classification-2.html#cb559-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb559-2"><a href="classification-2.html#cb559-2" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">50</span>)</span>
<span id="cb559-3"><a href="classification-2.html#cb559-3" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">50</span>)</span>
<span id="cb559-4"><a href="classification-2.html#cb559-4" tabindex="-1"></a></span>
<span id="cb559-5"><a href="classification-2.html#cb559-5" tabindex="-1"></a><span class="fu">library</span>(deldir)</span>
<span id="cb559-6"><a href="classification-2.html#cb559-6" tabindex="-1"></a>tesselation <span class="ot">&lt;-</span> <span class="fu">deldir</span>(x1, x2)</span>
<span id="cb559-7"><a href="classification-2.html#cb559-7" tabindex="-1"></a>tiles <span class="ot">&lt;-</span> <span class="fu">tile.list</span>(tesselation)</span>
<span id="cb559-8"><a href="classification-2.html#cb559-8" tabindex="-1"></a></span>
<span id="cb559-9"><a href="classification-2.html#cb559-9" tabindex="-1"></a><span class="fu">plot</span>(tiles, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">close =</span> <span class="cn">TRUE</span>,</span>
<span id="cb559-10"><a href="classification-2.html#cb559-10" tabindex="-1"></a>     <span class="at">fillcol =</span>  <span class="fu">hcl.colors</span>(<span class="dv">4</span>, <span class="st">&quot;Sunset&quot;</span>),</span>
<span id="cb559-11"><a href="classification-2.html#cb559-11" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.2</span><span class="sc">:</span><span class="fl">1.1</span>))</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/knn12-1.png" width="672" /></p>
<p>These are called Voronoi cells associated with 1-NN, which is the set of polygons whose edges are the perpendicular bisectors of the lines joining the neighboring points. Thus, the decision boundary is the result of fusing adjacent Voronoi cells that are associated with same class. In the example above, it’s the boundary of unions of each colors. Finding the boundaries that trace each adjacent Vorono regions can be done with additional several steps.</p>
<p>To see all in an application, we will use <code>knn3()</code> from the <em>Caret</em> package. We will not train a model but only see how the separation between classes will be nonlinear and different for different <span class="math inline">\(k\)</span>.</p>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="classification-2.html#cb560-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb560-2"><a href="classification-2.html#cb560-2" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb560-3"><a href="classification-2.html#cb560-3" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb560-4"><a href="classification-2.html#cb560-4" tabindex="-1"></a></span>
<span id="cb560-5"><a href="classification-2.html#cb560-5" tabindex="-1"></a><span class="co">#With k = 50</span></span>
<span id="cb560-6"><a href="classification-2.html#cb560-6" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">knn3</span>(y <span class="sc">~</span> ., <span class="at">data =</span> mnist_27<span class="sc">$</span>train, <span class="at">k =</span> <span class="dv">2</span>)</span>
<span id="cb560-7"><a href="classification-2.html#cb560-7" tabindex="-1"></a></span>
<span id="cb560-8"><a href="classification-2.html#cb560-8" tabindex="-1"></a>x_1 <span class="ot">&lt;-</span> mnist_27<span class="sc">$</span>true_p<span class="sc">$</span>x_1</span>
<span id="cb560-9"><a href="classification-2.html#cb560-9" tabindex="-1"></a>x_2 <span class="ot">&lt;-</span> mnist_27<span class="sc">$</span>true_p<span class="sc">$</span>x_2</span>
<span id="cb560-10"><a href="classification-2.html#cb560-10" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x_1, x_2) <span class="co">#This is whole data 22500 obs.</span></span>
<span id="cb560-11"><a href="classification-2.html#cb560-11" tabindex="-1"></a></span>
<span id="cb560-12"><a href="classification-2.html#cb560-12" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, df, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>) <span class="co"># Predicting probabilities in each bin</span></span>
<span id="cb560-13"><a href="classification-2.html#cb560-13" tabindex="-1"></a>p_7 <span class="ot">&lt;-</span> p_hat[,<span class="dv">2</span>] <span class="co">#Selecting the p_hat for 7</span></span>
<span id="cb560-14"><a href="classification-2.html#cb560-14" tabindex="-1"></a></span>
<span id="cb560-15"><a href="classification-2.html#cb560-15" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x_1, x_2, p_7)</span>
<span id="cb560-16"><a href="classification-2.html#cb560-16" tabindex="-1"></a></span>
<span id="cb560-17"><a href="classification-2.html#cb560-17" tabindex="-1"></a>my_colors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>)</span>
<span id="cb560-18"><a href="classification-2.html#cb560-18" tabindex="-1"></a></span>
<span id="cb560-19"><a href="classification-2.html#cb560-19" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb560-20"><a href="classification-2.html#cb560-20" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> mnist_27<span class="sc">$</span>train, <span class="fu">aes</span>(<span class="at">x =</span> x_1, <span class="at">y =</span> x_2, <span class="at">colour =</span> <span class="fu">factor</span>(y)),</span>
<span id="cb560-21"><a href="classification-2.html#cb560-21" tabindex="-1"></a>             <span class="at">shape =</span> <span class="dv">21</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">stroke =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb560-22"><a href="classification-2.html#cb560-22" tabindex="-1"></a>  <span class="fu">stat_contour</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> x_1, <span class="at">y =</span> x_2, <span class="at">z =</span> p_7), <span class="at">breaks=</span><span class="fu">c</span>(<span class="fl">0.5</span>), <span class="at">color=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb560-23"><a href="classification-2.html#cb560-23" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> my_colors)</span>
<span id="cb560-24"><a href="classification-2.html#cb560-24" tabindex="-1"></a><span class="fu">plot</span>(p1)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/knn13-1.png" width="672" /></p>
<div class="sourceCode" id="cb561"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb561-1"><a href="classification-2.html#cb561-1" tabindex="-1"></a><span class="co">#With k = 400</span></span>
<span id="cb561-2"><a href="classification-2.html#cb561-2" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">knn3</span>(y <span class="sc">~</span> ., <span class="at">data =</span> mnist_27<span class="sc">$</span>train, <span class="at">k =</span> <span class="dv">400</span>)</span>
<span id="cb561-3"><a href="classification-2.html#cb561-3" tabindex="-1"></a></span>
<span id="cb561-4"><a href="classification-2.html#cb561-4" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(model2, df, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>) <span class="co"># Prediciting probabilities in each bin</span></span>
<span id="cb561-5"><a href="classification-2.html#cb561-5" tabindex="-1"></a>p_7 <span class="ot">&lt;-</span> p_hat[,<span class="dv">2</span>] <span class="co">#Selecting the p_hat for 7</span></span>
<span id="cb561-6"><a href="classification-2.html#cb561-6" tabindex="-1"></a></span>
<span id="cb561-7"><a href="classification-2.html#cb561-7" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x_1, x_2, p_7)</span>
<span id="cb561-8"><a href="classification-2.html#cb561-8" tabindex="-1"></a></span>
<span id="cb561-9"><a href="classification-2.html#cb561-9" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb561-10"><a href="classification-2.html#cb561-10" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> mnist_27<span class="sc">$</span>train, <span class="fu">aes</span>(<span class="at">x =</span> x_1, <span class="at">y =</span> x_2, <span class="at">colour =</span> <span class="fu">factor</span>(y)),</span>
<span id="cb561-11"><a href="classification-2.html#cb561-11" tabindex="-1"></a>             <span class="at">shape =</span> <span class="dv">21</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">stroke =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb561-12"><a href="classification-2.html#cb561-12" tabindex="-1"></a>  <span class="fu">stat_contour</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> x_1, <span class="at">y =</span> x_2, <span class="at">z =</span> p_7), <span class="at">breaks=</span><span class="fu">c</span>(<span class="fl">0.5</span>), <span class="at">color=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb561-13"><a href="classification-2.html#cb561-13" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> my_colors)</span>
<span id="cb561-14"><a href="classification-2.html#cb561-14" tabindex="-1"></a><span class="fu">plot</span>(p1)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/knn13-2.png" width="672" /></p>
<p>One with <span class="math inline">\(k=2\)</span> shows signs for overfitting, the other one with <span class="math inline">\(k=400\)</span> indicates oversmoothing or underfitting. We need to tune <span class="math inline">\(k\)</span> such a way that it will be best in terms of prediction accuracy.</p>
</div>
<div id="knn-with-caret" class="section level2 hasAnchor" number="21.5">
<h2><span class="header-section-number">21.5</span> kNN with caret<a href="classification-2.html#knn-with-caret" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are many different learning algorithms developed by different authors and often with different parametric structures. The <code>caret</code>, <strong>Classification And Regression Training</strong> package tries to consolidate these differences and provide consistency. It currently includes 237 (and growing) different methods which are summarized in the caret <a href="https://topepo.github.io/caret/available-models.html">package manual</a> <span class="citation">(<a href="#ref-Kuhn_2019"><strong>Kuhn_2019?</strong></a>)</span>. Here, we will use <code>mnset_27</code> to illustrate how we can use <code>caret</code> for kNN. For now, we will use the caret’s <code>train()</code> function to find the optimal <code>k</code> in kNN, which is basically an automated version of cross-validation that we will see in the next chapter.</p>
<div id="mnist_27" class="section level3 hasAnchor" number="21.5.1">
<h3><span class="header-section-number">21.5.1</span> <code>mnist_27</code><a href="classification-2.html#mnist_27" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since, our dataset, <code>mnist_27</code>, is already split into train and test sets, we do not need to do it again. Here is the starting point:</p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="classification-2.html#cb562-1" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb562-2"><a href="classification-2.html#cb562-2" tabindex="-1"></a></span>
<span id="cb562-3"><a href="classification-2.html#cb562-3" tabindex="-1"></a><span class="co">#Training/Model building</span></span>
<span id="cb562-4"><a href="classification-2.html#cb562-4" tabindex="-1"></a>model_knn <span class="ot">&lt;-</span> <span class="fu">train</span>(y <span class="sc">~</span> ., <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="at">data =</span> mnist_27<span class="sc">$</span>train)</span>
<span id="cb562-5"><a href="classification-2.html#cb562-5" tabindex="-1"></a>model_knn</span></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 800 samples
##   2 predictor
##   2 classes: &#39;2&#39;, &#39;7&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 800, 800, 800, 800, 800, 800, ... 
## Resampling results across tuning parameters:
## 
##   k  Accuracy   Kappa    
##   5  0.8075980  0.6135168
##   7  0.8157975  0.6300494
##   9  0.8205824  0.6396302
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 9.</code></pre>
<p>By default, the cross validation is performed by taking 25 bootstrap samples comprised of 25% of the observations. Moreover, the default is to try <span class="math inline">\(k=5,7,9\)</span>. We can to expand it:</p>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb564-1"><a href="classification-2.html#cb564-1" tabindex="-1"></a><span class="co">#Training/Model building with our own grid</span></span>
<span id="cb564-2"><a href="classification-2.html#cb564-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2008</span>)</span>
<span id="cb564-3"><a href="classification-2.html#cb564-3" tabindex="-1"></a>model_knn1 <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb564-4"><a href="classification-2.html#cb564-4" tabindex="-1"></a>  y <span class="sc">~</span> .,</span>
<span id="cb564-5"><a href="classification-2.html#cb564-5" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb564-6"><a href="classification-2.html#cb564-6" tabindex="-1"></a>  <span class="at">data =</span> mnist_27<span class="sc">$</span>train,</span>
<span id="cb564-7"><a href="classification-2.html#cb564-7" tabindex="-1"></a>  <span class="at">tuneGrid =</span> <span class="fu">data.frame</span>(<span class="at">k =</span> <span class="fu">seq</span>(<span class="dv">9</span>, <span class="dv">71</span>, <span class="dv">2</span>))</span>
<span id="cb564-8"><a href="classification-2.html#cb564-8" tabindex="-1"></a>)</span>
<span id="cb564-9"><a href="classification-2.html#cb564-9" tabindex="-1"></a><span class="fu">ggplot</span>(model_knn1, <span class="at">highlight =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/knn15-1.png" width="672" /></p>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb565-1"><a href="classification-2.html#cb565-1" tabindex="-1"></a>model_knn1<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##     k
## 10 27</code></pre>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb567-1"><a href="classification-2.html#cb567-1" tabindex="-1"></a>model_knn1<span class="sc">$</span>finalModel</span></code></pre></div>
<pre><code>## 27-nearest neighbor model
## Training set outcome distribution:
## 
##   2   7 
## 379 421</code></pre>
<p>We can change its tuning to cross-validation:</p>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb569-1"><a href="classification-2.html#cb569-1" tabindex="-1"></a><span class="co">#Training/Model building with 10-k cross validation</span></span>
<span id="cb569-2"><a href="classification-2.html#cb569-2" tabindex="-1"></a>cv <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>, <span class="at">p =</span> <span class="fl">0.9</span>)</span>
<span id="cb569-3"><a href="classification-2.html#cb569-3" tabindex="-1"></a>model_knn2 <span class="ot">&lt;-</span> <span class="fu">train</span>(y <span class="sc">~</span> ., <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="at">data =</span> mnist_27<span class="sc">$</span>train,</span>
<span id="cb569-4"><a href="classification-2.html#cb569-4" tabindex="-1"></a>                   <span class="at">tuneGrid =</span> <span class="fu">data.frame</span>(<span class="at">k=</span><span class="fu">seq</span>(<span class="dv">9</span>,<span class="dv">71</span>,<span class="dv">2</span>)),</span>
<span id="cb569-5"><a href="classification-2.html#cb569-5" tabindex="-1"></a>                   <span class="at">trControl =</span> cv)</span>
<span id="cb569-6"><a href="classification-2.html#cb569-6" tabindex="-1"></a><span class="fu">ggplot</span>(model_knn2, <span class="at">highlight =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/knn16-1.png" width="672" /></p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="classification-2.html#cb570-1" tabindex="-1"></a>model_knn2<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##     k
## 11 29</code></pre>
<p>It seems like <span class="math inline">\(k=27\)</span> (<span class="math inline">\(k=29\)</span> with CV) gives us the best performing prediction model. We can see their prediction performance on the test set:</p>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb572-1"><a href="classification-2.html#cb572-1" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="fu">predict</span>(model_knn1, mnist_27<span class="sc">$</span>test, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb572-2"><a href="classification-2.html#cb572-2" tabindex="-1"></a>                mnist_27<span class="sc">$</span>test<span class="sc">$</span>y)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  2  7
##          2 92 19
##          7 14 75
##                                           
##                Accuracy : 0.835           
##                  95% CI : (0.7762, 0.8836)
##     No Information Rate : 0.53            
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.6678          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.4862          
##                                           
##             Sensitivity : 0.8679          
##             Specificity : 0.7979          
##          Pos Pred Value : 0.8288          
##          Neg Pred Value : 0.8427          
##              Prevalence : 0.5300          
##          Detection Rate : 0.4600          
##    Detection Prevalence : 0.5550          
##       Balanced Accuracy : 0.8329          
##                                           
##        &#39;Positive&#39; Class : 2               
## </code></pre>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="classification-2.html#cb574-1" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="fu">predict</span>(model_knn2, mnist_27<span class="sc">$</span>test, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb574-2"><a href="classification-2.html#cb574-2" tabindex="-1"></a>                mnist_27<span class="sc">$</span>test<span class="sc">$</span>y)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  2  7
##          2 91 18
##          7 15 76
##                                           
##                Accuracy : 0.835           
##                  95% CI : (0.7762, 0.8836)
##     No Information Rate : 0.53            
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.6682          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.7277          
##                                           
##             Sensitivity : 0.8585          
##             Specificity : 0.8085          
##          Pos Pred Value : 0.8349          
##          Neg Pred Value : 0.8352          
##              Prevalence : 0.5300          
##          Detection Rate : 0.4550          
##    Detection Prevalence : 0.5450          
##       Balanced Accuracy : 0.8335          
##                                           
##        &#39;Positive&#39; Class : 2               
## </code></pre>
<p>What are these measures? What is a “Confusion Matrix”? We will see them in the next section. But for now, let’s use another example.</p>
</div>
<div id="adult-dataset-1" class="section level3 hasAnchor" number="21.5.2">
<h3><span class="header-section-number">21.5.2</span> Adult dataset<a href="classification-2.html#adult-dataset-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This dataset provides information on income earning and attributes that may effect it. Information on the dataset is given at its <a href="https://archive.ics.uci.edu/ml/datasets/Adult">website</a> <span class="citation">(<a href="#ref-Kohavi_1996"><strong>Kohavi_1996?</strong></a>)</span>:</p>
<blockquote>
<p>Extraction from 1994 US. Census database. A set of reasonably clean records was extracted using the following conditions: ((<code>AAGE</code>&gt;16) &amp;&amp; (<code>AGI</code>&gt;100) &amp;&amp; (<code>AFNLWGT</code>&gt;1)&amp;&amp; (<code>HRSWK</code>&gt;0)).</p>
</blockquote>
<p>The prediction task is to determine whether a person makes over 50K a year.</p>
<div class="sourceCode" id="cb576"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb576-1"><a href="classification-2.html#cb576-1" tabindex="-1"></a><span class="co"># Download adult income data</span></span>
<span id="cb576-2"><a href="classification-2.html#cb576-2" tabindex="-1"></a><span class="co"># SET YOUR WORKING DIRECTORY FIRST</span></span>
<span id="cb576-3"><a href="classification-2.html#cb576-3" tabindex="-1"></a></span>
<span id="cb576-4"><a href="classification-2.html#cb576-4" tabindex="-1"></a><span class="co"># url.train &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&quot;</span></span>
<span id="cb576-5"><a href="classification-2.html#cb576-5" tabindex="-1"></a><span class="co"># url.test &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test&quot;</span></span>
<span id="cb576-6"><a href="classification-2.html#cb576-6" tabindex="-1"></a><span class="co"># url.names &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&quot;</span></span>
<span id="cb576-7"><a href="classification-2.html#cb576-7" tabindex="-1"></a><span class="co"># download.file(url.train, destfile = &quot;adult_train.csv&quot;)</span></span>
<span id="cb576-8"><a href="classification-2.html#cb576-8" tabindex="-1"></a><span class="co"># download.file(url.test, destfile = &quot;adult_test.csv&quot;)</span></span>
<span id="cb576-9"><a href="classification-2.html#cb576-9" tabindex="-1"></a><span class="co"># download.file(url.names, destfile = &quot;adult_names.txt&quot;)</span></span>
<span id="cb576-10"><a href="classification-2.html#cb576-10" tabindex="-1"></a></span>
<span id="cb576-11"><a href="classification-2.html#cb576-11" tabindex="-1"></a><span class="co"># Read the training set into memory</span></span>
<span id="cb576-12"><a href="classification-2.html#cb576-12" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;adult_train.csv&quot;</span>, <span class="at">header =</span> <span class="cn">FALSE</span>)</span>
<span id="cb576-13"><a href="classification-2.html#cb576-13" tabindex="-1"></a><span class="fu">str</span>(train)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32561 obs. of  15 variables:
##  $ V1 : int  39 50 38 53 28 37 49 52 31 42 ...
##  $ V2 : chr  &quot; State-gov&quot; &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; ...
##  $ V3 : int  77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ...
##  $ V4 : chr  &quot; Bachelors&quot; &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; ...
##  $ V5 : int  13 13 9 7 13 14 5 9 14 13 ...
##  $ V6 : chr  &quot; Never-married&quot; &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; ...
##  $ V7 : chr  &quot; Adm-clerical&quot; &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; ...
##  $ V8 : chr  &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; ...
##  $ V9 : chr  &quot; White&quot; &quot; White&quot; &quot; White&quot; &quot; Black&quot; ...
##  $ V10: chr  &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; ...
##  $ V11: int  2174 0 0 0 0 0 0 0 14084 5178 ...
##  $ V12: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ V13: int  40 13 40 40 40 40 16 45 50 40 ...
##  $ V14: chr  &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; ...
##  $ V15: chr  &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ...</code></pre>
<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb578-1"><a href="classification-2.html#cb578-1" tabindex="-1"></a><span class="co"># Read the test set into memory</span></span>
<span id="cb578-2"><a href="classification-2.html#cb578-2" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;adult_test.csv&quot;</span>, <span class="at">header =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>The data doesn’t have the variable names. That’s bad because we don’t know which one is which. Check the <strong>adult_names.txt</strong> file. The list of variables is given in that file. Thanks to <a href="https://rpubs.com/mbaumer/knn">Matthew Baumer</a> <span class="citation">(<a href="#ref-Baumer_2015"><strong>Baumer_2015?</strong></a>)</span>, we can write them manually:</p>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb579-1"><a href="classification-2.html#cb579-1" tabindex="-1"></a>varNames <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Age&quot;</span>, </span>
<span id="cb579-2"><a href="classification-2.html#cb579-2" tabindex="-1"></a>              <span class="st">&quot;WorkClass&quot;</span>,</span>
<span id="cb579-3"><a href="classification-2.html#cb579-3" tabindex="-1"></a>              <span class="st">&quot;fnlwgt&quot;</span>,</span>
<span id="cb579-4"><a href="classification-2.html#cb579-4" tabindex="-1"></a>              <span class="st">&quot;Education&quot;</span>,</span>
<span id="cb579-5"><a href="classification-2.html#cb579-5" tabindex="-1"></a>              <span class="st">&quot;EducationNum&quot;</span>,</span>
<span id="cb579-6"><a href="classification-2.html#cb579-6" tabindex="-1"></a>              <span class="st">&quot;MaritalStatus&quot;</span>,</span>
<span id="cb579-7"><a href="classification-2.html#cb579-7" tabindex="-1"></a>              <span class="st">&quot;Occupation&quot;</span>,</span>
<span id="cb579-8"><a href="classification-2.html#cb579-8" tabindex="-1"></a>              <span class="st">&quot;Relationship&quot;</span>,</span>
<span id="cb579-9"><a href="classification-2.html#cb579-9" tabindex="-1"></a>              <span class="st">&quot;Race&quot;</span>,</span>
<span id="cb579-10"><a href="classification-2.html#cb579-10" tabindex="-1"></a>              <span class="st">&quot;Sex&quot;</span>,</span>
<span id="cb579-11"><a href="classification-2.html#cb579-11" tabindex="-1"></a>              <span class="st">&quot;CapitalGain&quot;</span>,</span>
<span id="cb579-12"><a href="classification-2.html#cb579-12" tabindex="-1"></a>              <span class="st">&quot;CapitalLoss&quot;</span>,</span>
<span id="cb579-13"><a href="classification-2.html#cb579-13" tabindex="-1"></a>              <span class="st">&quot;HoursPerWeek&quot;</span>,</span>
<span id="cb579-14"><a href="classification-2.html#cb579-14" tabindex="-1"></a>              <span class="st">&quot;NativeCountry&quot;</span>,</span>
<span id="cb579-15"><a href="classification-2.html#cb579-15" tabindex="-1"></a>              <span class="st">&quot;IncomeLevel&quot;</span>)</span>
<span id="cb579-16"><a href="classification-2.html#cb579-16" tabindex="-1"></a><span class="fu">names</span>(train) <span class="ot">&lt;-</span> varNames</span>
<span id="cb579-17"><a href="classification-2.html#cb579-17" tabindex="-1"></a><span class="fu">names</span>(test) <span class="ot">&lt;-</span> varNames</span>
<span id="cb579-18"><a href="classification-2.html#cb579-18" tabindex="-1"></a><span class="fu">str</span>(train)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32561 obs. of  15 variables:
##  $ Age          : int  39 50 38 53 28 37 49 52 31 42 ...
##  $ WorkClass    : chr  &quot; State-gov&quot; &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; ...
##  $ fnlwgt       : int  77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ...
##  $ Education    : chr  &quot; Bachelors&quot; &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; ...
##  $ EducationNum : int  13 13 9 7 13 14 5 9 14 13 ...
##  $ MaritalStatus: chr  &quot; Never-married&quot; &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; ...
##  $ Occupation   : chr  &quot; Adm-clerical&quot; &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; ...
##  $ Relationship : chr  &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; ...
##  $ Race         : chr  &quot; White&quot; &quot; White&quot; &quot; White&quot; &quot; Black&quot; ...
##  $ Sex          : chr  &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; ...
##  $ CapitalGain  : int  2174 0 0 0 0 0 0 0 14084 5178 ...
##  $ CapitalLoss  : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ HoursPerWeek : int  40 13 40 40 40 40 16 45 50 40 ...
##  $ NativeCountry: chr  &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; ...
##  $ IncomeLevel  : chr  &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ...</code></pre>
<p>Since the dataset is large we are not going to use the test set but split the train set into our own test and train sets. Note that, however, if we had used the original test set, we would have had to make some adjustments/cleaning before using it. For example, if you look at <code>Age</code> variable, it seems as a factor variable. It is an integer in the training set. We have to change it first. Moreover, our <span class="math inline">\(Y\)</span> has two levels in the train set, it has 3 levels in the test set. We have to go over each variable and make sure that the test and train sets have the same features and class types. This task is left to you if you want to use the original train and test sets. A final tip: remove the first row in the original test set!</p>
<div class="sourceCode" id="cb581"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb581-1"><a href="classification-2.html#cb581-1" tabindex="-1"></a><span class="co">#Caret needs some preparations!</span></span>
<span id="cb581-2"><a href="classification-2.html#cb581-2" tabindex="-1"></a><span class="fu">table</span>(train<span class="sc">$</span>IncomeLevel)</span></code></pre></div>
<pre><code>## 
##  &lt;=50K   &gt;50K 
##  24720   7841</code></pre>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb583-1"><a href="classification-2.html#cb583-1" tabindex="-1"></a><span class="co"># This is b/c we will use the same data for LPM</span></span>
<span id="cb583-2"><a href="classification-2.html#cb583-2" tabindex="-1"></a>train<span class="sc">$</span>Y <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(train<span class="sc">$</span>IncomeLevel <span class="sc">==</span> <span class="st">&quot; &lt;=50K&quot;</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb583-3"><a href="classification-2.html#cb583-3" tabindex="-1"></a>train <span class="ot">&lt;-</span> train[,<span class="sc">-</span><span class="dv">15</span>]</span>
<span id="cb583-4"><a href="classification-2.html#cb583-4" tabindex="-1"></a></span>
<span id="cb583-5"><a href="classification-2.html#cb583-5" tabindex="-1"></a><span class="co"># kNN needs Y to be a factor variable</span></span>
<span id="cb583-6"><a href="classification-2.html#cb583-6" tabindex="-1"></a>train<span class="sc">$</span>Y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(train<span class="sc">$</span>Y)</span>
<span id="cb583-7"><a href="classification-2.html#cb583-7" tabindex="-1"></a><span class="fu">levels</span>(train<span class="sc">$</span>Y)[<span class="fu">levels</span>(train<span class="sc">$</span>Y) <span class="sc">==</span> <span class="st">&quot;0&quot;</span>] <span class="ot">&lt;-</span> <span class="st">&quot;Less&quot;</span></span>
<span id="cb583-8"><a href="classification-2.html#cb583-8" tabindex="-1"></a><span class="fu">levels</span>(train<span class="sc">$</span>Y)[<span class="fu">levels</span>(train<span class="sc">$</span>Y) <span class="sc">==</span> <span class="st">&quot;1&quot;</span>] <span class="ot">&lt;-</span> <span class="st">&quot;More&quot;</span></span>
<span id="cb583-9"><a href="classification-2.html#cb583-9" tabindex="-1"></a><span class="fu">levels</span>(train<span class="sc">$</span>Y)</span></code></pre></div>
<pre><code>## [1] &quot;Less&quot; &quot;More&quot;</code></pre>
<div class="sourceCode" id="cb585"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb585-1"><a href="classification-2.html#cb585-1" tabindex="-1"></a><span class="co">#kNN</span></span>
<span id="cb585-2"><a href="classification-2.html#cb585-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">3033</span>)</span>
<span id="cb585-3"><a href="classification-2.html#cb585-3" tabindex="-1"></a>train_df <span class="ot">&lt;-</span></span>
<span id="cb585-4"><a href="classification-2.html#cb585-4" tabindex="-1"></a>  caret<span class="sc">::</span><span class="fu">createDataPartition</span>(<span class="at">y =</span> train<span class="sc">$</span>Y, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb585-5"><a href="classification-2.html#cb585-5" tabindex="-1"></a>training <span class="ot">&lt;-</span> train[train_df, ]</span>
<span id="cb585-6"><a href="classification-2.html#cb585-6" tabindex="-1"></a>testing <span class="ot">&lt;-</span> train[<span class="sc">-</span>train_df, ]</span>
<span id="cb585-7"><a href="classification-2.html#cb585-7" tabindex="-1"></a></span>
<span id="cb585-8"><a href="classification-2.html#cb585-8" tabindex="-1"></a><span class="co">#Training/Model building with 10-k cross validation</span></span>
<span id="cb585-9"><a href="classification-2.html#cb585-9" tabindex="-1"></a>cv <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>, <span class="at">p =</span> <span class="fl">0.9</span>)</span>
<span id="cb585-10"><a href="classification-2.html#cb585-10" tabindex="-1"></a>model_knn3 <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(</span>
<span id="cb585-11"><a href="classification-2.html#cb585-11" tabindex="-1"></a>  Y <span class="sc">~</span> .,</span>
<span id="cb585-12"><a href="classification-2.html#cb585-12" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb585-13"><a href="classification-2.html#cb585-13" tabindex="-1"></a>  <span class="at">data =</span> training,</span>
<span id="cb585-14"><a href="classification-2.html#cb585-14" tabindex="-1"></a>  <span class="at">tuneGrid =</span> <span class="fu">data.frame</span>(<span class="at">k =</span> <span class="fu">seq</span>(<span class="dv">9</span>, <span class="dv">41</span> , <span class="dv">2</span>)),</span>
<span id="cb585-15"><a href="classification-2.html#cb585-15" tabindex="-1"></a>  <span class="at">trControl =</span> cv</span>
<span id="cb585-16"><a href="classification-2.html#cb585-16" tabindex="-1"></a>)</span>
<span id="cb585-17"><a href="classification-2.html#cb585-17" tabindex="-1"></a><span class="fu">ggplot</span>(model_knn3, <span class="at">highlight =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/knn20-1.png" width="672" /></p>
<p>Now we are going to use the test set to see the model’s performance.</p>
<div class="sourceCode" id="cb586"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb586-1"><a href="classification-2.html#cb586-1" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="fu">predict</span>(model_knn3, testing, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb586-2"><a href="classification-2.html#cb586-2" tabindex="-1"></a>                testing<span class="sc">$</span>Y)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Less More
##       Less 7311 1871
##       More  105  481
##                                           
##                Accuracy : 0.7977          
##                  95% CI : (0.7896, 0.8056)
##     No Information Rate : 0.7592          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.256           
##                                           
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
##                                           
##             Sensitivity : 0.9858          
##             Specificity : 0.2045          
##          Pos Pred Value : 0.7962          
##          Neg Pred Value : 0.8208          
##              Prevalence : 0.7592          
##          Detection Rate : 0.7485          
##    Detection Prevalence : 0.9400          
##       Balanced Accuracy : 0.5952          
##                                           
##        &#39;Positive&#39; Class : Less            
## </code></pre>
<p>Next, as you can guess, we will delve into these performance measures.</p>
<p>Learning algorithm may not be evaluated only by its predictive capacity. We may want to interpret the results by identifying the important predictors and their importance. <strong>There is always a trade-off between interpretability and predictive accuracy</strong>. Here is a an illustration. We will talk about this later in the book.</p>
<p><img src="png/tradeoff.png" width="130%" height="130%" /></p>
</div>
</div>
<div id="tuning-in-classification" class="section level2 hasAnchor" number="21.6">
<h2><span class="header-section-number">21.6</span> Tuning in Classification<a href="classification-2.html#tuning-in-classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What metrics are we going to use when we <em>train</em> our classification models? In kNN, for example, our hyperparameter is <span class="math inline">\(k\)</span>, the number of observations in each bin. In our applications with <code>mnist_27</code> and <code>Adult</code> datasets, <span class="math inline">\(k\)</span> was determined by a metric called as <strong>accuracy</strong>. What is it? If the choice of <span class="math inline">\(k\)</span> depends on what metrics we use in tuning, can we improve our prediction performance by using a different metric? Moreover, the accuracy is calculated from the confusion table. Yet, the confusion table will be different for a range of discriminating thresholds used for labeling predicted probabilities. These are important questions in classification problems. We will begin answering them in this chapter.</p>
</div>
<div id="confusion-matrix" class="section level2 hasAnchor" number="21.7">
<h2><span class="header-section-number">21.7</span> Confusion matrix<a href="classification-2.html#confusion-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In general, whether it is for training or not, measuring the performance of a classification model is an important issue and has to be well understood before fitting or training a model.</p>
<p>To evaluate a model’s fit, we can look at its predictive accuracy. In classification problems, this requires predicting <span class="math inline">\(Y\)</span>, as either 0 or 1, from the predicted value of <span class="math inline">\(p(x)\)</span>, such as</p>
<p><span class="math display">\[
\hat{Y}=\left\{\begin{array}{ll}{1,} &amp; {\hat{p}\left(x_{1}, \ldots, x_{k}\right)&gt;\frac{1}{2}} \\ {0,} &amp; {\hat{p}\left(x_{1}, \ldots, x_{k}\right)&lt;\frac{1}{2}}\end{array}\right.
\]</span></p>
<p>From this transformation of <span class="math inline">\(\hat{p}(x)\)</span> to <span class="math inline">\(\hat{Y}\)</span>, the overall predictive accuracy can be summarized with a matrix,</p>
<p><span class="math display">\[
\begin{array}{ccc}{\text { Predicted vs. Reality}} &amp; {Y=1} &amp; {Y=0} \\ {\hat{Y}=1} &amp; {\text { TP }_{}} &amp; {\text { FP }_{}} \\ {\hat{Y}=0} &amp; {\text { FN }_{}} &amp; {\text { TN }_{}}\end{array}
\]</span></p>
<p>where, TP, FP, FN, TN are True positives, False Positives, False Negatives, and True Negatives, respectively. This table is also know as <strong>Confusion Table</strong> or confusion matrix. The name, <em>confusion</em>, is very intuitive because it is easy to see how the system is <strong>confusing</strong> two classes.</p>
<p>There are many metrics that can be calculated from this table. Let’s use an example given in <a href="https://en.wikipedia.org/wiki/Confusion_matrix">Wikipedia</a></p>
<p><span class="math display">\[
\begin{array}{ccc}{\text { Predicted vs. Reality}} &amp; {Y=Cat} &amp; {Y=Dog} \\ {\hat{Y}=Cat} &amp; {\text { 5 }_{}} &amp; {\text { 2 }_{}} \\ {\hat{Y}=Dog} &amp; {\text { 3 }_{}} &amp; {\text { 3 }_{}}\end{array}
\]</span></p>
<p>According to this confusion matrix, there are 8 actual cats and 5 actual dogs (column totals). The learning algorithm, however, predicts only 5 cats and 3 dogs correctly. The model predicts 3 cats as dogs and 2 dogs as cats. All correct predictions are located in the diagonal of the table, so it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal.</p>
<p>In predictive analytics, this table (matrix) allows more detailed analysis than mere proportion of correct classifications (accuracy). <strong>Accuracy</strong> (<span class="math inline">\((TP+TN)/n\)</span>) is not a reliable metric for the real performance of a classifier, when the dataset is unbalanced in terms of numbers of observations in each class.</p>
<p>It can be seen how misleading the use of <span class="math inline">\((TP+TN)/n\)</span> could be, if there were 95 cats and only 5 dogs in our example. If we choose <em>accuracy</em> as the performance measure in our training, our learning algorithm might classify all the observations as cats, because the overall accuracy would be 95%. In that case, however, all the dog would be misclassified as cats.</p>
</div>
<div id="performance-measures" class="section level2 hasAnchor" number="21.8">
<h2><span class="header-section-number">21.8</span> Performance measures<a href="classification-2.html#performance-measures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Which metrics should we be using in training our classification models? These questions are more important when the classes are not in balance. Moreover, in some situation, false predictions would be more important then true predictions. In a situation that you try to predict, for example, cancer, minimizing false negatives (the model misses cancer patients) would be more important than minimizing false positives (the model wrongly predicts cancer). When we have an algorithm to predict spam emails, however, false positives would be the target to minimize rather than false negatives.</p>
<p>Here is the full picture of various metrics using the same confusion table from <a href="https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers">Wikipedia</a>:</p>
<p><img src="png/confusion.png" width="140%" height="140%" /></p>
<p>Let’s summarize some of the metrics and their use with examples for detecting cancer:</p>
<ul>
<li><strong>Accuracy</strong>: the number of correct predictions (with and without cancer) relative to the number of observations (patients). This can be used when the classes are balanced with not less than a 60-40% split. <span class="math inline">\((TP+TN)/n\)</span>.<br />
</li>
<li><strong>Balanced Accuracy</strong>: when the class balance is worse than 60-40% split, <span class="math inline">\((TP/P + TN/N)/2\)</span>.<br />
</li>
<li><strong>Precision</strong>: the percentage positive predictions that are correct. That is, the proportion of patients that we predict as having cancer, actually have cancer, <span class="math inline">\(TP/(TP+FP)\)</span>.<br />
</li>
<li><strong>Sensitivity</strong>: the percentage of positives that are predicted correctly. That is, the proportion of patients that actually have cancer was correctly predicted by the algorithm as having cancer, <span class="math inline">\(TP/(TP+FN)\)</span>. This measure is also called as <em>True Positive Rate</em> or as <em>Recall</em>.</li>
<li><strong>Specificity</strong>: the percentage of negatives that are predicted correctly. Proportion of patients that do not have cancer, are predicted by the model as non-cancerous, This measure is also called as <em>True Positive Rate</em> = <span class="math inline">\(TN/(TN+FP)\)</span>.</li>
</ul>
<p>Here is the summary:</p>
<p><span class="math display">\[
\begin{array}{ccc}{\text { Predicted vs. Reality}} &amp; {Y=Cat} &amp; {Y=Dog} \\ {\hat{Y}=Cat} &amp; {\text {TPR or Sensitivity }_{}} &amp; {\text { FPR or Fall-out }_{}} \\ {\hat{Y}=Dog} &amp; {\text { FNR or Miss Rate }_{}} &amp; {\text { TNR or Specificity }_{}}\end{array}
\]</span></p>
<p><strong>Kappa</strong> is also calculated in most cases. It is an interesting measure because it compares the actual performance of prediction with what it would be if a random prediction was carried out. For example, suppose that your model predicts <span class="math inline">\(Y\)</span> with 95% accuracy. How good your prediction power would be if a random choice would also predict 70% of <span class="math inline">\(Y\)</span>s correctly? Let’s use an example:</p>
<p><span class="math display">\[
\begin{array}{ccc}{\text { Predicted vs. Reality}} &amp; {Y=Cat} &amp; {Y=Dog} \\ {\hat{Y}=Cat} &amp; {\text { 22 }_{}} &amp; {\text { 9 }_{}} \\ {\hat{Y}=Dog} &amp; {\text { 7 }_{}} &amp; {\text { 13 }_{}}\end{array}
\]</span></p>
<p>In this case the accuracy is <span class="math inline">\((22+13)/51 = 0.69\)</span> But how much of it is due the model’s performance itself? In other words, the distribution of cats and dogs can also give a predictive clue such that a certain level of prediction accuracy can be achieved by chance without any learning algorithm. For the TP cell in the table, this can be calculated as the difference between observed accuracy (OA) and expected accuracy (EA),</p>
<p><span class="math display">\[
\mathrm{(OA-EA)_{TP}}=\mathrm{Pr}(\hat{Y}=Cat)[\mathrm{Pr}(Y=Cat |\hat{Y}= Cat)-\mathrm{P}(Y=Cat)],
\]</span></p>
<p>Remember from your statistics class, if the two variables are independent, the conditional probability of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> has to be equal to the marginal probability of <span class="math inline">\(X\)</span>. Therefore, inside the brackets, the difference between the conditional probability, which reflects the probability of predicting cats due to the model, and the marginal probability of observing actual cats reflects the <em>true</em> level of predictive power of the model by removing the randomness in prediction.</p>
<p><span class="math display">\[
\mathrm{(OA-EA)_{TN}}=\mathrm{Pr}(\hat{Y}=Dog)[\mathrm{Pr}(Y=Dog |\hat{Y}= Dog)-\mathrm{P}(Y=Dog)],
\]</span></p>
<p>If we use the joint and marginal probability definitions, these can be written as:</p>
<p><span class="math display">\[
OA-EA=\frac{m_{i j}}{n}-\frac{m_{i} m_{j}}{n^{2}}
\]</span></p>
<p>Here is the calculation of <strong>Kappa</strong> for our example:</p>
<p>Total, <span class="math inline">\(n = 51\)</span>,<br />
<span class="math inline">\(OA-EA\)</span> for <span class="math inline">\(TP\)</span> = <span class="math inline">\(22/51-31 \times (29/51^2) = 0.0857\)</span><br />
<span class="math inline">\(OA-EA\)</span> for <span class="math inline">\(TN\)</span> = <span class="math inline">\(13/51-20 \times (21/51^2) = 0.0934\)</span></p>
<p>And we normalize it by <span class="math inline">\(1-EA = 1- 31 \times (29/51^2) + 20 \times (21/51^2) = 0.51\)</span>, which is the value if the prediction was 100% successful.</p>
<p>Hence, <strong>Kappa</strong>: <span class="math inline">\((0.0857+0.0934) / (1 - 0.51) = 0.3655\)</span></p>
<p>Finally, <strong>Jouden’s J statistics</strong> also as known as <strong>Youden’s index</strong> or <strong>Informedness</strong>, is a single statistics that captures the performance of prediction. It’s simply <span class="math inline">\(J=TPR+TNR-1\)</span> and ranges between 0 and 1 indicating useless and perfect prediction performance, respectively. This metric is also related to <strong>Receiver Operating Characteristics (ROC) curve</strong> analysis, which is the subject of next section.</p>
</div>
<div id="roc-curve" class="section level2 hasAnchor" number="21.9">
<h2><span class="header-section-number">21.9</span> ROC Curve<a href="classification-2.html#roc-curve" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our outcome variable is categorical (<span class="math inline">\(Y = 1\)</span> or <span class="math inline">\(0\)</span>). Most classification algorithms calculate the predicted probability of success (<span class="math inline">\(Y = 1\)</span>). If the probability is larger than a fixed cut-off threshold (discriminating threshold), then we assume that the model predicts success (Y = 1); otherwise, we assume that it predicts failure. As a result of such a procedure, the comparison of the observed and predicted values summarized in a confusion table depends on the threshold. The predictive accuracy of a model as a function of threshold can be summarized by Area Under Curve (AUC) of Receiver Operating Characteristics (ROC). The ROC curve, which is is a graphical plot that illustrates the diagnostic ability of a binary classifier, indicates a trade-off between True Positive Rate (TPR) and False Positive Rate (FPR). Hence, the success of a model comes with its predictions that increases TPR without raising FPR. The ROC curve was first used during World War II for the analysis of radar signals before it was employed in signal detection theory.</p>
<p>Here is a visualization:</p>
<p><img src="png/ROC1.png" width="140%" height="140%" /></p>
<p>Let’s start with an example, where we have 100 individuals, 50 with <span class="math inline">\(y_i=1\)</span> and 50 with <span class="math inline">\(y_i=0\)</span>, which is well-balanced. If we use a discriminating threshold (0%) that puts everybody into Category 1 or a threshold (100%) that puts everybody into Category 2, that is,</p>
<p><span class="math display">\[
\hat{Y}=\left\{\begin{array}{ll}{1,} &amp; {\hat{p}\left(x_{1}, \ldots, x_{k}\right)&gt;0 \%} \\ {0,} &amp; {\hat{p}\left(x_{1}, \ldots, x_{k}\right)\leq0 \%}\end{array}\right.
\]</span></p>
<p>and,</p>
<p><span class="math display">\[
\hat{Y}=\left\{\begin{array}{ll}{1,} &amp; {\hat{p}\left(x_{1}, \ldots, x_{k}\right)&gt;100 \%} \\ {0,} &amp; {\hat{p}\left(x_{1}, \ldots, x_{k}\right)\leq100 \%}\end{array}\right.
\]</span></p>
<p>this would have led to the following confusing tables, respectively:</p>
<p><span class="math display">\[
\begin{array}{ccc}{\text { Predicted vs. Reality}} &amp; {Y=1} &amp; {Y=0} \\ {\hat{Y}=1} &amp; {\text { 50 }_{}} &amp; {\text { 50 }_{}} \\ {\hat{Y}=0} &amp; {\text { 0 }_{}} &amp; {\text { 0 }_{}}\end{array}
\]</span>
<span class="math display">\[
\begin{array}{ccc}{\text { Predicted vs. Reality}} &amp; {Y=1} &amp; {Y=0} \\ {\hat{Y}=1} &amp; {\text { 0 }_{}} &amp; {\text { 0 }_{}} \\ {\hat{Y}=0} &amp; {\text { 50 }_{}} &amp; {\text { 50 }_{}}\end{array}
\]</span></p>
<p>In the first case, <span class="math inline">\(TPR = 1\)</span> and <span class="math inline">\(FPR = 1\)</span>; and in the second case, <span class="math inline">\(TPR = 0\)</span> and <span class="math inline">\(FPR = 0\)</span>. So when we calculate all possible confusion tables with different values of thresholds ranging from 0% to 100%, we will have the same number of (<span class="math inline">\(TPR, FPR\)</span>) points each corresponding to one threshold. <strong>The ROC curve is the curve that connects these points</strong>.</p>
<p>Let’s use an example with the <em>Boston Housing Market</em> dataset to illustrate ROC:</p>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb588-1"><a href="classification-2.html#cb588-1" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb588-2"><a href="classification-2.html#cb588-2" tabindex="-1"></a><span class="fu">data</span>(Boston)</span>
<span id="cb588-3"><a href="classification-2.html#cb588-3" tabindex="-1"></a></span>
<span id="cb588-4"><a href="classification-2.html#cb588-4" tabindex="-1"></a><span class="co"># Create our binary outcome</span></span>
<span id="cb588-5"><a href="classification-2.html#cb588-5" tabindex="-1"></a>data <span class="ot">&lt;-</span> Boston[, <span class="sc">-</span><span class="dv">14</span>] <span class="co">#Dropping &quot;medv&quot;</span></span>
<span id="cb588-6"><a href="classification-2.html#cb588-6" tabindex="-1"></a>data<span class="sc">$</span>dummy <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">ifelse</span>(Boston<span class="sc">$</span>medv <span class="sc">&gt;</span> <span class="dv">25</span>, <span class="dv">1</span>, <span class="dv">0</span>))</span>
<span id="cb588-7"><a href="classification-2.html#cb588-7" tabindex="-1"></a></span>
<span id="cb588-8"><a href="classification-2.html#cb588-8" tabindex="-1"></a><span class="co"># Use logistic regression for classification</span></span>
<span id="cb588-9"><a href="classification-2.html#cb588-9" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glm</span>(dummy <span class="sc">~</span> ., <span class="at">data =</span> data, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb588-10"><a href="classification-2.html#cb588-10" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = dummy ~ ., family = &quot;binomial&quot;, data = data)
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  5.312511   4.876070   1.090 0.275930    
## crim        -0.011101   0.045322  -0.245 0.806503    
## zn           0.010917   0.010834   1.008 0.313626    
## indus       -0.110452   0.058740  -1.880 0.060060 .  
## chas         0.966337   0.808960   1.195 0.232266    
## nox         -6.844521   4.483514  -1.527 0.126861    
## rm           1.886872   0.452692   4.168 3.07e-05 ***
## age          0.003491   0.011133   0.314 0.753853    
## dis         -0.589016   0.164013  -3.591 0.000329 ***
## rad          0.318042   0.082623   3.849 0.000118 ***
## tax         -0.010826   0.004036  -2.682 0.007314 ** 
## ptratio     -0.353017   0.122259  -2.887 0.003884 ** 
## black       -0.002264   0.003826  -0.592 0.554105    
## lstat       -0.367355   0.073020  -5.031 4.88e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 563.52  on 505  degrees of freedom
## Residual deviance: 209.11  on 492  degrees of freedom
## AIC: 237.11
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>And our prediction (in-sample):</p>
<div class="sourceCode" id="cb590"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb590-1"><a href="classification-2.html#cb590-1" tabindex="-1"></a><span class="co"># Classified Y&#39;s by TRUE and FALSE</span></span>
<span id="cb590-2"><a href="classification-2.html#cb590-2" tabindex="-1"></a>yHat <span class="ot">&lt;-</span> model<span class="sc">$</span>fitted.values <span class="sc">&gt;</span> <span class="fl">0.5</span></span>
<span id="cb590-3"><a href="classification-2.html#cb590-3" tabindex="-1"></a>conf_table <span class="ot">&lt;-</span> <span class="fu">table</span>(yHat, data<span class="sc">$</span>dummy)</span>
<span id="cb590-4"><a href="classification-2.html#cb590-4" tabindex="-1"></a></span>
<span id="cb590-5"><a href="classification-2.html#cb590-5" tabindex="-1"></a><span class="co">#let&#39;s change the order of cells</span></span>
<span id="cb590-6"><a href="classification-2.html#cb590-6" tabindex="-1"></a>ctt <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(conf_table)</span>
<span id="cb590-7"><a href="classification-2.html#cb590-7" tabindex="-1"></a>ct <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb590-8"><a href="classification-2.html#cb590-8" tabindex="-1"></a>ct[<span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> ctt[<span class="dv">2</span>,<span class="dv">2</span>]</span>
<span id="cb590-9"><a href="classification-2.html#cb590-9" tabindex="-1"></a>ct[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> ctt[<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb590-10"><a href="classification-2.html#cb590-10" tabindex="-1"></a>ct[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> ctt[<span class="dv">2</span>,<span class="dv">1</span>]</span>
<span id="cb590-11"><a href="classification-2.html#cb590-11" tabindex="-1"></a>ct[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> ctt[<span class="dv">1</span>,<span class="dv">2</span>]</span>
<span id="cb590-12"><a href="classification-2.html#cb590-12" tabindex="-1"></a></span>
<span id="cb590-13"><a href="classification-2.html#cb590-13" tabindex="-1"></a><span class="fu">rownames</span>(ct) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Yhat = 1&quot;</span>, <span class="st">&quot;Yhat = 0&quot;</span>)</span>
<span id="cb590-14"><a href="classification-2.html#cb590-14" tabindex="-1"></a><span class="fu">colnames</span>(ct) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Y = 1&quot;</span>, <span class="st">&quot;Y = 0&quot;</span>)</span>
<span id="cb590-15"><a href="classification-2.html#cb590-15" tabindex="-1"></a>ct</span></code></pre></div>
<pre><code>##          Y = 1 Y = 0
## Yhat = 1   100    16
## Yhat = 0    24   366</code></pre>
<p>It would be much easier if we create our own function to rotate a matrix/table:</p>
<div class="sourceCode" id="cb592"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb592-1"><a href="classification-2.html#cb592-1" tabindex="-1"></a>rot <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb592-2"><a href="classification-2.html#cb592-2" tabindex="-1"></a>  t <span class="ot">&lt;-</span> <span class="fu">apply</span>(x, <span class="dv">2</span>, rev)</span>
<span id="cb592-3"><a href="classification-2.html#cb592-3" tabindex="-1"></a>  tt <span class="ot">&lt;-</span> <span class="fu">apply</span>(t, <span class="dv">1</span>, rev)</span>
<span id="cb592-4"><a href="classification-2.html#cb592-4" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">t</span>(tt))</span>
<span id="cb592-5"><a href="classification-2.html#cb592-5" tabindex="-1"></a>}</span>
<span id="cb592-6"><a href="classification-2.html#cb592-6" tabindex="-1"></a>ct <span class="ot">&lt;-</span> <span class="fu">rot</span>(conf_table)</span>
<span id="cb592-7"><a href="classification-2.html#cb592-7" tabindex="-1"></a><span class="fu">rownames</span>(ct) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Yhat = 1&quot;</span>, <span class="st">&quot;Yhat = 0&quot;</span>)</span>
<span id="cb592-8"><a href="classification-2.html#cb592-8" tabindex="-1"></a><span class="fu">colnames</span>(ct) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Y = 1&quot;</span>, <span class="st">&quot;Y = 0&quot;</span>)</span>
<span id="cb592-9"><a href="classification-2.html#cb592-9" tabindex="-1"></a>ct</span></code></pre></div>
<pre><code>##           
## yHat       Y = 1 Y = 0
##   Yhat = 1   100    16
##   Yhat = 0    24   366</code></pre>
<p>Now we calculate our TPR, FPR, and J-Index:</p>
<div class="sourceCode" id="cb594"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb594-1"><a href="classification-2.html#cb594-1" tabindex="-1"></a><span class="co">#TPR</span></span>
<span id="cb594-2"><a href="classification-2.html#cb594-2" tabindex="-1"></a>TPR <span class="ot">&lt;-</span> ct[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span>(ct[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">+</span>ct[<span class="dv">2</span>,<span class="dv">1</span>])</span>
<span id="cb594-3"><a href="classification-2.html#cb594-3" tabindex="-1"></a>TPR</span></code></pre></div>
<pre><code>## [1] 0.8064516</code></pre>
<div class="sourceCode" id="cb596"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb596-1"><a href="classification-2.html#cb596-1" tabindex="-1"></a><span class="co">#FPR</span></span>
<span id="cb596-2"><a href="classification-2.html#cb596-2" tabindex="-1"></a>FPR <span class="ot">&lt;-</span> ct[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(ct[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">+</span>ct[<span class="dv">2</span>,<span class="dv">2</span>])</span>
<span id="cb596-3"><a href="classification-2.html#cb596-3" tabindex="-1"></a>FPR</span></code></pre></div>
<pre><code>## [1] 0.04188482</code></pre>
<div class="sourceCode" id="cb598"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb598-1"><a href="classification-2.html#cb598-1" tabindex="-1"></a><span class="co">#J-Index</span></span>
<span id="cb598-2"><a href="classification-2.html#cb598-2" tabindex="-1"></a>TPR<span class="sc">-</span>FPR</span></code></pre></div>
<pre><code>## [1] 0.7645668</code></pre>
<p>These rates are calculated for the threshold of 0.5. We can have all pairs of <span class="math inline">\(TPR\)</span> and <span class="math inline">\(FPR\)</span> for all possible discrimination thresholds. What’s the possible set? We will use our <span class="math inline">\(\hat{P}\)</span> values for this.</p>
<div class="sourceCode" id="cb600"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb600-1"><a href="classification-2.html#cb600-1" tabindex="-1"></a><span class="co">#We create an ordered grid from our fitted values</span></span>
<span id="cb600-2"><a href="classification-2.html#cb600-2" tabindex="-1"></a><span class="fu">summary</span>(model<span class="sc">$</span>fitted.values)</span></code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## 0.000000 0.004205 0.035602 0.245059 0.371758 0.999549</code></pre>
<div class="sourceCode" id="cb602"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb602-1"><a href="classification-2.html#cb602-1" tabindex="-1"></a>phat <span class="ot">&lt;-</span> model<span class="sc">$</span>fitted.values[<span class="fu">order</span>(model<span class="sc">$</span>fitted.values)]</span>
<span id="cb602-2"><a href="classification-2.html#cb602-2" tabindex="-1"></a><span class="fu">length</span>(phat)</span></code></pre></div>
<pre><code>## [1] 506</code></pre>
<div class="sourceCode" id="cb604"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb604-1"><a href="classification-2.html#cb604-1" tabindex="-1"></a><span class="co">#We need to have containers for the pairs of TPR and FPR</span></span>
<span id="cb604-2"><a href="classification-2.html#cb604-2" tabindex="-1"></a>TPR <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb604-3"><a href="classification-2.html#cb604-3" tabindex="-1"></a>FPR <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb604-4"><a href="classification-2.html#cb604-4" tabindex="-1"></a></span>
<span id="cb604-5"><a href="classification-2.html#cb604-5" tabindex="-1"></a><span class="co">#Now the loop</span></span>
<span id="cb604-6"><a href="classification-2.html#cb604-6" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(phat)) {</span>
<span id="cb604-7"><a href="classification-2.html#cb604-7" tabindex="-1"></a>  yHat <span class="ot">&lt;-</span> model<span class="sc">$</span>fitted.values <span class="sc">&gt;</span> phat[i]</span>
<span id="cb604-8"><a href="classification-2.html#cb604-8" tabindex="-1"></a>  conf_table <span class="ot">&lt;-</span> <span class="fu">table</span>(yHat, data<span class="sc">$</span>dummy)</span>
<span id="cb604-9"><a href="classification-2.html#cb604-9" tabindex="-1"></a>  ct <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(conf_table) </span>
<span id="cb604-10"><a href="classification-2.html#cb604-10" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">sum</span>(<span class="fu">dim</span>(ct))<span class="sc">&gt;</span><span class="dv">3</span>){ <span class="co">#here we ignore the thresholds 0 and 1</span></span>
<span id="cb604-11"><a href="classification-2.html#cb604-11" tabindex="-1"></a>    TPR[i] <span class="ot">&lt;-</span> ct[<span class="dv">2</span>,<span class="dv">2</span>]<span class="sc">/</span>(ct[<span class="dv">2</span>,<span class="dv">2</span>]<span class="sc">+</span>ct[<span class="dv">1</span>,<span class="dv">2</span>])</span>
<span id="cb604-12"><a href="classification-2.html#cb604-12" tabindex="-1"></a>    FPR[i] <span class="ot">&lt;-</span> ct[<span class="dv">2</span>,<span class="dv">1</span>]<span class="sc">/</span>(ct[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">+</span>ct[<span class="dv">2</span>,<span class="dv">1</span>])</span>
<span id="cb604-13"><a href="classification-2.html#cb604-13" tabindex="-1"></a>  }</span>
<span id="cb604-14"><a href="classification-2.html#cb604-14" tabindex="-1"></a>}</span>
<span id="cb604-15"><a href="classification-2.html#cb604-15" tabindex="-1"></a><span class="fu">plot</span>(FPR, TPR, <span class="at">col=</span> <span class="st">&quot;blue&quot;</span>, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">main =</span> <span class="st">&quot;ROC&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb604-16"><a href="classification-2.html#cb604-16" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">1</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/tc7-1.png" width="672" /></p>
<p>Several things we observe on this curve. First, there is a trade-off between TPF and FPR. Approximately, after 70% of TPR, an increase in TPF can be achieved by increasing FPR, which means that if we care more about the possible lowest FPR, we can fix the discriminating rate at that point.</p>
<p>Second, we can identify the best discriminating threshold that makes the distance between TPR and FPR largest. In other words, we can identify the threshold where the marginal gain on TPR would be equal to the marginal cost of FPR. This can be achieved by the <strong>Jouden’s J statistics</strong>, <span class="math inline">\(J=TPR+TNR-1\)</span>, which identifies the best discriminating threshold. Note that <span class="math inline">\(TNR= 1-FPR\)</span>. Hence <span class="math inline">\(J = TPR-FPR\)</span>.</p>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb605-1"><a href="classification-2.html#cb605-1" tabindex="-1"></a><span class="co"># Youden&#39;s J Statistics</span></span>
<span id="cb605-2"><a href="classification-2.html#cb605-2" tabindex="-1"></a>J <span class="ot">&lt;-</span> TPR <span class="sc">-</span> FPR</span>
<span id="cb605-3"><a href="classification-2.html#cb605-3" tabindex="-1"></a><span class="co"># The best discriminating threshold</span></span>
<span id="cb605-4"><a href="classification-2.html#cb605-4" tabindex="-1"></a>phat[<span class="fu">which.max</span>(J)]</span></code></pre></div>
<pre><code>##       231 
## 0.1786863</code></pre>
<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb607-1"><a href="classification-2.html#cb607-1" tabindex="-1"></a><span class="co">#TPR and FPR at this threshold</span></span>
<span id="cb607-2"><a href="classification-2.html#cb607-2" tabindex="-1"></a>TPR[<span class="fu">which.max</span>(J)]</span></code></pre></div>
<pre><code>## [1] 0.9354839</code></pre>
<div class="sourceCode" id="cb609"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb609-1"><a href="classification-2.html#cb609-1" tabindex="-1"></a>FPR[<span class="fu">which.max</span>(J)]</span></code></pre></div>
<pre><code>## [1] 0.1361257</code></pre>
<div class="sourceCode" id="cb611"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb611-1"><a href="classification-2.html#cb611-1" tabindex="-1"></a>J[<span class="fu">which.max</span>(J)]</span></code></pre></div>
<pre><code>## [1] 0.7993582</code></pre>
<p>This simple example shows that the best (in-sample) fit can be achieved by</p>
<p><span class="math display">\[
\hat{Y}=\left\{\begin{array}{ll}{1,} &amp; {\hat{p}\left(x_{1}, \ldots, x_{k}\right)&gt;17.86863 \%} \\ {0,} &amp; {\hat{p}\left(x_{1}, \ldots, x_{k}\right)\leq17.86863 \%}\end{array}\right.
\]</span></p>
</div>
<div id="auc---area-under-the-curve" class="section level2 hasAnchor" number="21.10">
<h2><span class="header-section-number">21.10</span> AUC - Area Under the Curve<a href="classification-2.html#auc---area-under-the-curve" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Finally, we measure the predictive accuracy by the area under the ROC curve. An area of 1 represents a perfect performance; an area of 0.5 represents a worthless prediction. This is because an area of 0.5 suggests its performance is no better than random chance.</p>
<p><img src="png/AUC.png" width="130%" height="130%" /></p>
<p>For example, an accepted rough guide for classifying the accuracy of a diagnostic test in medical procedures is</p>
<p>0.90-1.00 = Excellent (A)<br />
0.80-0.90 = Good (B)<br />
0.70-0.80 = Fair (C)<br />
0.60-0.70 = Poor (D)<br />
0.50-0.60 = Fail (F)</p>
<p>Since the formula and its derivation is beyond the scope of this chapter, we will use the package <code>ROCR</code> to calculate it.</p>
<div class="sourceCode" id="cb613"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb613-1"><a href="classification-2.html#cb613-1" tabindex="-1"></a><span class="fu">library</span>(ROCR)</span>
<span id="cb613-2"><a href="classification-2.html#cb613-2" tabindex="-1"></a></span>
<span id="cb613-3"><a href="classification-2.html#cb613-3" tabindex="-1"></a>data<span class="sc">$</span>dummy <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">ifelse</span>(Boston<span class="sc">$</span>medv <span class="sc">&gt;</span> <span class="dv">25</span>, <span class="dv">1</span>, <span class="dv">0</span>))</span>
<span id="cb613-4"><a href="classification-2.html#cb613-4" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glm</span>(dummy <span class="sc">~</span> ., <span class="at">data =</span> data, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb613-5"><a href="classification-2.html#cb613-5" tabindex="-1"></a>phat <span class="ot">&lt;-</span> model<span class="sc">$</span>fitted.values</span>
<span id="cb613-6"><a href="classification-2.html#cb613-6" tabindex="-1"></a></span>
<span id="cb613-7"><a href="classification-2.html#cb613-7" tabindex="-1"></a>phat_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(phat, <span class="st">&quot;Y&quot;</span> <span class="ot">=</span> data<span class="sc">$</span>dummy)</span>
<span id="cb613-8"><a href="classification-2.html#cb613-8" tabindex="-1"></a>pred_rocr <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat_df[,<span class="dv">1</span>], phat_df[,<span class="dv">2</span>])</span>
<span id="cb613-9"><a href="classification-2.html#cb613-9" tabindex="-1"></a>perf <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr,<span class="st">&quot;tpr&quot;</span>,<span class="st">&quot;fpr&quot;</span>)</span>
<span id="cb613-10"><a href="classification-2.html#cb613-10" tabindex="-1"></a></span>
<span id="cb613-11"><a href="classification-2.html#cb613-11" tabindex="-1"></a><span class="fu">plot</span>(perf, <span class="at">colorize=</span><span class="cn">TRUE</span>)</span>
<span id="cb613-12"><a href="classification-2.html#cb613-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/tc10-1.png" width="672" /></p>
<div class="sourceCode" id="cb614"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb614-1"><a href="classification-2.html#cb614-1" tabindex="-1"></a>auc_ROCR <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb614-2"><a href="classification-2.html#cb614-2" tabindex="-1"></a>AUC <span class="ot">&lt;-</span> auc_ROCR<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb614-3"><a href="classification-2.html#cb614-3" tabindex="-1"></a>AUC</span></code></pre></div>
<pre><code>## [1] 0.9600363</code></pre>
<p>This ROC curve is the same as the one that we developed earlier.</p>
<p>When we train a model, in each run (different train and test sets) we will obtain a different AUC. Differences in AUC across train and validation sets creates an uncertainty about AUC. Consequently, the asymptotic properties of AUC for comparing alternative models has become a subject of discussions in the literature.</p>
<p>Another important point is that, while AUC represents the entire area under the curve, our interest would be on a specific location of TPR or FPR. Hence it’s possible that, for any given two competing algorithms, while one prediction algorithm has a higher overall AUC, the other one could have a better AUC in that specific location. This issue can be seen in the following figure taken from <a href="https://arxiv.org/pdf/1812.01388.pdf">Bad practices in evaluation methodology relevant to class-imbalanced problems</a> by Jan Brabec and Lukas Machlica <span class="citation">(<a href="#ref-Brab_2018"><strong>Brab_2018?</strong></a>)</span>.</p>
<p><img src="png/AUCs.png" width="140%" height="140%" /></p>
<blockquote>
<p>For example, in the domain of network traffic intrusion-detection, the imbalance ratio is often higher than 1:1000, and the cost of a false alarm for an applied system is very high. This is due to increased analysis and remediation costs of infected devices. In such systems, the region of interest on the ROC curve is for false positive rate at most 0.0001. If AUC was computed in the usual way over the complete ROC curve then 99.99% of the area would be irrelevant and would represent only noise in the final outcome. We demonstrate this phenomenon in Figure 1.</p>
<p>If AUC has to be used, we suggest to discuss the region of interest, and eventually compute the area only at this region. This is even more important if ROC curves are not presented, but only AUCs of the compared algorithms are reported.</p>
</blockquote>
<p>Most of the challenges in classification problems are related to class imbalances in the data. We look at this issue in Cahpter 39.</p>
</div>
<div id="classification-example" class="section level2 hasAnchor" number="21.11">
<h2><span class="header-section-number">21.11</span> Classification Example<a href="classification-2.html#classification-example" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can conclude this section with a classification example. We will use <code>Adult</code> dataset. The information on the dataset is given at the <a href="https://archive.ics.uci.edu/ml/datasets/Adult">Machine Learning Repository at UCI</a> <span class="citation">(<a href="#ref-Kohavi_1996"><strong>Kohavi_1996?</strong></a>)</span>:</p>
<p>The prediction task is to determine whether a person makes over $50K a year. This question would be similar to the question of <em>whether the person makes less than 50K</em>. However, we need to be careful in defining which class will be <strong>positive</strong> or <strong>negative</strong>. Suppose we have <span class="math inline">\(Y\)</span>, 0 and 1, and we define 1 as a <em>positive</em> class:</p>
<p><span class="math display">\[
\begin{array}{ccc}{\text { Predicted vs. Reality}} &amp; {Y=1+} &amp; {Y=0-} \\ {\hat{Y}=1+} &amp; {\text { TP }_{}} &amp; {\text { FP }_{}} \\ {\hat{Y}=0-} &amp; {\text { FN }_{}} &amp; {\text { TN }_{}}\end{array}
\]</span>
Now suppose we define 1 as a negative class:</p>
<p><span class="math display">\[
\begin{array}{ccc}{\text { Predicted vs. Reality}} &amp; {Y=0+} &amp; {Y=1-} \\ {\hat{Y}=0+} &amp; {\text { TP }_{}} &amp; {\text { FP }_{}} \\ {\hat{Y}=1-} &amp; {\text { FN }_{}} &amp; {\text { TN }_{}}\end{array}
\]</span>
Of course this is just a notational difference and nothing changes in calculations. But some performance measures, especially, sensitivity (TPR) and fall-out (FPR) will be different.</p>
<p>We are going to use the original train set again to avoid some data cleaning jobs that we mentioned in Chapter 5.</p>
<div class="sourceCode" id="cb616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb616-1"><a href="classification-2.html#cb616-1" tabindex="-1"></a><span class="co"># Download adult income data</span></span>
<span id="cb616-2"><a href="classification-2.html#cb616-2" tabindex="-1"></a></span>
<span id="cb616-3"><a href="classification-2.html#cb616-3" tabindex="-1"></a><span class="co"># url.train &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&quot;</span></span>
<span id="cb616-4"><a href="classification-2.html#cb616-4" tabindex="-1"></a><span class="co"># url.names &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&quot;</span></span>
<span id="cb616-5"><a href="classification-2.html#cb616-5" tabindex="-1"></a><span class="co"># download.file(url.train, destfile = &quot;adult_train.csv&quot;)</span></span>
<span id="cb616-6"><a href="classification-2.html#cb616-6" tabindex="-1"></a><span class="co"># download.file(url.names, destfile = &quot;adult_names.txt&quot;)</span></span>
<span id="cb616-7"><a href="classification-2.html#cb616-7" tabindex="-1"></a></span>
<span id="cb616-8"><a href="classification-2.html#cb616-8" tabindex="-1"></a><span class="co"># Read the training set into memory</span></span>
<span id="cb616-9"><a href="classification-2.html#cb616-9" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;adult_train.csv&quot;</span>, <span class="at">header =</span> <span class="cn">FALSE</span>)</span>
<span id="cb616-10"><a href="classification-2.html#cb616-10" tabindex="-1"></a></span>
<span id="cb616-11"><a href="classification-2.html#cb616-11" tabindex="-1"></a>varNames <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Age&quot;</span>, </span>
<span id="cb616-12"><a href="classification-2.html#cb616-12" tabindex="-1"></a>              <span class="st">&quot;WorkClass&quot;</span>,</span>
<span id="cb616-13"><a href="classification-2.html#cb616-13" tabindex="-1"></a>              <span class="st">&quot;fnlwgt&quot;</span>,</span>
<span id="cb616-14"><a href="classification-2.html#cb616-14" tabindex="-1"></a>              <span class="st">&quot;Education&quot;</span>,</span>
<span id="cb616-15"><a href="classification-2.html#cb616-15" tabindex="-1"></a>              <span class="st">&quot;EducationNum&quot;</span>,</span>
<span id="cb616-16"><a href="classification-2.html#cb616-16" tabindex="-1"></a>              <span class="st">&quot;MaritalStatus&quot;</span>,</span>
<span id="cb616-17"><a href="classification-2.html#cb616-17" tabindex="-1"></a>              <span class="st">&quot;Occupation&quot;</span>,</span>
<span id="cb616-18"><a href="classification-2.html#cb616-18" tabindex="-1"></a>              <span class="st">&quot;Relationship&quot;</span>,</span>
<span id="cb616-19"><a href="classification-2.html#cb616-19" tabindex="-1"></a>              <span class="st">&quot;Race&quot;</span>,</span>
<span id="cb616-20"><a href="classification-2.html#cb616-20" tabindex="-1"></a>              <span class="st">&quot;Sex&quot;</span>,</span>
<span id="cb616-21"><a href="classification-2.html#cb616-21" tabindex="-1"></a>              <span class="st">&quot;CapitalGain&quot;</span>,</span>
<span id="cb616-22"><a href="classification-2.html#cb616-22" tabindex="-1"></a>              <span class="st">&quot;CapitalLoss&quot;</span>,</span>
<span id="cb616-23"><a href="classification-2.html#cb616-23" tabindex="-1"></a>              <span class="st">&quot;HoursPerWeek&quot;</span>,</span>
<span id="cb616-24"><a href="classification-2.html#cb616-24" tabindex="-1"></a>              <span class="st">&quot;NativeCountry&quot;</span>,</span>
<span id="cb616-25"><a href="classification-2.html#cb616-25" tabindex="-1"></a>              <span class="st">&quot;IncomeLevel&quot;</span>)</span>
<span id="cb616-26"><a href="classification-2.html#cb616-26" tabindex="-1"></a></span>
<span id="cb616-27"><a href="classification-2.html#cb616-27" tabindex="-1"></a><span class="fu">names</span>(df) <span class="ot">&lt;-</span> varNames</span>
<span id="cb616-28"><a href="classification-2.html#cb616-28" tabindex="-1"></a>data <span class="ot">&lt;-</span> df</span></code></pre></div>
<p>In each machine learning application, the data preparation stage (i.e. cleaning the data, organizing the columns and rows, checking out the columns’ names, checking the types of each feature, identifying and handling the missing observations, etc) is a very important step and should be dealt with a good care.</p>
<p>First, let’s see if the data balanced or not:</p>
<div class="sourceCode" id="cb617"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb617-1"><a href="classification-2.html#cb617-1" tabindex="-1"></a>tbl <span class="ot">&lt;-</span> <span class="fu">table</span>(data<span class="sc">$</span>IncomeLevel)</span>
<span id="cb617-2"><a href="classification-2.html#cb617-2" tabindex="-1"></a>tbl</span></code></pre></div>
<pre><code>## 
##  &lt;=50K   &gt;50K 
##  24720   7841</code></pre>
<div class="sourceCode" id="cb619"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb619-1"><a href="classification-2.html#cb619-1" tabindex="-1"></a>tbl[<span class="dv">2</span>] <span class="sc">/</span> tbl[<span class="dv">1</span>]</span></code></pre></div>
<pre><code>##      &gt;50K 
## 0.3171926</code></pre>
<p>There are multiple variables that are <code>chr</code> in the data.</p>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb621-1"><a href="classification-2.html#cb621-1" tabindex="-1"></a><span class="fu">str</span>(data)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32561 obs. of  15 variables:
##  $ Age          : int  39 50 38 53 28 37 49 52 31 42 ...
##  $ WorkClass    : chr  &quot; State-gov&quot; &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; ...
##  $ fnlwgt       : int  77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ...
##  $ Education    : chr  &quot; Bachelors&quot; &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; ...
##  $ EducationNum : int  13 13 9 7 13 14 5 9 14 13 ...
##  $ MaritalStatus: chr  &quot; Never-married&quot; &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; ...
##  $ Occupation   : chr  &quot; Adm-clerical&quot; &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; ...
##  $ Relationship : chr  &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; ...
##  $ Race         : chr  &quot; White&quot; &quot; White&quot; &quot; White&quot; &quot; Black&quot; ...
##  $ Sex          : chr  &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; ...
##  $ CapitalGain  : int  2174 0 0 0 0 0 0 0 14084 5178 ...
##  $ CapitalLoss  : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ HoursPerWeek : int  40 13 40 40 40 40 16 45 50 40 ...
##  $ NativeCountry: chr  &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; ...
##  $ IncomeLevel  : chr  &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ...</code></pre>
<div class="sourceCode" id="cb623"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb623-1"><a href="classification-2.html#cb623-1" tabindex="-1"></a><span class="fu">table</span>(data<span class="sc">$</span>WorkClass)</span></code></pre></div>
<pre><code>## 
##                 ?       Federal-gov         Local-gov      Never-worked 
##              1836               960              2093                 7 
##           Private      Self-emp-inc  Self-emp-not-inc         State-gov 
##             22696              1116              2541              1298 
##       Without-pay 
##                14</code></pre>
<div class="sourceCode" id="cb625"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb625-1"><a href="classification-2.html#cb625-1" tabindex="-1"></a><span class="fu">table</span>(data<span class="sc">$</span>NativeCountry)</span></code></pre></div>
<pre><code>## 
##                           ?                    Cambodia 
##                         583                          19 
##                      Canada                       China 
##                         121                          75 
##                    Columbia                        Cuba 
##                          59                          95 
##          Dominican-Republic                     Ecuador 
##                          70                          28 
##                 El-Salvador                     England 
##                         106                          90 
##                      France                     Germany 
##                          29                         137 
##                      Greece                   Guatemala 
##                          29                          64 
##                       Haiti          Holand-Netherlands 
##                          44                           1 
##                    Honduras                        Hong 
##                          13                          20 
##                     Hungary                       India 
##                          13                         100 
##                        Iran                     Ireland 
##                          43                          24 
##                       Italy                     Jamaica 
##                          73                          81 
##                       Japan                        Laos 
##                          62                          18 
##                      Mexico                   Nicaragua 
##                         643                          34 
##  Outlying-US(Guam-USVI-etc)                        Peru 
##                          14                          31 
##                 Philippines                      Poland 
##                         198                          60 
##                    Portugal                 Puerto-Rico 
##                          37                         114 
##                    Scotland                       South 
##                          12                          80 
##                      Taiwan                    Thailand 
##                          51                          18 
##             Trinadad&amp;Tobago               United-States 
##                          19                       29170 
##                     Vietnam                  Yugoslavia 
##                          67                          16</code></pre>
<p>We can see that there is only one observation in <code>Holand-Netherlands</code>. This is a problem because it will be either in the training set or the test set. Therefore, when you estimate without taking care of it, it will give this error:</p>
<p><code>Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : factor NativeCountry has new levels Holand-Netherlands</code></p>
<p>We will see later how to take care of these issues in a loop with several error handling options. But now, let’s drop this observation:</p>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb627-1"><a href="classification-2.html#cb627-1" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">which</span>(data<span class="sc">$</span>NativeCountry <span class="sc">==</span><span class="st">&quot; Holand-Netherlands&quot;</span>)</span>
<span id="cb627-2"><a href="classification-2.html#cb627-2" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[<span class="sc">-</span>ind, ]</span></code></pre></div>
<p>Although some packages like <code>lm()</code> and <code>glm()</code> can use character variables, we should take care of them properly before any type of data analysis. Here is an example:</p>
<div class="sourceCode" id="cb628"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb628-1"><a href="classification-2.html#cb628-1" tabindex="-1"></a>df <span class="ot">&lt;-</span> data</span>
<span id="cb628-2"><a href="classification-2.html#cb628-2" tabindex="-1"></a><span class="co">#converting by a loop</span></span>
<span id="cb628-3"><a href="classification-2.html#cb628-3" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(df)) {</span>
<span id="cb628-4"><a href="classification-2.html#cb628-4" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">is.character</span>(df[, i]))</span>
<span id="cb628-5"><a href="classification-2.html#cb628-5" tabindex="-1"></a>    df[, i] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(df[, i])</span>
<span id="cb628-6"><a href="classification-2.html#cb628-6" tabindex="-1"></a>}</span>
<span id="cb628-7"><a href="classification-2.html#cb628-7" tabindex="-1"></a></span>
<span id="cb628-8"><a href="classification-2.html#cb628-8" tabindex="-1"></a>df <span class="ot">&lt;-</span> data</span>
<span id="cb628-9"><a href="classification-2.html#cb628-9" tabindex="-1"></a><span class="co">#Converting with `apply()` family</span></span>
<span id="cb628-10"><a href="classification-2.html#cb628-10" tabindex="-1"></a>df[<span class="fu">sapply</span>(df, is.character)] <span class="ot">&lt;-</span> <span class="fu">lapply</span>(df[<span class="fu">sapply</span>(df, is.character)],</span>
<span id="cb628-11"><a href="classification-2.html#cb628-11" tabindex="-1"></a>                                       as.factor)</span></code></pre></div>
<p>The job is to use LPM, Logistic, and kNN models to see which one could be a better predictive model for the data. In LPM and Logistic, we do not (yet) have any parameter to tune for a better prediction. Although we could use a degree of polynomials for selected features, we will set aside that option for now. We will later see regularization methods for parametric models, which will make LPM and logistic models “trainable”. In kNN, <span class="math inline">\(k\)</span> is the hyperparameter to train the model.</p>
<p>There are several key points to keep in mind in this classification practice:</p>
<ul>
<li>What performance metric(s) are we going to use for comparing the alternative models?</li>
<li>How are we going to transform the predicted probabilities to classes (0’s and 1’s) so that we can have the confusion matrix?</li>
</ul>
<p>Let’s start with LPM first.</p>
</div>
<div id="lpm-1" class="section level2 hasAnchor" number="21.12">
<h2><span class="header-section-number">21.12</span> LPM<a href="classification-2.html#lpm-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb629-1"><a href="classification-2.html#cb629-1" tabindex="-1"></a><span class="fu">anyNA</span>(data)</span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb631-1"><a href="classification-2.html#cb631-1" tabindex="-1"></a><span class="co"># Our LPM requires</span></span>
<span id="cb631-2"><a href="classification-2.html#cb631-2" tabindex="-1"></a>data<span class="sc">$</span>Y <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(data<span class="sc">$</span>IncomeLevel<span class="sc">==</span><span class="st">&quot; &lt;=50K&quot;</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb631-3"><a href="classification-2.html#cb631-3" tabindex="-1"></a>data <span class="ot">&lt;-</span> data[, <span class="sc">-</span><span class="dv">15</span>]</span></code></pre></div>
<p>Now, we are ready. We will use ROC and AUC for comparing the models.</p>
<div class="sourceCode" id="cb632"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb632-1"><a href="classification-2.html#cb632-1" tabindex="-1"></a><span class="fu">library</span>(ROCR)</span>
<span id="cb632-2"><a href="classification-2.html#cb632-2" tabindex="-1"></a></span>
<span id="cb632-3"><a href="classification-2.html#cb632-3" tabindex="-1"></a>AUC <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb632-4"><a href="classification-2.html#cb632-4" tabindex="-1"></a>t <span class="ot">=</span> <span class="dv">100</span> <span class="co"># number of times we loop</span></span>
<span id="cb632-5"><a href="classification-2.html#cb632-5" tabindex="-1"></a></span>
<span id="cb632-6"><a href="classification-2.html#cb632-6" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>t) {</span>
<span id="cb632-7"><a href="classification-2.html#cb632-7" tabindex="-1"></a>  <span class="fu">set.seed</span>(i)</span>
<span id="cb632-8"><a href="classification-2.html#cb632-8" tabindex="-1"></a>  shuffle <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(data), <span class="fu">nrow</span>(data), <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb632-9"><a href="classification-2.html#cb632-9" tabindex="-1"></a>  k <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb632-10"><a href="classification-2.html#cb632-10" tabindex="-1"></a>  testind <span class="ot">&lt;-</span> shuffle[<span class="dv">1</span><span class="sc">:</span>(<span class="fu">nrow</span>(data) <span class="sc">/</span> k)]</span>
<span id="cb632-11"><a href="classification-2.html#cb632-11" tabindex="-1"></a>  trainind <span class="ot">&lt;-</span> shuffle[<span class="sc">-</span>testind]</span>
<span id="cb632-12"><a href="classification-2.html#cb632-12" tabindex="-1"></a>  trdf <span class="ot">&lt;-</span> data[trainind, ] <span class="co">#80% of the data</span></span>
<span id="cb632-13"><a href="classification-2.html#cb632-13" tabindex="-1"></a>  tsdf <span class="ot">&lt;-</span> data[testind, ] <span class="co">#20% of data set a side</span></span>
<span id="cb632-14"><a href="classification-2.html#cb632-14" tabindex="-1"></a>  </span>
<span id="cb632-15"><a href="classification-2.html#cb632-15" tabindex="-1"></a>  <span class="co">#LPM</span></span>
<span id="cb632-16"><a href="classification-2.html#cb632-16" tabindex="-1"></a>  model1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Y <span class="sc">~</span> ., <span class="at">data =</span> trdf, <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>)</span>
<span id="cb632-17"><a href="classification-2.html#cb632-17" tabindex="-1"></a>  phat <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, tsdf)</span>
<span id="cb632-18"><a href="classification-2.html#cb632-18" tabindex="-1"></a>  phat[phat <span class="sc">&lt;</span> <span class="dv">0</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb632-19"><a href="classification-2.html#cb632-19" tabindex="-1"></a>  phat[phat <span class="sc">&gt;</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb632-20"><a href="classification-2.html#cb632-20" tabindex="-1"></a>  </span>
<span id="cb632-21"><a href="classification-2.html#cb632-21" tabindex="-1"></a>  <span class="co"># ROC &amp; AUC (from ROCR)</span></span>
<span id="cb632-22"><a href="classification-2.html#cb632-22" tabindex="-1"></a>  phat_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(phat, <span class="st">&quot;Y&quot;</span> <span class="ot">=</span> tsdf<span class="sc">$</span>Y)</span>
<span id="cb632-23"><a href="classification-2.html#cb632-23" tabindex="-1"></a>  pred_rocr <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat_df[, <span class="dv">1</span>], phat_df[, <span class="dv">2</span>])</span>
<span id="cb632-24"><a href="classification-2.html#cb632-24" tabindex="-1"></a>  </span>
<span id="cb632-25"><a href="classification-2.html#cb632-25" tabindex="-1"></a>  auc_ROCR <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb632-26"><a href="classification-2.html#cb632-26" tabindex="-1"></a>  AUC[i] <span class="ot">&lt;-</span> auc_ROCR<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span>
<span id="cb632-27"><a href="classification-2.html#cb632-27" tabindex="-1"></a>}</span>
<span id="cb632-28"><a href="classification-2.html#cb632-28" tabindex="-1"></a></span>
<span id="cb632-29"><a href="classification-2.html#cb632-29" tabindex="-1"></a><span class="fu">plot</span>(AUC, <span class="at">col =</span> <span class="st">&quot;grey&quot;</span>)</span>
<span id="cb632-30"><a href="classification-2.html#cb632-30" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="fu">mean</span>(AUC), <span class="at">b =</span> <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/tc18-1.png" width="672" /></p>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb633-1"><a href="classification-2.html#cb633-1" tabindex="-1"></a><span class="fu">mean</span>(AUC)</span></code></pre></div>
<pre><code>## [1] 0.8936181</code></pre>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb635-1"><a href="classification-2.html#cb635-1" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">var</span>(AUC))</span></code></pre></div>
<pre><code>## [1] 0.003810335</code></pre>
<p>Let’s see the ROC curve from the last run.</p>
<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb637-1"><a href="classification-2.html#cb637-1" tabindex="-1"></a><span class="co"># ROC from the last run by `ROCR`</span></span>
<span id="cb637-2"><a href="classification-2.html#cb637-2" tabindex="-1"></a>perf <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr, <span class="st">&quot;tpr&quot;</span>, <span class="st">&quot;fpr&quot;</span>)</span>
<span id="cb637-3"><a href="classification-2.html#cb637-3" tabindex="-1"></a><span class="fu">plot</span>(perf, <span class="at">colorize =</span> <span class="cn">TRUE</span>)</span>
<span id="cb637-4"><a href="classification-2.html#cb637-4" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/tc19-1.png" width="672" /></p>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb638-1"><a href="classification-2.html#cb638-1" tabindex="-1"></a><span class="co"># And our &quot;own&quot; ROC</span></span>
<span id="cb638-2"><a href="classification-2.html#cb638-2" tabindex="-1"></a>phator <span class="ot">&lt;-</span> phat[<span class="fu">order</span>(phat)]</span>
<span id="cb638-3"><a href="classification-2.html#cb638-3" tabindex="-1"></a>phator[phator <span class="sc">&lt;</span> <span class="dv">0</span>] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb638-4"><a href="classification-2.html#cb638-4" tabindex="-1"></a>phator[phator <span class="sc">&gt;</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb638-5"><a href="classification-2.html#cb638-5" tabindex="-1"></a>phator <span class="ot">&lt;-</span> <span class="fu">unique</span>(phator)</span>
<span id="cb638-6"><a href="classification-2.html#cb638-6" tabindex="-1"></a></span>
<span id="cb638-7"><a href="classification-2.html#cb638-7" tabindex="-1"></a>TPR <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb638-8"><a href="classification-2.html#cb638-8" tabindex="-1"></a>FPR <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb638-9"><a href="classification-2.html#cb638-9" tabindex="-1"></a></span>
<span id="cb638-10"><a href="classification-2.html#cb638-10" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(phator)) {</span>
<span id="cb638-11"><a href="classification-2.html#cb638-11" tabindex="-1"></a>  yHat <span class="ot">&lt;-</span> phat <span class="sc">&gt;</span> phator[i]</span>
<span id="cb638-12"><a href="classification-2.html#cb638-12" tabindex="-1"></a>  conf_table <span class="ot">&lt;-</span> <span class="fu">table</span>(yHat, tsdf<span class="sc">$</span>Y)</span>
<span id="cb638-13"><a href="classification-2.html#cb638-13" tabindex="-1"></a>  ct <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(conf_table)</span>
<span id="cb638-14"><a href="classification-2.html#cb638-14" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">sum</span>(<span class="fu">dim</span>(ct)) <span class="sc">&gt;</span> <span class="dv">3</span>) {</span>
<span id="cb638-15"><a href="classification-2.html#cb638-15" tabindex="-1"></a>    <span class="co">#here we ignore the min and max thresholds</span></span>
<span id="cb638-16"><a href="classification-2.html#cb638-16" tabindex="-1"></a>    TPR[i] <span class="ot">&lt;-</span> ct[<span class="dv">2</span>, <span class="dv">2</span>] <span class="sc">/</span> (ct[<span class="dv">2</span>, <span class="dv">2</span>] <span class="sc">+</span> ct[<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb638-17"><a href="classification-2.html#cb638-17" tabindex="-1"></a>    FPR[i] <span class="ot">&lt;-</span> ct[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">/</span> (ct[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span> ct[<span class="dv">2</span>, <span class="dv">1</span>])</span>
<span id="cb638-18"><a href="classification-2.html#cb638-18" tabindex="-1"></a>  }</span>
<span id="cb638-19"><a href="classification-2.html#cb638-19" tabindex="-1"></a>}</span>
<span id="cb638-20"><a href="classification-2.html#cb638-20" tabindex="-1"></a></span>
<span id="cb638-21"><a href="classification-2.html#cb638-21" tabindex="-1"></a><span class="co"># Flat and vertical sections are omitted</span></span>
<span id="cb638-22"><a href="classification-2.html#cb638-22" tabindex="-1"></a><span class="fu">plot</span>(FPR,</span>
<span id="cb638-23"><a href="classification-2.html#cb638-23" tabindex="-1"></a>     TPR,</span>
<span id="cb638-24"><a href="classification-2.html#cb638-24" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb638-25"><a href="classification-2.html#cb638-25" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;l&quot;</span>,</span>
<span id="cb638-26"><a href="classification-2.html#cb638-26" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;ROC&quot;</span>)</span>
<span id="cb638-27"><a href="classification-2.html#cb638-27" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="21-Classification_files/figure-html/tc19-2.png" width="672" /></p>
<p>What’s the confusion table at the “best” discriminating threshold? The answer is the one where the difference between TPR and FPR is maximized: <strong>Youden’s J Statistics</strong>. Note that this answers would be different if we have different weights in TPR and FPR. We may also have different targets, maximum FPR, for example.</p>
<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb639-1"><a href="classification-2.html#cb639-1" tabindex="-1"></a><span class="co"># Youden&#39;s J Statistics</span></span>
<span id="cb639-2"><a href="classification-2.html#cb639-2" tabindex="-1"></a>J <span class="ot">&lt;-</span> TPR <span class="sc">-</span> FPR</span>
<span id="cb639-3"><a href="classification-2.html#cb639-3" tabindex="-1"></a></span>
<span id="cb639-4"><a href="classification-2.html#cb639-4" tabindex="-1"></a><span class="co"># The best discriminating threshold</span></span>
<span id="cb639-5"><a href="classification-2.html#cb639-5" tabindex="-1"></a>opt_th <span class="ot">&lt;-</span> phator[<span class="fu">which.max</span>(J)]</span>
<span id="cb639-6"><a href="classification-2.html#cb639-6" tabindex="-1"></a>opt_th</span></code></pre></div>
<pre><code>## [1] 0.318723</code></pre>
<div class="sourceCode" id="cb641"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb641-1"><a href="classification-2.html#cb641-1" tabindex="-1"></a><span class="co">#TPR and FPR at this threshold</span></span>
<span id="cb641-2"><a href="classification-2.html#cb641-2" tabindex="-1"></a>TPR[<span class="fu">which.max</span>(J)]</span></code></pre></div>
<pre><code>## [1] 0.8494898</code></pre>
<div class="sourceCode" id="cb643"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb643-1"><a href="classification-2.html#cb643-1" tabindex="-1"></a>FPR[<span class="fu">which.max</span>(J)]</span></code></pre></div>
<pre><code>## [1] 0.2024676</code></pre>
<div class="sourceCode" id="cb645"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb645-1"><a href="classification-2.html#cb645-1" tabindex="-1"></a>J[<span class="fu">which.max</span>(J)]</span></code></pre></div>
<pre><code>## [1] 0.6470222</code></pre>
<p>And the confusion table (from the last run):</p>
<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb647-1"><a href="classification-2.html#cb647-1" tabindex="-1"></a>yHat <span class="ot">&lt;-</span> phat <span class="sc">&gt;</span> opt_th</span>
<span id="cb647-2"><a href="classification-2.html#cb647-2" tabindex="-1"></a>conf_table <span class="ot">&lt;-</span> <span class="fu">table</span>(yHat, tsdf<span class="sc">$</span>Y)</span>
<span id="cb647-3"><a href="classification-2.html#cb647-3" tabindex="-1"></a></span>
<span id="cb647-4"><a href="classification-2.html#cb647-4" tabindex="-1"></a><span class="co"># Function to rotate the table</span></span>
<span id="cb647-5"><a href="classification-2.html#cb647-5" tabindex="-1"></a>rot <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb647-6"><a href="classification-2.html#cb647-6" tabindex="-1"></a>  t <span class="ot">&lt;-</span> <span class="fu">apply</span>(x, <span class="dv">2</span>, rev)</span>
<span id="cb647-7"><a href="classification-2.html#cb647-7" tabindex="-1"></a>  tt <span class="ot">&lt;-</span> <span class="fu">apply</span>(t, <span class="dv">1</span>, rev)</span>
<span id="cb647-8"><a href="classification-2.html#cb647-8" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">t</span>(tt))</span>
<span id="cb647-9"><a href="classification-2.html#cb647-9" tabindex="-1"></a>}</span>
<span id="cb647-10"><a href="classification-2.html#cb647-10" tabindex="-1"></a></span>
<span id="cb647-11"><a href="classification-2.html#cb647-11" tabindex="-1"></a><span class="co"># Better looking table</span></span>
<span id="cb647-12"><a href="classification-2.html#cb647-12" tabindex="-1"></a>ct <span class="ot">&lt;-</span> <span class="fu">rot</span>(conf_table)</span>
<span id="cb647-13"><a href="classification-2.html#cb647-13" tabindex="-1"></a><span class="fu">rownames</span>(ct) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Yhat = 1&quot;</span>, <span class="st">&quot;Yhat = 0&quot;</span>)</span>
<span id="cb647-14"><a href="classification-2.html#cb647-14" tabindex="-1"></a><span class="fu">colnames</span>(ct) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Y = 1&quot;</span>, <span class="st">&quot;Y = 0&quot;</span>)</span>
<span id="cb647-15"><a href="classification-2.html#cb647-15" tabindex="-1"></a>ct</span></code></pre></div>
<pre><code>##           
## yHat       Y = 1 Y = 0
##   Yhat = 1  1332  1001
##   Yhat = 0   236  3943</code></pre>
<p>Note that the optimal threshold is almost the ratio of cases in the data around 31%. We will come back to this issue later.</p>
</div>
<div id="logistic-regression-1" class="section level2 hasAnchor" number="21.13">
<h2><span class="header-section-number">21.13</span> Logistic Regression<a href="classification-2.html#logistic-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>TBA FROM ORIGINAL CHAPTER</p>
<p>Both LPM and Logistic methods are linear classifiers. We can add polynomials and interactions manually to capture possible nonlinearities in the data but that would be an impossible job as the number of features would grow exponentially. This brings us to a nonparametric classifier, kNN.</p>
</div>
<div id="knn-1" class="section level2 hasAnchor" number="21.14">
<h2><span class="header-section-number">21.14</span> kNN<a href="classification-2.html#knn-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will train kNN with the choice of <span class="math inline">\(k\)</span> and use AUC as our performance criteria in choosing <span class="math inline">\(k\)</span>.</p>
<div id="knn-10-fold-cv" class="section level3 hasAnchor" number="21.14.1">
<h3><span class="header-section-number">21.14.1</span> kNN 10-fold CV<a href="classification-2.html#knn-10-fold-cv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are several packages in R for kNN applications: <code>knn()</code> from the <code>class</code> package and <code>knn3()</code> in the <code>caret</code> package. We will use <code>knn3()</code> in the caret package. Since kNN use distances, we should scale the numerical variables first to make their magnitudes on the same scale.</p>
<p>TBA FROM ORIGINAL CHAPTER</p>
<p>Now, it can also be used with <code>knn()</code> from the <code>class</code> package. Note that kNN gets unstable as the number of variables increases. We can see it by calculating test AUC multiple times by adding an outer loop to our algorithm.</p>
</div>
<div id="knn-with-caret-1" class="section level3 hasAnchor" number="21.14.2">
<h3><span class="header-section-number">21.14.2</span> kNN with <code>caret</code><a href="classification-2.html#knn-with-caret-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>TBA FROM ORIGINAL CHAPTER</p>
<p>We now know two things: (1) how good the prediction is with kNN; (2) how good it is relative to other “base” or “benchmark” models. These two questions must be answered every time to evaluate the prediction performance of a machine learning algorithm. Although we didn’t calculate the test AUC in our own kNN algorithm, we can accept that kNN performance is good with AUC that is close to 90%. However, it is not significantly better than LPM and Logistic</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="model-selection-and-sparsity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="time-series.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuk/machinemetrics/edit/master/21-Classification.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
