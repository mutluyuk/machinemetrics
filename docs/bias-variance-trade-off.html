<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Bias-Variance Trade-off | Causal MachineMetrics</title>
  <meta name="description" content="Chapter 6 Bias-Variance Trade-off | Causal MachineMetrics" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Bias-Variance Trade-off | Causal MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Bias-Variance Trade-off | Causal MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="error.html"/>
<link rel="next" href="overfitting.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html"><i class="fa fa-check"></i><b>2</b> Spectrum of Data Modeling:</a>
<ul>
<li class="chapter" data-level="2.1" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html#statistical-vs.-machine-learning-approaches"><i class="fa fa-check"></i><b>2.1</b> Statistical vs. Machine Learning Approaches:</a></li>
<li class="chapter" data-level="2.2" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models:</a></li>
<li class="chapter" data-level="2.4" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html#model-selection"><i class="fa fa-check"></i><b>2.4</b> Model Selection:</a></li>
<li class="chapter" data-level="2.5" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation:</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>3.5.3</b> Maximum Likelihood Estimation (MLE)}</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error"><i class="fa fa-check"></i><b>5.1</b> Estimation error</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#efficiency"><i class="fa fa-check"></i><b>5.2</b> Efficiency</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#mean-square-error"><i class="fa fa-check"></i><b>5.3</b> Mean Square Error</a></li>
<li class="chapter" data-level="5.4" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.4</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.5" data-path="error.html"><a href="error.html#technical-points-and-proofs"><i class="fa fa-check"></i><b>5.5</b> Technical points and proofs</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#formal-definition"><i class="fa fa-check"></i><b>6.1</b> Formal Definition</a></li>
<li class="chapter" data-level="6.2" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#simulated-breakdown-of-the-mspe"><i class="fa fa-check"></i><b>6.2</b> Simulated Breakdown of the MSPE</a></li>
<li class="chapter" data-level="6.3" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.3</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#the-dichotomy-of-statistical-modeling-data-versus-algorithmic-approaches"><i class="fa fa-check"></i><b>9.1</b> The Dichotomy of Statistical Modeling: Data versus Algorithmic Approaches:</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.2</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.3</b> LPM</a></li>
<li class="chapter" data-level="9.4" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.4</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.2</b> Pruning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.3</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.2</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.2.1</b> AdaBoost</a></li>
<li class="chapter" data-level="17.2.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.2.2</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.3</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.4</b> Classification</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.5</b> Regression</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.6</b> Exploration</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.7</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.7.1</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.7.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.7.2</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.7.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.7.3</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection-1"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-1.html"><a href="classification-1.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-1.html"><a href="classification-1.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-1.html"><a href="classification-1.html#linear-classifiers"><i class="fa fa-check"></i><b>21.2</b> Linear classifiers</a></li>
<li class="chapter" data-level="21.3" data-path="classification-1.html"><a href="classification-1.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.4" data-path="classification-1.html"><a href="classification-1.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.4</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.5" data-path="classification-1.html"><a href="classification-1.html#confusion-matrix"><i class="fa fa-check"></i><b>21.5</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.6" data-path="classification-1.html"><a href="classification-1.html#performance-measures"><i class="fa fa-check"></i><b>21.6</b> Performance measures</a></li>
<li class="chapter" data-level="21.7" data-path="classification-1.html"><a href="classification-1.html#roc-curve"><i class="fa fa-check"></i><b>21.7</b> ROC Curve</a></li>
<li class="chapter" data-level="21.8" data-path="classification-1.html"><a href="classification-1.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.8</b> AUC - Area Under the Curve</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html"><i class="fa fa-check"></i><b>22</b> Causal Inference for Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.5</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.6" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.6</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.7</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.8" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#speed"><i class="fa fa-check"></i><b>22.8</b> Speed</a></li>
<li class="chapter" data-level="22.9" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#ci-for-ts"><i class="fa fa-check"></i><b>22.9</b> CI for TS</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="causal-forecasting.html"><a href="causal-forecasting.html"><i class="fa fa-check"></i><b>23</b> Causal Forecasting</a>
<ul>
<li class="chapter" data-level="23.1" data-path="causal-forecasting.html"><a href="causal-forecasting.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="causal-forecasting.html"><a href="causal-forecasting.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="causal-forecasting.html"><a href="causal-forecasting.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="causal-forecasting.html"><a href="causal-forecasting.html#random-forest"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="causal-forecasting.html"><a href="causal-forecasting.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.5</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="ate-with-support-vector-machine.html"><a href="ate-with-support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> ATE with Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="ate-with-support-vector-machine.html"><a href="ate-with-support-vector-machine.html#support-vector-machine"><i class="fa fa-check"></i><b>24.1</b> Support Vector Machine</a></li>
<li class="chapter" data-level="24.2" data-path="ate-with-support-vector-machine.html"><a href="ate-with-support-vector-machine.html#ate-with-svm"><i class="fa fa-check"></i><b>24.2</b> ATE with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.4</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.5</b> Regularized Covariance Matrix</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.4</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="causal-component-analysis.html"><a href="causal-component-analysis.html"><i class="fa fa-check"></i><b>29</b> Causal Component Analysis</a>
<ul>
<li class="chapter" data-level="29.1" data-path="causal-component-analysis.html"><a href="causal-component-analysis.html#pca-principle-component-analysis"><i class="fa fa-check"></i><b>29.1</b> PCA (Principle Component Analysis)</a></li>
<li class="chapter" data-level="29.2" data-path="causal-component-analysis.html"><a href="causal-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.2</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.1</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.2</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a></li>
<li class="chapter" data-level="32" data-path="text-based-causal-inference.html"><a href="text-based-causal-inference.html"><i class="fa fa-check"></i><b>32</b> Text-based Causal Inference</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.1</b> Regression splines</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.2</b> MARS</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.3</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuksel/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Causal MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bias-variance-trade-off" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Bias-Variance Trade-off<a href="bias-variance-trade-off.html#bias-variance-trade-off" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this chapter, we explore the bias-variance tradeoff and its critical role in model performance. In this chapter, we still assume prediction functions (i.e. we assume we know the prediction functions in simulations) to find MSPE using the given sample dataset. We use single data set to calculate MSPE for that given data set (in-sample MSPE for a test data). In this chapter, we show calculating bias-variance trade-off for each assumed function using the sample data. Bias-Variance Trade-off, while complex, is reflected in everyday life situations, offering insights into its practical significance. We begin with the story of Baran, whose experiences in planning his biweekly work trips vividly illustrate this tradeoff.</p>
<p>Baran aims to catch a flight for his biweekly work trips, with the gate closing at 8 AM. During his first year, he developed a routine to leave his house around 7:00 AM, planning to depart either 30 minutes before or after this time. However, he discovered that this approach sometimes led to him missing his flight. Even though his average or expected departure time from home was 7:00 AM, this process occasionally resulted in arrival times after the flight’s gate closure, making his goal of catching the flight more error-prone. This approach represented a scenario with higher variance and lower bias, leading to costly errors concerning his airport arrival time and missed flights. In contrast, with his revised preparation routine, Baran aimed to leave around 7:00 AM but allowed a margin of 10 minutes either way. Consequently, his average departure time shifted to 7:05 AM, introducing a 5-minute bias. However, this new strategy led to more consistent success in catching his flight, reflecting higher bias but lower variance. This approach nearly eliminated all instances of missing the flight. This whole experience demonstrates the bias-variance tradeoff. In the first case, a preparation process with higher variance and lower bias resulted in occasional flight misses. In the second case, adopting a process with reduced variance, even though it introduces a degree of bias, ultimately enhances the overall result. Baran’s experience illustrates that consistently opting for lower bias (even unbiased), is not always the best strategy to predict his arrival time, as a balance between bias and variance can lead to more effective real-world results.</p>
<p>As seen from Baran’s narrative, the bias-variance tradeoff is not an abstract statistical concept but a tangible factor in our everyday decision-making processes. Grasping this balance is key to understanding how to craft models that are not only accurate but also reliable across various situations. In the next chapter, we’ll explore another critical concept in machine learning: overfitting. This concept is also very crucial for crafting precise and dependable models. We will explore how overfitting relates to and differs from the bias-variance tradeoff. This exploration will further enhance our understanding of model development, providing deeper insights into creating effective and reliable machine learning algorithms.</p>
<p>We already discuss bias and variance of estimator and predictor, and decomposition of MSE for estimation and MSPE for prediction in the previous chapter. Now, Lets discuss what is bias and variance and trade-off between them in predictive models. The bias-variance tradeoff is a fundamental concept in statistical learning and machine learning, which deals with the problem of minimizing and balancing two sources of error that can affect the performance of a model. Bias occurs from oversimplifying the model, leading to underfitting and missing relevant relations in the data. Variance occurs from overcomplicating the model, leading to overfitting and capturing random noise instead of the intended outputs. High bias results in poor performance on training data, while high variance causes poor generalization to new data. The goal is to find a balance, creating a model that is complex enough to capture true patterns but simple enough to generalize well. This balance is achieved through choosing appropriate model complexity, using techniques like cross-validation, applying regularization, and refining data or features.</p>
<p>Following is a formal discussion of bias-variance tradeoff.</p>
<div id="formal-definition" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Formal Definition<a href="bias-variance-trade-off.html#formal-definition" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To remind,our task is prediction of an outcome, Y using the data (more accurately test/train data) .</p>
<p>To “predict” Y using features X, means to find some <span class="math inline">\(f\)</span> which is close to <span class="math inline">\(Y\)</span>. We assume that <span class="math inline">\(Y\)</span> is some function of <span class="math inline">\(X\)</span> plus some random noise.</p>
<p><span class="math display">\[Y=f(X)+ \epsilon\]</span>
However, we can never know real <span class="math inline">\(f(X)\)</span>. Thus our goal becomes to finding some <span class="math inline">\(\hat{f(X)}\)</span> that is a good estimate of the regression function <span class="math inline">\(f(X)\)</span>. There will be always difference between real <span class="math inline">\(f(X)\)</span> and <span class="math inline">\(\hat{f(X)}\)</span>. That difference is called reducible error. We find a good estimate of <span class="math inline">\(f(X)\)</span> by reducing the expected mean square of error of test data as much as possible. Thus, we can write</p>
<p><strong>MSE for prediction function using training data</strong> (for test data, validation sample):</p>
<p><span class="math display">\[
\operatorname{MSE}(f(x), \hat{f}(x))= \underbrace{(\mathbb{E}[f(x)-\hat{f}(x)])^2}_{\operatorname{bias}^2(\hat{f}(x))}+\underbrace{\mathbb{E}\left[(\hat{f}(x)-\mathbb{E}[\hat{f}(x)])^2\right]}_{\operatorname{var}(\hat{f}(x))}
\]</span></p>
<p>Then,</p>
<p><strong>Mean Square of Prediction Error</strong> can be written as:</p>
<p><span class="math display">\[
\text{MSPE}=\mathbb{E}\left[(Y-\hat{f(X)})^{2}\right]=(\text{Bias}[\hat{f(X)}])^{2}+\text{Var}[\hat{f(X)}]+\sigma^{2}
\]</span>
<span class="math inline">\(\sigma^{2}=E[\varepsilon^{2}]\)</span></p>
<p>The expected mean-squared prediction error (MSPE) on the validation/training sample consists of the sum of the squared bias of the fit and the variance of both the fit and the error/noise term. The error term <span class="math inline">\(\sigma^2\)</span>, also referred to as irreducible error or uncertainty, represents the variance of the target variable <span class="math inline">\(Y\)</span> around its true mean <span class="math inline">\(f(x)\)</span>. It is inherent in the problem and does not depend on the model or training data. When the data generation process is known, <span class="math inline">\(\sigma^2\)</span> can be determined. Alternatively, estimation of <span class="math inline">\(\sigma^2\)</span> is possible using the sample variance of <span class="math inline">\(y\)</span> at duplicated (or nearby) inputs <span class="math inline">\(x\)</span>.</p>
<p><strong>Variance</strong> is the amount by which <span class="math inline">\(\hat{f(X)}\)</span> could change if we estimated it using different test/training data set. <span class="math inline">\(\hat{f(X)}\)</span> depends on the training data. (More complete notation would be <span class="math inline">\(\hat{f}(X; Y_{train},X_{train})\)</span> )If the <span class="math inline">\(\hat{f(X)}\)</span> is less complex/less flexible, then it is more likely to change if we use different samples to estimate it. However, if <span class="math inline">\(\hat{f(X)}\)</span> is more complex/more flexible function, then it is more likely to change between different test/training samples.</p>
<p>For instance, Lets assume we have a data set (test or training data) and we want to “predict” Y using features X, thus estimate function <span class="math inline">\(f(X)\)</span>. and lets assume we have 1-degree and 10-degree polynomial functions as a potential prediction functions. We say <span class="math inline">\(\hat{f(X)}=\hat{\beta_{0}} + \hat{\beta_{1}} X\)</span> is less complex than 10-degree polynomial <span class="math inline">\(\hat{f(X)}=\hat{\beta_{0}} + \hat{\beta_{1}} X...+ \hat{\beta_{10}} X^{10}\)</span> function. As, 10-degree polynomial function has more parameters <span class="math inline">\(\beta_{0},..\beta_{10}\)</span>, it is more flexible. That also means it has high variance (As it has more parameters, all these parameters are more inclined to have different values in different training data sets). Thus, a prediction function has high variance if it can change substantially when we use different training samples to estimate <span class="math inline">\(f(X)\)</span>. We can also say less flexible functions (functions with less parameters) have low variance. Low variance functions are less likely to change when we use different training sample or adding new data to the test sample. We will show all these with simulation in overfitting chapter as well.</p>
<p><strong>Bias</strong> is the difference between the real! prediction function and expected estimated function. If the <span class="math inline">\(\hat{f(X)}\)</span> is less flexible, then it is more likely to have higher bias. We can think this as real function(reality) is always more complex than the function approximates the reality. So, it is more prone to have higher error, i.e. more bias.</p>
<p>In the context of regression, Parametric models are biased when the form of the model does not incorporate all the necessary variables, or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic. In non-parametric models when the model provides too much smoothing.</p>
<p>There is a bias-variance tradeoff. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. Flexible(i.e. complex) models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance.</p>
<p>So for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate flexibility for the data. However, this selected model should not overfit the data as well which we will discuss in the next chapter. Read <a href="https://threadreaderapp.com/thread/1584515105374339073.html">1</a> and <a href="https://www.simplilearn.com/tutorials/machine-learning-tutorial/bias-and-variance">2</a></p>
</div>
<div id="simulated-breakdown-of-the-mspe" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Simulated Breakdown of the MSPE<a href="bias-variance-trade-off.html#simulated-breakdown-of-the-mspe" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Although the variance-bias trade-off conceptually seems intuitive, at least from a mathematical standpoint, another practical question arises: Is it possible to observe the components of the decomposed Mean Squared Prediction Error (MSPE)? In real-world data, observing these components is challenging as we do not know the true function and irreducible error. However, we can illustrate this concept through simulations.</p>
<p>For our analysis, we revisit the example discussed earlier. We consider years of schooling, varying between 9 and 16 years, as our variable of interest. From this ‘population’, we repeatedly take random samples. The objective now is to utilize each sample to develop a predictor or a set of prediction rules. These rules aim to predict a number or a series of numbers (years of schooling in this simulation) that are also drawn from the same population. By doing so, we can effectively simulate and analyze the decomposition of the MSPE, providing a clearer understanding of its components and their interplay.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="bias-variance-trade-off.html#cb55-1" tabindex="-1"></a><span class="co"># Here is our population</span></span>
<span id="cb55-2"><a href="bias-variance-trade-off.html#cb55-2" tabindex="-1"></a>populationX <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">11</span>,<span class="dv">12</span>,<span class="dv">13</span>,<span class="dv">14</span>,<span class="dv">15</span>,<span class="dv">16</span>)</span>
<span id="cb55-3"><a href="bias-variance-trade-off.html#cb55-3" tabindex="-1"></a></span>
<span id="cb55-4"><a href="bias-variance-trade-off.html#cb55-4" tabindex="-1"></a></span>
<span id="cb55-5"><a href="bias-variance-trade-off.html#cb55-5" tabindex="-1"></a><span class="co">#Let&#39;s have a containers to have repeated samples (2000)</span></span>
<span id="cb55-6"><a href="bias-variance-trade-off.html#cb55-6" tabindex="-1"></a>Ms <span class="ot">&lt;-</span> <span class="dv">5000</span></span>
<span id="cb55-7"><a href="bias-variance-trade-off.html#cb55-7" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, Ms, <span class="dv">10</span>)</span>
<span id="cb55-8"><a href="bias-variance-trade-off.html#cb55-8" tabindex="-1"></a><span class="fu">colnames</span>(samples) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>, <span class="st">&quot;X3&quot;</span>, <span class="st">&quot;X4&quot;</span>, <span class="st">&quot;X5&quot;</span>, <span class="st">&quot;X6&quot;</span>, <span class="st">&quot;X7&quot;</span>, <span class="st">&quot;X8&quot;</span>, <span class="st">&quot;X9&quot;</span>, <span class="st">&quot;X10&quot;</span>)</span>
<span id="cb55-9"><a href="bias-variance-trade-off.html#cb55-9" tabindex="-1"></a></span>
<span id="cb55-10"><a href="bias-variance-trade-off.html#cb55-10" tabindex="-1"></a><span class="co"># Let&#39;s have samples (with replacement always)</span></span>
<span id="cb55-11"><a href="bias-variance-trade-off.html#cb55-11" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb55-12"><a href="bias-variance-trade-off.html#cb55-12" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(samples)) {</span>
<span id="cb55-13"><a href="bias-variance-trade-off.html#cb55-13" tabindex="-1"></a>  samples[i,] <span class="ot">&lt;-</span> <span class="fu">sample</span>(populationX, <span class="dv">10</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb55-14"><a href="bias-variance-trade-off.html#cb55-14" tabindex="-1"></a>}</span>
<span id="cb55-15"><a href="bias-variance-trade-off.html#cb55-15" tabindex="-1"></a><span class="fu">head</span>(samples)</span></code></pre></div>
<pre><code>##      X1 X2 X3 X4 X5 X6 X7 X8 X9 X10
## [1,] 15 15 11 14 11 10 10 14 11  13
## [2,] 12 14 14  9 10 11 16 13 11  11
## [3,]  9 12  9  9 13 11 16 10 15  10
## [4,]  9 14 11 12 14  9 11 15 13  12
## [5,] 15 16 10 13 15  9  9 10 15  11
## [6,] 12 13 15 13 11 16 14  9 10  13</code></pre>
<p>Now, Let’s use our predictors:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="bias-variance-trade-off.html#cb57-1" tabindex="-1"></a><span class="co"># Container to record all predictions</span></span>
<span id="cb57-2"><a href="bias-variance-trade-off.html#cb57-2" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, Ms, <span class="dv">2</span>)</span>
<span id="cb57-3"><a href="bias-variance-trade-off.html#cb57-3" tabindex="-1"></a></span>
<span id="cb57-4"><a href="bias-variance-trade-off.html#cb57-4" tabindex="-1"></a><span class="co"># fhat_1 = 10</span></span>
<span id="cb57-5"><a href="bias-variance-trade-off.html#cb57-5" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Ms) {</span>
<span id="cb57-6"><a href="bias-variance-trade-off.html#cb57-6" tabindex="-1"></a>  predictions[i,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb57-7"><a href="bias-variance-trade-off.html#cb57-7" tabindex="-1"></a>}</span>
<span id="cb57-8"><a href="bias-variance-trade-off.html#cb57-8" tabindex="-1"></a></span>
<span id="cb57-9"><a href="bias-variance-trade-off.html#cb57-9" tabindex="-1"></a><span class="co"># fhat_2 - mean</span></span>
<span id="cb57-10"><a href="bias-variance-trade-off.html#cb57-10" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Ms) {</span>
<span id="cb57-11"><a href="bias-variance-trade-off.html#cb57-11" tabindex="-1"></a>  predictions[i,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">sum</span>(samples[i,])<span class="sc">/</span><span class="fu">length</span>(samples[i,])</span>
<span id="cb57-12"><a href="bias-variance-trade-off.html#cb57-12" tabindex="-1"></a>}</span>
<span id="cb57-13"><a href="bias-variance-trade-off.html#cb57-13" tabindex="-1"></a></span>
<span id="cb57-14"><a href="bias-variance-trade-off.html#cb57-14" tabindex="-1"></a><span class="fu">head</span>(predictions)</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]   10 12.4
## [2,]   10 12.1
## [3,]   10 11.4
## [4,]   10 12.0
## [5,]   10 12.3
## [6,]   10 12.6</code></pre>
<p>Now let’s have our MSPE decomposition:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="bias-variance-trade-off.html#cb59-1" tabindex="-1"></a><span class="co"># MSPE</span></span>
<span id="cb59-2"><a href="bias-variance-trade-off.html#cb59-2" tabindex="-1"></a>MSPE <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, Ms, <span class="dv">2</span>)</span>
<span id="cb59-3"><a href="bias-variance-trade-off.html#cb59-3" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Ms) {</span>
<span id="cb59-4"><a href="bias-variance-trade-off.html#cb59-4" tabindex="-1"></a>  MSPE[i,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>((populationX<span class="sc">-</span>predictions[i,<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb59-5"><a href="bias-variance-trade-off.html#cb59-5" tabindex="-1"></a>  MSPE[i,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>((populationX<span class="sc">-</span>predictions[i,<span class="dv">2</span>])<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb59-6"><a href="bias-variance-trade-off.html#cb59-6" tabindex="-1"></a>}</span>
<span id="cb59-7"><a href="bias-variance-trade-off.html#cb59-7" tabindex="-1"></a><span class="fu">head</span>(MSPE)</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,] 11.5 5.26
## [2,] 11.5 5.41
## [3,] 11.5 6.46
## [4,] 11.5 5.50
## [5,] 11.5 5.29
## [6,] 11.5 5.26</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="bias-variance-trade-off.html#cb61-1" tabindex="-1"></a><span class="co"># Bias</span></span>
<span id="cb61-2"><a href="bias-variance-trade-off.html#cb61-2" tabindex="-1"></a>bias1 <span class="ot">&lt;-</span> <span class="fu">mean</span>(populationX)<span class="sc">-</span><span class="fu">mean</span>(predictions[,<span class="dv">1</span>])</span>
<span id="cb61-3"><a href="bias-variance-trade-off.html#cb61-3" tabindex="-1"></a>bias2 <span class="ot">&lt;-</span> <span class="fu">mean</span>(populationX)<span class="sc">-</span><span class="fu">mean</span>(predictions[,<span class="dv">2</span>])</span>
<span id="cb61-4"><a href="bias-variance-trade-off.html#cb61-4" tabindex="-1"></a></span>
<span id="cb61-5"><a href="bias-variance-trade-off.html#cb61-5" tabindex="-1"></a><span class="co"># Variance (predictor)</span></span>
<span id="cb61-6"><a href="bias-variance-trade-off.html#cb61-6" tabindex="-1"></a>var1 <span class="ot">&lt;-</span> <span class="fu">var</span>(predictions[,<span class="dv">1</span>])</span>
<span id="cb61-7"><a href="bias-variance-trade-off.html#cb61-7" tabindex="-1"></a>var1</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="bias-variance-trade-off.html#cb63-1" tabindex="-1"></a>var2 <span class="ot">&lt;-</span> <span class="fu">var</span>(predictions[,<span class="dv">2</span>])</span>
<span id="cb63-2"><a href="bias-variance-trade-off.html#cb63-2" tabindex="-1"></a>var2</span></code></pre></div>
<pre><code>## [1] 0.5385286</code></pre>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="bias-variance-trade-off.html#cb65-1" tabindex="-1"></a><span class="co"># Variance (epsilon)</span></span>
<span id="cb65-2"><a href="bias-variance-trade-off.html#cb65-2" tabindex="-1"></a>var_eps <span class="ot">&lt;-</span> <span class="fu">mean</span>((populationX<span class="fl">-12.5</span>)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb65-3"><a href="bias-variance-trade-off.html#cb65-3" tabindex="-1"></a>var_eps</span></code></pre></div>
<pre><code>## [1] 5.25</code></pre>
<p>Let’s put them in a table:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="bias-variance-trade-off.html#cb67-1" tabindex="-1"></a>VBtradeoff <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb67-2"><a href="bias-variance-trade-off.html#cb67-2" tabindex="-1"></a><span class="fu">rownames</span>(VBtradeoff) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;fhat_1&quot;</span>, <span class="st">&quot;fhat_2&quot;</span>)</span>
<span id="cb67-3"><a href="bias-variance-trade-off.html#cb67-3" tabindex="-1"></a><span class="fu">colnames</span>(VBtradeoff) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Bias&quot;</span>, <span class="st">&quot;Var(fhat)&quot;</span>, <span class="st">&quot;Var(eps)&quot;</span>, <span class="st">&quot;MSPE&quot;</span>)</span>
<span id="cb67-4"><a href="bias-variance-trade-off.html#cb67-4" tabindex="-1"></a>VBtradeoff[<span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> bias1<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb67-5"><a href="bias-variance-trade-off.html#cb67-5" tabindex="-1"></a>VBtradeoff[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> bias2<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb67-6"><a href="bias-variance-trade-off.html#cb67-6" tabindex="-1"></a>VBtradeoff[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> var1</span>
<span id="cb67-7"><a href="bias-variance-trade-off.html#cb67-7" tabindex="-1"></a>VBtradeoff[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> var2</span>
<span id="cb67-8"><a href="bias-variance-trade-off.html#cb67-8" tabindex="-1"></a>VBtradeoff[<span class="dv">1</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> var_eps</span>
<span id="cb67-9"><a href="bias-variance-trade-off.html#cb67-9" tabindex="-1"></a>VBtradeoff[<span class="dv">2</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> var_eps</span>
<span id="cb67-10"><a href="bias-variance-trade-off.html#cb67-10" tabindex="-1"></a>VBtradeoff[<span class="dv">1</span>,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(MSPE[,<span class="dv">1</span>])</span>
<span id="cb67-11"><a href="bias-variance-trade-off.html#cb67-11" tabindex="-1"></a>VBtradeoff[<span class="dv">2</span>,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(MSPE[,<span class="dv">2</span>])</span>
<span id="cb67-12"><a href="bias-variance-trade-off.html#cb67-12" tabindex="-1"></a><span class="fu">round</span>(VBtradeoff, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##        Bias Var(fhat) Var(eps)   MSPE
## fhat_1 6.25     0.000     5.25 11.500
## fhat_2 0.00     0.539     5.25  5.788</code></pre>
<p>This table clearly shows the decomposition of MSPE. The first column is the contribution to the MSPE from the bias, and the second column is the contribution from the variance of the predictor. These together make up the reducible error. The third column is the variance that comes from the data, the irreducible error. The last column is, of course, the total MSPE, and we can see that <span class="math inline">\(\hat{f}_2\)</span> is the better predictor because of its lower MSPE.</p>
<p>Here is an example that shows the trade-off between bias and variance using prediction functions in a simulation.</p>
<p>In a seminal study, Angrist and Krueger (1991) tackled the problems associated with ability bias in Mincer’s equation by creating an instrumental variable based on students’ quarter of birth. The data is provided as “ak91” in the package “masteringmetrics”. Instead of using actual data, we simulate only education and income data using the same method of moments. One purpose is to show that we may perform simulations using only descriptive tables and regression results even if we do not have the actual data, allowing us to discuss the underlying concepts effectively.</p>
<p>In the simulation below, we generate a dataset based on the descriptive statistics of years of schooling (s) and the logarithm of wage (lnw). We then create four different prediction functions to evaluate their MSPE, bias, and variance.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="bias-variance-trade-off.html#cb69-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb69-2"><a href="bias-variance-trade-off.html#cb69-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb69-3"><a href="bias-variance-trade-off.html#cb69-3" tabindex="-1"></a></span>
<span id="cb69-4"><a href="bias-variance-trade-off.html#cb69-4" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb69-5"><a href="bias-variance-trade-off.html#cb69-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">26732</span></span>
<span id="cb69-6"><a href="bias-variance-trade-off.html#cb69-6" tabindex="-1"></a>mean_lnw <span class="ot">&lt;-</span> <span class="fl">5.9</span></span>
<span id="cb69-7"><a href="bias-variance-trade-off.html#cb69-7" tabindex="-1"></a>sd_lnw <span class="ot">&lt;-</span> <span class="fl">0.7</span></span>
<span id="cb69-8"><a href="bias-variance-trade-off.html#cb69-8" tabindex="-1"></a>mean_s <span class="ot">&lt;-</span> <span class="fl">12.8</span></span>
<span id="cb69-9"><a href="bias-variance-trade-off.html#cb69-9" tabindex="-1"></a>sd_s <span class="ot">&lt;-</span> <span class="fl">3.3</span></span>
<span id="cb69-10"><a href="bias-variance-trade-off.html#cb69-10" tabindex="-1"></a>min_s <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb69-11"><a href="bias-variance-trade-off.html#cb69-11" tabindex="-1"></a>max_s <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb69-12"><a href="bias-variance-trade-off.html#cb69-12" tabindex="-1"></a></span>
<span id="cb69-13"><a href="bias-variance-trade-off.html#cb69-13" tabindex="-1"></a><span class="co"># Generate schooling data</span></span>
<span id="cb69-14"><a href="bias-variance-trade-off.html#cb69-14" tabindex="-1"></a><span class="co">#schooling &lt;- runif(n, min = min_s, max = max_s)</span></span>
<span id="cb69-15"><a href="bias-variance-trade-off.html#cb69-15" tabindex="-1"></a>  <span class="co"># Generate schooling data and round to nearest 0.5</span></span>
<span id="cb69-16"><a href="bias-variance-trade-off.html#cb69-16" tabindex="-1"></a>  schooling <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">runif</span>(n, <span class="at">min =</span> min_s, <span class="at">max =</span> max_s) <span class="sc">*</span> <span class="dv">2</span>) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb69-17"><a href="bias-variance-trade-off.html#cb69-17" tabindex="-1"></a></span>
<span id="cb69-18"><a href="bias-variance-trade-off.html#cb69-18" tabindex="-1"></a><span class="co"># Generate lnw data using the provided regression coefficients</span></span>
<span id="cb69-19"><a href="bias-variance-trade-off.html#cb69-19" tabindex="-1"></a><span class="co"># Intercept = 5, Coefficient of s = 0.07</span></span>
<span id="cb69-20"><a href="bias-variance-trade-off.html#cb69-20" tabindex="-1"></a>lnw <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fl">0.07</span> <span class="sc">*</span> schooling <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sd_lnw)</span>
<span id="cb69-21"><a href="bias-variance-trade-off.html#cb69-21" tabindex="-1"></a></span>
<span id="cb69-22"><a href="bias-variance-trade-off.html#cb69-22" tabindex="-1"></a><span class="co"># Create data frame</span></span>
<span id="cb69-23"><a href="bias-variance-trade-off.html#cb69-23" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">schooling =</span> schooling, <span class="at">lnw =</span> lnw)</span>
<span id="cb69-24"><a href="bias-variance-trade-off.html#cb69-24" tabindex="-1"></a></span>
<span id="cb69-25"><a href="bias-variance-trade-off.html#cb69-25" tabindex="-1"></a><span class="co"># Define prediction functions for log income</span></span>
<span id="cb69-26"><a href="bias-variance-trade-off.html#cb69-26" tabindex="-1"></a>predictor1 <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="fu">mean</span>(lnw), n)  <span class="co"># Constant predictor based on the mean of lnw</span></span>
<span id="cb69-27"><a href="bias-variance-trade-off.html#cb69-27" tabindex="-1"></a>predictor2 <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fl">0.10</span> <span class="sc">*</span> schooling <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fl">0.1</span>)  <span class="co"># Linear predictor with noise</span></span>
<span id="cb69-28"><a href="bias-variance-trade-off.html#cb69-28" tabindex="-1"></a>predictor3 <span class="ot">&lt;-</span> <span class="fl">4.5</span> <span class="sc">+</span> <span class="fl">0.06</span> <span class="sc">*</span> schooling  <span class="co"># Biased predictor with lower intercept and coefficient</span></span>
<span id="cb69-29"><a href="bias-variance-trade-off.html#cb69-29" tabindex="-1"></a>predictor4 <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fl">0.07</span> <span class="sc">*</span> schooling  <span class="co"># True predictor (linear)</span></span>
<span id="cb69-30"><a href="bias-variance-trade-off.html#cb69-30" tabindex="-1"></a></span>
<span id="cb69-31"><a href="bias-variance-trade-off.html#cb69-31" tabindex="-1"></a><span class="co"># Function to calculate MSPE components</span></span>
<span id="cb69-32"><a href="bias-variance-trade-off.html#cb69-32" tabindex="-1"></a>calculate_mspe <span class="ot">&lt;-</span> <span class="cf">function</span>(true, pred) {</span>
<span id="cb69-33"><a href="bias-variance-trade-off.html#cb69-33" tabindex="-1"></a>  bias <span class="ot">&lt;-</span> <span class="fu">mean</span>(pred <span class="sc">-</span> true)</span>
<span id="cb69-34"><a href="bias-variance-trade-off.html#cb69-34" tabindex="-1"></a>  variance <span class="ot">&lt;-</span> <span class="fu">var</span>(pred)</span>
<span id="cb69-35"><a href="bias-variance-trade-off.html#cb69-35" tabindex="-1"></a>  irreducible_error <span class="ot">&lt;-</span> <span class="fu">var</span>(true <span class="sc">-</span> <span class="fu">mean</span>(true))</span>
<span id="cb69-36"><a href="bias-variance-trade-off.html#cb69-36" tabindex="-1"></a>  mspe <span class="ot">&lt;-</span> <span class="fu">mean</span>((true <span class="sc">-</span> pred)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb69-37"><a href="bias-variance-trade-off.html#cb69-37" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(bias<span class="sc">^</span><span class="dv">2</span>, variance, irreducible_error, mspe))</span>
<span id="cb69-38"><a href="bias-variance-trade-off.html#cb69-38" tabindex="-1"></a>}</span>
<span id="cb69-39"><a href="bias-variance-trade-off.html#cb69-39" tabindex="-1"></a></span>
<span id="cb69-40"><a href="bias-variance-trade-off.html#cb69-40" tabindex="-1"></a><span class="co"># Calculate MSPE components for each predictor</span></span>
<span id="cb69-41"><a href="bias-variance-trade-off.html#cb69-41" tabindex="-1"></a>true_values <span class="ot">&lt;-</span> lnw</span>
<span id="cb69-42"><a href="bias-variance-trade-off.html#cb69-42" tabindex="-1"></a></span>
<span id="cb69-43"><a href="bias-variance-trade-off.html#cb69-43" tabindex="-1"></a>mspe1 <span class="ot">&lt;-</span> <span class="fu">calculate_mspe</span>(true_values, predictor1)</span>
<span id="cb69-44"><a href="bias-variance-trade-off.html#cb69-44" tabindex="-1"></a>mspe2 <span class="ot">&lt;-</span> <span class="fu">calculate_mspe</span>(true_values, predictor2)</span>
<span id="cb69-45"><a href="bias-variance-trade-off.html#cb69-45" tabindex="-1"></a>mspe3 <span class="ot">&lt;-</span> <span class="fu">calculate_mspe</span>(true_values, predictor3)</span>
<span id="cb69-46"><a href="bias-variance-trade-off.html#cb69-46" tabindex="-1"></a>mspe4 <span class="ot">&lt;-</span> <span class="fu">calculate_mspe</span>(true_values, predictor4)</span>
<span id="cb69-47"><a href="bias-variance-trade-off.html#cb69-47" tabindex="-1"></a></span>
<span id="cb69-48"><a href="bias-variance-trade-off.html#cb69-48" tabindex="-1"></a><span class="co"># Combine results into a data frame</span></span>
<span id="cb69-49"><a href="bias-variance-trade-off.html#cb69-49" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb69-50"><a href="bias-variance-trade-off.html#cb69-50" tabindex="-1"></a>  <span class="at">Predictor =</span> <span class="fu">c</span>(<span class="st">&quot;Predictor 1&quot;</span>, <span class="st">&quot;Predictor 2&quot;</span>, <span class="st">&quot;Predictor 3&quot;</span>, <span class="st">&quot;Predictor 4&quot;</span>),</span>
<span id="cb69-51"><a href="bias-variance-trade-off.html#cb69-51" tabindex="-1"></a>  <span class="at">Bias2 =</span> <span class="fu">c</span>(mspe1[<span class="dv">1</span>], mspe2[<span class="dv">1</span>], mspe3[<span class="dv">1</span>], mspe4[<span class="dv">1</span>]),</span>
<span id="cb69-52"><a href="bias-variance-trade-off.html#cb69-52" tabindex="-1"></a>  <span class="at">Variance =</span> <span class="fu">c</span>(mspe1[<span class="dv">2</span>], mspe2[<span class="dv">2</span>], mspe3[<span class="dv">2</span>], mspe4[<span class="dv">2</span>]),</span>
<span id="cb69-53"><a href="bias-variance-trade-off.html#cb69-53" tabindex="-1"></a>  <span class="at">IrreducibleError =</span> <span class="fu">c</span>(mspe1[<span class="dv">3</span>], mspe2[<span class="dv">3</span>], mspe3[<span class="dv">3</span>], mspe4[<span class="dv">3</span>]),</span>
<span id="cb69-54"><a href="bias-variance-trade-off.html#cb69-54" tabindex="-1"></a>  <span class="at">MSPE =</span> <span class="fu">c</span>(mspe1[<span class="dv">4</span>], mspe2[<span class="dv">4</span>], mspe3[<span class="dv">4</span>], mspe4[<span class="dv">4</span>])</span>
<span id="cb69-55"><a href="bias-variance-trade-off.html#cb69-55" tabindex="-1"></a>)</span>
<span id="cb69-56"><a href="bias-variance-trade-off.html#cb69-56" tabindex="-1"></a></span>
<span id="cb69-57"><a href="bias-variance-trade-off.html#cb69-57" tabindex="-1"></a><span class="co"># Print results with 3 decimal points</span></span>
<span id="cb69-58"><a href="bias-variance-trade-off.html#cb69-58" tabindex="-1"></a>results_rounded <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb69-59"><a href="bias-variance-trade-off.html#cb69-59" tabindex="-1"></a>  <span class="at">Predictor =</span> results<span class="sc">$</span>Predictor,</span>
<span id="cb69-60"><a href="bias-variance-trade-off.html#cb69-60" tabindex="-1"></a>  <span class="at">Bias2 =</span> <span class="fu">round</span>(results<span class="sc">$</span>Bias2, <span class="dv">3</span>),</span>
<span id="cb69-61"><a href="bias-variance-trade-off.html#cb69-61" tabindex="-1"></a>  <span class="at">Variance =</span> <span class="fu">round</span>(results<span class="sc">$</span>Variance, <span class="dv">3</span>),</span>
<span id="cb69-62"><a href="bias-variance-trade-off.html#cb69-62" tabindex="-1"></a>  <span class="at">IrreducibleError =</span> <span class="fu">round</span>(results<span class="sc">$</span>IrreducibleError, <span class="dv">3</span>),</span>
<span id="cb69-63"><a href="bias-variance-trade-off.html#cb69-63" tabindex="-1"></a>  <span class="at">MSPE =</span> <span class="fu">round</span>(results<span class="sc">$</span>MSPE, <span class="dv">3</span>)</span>
<span id="cb69-64"><a href="bias-variance-trade-off.html#cb69-64" tabindex="-1"></a>)</span>
<span id="cb69-65"><a href="bias-variance-trade-off.html#cb69-65" tabindex="-1"></a></span>
<span id="cb69-66"><a href="bias-variance-trade-off.html#cb69-66" tabindex="-1"></a><span class="fu">print</span>(results_rounded)</span></code></pre></div>
<pre><code>##     Predictor Bias2 Variance IrreducibleError  MSPE
## 1 Predictor 1 0.000    0.000            0.661 0.661
## 2 Predictor 2 0.094    0.342            0.661 0.620
## 3 Predictor 3 0.353    0.120            0.661 0.847
## 4 Predictor 4 0.000    0.163            0.661 0.490</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="bias-variance-trade-off.html#cb71-1" tabindex="-1"></a><span class="co"># Clear the environment</span></span>
<span id="cb71-2"><a href="bias-variance-trade-off.html#cb71-2" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span></code></pre></div>
<p>The simulation demonstrates the performance of four different prediction functions. Among these, the true linear predictor (Predictor 4) achieved the smallest MSPE, indicating it provides the most accurate predictions. This is expected since it closely aligns with the underlying data generation process.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="bias-variance-trade-off.html#cb72-1" tabindex="-1"></a><span class="co"># Load necessary library</span></span>
<span id="cb72-2"><a href="bias-variance-trade-off.html#cb72-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb72-3"><a href="bias-variance-trade-off.html#cb72-3" tabindex="-1"></a><span class="fu">library</span>(gridExtra)  <span class="co"># For arranging plots side by side</span></span>
<span id="cb72-4"><a href="bias-variance-trade-off.html#cb72-4" tabindex="-1"></a></span>
<span id="cb72-5"><a href="bias-variance-trade-off.html#cb72-5" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb72-6"><a href="bias-variance-trade-off.html#cb72-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb72-7"><a href="bias-variance-trade-off.html#cb72-7" tabindex="-1"></a></span>
<span id="cb72-8"><a href="bias-variance-trade-off.html#cb72-8" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb72-9"><a href="bias-variance-trade-off.html#cb72-9" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">26732</span></span>
<span id="cb72-10"><a href="bias-variance-trade-off.html#cb72-10" tabindex="-1"></a>mean_lnw <span class="ot">&lt;-</span> <span class="fl">5.9</span></span>
<span id="cb72-11"><a href="bias-variance-trade-off.html#cb72-11" tabindex="-1"></a>sd_lnw <span class="ot">&lt;-</span> <span class="fl">0.7</span></span>
<span id="cb72-12"><a href="bias-variance-trade-off.html#cb72-12" tabindex="-1"></a>mean_s <span class="ot">&lt;-</span> <span class="fl">12.8</span></span>
<span id="cb72-13"><a href="bias-variance-trade-off.html#cb72-13" tabindex="-1"></a>sd_s <span class="ot">&lt;-</span> <span class="fl">3.3</span></span>
<span id="cb72-14"><a href="bias-variance-trade-off.html#cb72-14" tabindex="-1"></a>min_s <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb72-15"><a href="bias-variance-trade-off.html#cb72-15" tabindex="-1"></a>max_s <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb72-16"><a href="bias-variance-trade-off.html#cb72-16" tabindex="-1"></a></span>
<span id="cb72-17"><a href="bias-variance-trade-off.html#cb72-17" tabindex="-1"></a><span class="co"># Initialize storage for beta0 and beta1</span></span>
<span id="cb72-18"><a href="bias-variance-trade-off.html#cb72-18" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">100</span>)</span>
<span id="cb72-19"><a href="bias-variance-trade-off.html#cb72-19" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">100</span>)</span>
<span id="cb72-20"><a href="bias-variance-trade-off.html#cb72-20" tabindex="-1"></a>mspe_values <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">100</span>)</span>
<span id="cb72-21"><a href="bias-variance-trade-off.html#cb72-21" tabindex="-1"></a>bias_values <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">100</span>)</span>
<span id="cb72-22"><a href="bias-variance-trade-off.html#cb72-22" tabindex="-1"></a>variance_values <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">100</span>)</span>
<span id="cb72-23"><a href="bias-variance-trade-off.html#cb72-23" tabindex="-1"></a>irreducible_error_values <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">100</span>)</span>
<span id="cb72-24"><a href="bias-variance-trade-off.html#cb72-24" tabindex="-1"></a></span>
<span id="cb72-25"><a href="bias-variance-trade-off.html#cb72-25" tabindex="-1"></a><span class="co"># Function to calculate MSPE components</span></span>
<span id="cb72-26"><a href="bias-variance-trade-off.html#cb72-26" tabindex="-1"></a>calculate_mspe <span class="ot">&lt;-</span> <span class="cf">function</span>(true, pred) {</span>
<span id="cb72-27"><a href="bias-variance-trade-off.html#cb72-27" tabindex="-1"></a>  bias <span class="ot">&lt;-</span> <span class="fu">mean</span>(pred <span class="sc">-</span> true)</span>
<span id="cb72-28"><a href="bias-variance-trade-off.html#cb72-28" tabindex="-1"></a>  variance <span class="ot">&lt;-</span> <span class="fu">var</span>(pred)</span>
<span id="cb72-29"><a href="bias-variance-trade-off.html#cb72-29" tabindex="-1"></a>  irreducible_error <span class="ot">&lt;-</span> <span class="fu">var</span>(true <span class="sc">-</span> <span class="fu">mean</span>(true))</span>
<span id="cb72-30"><a href="bias-variance-trade-off.html#cb72-30" tabindex="-1"></a>  mspe <span class="ot">&lt;-</span> <span class="fu">mean</span>((true <span class="sc">-</span> pred)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb72-31"><a href="bias-variance-trade-off.html#cb72-31" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(bias<span class="sc">^</span><span class="dv">2</span>, variance, irreducible_error, mspe))</span>
<span id="cb72-32"><a href="bias-variance-trade-off.html#cb72-32" tabindex="-1"></a>}</span>
<span id="cb72-33"><a href="bias-variance-trade-off.html#cb72-33" tabindex="-1"></a></span>
<span id="cb72-34"><a href="bias-variance-trade-off.html#cb72-34" tabindex="-1"></a><span class="co"># Generate 100 different datasets and fit models</span></span>
<span id="cb72-35"><a href="bias-variance-trade-off.html#cb72-35" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb72-36"><a href="bias-variance-trade-off.html#cb72-36" tabindex="-1"></a>  <span class="co"># Generate schooling data and round to nearest 0.5</span></span>
<span id="cb72-37"><a href="bias-variance-trade-off.html#cb72-37" tabindex="-1"></a>  schooling <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">runif</span>(n, <span class="at">min =</span> min_s, <span class="at">max =</span> max_s) <span class="sc">*</span> <span class="dv">2</span>) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb72-38"><a href="bias-variance-trade-off.html#cb72-38" tabindex="-1"></a>  </span>
<span id="cb72-39"><a href="bias-variance-trade-off.html#cb72-39" tabindex="-1"></a>  <span class="co"># Generate lnw data using the provided regression coefficients</span></span>
<span id="cb72-40"><a href="bias-variance-trade-off.html#cb72-40" tabindex="-1"></a>  lnw <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="sc">+</span> <span class="fl">0.07</span> <span class="sc">*</span> schooling <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sd_lnw)</span>
<span id="cb72-41"><a href="bias-variance-trade-off.html#cb72-41" tabindex="-1"></a>  </span>
<span id="cb72-42"><a href="bias-variance-trade-off.html#cb72-42" tabindex="-1"></a>  <span class="co"># Fit the linear model</span></span>
<span id="cb72-43"><a href="bias-variance-trade-off.html#cb72-43" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">lm</span>(lnw <span class="sc">~</span> schooling)</span>
<span id="cb72-44"><a href="bias-variance-trade-off.html#cb72-44" tabindex="-1"></a>  beta0[i] <span class="ot">&lt;-</span> <span class="fu">coef</span>(model)[<span class="dv">1</span>]</span>
<span id="cb72-45"><a href="bias-variance-trade-off.html#cb72-45" tabindex="-1"></a>  beta1[i] <span class="ot">&lt;-</span> <span class="fu">coef</span>(model)[<span class="dv">2</span>]</span>
<span id="cb72-46"><a href="bias-variance-trade-off.html#cb72-46" tabindex="-1"></a>  </span>
<span id="cb72-47"><a href="bias-variance-trade-off.html#cb72-47" tabindex="-1"></a>  <span class="co"># Generate predictions using the model</span></span>
<span id="cb72-48"><a href="bias-variance-trade-off.html#cb72-48" tabindex="-1"></a>  predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">schooling =</span> schooling))</span>
<span id="cb72-49"><a href="bias-variance-trade-off.html#cb72-49" tabindex="-1"></a>  </span>
<span id="cb72-50"><a href="bias-variance-trade-off.html#cb72-50" tabindex="-1"></a>  <span class="co"># Calculate MSPE components</span></span>
<span id="cb72-51"><a href="bias-variance-trade-off.html#cb72-51" tabindex="-1"></a>  mspe_components <span class="ot">&lt;-</span> <span class="fu">calculate_mspe</span>(lnw, predictions)</span>
<span id="cb72-52"><a href="bias-variance-trade-off.html#cb72-52" tabindex="-1"></a>  mspe_values[i] <span class="ot">&lt;-</span> mspe_components[<span class="dv">4</span>]</span>
<span id="cb72-53"><a href="bias-variance-trade-off.html#cb72-53" tabindex="-1"></a>  bias_values[i] <span class="ot">&lt;-</span> mspe_components[<span class="dv">1</span>]</span>
<span id="cb72-54"><a href="bias-variance-trade-off.html#cb72-54" tabindex="-1"></a>  variance_values[i] <span class="ot">&lt;-</span> mspe_components[<span class="dv">2</span>]</span>
<span id="cb72-55"><a href="bias-variance-trade-off.html#cb72-55" tabindex="-1"></a>  irreducible_error_values[i] <span class="ot">&lt;-</span> mspe_components[<span class="dv">3</span>]</span>
<span id="cb72-56"><a href="bias-variance-trade-off.html#cb72-56" tabindex="-1"></a>  </span>
<span id="cb72-57"><a href="bias-variance-trade-off.html#cb72-57" tabindex="-1"></a>  <span class="co"># Print initial 50 observations and other calculated variables for the first dataset</span></span>
<span id="cb72-58"><a href="bias-variance-trade-off.html#cb72-58" tabindex="-1"></a>  <span class="cf">if</span> (i <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb72-59"><a href="bias-variance-trade-off.html#cb72-59" tabindex="-1"></a>    initial_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">schooling =</span> schooling[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>], <span class="at">lnw =</span> lnw[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>], <span class="at">predicted_lnw =</span> predictions[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>])</span>
<span id="cb72-60"><a href="bias-variance-trade-off.html#cb72-60" tabindex="-1"></a>    <span class="fu">print</span>(<span class="st">&quot;Initial 50 observations of schooling, lnw, and predicted lnw:&quot;</span>)</span>
<span id="cb72-61"><a href="bias-variance-trade-off.html#cb72-61" tabindex="-1"></a>    <span class="fu">print</span>(initial_data)</span>
<span id="cb72-62"><a href="bias-variance-trade-off.html#cb72-62" tabindex="-1"></a>    </span>
<span id="cb72-63"><a href="bias-variance-trade-off.html#cb72-63" tabindex="-1"></a>    <span class="fu">print</span>(<span class="st">&quot;Coefficients for the first generated dataset:&quot;</span>)</span>
<span id="cb72-64"><a href="bias-variance-trade-off.html#cb72-64" tabindex="-1"></a>    <span class="fu">print</span>(<span class="fu">coef</span>(model))</span>
<span id="cb72-65"><a href="bias-variance-trade-off.html#cb72-65" tabindex="-1"></a>    </span>
<span id="cb72-66"><a href="bias-variance-trade-off.html#cb72-66" tabindex="-1"></a>    <span class="fu">print</span>(<span class="st">&quot;MSPE components for the first generated dataset:&quot;</span>)</span>
<span id="cb72-67"><a href="bias-variance-trade-off.html#cb72-67" tabindex="-1"></a>    <span class="fu">print</span>(mspe_components)</span>
<span id="cb72-68"><a href="bias-variance-trade-off.html#cb72-68" tabindex="-1"></a>  }</span>
<span id="cb72-69"><a href="bias-variance-trade-off.html#cb72-69" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## [1] &quot;Initial 50 observations of schooling, lnw, and predicted lnw:&quot;
##    schooling      lnw predicted_lnw
## 1        6.0 4.309461      5.407463
## 2       16.0 5.252208      6.124208
## 3        8.0 5.800693      5.550812
## 4       17.5 6.868000      6.231720
## 5       19.0 6.337707      6.339232
## 6        1.0 5.271255      5.049090
## 7       10.5 6.936026      5.729998
## 8       18.0 6.449974      6.267557
## 9       11.0 6.726069      5.765835
## 10       9.0 5.014647      5.622486
## 11      19.0 6.781244      6.339232
## 12       9.0 4.850522      5.622486
## 13      13.5 6.106988      5.945022
## 14      11.5 5.621766      5.801673
## 15       2.0 4.888290      5.120764
## 16      18.0 6.967818      6.267557
## 17       5.0 5.339867      5.335788
## 18       1.0 4.622575      5.049090
## 19       6.5 6.853711      5.443300
## 20      19.0 7.802680      6.339232
## 21      18.0 5.595551      6.267557
## 22      14.0 6.682397      5.980859
## 23      13.0 6.445602      5.909184
## 24      20.0 6.644234      6.410906
## 25      13.0 6.194610      5.909184
## 26      14.0 6.800737      5.980859
## 27      11.0 6.830699      5.765835
## 28      12.0 4.844341      5.837510
## 29       6.0 5.324725      5.407463
## 30       3.0 5.177032      5.192439
## 31      19.5 7.673262      6.375069
## 32      18.0 6.750589      6.267557
## 33      14.0 5.300752      5.980859
## 34      16.0 6.542941      6.124208
## 35       0.5 3.807720      5.013253
## 36       9.5 6.065694      5.658324
## 37      15.0 6.444348      6.052534
## 38       4.5 6.583272      5.299951
## 39       6.5 5.447284      5.443300
## 40       4.5 5.759566      5.299951
## 41       3.0 5.401111      5.192439
## 42       8.5 5.123768      5.586649
## 43       8.5 5.225697      5.586649
## 44       7.5 4.734100      5.514974
## 45       3.0 5.616073      5.192439
## 46       3.0 5.388677      5.192439
## 47       4.5 4.772368      5.299951
## 48       9.5 4.891484      5.658324
## 49       5.5 4.781807      5.371625
## 50      17.0 5.894623      6.195883
## [1] &quot;Coefficients for the first generated dataset:&quot;
## (Intercept)   schooling 
##  4.97741540  0.07167454 
## [1] &quot;MSPE components for the first generated dataset:&quot;
## [1] 1.608257e-29 1.709228e-01 6.609322e-01 4.899911e-01</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="bias-variance-trade-off.html#cb74-1" tabindex="-1"></a><span class="co"># Calculate expected MSPE, bias, and variance</span></span>
<span id="cb74-2"><a href="bias-variance-trade-off.html#cb74-2" tabindex="-1"></a>expected_mspe <span class="ot">&lt;-</span> <span class="fu">mean</span>(mspe_values)</span>
<span id="cb74-3"><a href="bias-variance-trade-off.html#cb74-3" tabindex="-1"></a>expected_bias <span class="ot">&lt;-</span> <span class="fu">mean</span>(bias_values)</span>
<span id="cb74-4"><a href="bias-variance-trade-off.html#cb74-4" tabindex="-1"></a>expected_variance <span class="ot">&lt;-</span> <span class="fu">mean</span>(variance_values)</span>
<span id="cb74-5"><a href="bias-variance-trade-off.html#cb74-5" tabindex="-1"></a>expected_irreducible_error <span class="ot">&lt;-</span> <span class="fu">mean</span>(irreducible_error_values)</span>
<span id="cb74-6"><a href="bias-variance-trade-off.html#cb74-6" tabindex="-1"></a></span>
<span id="cb74-7"><a href="bias-variance-trade-off.html#cb74-7" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb74-8"><a href="bias-variance-trade-off.html#cb74-8" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Expected MSPE:&quot;</span>, <span class="fu">round</span>(expected_mspe, <span class="dv">3</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Expected MSPE: 0.491</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="bias-variance-trade-off.html#cb76-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Expected Bias:&quot;</span>, <span class="fu">round</span>(expected_bias, <span class="dv">3</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Expected Bias: 0</code></pre>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="bias-variance-trade-off.html#cb78-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Expected Variance:&quot;</span>, <span class="fu">round</span>(expected_variance, <span class="dv">3</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Expected Variance: 0.163</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="bias-variance-trade-off.html#cb80-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Expected Irreducible Error:&quot;</span>, <span class="fu">round</span>(expected_irreducible_error, <span class="dv">3</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Expected Irreducible Error: 0.654</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="bias-variance-trade-off.html#cb82-1" tabindex="-1"></a><span class="co"># Clear the environment</span></span>
<span id="cb82-2"><a href="bias-variance-trade-off.html#cb82-2" tabindex="-1"></a><span class="fu">rm</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span></code></pre></div>
<p>Other simulation examples are <a href="https://blog.zenggyu.com/en/post/2018-03-11/understanding-the-bias-variance-decomposition-with-a-simulated-experiment/">1</a> , <a href="https://www.r-bloggers.com/2019/06/simulating-the-bias-variance-tradeoff-in-r/">2</a>, and <a href="https://daviddalpiaz.github.io/r4sl/simulating-the-biasvariance-tradeoff.html">3</a></p>
</div>
<div id="biased-estimator-as-a-predictor" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Biased estimator as a predictor<a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Up to this point, we have shown through simulation that a prediction function with zero bias but high variance often produces better predictions than a prediction function with zero variance but high bias. However, we can potentially obtain an even better prediction function that has some bias and some variance. A better prediction function means a smaller Mean Squared Prediction Error (MSPE). The key idea is that if the reduction in variance more than compensates for the increase in bias, then we have a better predictor.</p>
<p>To explore this, let’s define a biased estimator of <span class="math inline">\(\mu_x\)</span>:
<span class="math display">\[
\hat{X}_{\text{biased}} = \hat{\mu}_x = \alpha \bar{X}
\]</span>
where <span class="math inline">\(\bar{X}\)</span> is the sample mean. The sample mean <span class="math inline">\(\bar{X}\)</span> is an unbiased estimator of <span class="math inline">\(\mu_x\)</span>, and the parameter <span class="math inline">\(\alpha\)</span> introduces bias. When <span class="math inline">\(\alpha\)</span> is 1, <span class="math inline">\(\hat{\mu}_x\)</span> becomes the unbiased sample mean.</p>
<p>The bias of the estimator <span class="math inline">\(\hat{\mu}_x\)</span> is given by:
<span class="math display">\[
\operatorname{Bias}(\hat{\mu}_x) = \mathbb{E}[\hat{\mu}_x] - \mu_x = \alpha \mu_x - \mu_x = (\alpha - 1) \mu_x
\]</span>
The variance of the estimator <span class="math inline">\(\hat{\mu}_x\)</span> is:
<span class="math display">\[
\operatorname{Var}(\hat{\mu}_x) = \operatorname{Var}(\alpha \bar{X}) = \alpha^2 \operatorname{Var}(\bar{X})
\]</span>
Since the variance of the sample mean <span class="math inline">\(\bar{X}\)</span> is <span class="math inline">\(\frac{\sigma_{\varepsilon}^2}{n}\)</span> (Check chapter 5.4), we have:
<span class="math display">\[
\operatorname{Var}(\hat{\mu}_x) = \alpha^2 \frac{\sigma_{\varepsilon}^2}{n}
\]</span></p>
<p>The MSPE and its bias-variance components:
<span class="math display">\[
\text{MSPE} = \mathbb{E}[(\hat{\mu}_x - \mu_x)^2] = \operatorname{Bias}^2(\hat{\mu}_x) + \operatorname{Var}(\hat{\mu}_x) + \sigma_{\varepsilon}^2
\]</span>
where <span class="math inline">\(\sigma_{\varepsilon}^2\)</span> is the irreducible error.</p>
<p>First, calculate the bias squared:
<span class="math display">\[
\operatorname{Bias}^2(\hat{\mu}_x) = [(\alpha - 1) \mu_x]^2 = (1 - \alpha)^2 \mu_x^2
\]</span></p>
<p>Next, calculate the variance:
<span class="math display">\[
\operatorname{Var}(\hat{\mu}_x) = \alpha^2 \frac{\sigma_{\varepsilon}^2}{n}
\]</span></p>
<p>Finally, the irreducible error is <span class="math inline">\(\sigma_{\varepsilon}^2\)</span>. After combining these terms,we can shows that the MSPE for the biased estimator <span class="math inline">\(\hat{\mu}_x = \alpha \bar{X}\)</span> is:
<span class="math display">\[
\text{MSPE} = (1 - \alpha)^2 \mu_x^2 + \alpha^2 \frac{\sigma_{\varepsilon}^2}{n} + \sigma_{\varepsilon}^2 = [(1-\alpha) \mu_x]^2 + \frac{1}{n} \alpha^2 \sigma_{\varepsilon}^2 + \sigma_{\varepsilon}^2
\]</span></p>
<p>The final expression for the MSPE of the biased estimator <span class="math inline">\(\hat{\mu}_x = \alpha \bar{X}\)</span> combines the squared bias term <span class="math inline">\((1 - \alpha)^2 \mu_x^2\)</span>, the variance term <span class="math inline">\(\frac{\alpha^2 \sigma_{\varepsilon}^2}{n}\)</span>, and the irreducible error term <span class="math inline">\(\sigma_{\varepsilon}^2\)</span>. By adjusting <span class="math inline">\(\alpha\)</span>, we can balance the trade-off between bias and variance to minimize the MSPE. This highlights that a small amount of bias can be beneficial if it significantly reduces the variance, leading to a lower MSPE and thus a better predictor.</p>
<p>Our first observation is that when <span class="math inline">\(\alpha\)</span> is one, the bias will be zero. Since it seems that MSPE is a convex function of <span class="math inline">\(\alpha\)</span>, we can search for <span class="math inline">\(\alpha\)</span> that minimizes MSPE. The value of <span class="math inline">\(\alpha\)</span> that minimizes the MSPE is:</p>
<p><span class="math display">\[
\frac{\partial \text{MSPE}}{\partial \alpha} =0 \rightarrow ~~ \alpha = \frac{\mu^2_x}{\mu^2_x+\sigma^2_\varepsilon/n}&lt;1
\]</span></p>
<p>Check end of the chapter for step-by-step derivation of the optimal value of <span class="math inline">\(\alpha\)</span> that minimizes the MSPE.</p>
<p>Using the same simulation sample above , lets calculate alpha and MSPE with this new biased prediction function, and compare all 3 MSPEs.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="bias-variance-trade-off.html#cb83-1" tabindex="-1"></a>pred <span class="ot">&lt;-</span><span class="fu">rep</span>(<span class="dv">0</span>, Ms)</span>
<span id="cb83-2"><a href="bias-variance-trade-off.html#cb83-2" tabindex="-1"></a></span>
<span id="cb83-3"><a href="bias-variance-trade-off.html#cb83-3" tabindex="-1"></a><span class="co"># The magnitude of bias</span></span>
<span id="cb83-4"><a href="bias-variance-trade-off.html#cb83-4" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> (<span class="fu">mean</span>(populationX))<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>((<span class="fu">mean</span>(populationX)<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span>var_eps<span class="sc">/</span><span class="dv">10</span>))</span>
<span id="cb83-5"><a href="bias-variance-trade-off.html#cb83-5" tabindex="-1"></a>alpha</span></code></pre></div>
<pre><code>## [1] 0.9966513</code></pre>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="bias-variance-trade-off.html#cb85-1" tabindex="-1"></a><span class="co"># Biased predictor</span></span>
<span id="cb85-2"><a href="bias-variance-trade-off.html#cb85-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Ms) {</span>
<span id="cb85-3"><a href="bias-variance-trade-off.html#cb85-3" tabindex="-1"></a>  pred[i] <span class="ot">&lt;-</span> alpha<span class="sc">*</span>predictions[i,<span class="dv">2</span>]</span>
<span id="cb85-4"><a href="bias-variance-trade-off.html#cb85-4" tabindex="-1"></a>}</span>
<span id="cb85-5"><a href="bias-variance-trade-off.html#cb85-5" tabindex="-1"></a><span class="co"># Check if E(alpha*Xbar) = alpha*mu_x</span></span>
<span id="cb85-6"><a href="bias-variance-trade-off.html#cb85-6" tabindex="-1"></a><span class="fu">mean</span>(pred)</span></code></pre></div>
<pre><code>## [1] 12.45708</code></pre>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="bias-variance-trade-off.html#cb87-1" tabindex="-1"></a>alpha<span class="sc">*</span><span class="fu">mean</span>(populationX)</span></code></pre></div>
<pre><code>## [1] 12.45814</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="bias-variance-trade-off.html#cb89-1" tabindex="-1"></a><span class="co"># MSPE</span></span>
<span id="cb89-2"><a href="bias-variance-trade-off.html#cb89-2" tabindex="-1"></a>MSPE_biased <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, Ms)</span>
<span id="cb89-3"><a href="bias-variance-trade-off.html#cb89-3" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>Ms) {</span>
<span id="cb89-4"><a href="bias-variance-trade-off.html#cb89-4" tabindex="-1"></a>  MSPE_biased[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((populationX<span class="sc">-</span>pred[i])<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb89-5"><a href="bias-variance-trade-off.html#cb89-5" tabindex="-1"></a>}</span>
<span id="cb89-6"><a href="bias-variance-trade-off.html#cb89-6" tabindex="-1"></a><span class="fu">mean</span>(MSPE_biased)</span></code></pre></div>
<pre><code>## [1] 5.786663</code></pre>
<p>Let’s add this predictor into our table:</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="bias-variance-trade-off.html#cb91-1" tabindex="-1"></a>VBtradeoff <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb91-2"><a href="bias-variance-trade-off.html#cb91-2" tabindex="-1"></a><span class="fu">rownames</span>(VBtradeoff) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;fhat_1&quot;</span>, <span class="st">&quot;fhat_2&quot;</span>, <span class="st">&quot;fhat_3&quot;</span>)</span>
<span id="cb91-3"><a href="bias-variance-trade-off.html#cb91-3" tabindex="-1"></a><span class="fu">colnames</span>(VBtradeoff) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Bias&quot;</span>, <span class="st">&quot;Var(fhat)&quot;</span>, <span class="st">&quot;Var(eps)&quot;</span>, <span class="st">&quot;MSPE&quot;</span>)</span>
<span id="cb91-4"><a href="bias-variance-trade-off.html#cb91-4" tabindex="-1"></a>VBtradeoff[<span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> bias1<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb91-5"><a href="bias-variance-trade-off.html#cb91-5" tabindex="-1"></a>VBtradeoff[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> bias2<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb91-6"><a href="bias-variance-trade-off.html#cb91-6" tabindex="-1"></a>VBtradeoff[<span class="dv">3</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> (<span class="fu">mean</span>(populationX)<span class="sc">-</span><span class="fu">mean</span>(pred))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb91-7"><a href="bias-variance-trade-off.html#cb91-7" tabindex="-1"></a>VBtradeoff[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> var1</span>
<span id="cb91-8"><a href="bias-variance-trade-off.html#cb91-8" tabindex="-1"></a>VBtradeoff[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> var2</span>
<span id="cb91-9"><a href="bias-variance-trade-off.html#cb91-9" tabindex="-1"></a>VBtradeoff[<span class="dv">3</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">var</span>(pred)</span>
<span id="cb91-10"><a href="bias-variance-trade-off.html#cb91-10" tabindex="-1"></a>VBtradeoff[<span class="dv">1</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> var_eps</span>
<span id="cb91-11"><a href="bias-variance-trade-off.html#cb91-11" tabindex="-1"></a>VBtradeoff[<span class="dv">2</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> var_eps</span>
<span id="cb91-12"><a href="bias-variance-trade-off.html#cb91-12" tabindex="-1"></a>VBtradeoff[<span class="dv">3</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> var_eps</span>
<span id="cb91-13"><a href="bias-variance-trade-off.html#cb91-13" tabindex="-1"></a>VBtradeoff[<span class="dv">1</span>,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(MSPE[,<span class="dv">1</span>])</span>
<span id="cb91-14"><a href="bias-variance-trade-off.html#cb91-14" tabindex="-1"></a>VBtradeoff[<span class="dv">2</span>,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(MSPE[,<span class="dv">2</span>])</span>
<span id="cb91-15"><a href="bias-variance-trade-off.html#cb91-15" tabindex="-1"></a>VBtradeoff[<span class="dv">3</span>,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(MSPE_biased)</span>
<span id="cb91-16"><a href="bias-variance-trade-off.html#cb91-16" tabindex="-1"></a><span class="fu">round</span>(VBtradeoff, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##         Bias Var(fhat) Var(eps)   MSPE
## fhat_1 6.250     0.000     5.25 11.500
## fhat_2 0.000     0.539     5.25  5.788
## fhat_3 0.002     0.535     5.25  5.787</code></pre>
<p>As seen , increase in bias is lower than decrease in variance. The prediction function with some bias and variance is the <strong>best prediction function</strong> as it has the smallest MSPE. This example shows the difference between estimation and prediction for a simplest predictor, the mean of <span class="math inline">\(X\)</span>.</p>
<p>In the next chapter, we will explore overfitting and how it relates to and differs from the bias-variance tradeoff.</p>
<p><strong>Note:</strong> To find the value of <span class="math inline">\(\alpha\)</span> that minimizes the Mean Squared Prediction Error (MSPE), we take the derivative of the MSPE with respect to <span class="math inline">\(\alpha\)</span>, set it to zero, and solve for <span class="math inline">\(\alpha\)</span>.</p>
<p>The MSPE is given by:
<span class="math display">\[
\text{MSPE} = [(1-\alpha) \mu_x]^2 + \frac{1}{n} \alpha^2 \sigma_{\varepsilon}^2 + \sigma_{\varepsilon}^2
\]</span></p>
<p>First, we take the derivative of MSPE with respect to <span class="math inline">\(\alpha\)</span>:
<span class="math display">\[
\frac{\partial \text{MSPE}}{\partial \alpha} = \frac{\partial}{\partial \alpha} \left[(1-\alpha)^2 \mu_x^2 + \frac{1}{n} \alpha^2 \sigma_{\varepsilon}^2 + \sigma_{\varepsilon}^2\right]
\]</span></p>
<p>We compute the derivative term by term:
<span class="math display">\[
\frac{\partial \text{MSPE}}{\partial \alpha} = 2(1-\alpha)(-1) \mu_x^2 + 2 \frac{1}{n} \alpha \sigma_{\varepsilon}^2
\]</span>
Simplifying, we get:
<span class="math display">\[
\frac{\partial \text{MSPE}}{\partial \alpha} = -2(1-\alpha) \mu_x^2 + 2 \frac{1}{n} \alpha \sigma_{\varepsilon}^2
\]</span></p>
<p>To find the minimum MSPE, we set the derivative equal to zero:
<span class="math display">\[
-2(1-\alpha) \mu_x^2 + 2 \frac{1}{n} \alpha \sigma_{\varepsilon}^2 = 0
\]</span></p>
<p>Solving for <span class="math inline">\(\alpha\)</span>:
<span class="math display">\[
-2 \mu_x^2 + 2 \alpha \mu_x^2 + 2 \frac{1}{n} \alpha \sigma_{\varepsilon}^2 = 0
\]</span>
<span class="math display">\[
- \mu_x^2 + \alpha \mu_x^2 + \frac{1}{n} \alpha \sigma_{\varepsilon}^2 = 0
\]</span>
<span class="math display">\[
- \mu_x^2 + \alpha \left( \mu_x^2 + \frac{\sigma_{\varepsilon}^2}{n} \right) = 0
\]</span>
<span class="math display">\[
\alpha \left( \mu_x^2 + \frac{\sigma_{\varepsilon}^2}{n} \right) = \mu_x^2
\]</span>
<span class="math display">\[
\alpha = \frac{\mu_x^2}{\mu_x^2 + \frac{\sigma_{\varepsilon}^2}{n}}
\]</span></p>
<p>Thus, the value of <span class="math inline">\(\alpha\)</span> that minimizes the MSPE is:
<span class="math display">\[
\alpha = \frac{\mu_x^2}{\mu_x^2 + \frac{\sigma_{\varepsilon}^2}{n}}
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="error.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="overfitting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuksel/machinemetrics/edit/master/06-BiasVarTradeoff.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
