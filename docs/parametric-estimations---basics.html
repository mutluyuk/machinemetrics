<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Parametric Estimations - Basics | Causal MachineMetrics</title>
  <meta name="description" content="Chapter 9 Parametric Estimations - Basics | Causal MachineMetrics" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Parametric Estimations - Basics | Causal MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Parametric Estimations - Basics | Causal MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression-v.s.-classification.html"/>
<link rel="next" href="nonparametric-estimations---basics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html"><i class="fa fa-check"></i><b>2</b> Spectrum of Data Modeling:</a>
<ul>
<li class="chapter" data-level="2.1" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html#statistical-vs.-machine-learning-approaches"><i class="fa fa-check"></i><b>2.1</b> Statistical vs. Machine Learning Approaches:</a></li>
<li class="chapter" data-level="2.2" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models:</a></li>
<li class="chapter" data-level="2.4" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html#model-selection"><i class="fa fa-check"></i><b>2.4</b> Model Selection:</a></li>
<li class="chapter" data-level="2.5" data-path="spectrum-of-data-modeling.html"><a href="spectrum-of-data-modeling.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation:</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#maximum-likelihood-estimation-mle"><i class="fa fa-check"></i><b>3.5.3</b> Maximum Likelihood Estimation (MLE)}</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error"><i class="fa fa-check"></i><b>5.1</b> Estimation error</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#efficiency"><i class="fa fa-check"></i><b>5.2</b> Efficiency</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#mean-square-error"><i class="fa fa-check"></i><b>5.3</b> Mean Square Error</a></li>
<li class="chapter" data-level="5.4" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.4</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.5" data-path="error.html"><a href="error.html#technical-points-and-proofs"><i class="fa fa-check"></i><b>5.5</b> Technical points and proofs</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#formal-definition"><i class="fa fa-check"></i><b>6.1</b> Formal Definition</a></li>
<li class="chapter" data-level="6.2" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#simulated-breakdown-of-the-mspe"><i class="fa fa-check"></i><b>6.2</b> Simulated Breakdown of the MSPE</a></li>
<li class="chapter" data-level="6.3" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.3</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#the-dichotomy-of-statistical-modeling-data-versus-algorithmic-approaches"><i class="fa fa-check"></i><b>9.1</b> The Dichotomy of Statistical Modeling: Data versus Algorithmic Approaches:</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.2</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.3</b> LPM</a></li>
<li class="chapter" data-level="9.4" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.4</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.2</b> Pruning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.3</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.2</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.2.1</b> AdaBoost</a></li>
<li class="chapter" data-level="17.2.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.2.2</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.3</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.4</b> Classification</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.5</b> Regression</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.6</b> Exploration</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.7</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.7.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.7.1</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.7.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.7.2</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.7.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.7.3</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection-1"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.3</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-1.html"><a href="classification-1.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-1.html"><a href="classification-1.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-1.html"><a href="classification-1.html#linear-classifiers"><i class="fa fa-check"></i><b>21.2</b> Linear classifiers</a></li>
<li class="chapter" data-level="21.3" data-path="classification-1.html"><a href="classification-1.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.4" data-path="classification-1.html"><a href="classification-1.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.4</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.5" data-path="classification-1.html"><a href="classification-1.html#confusion-matrix"><i class="fa fa-check"></i><b>21.5</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.6" data-path="classification-1.html"><a href="classification-1.html#performance-measures"><i class="fa fa-check"></i><b>21.6</b> Performance measures</a></li>
<li class="chapter" data-level="21.7" data-path="classification-1.html"><a href="classification-1.html#roc-curve"><i class="fa fa-check"></i><b>21.7</b> ROC Curve</a></li>
<li class="chapter" data-level="21.8" data-path="classification-1.html"><a href="classification-1.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.8</b> AUC - Area Under the Curve</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html"><i class="fa fa-check"></i><b>22</b> Causal Inference for Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.5</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.6" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.6</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.7</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.8" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#speed"><i class="fa fa-check"></i><b>22.8</b> Speed</a></li>
<li class="chapter" data-level="22.9" data-path="causal-inference-for-time-series.html"><a href="causal-inference-for-time-series.html#ci-for-ts"><i class="fa fa-check"></i><b>22.9</b> CI for TS</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="causal-forecasting.html"><a href="causal-forecasting.html"><i class="fa fa-check"></i><b>23</b> Causal Forecasting</a>
<ul>
<li class="chapter" data-level="23.1" data-path="causal-forecasting.html"><a href="causal-forecasting.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="causal-forecasting.html"><a href="causal-forecasting.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="causal-forecasting.html"><a href="causal-forecasting.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="causal-forecasting.html"><a href="causal-forecasting.html#random-forest"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="causal-forecasting.html"><a href="causal-forecasting.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.5</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="ate-with-support-vector-machine.html"><a href="ate-with-support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> ATE with Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="ate-with-support-vector-machine.html"><a href="ate-with-support-vector-machine.html#support-vector-machine"><i class="fa fa-check"></i><b>24.1</b> Support Vector Machine</a></li>
<li class="chapter" data-level="24.2" data-path="ate-with-support-vector-machine.html"><a href="ate-with-support-vector-machine.html#ate-with-svm"><i class="fa fa-check"></i><b>24.2</b> ATE with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.4</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.5</b> Regularized Covariance Matrix</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.4</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="causal-component-analysis.html"><a href="causal-component-analysis.html"><i class="fa fa-check"></i><b>29</b> Causal Component Analysis</a>
<ul>
<li class="chapter" data-level="29.1" data-path="causal-component-analysis.html"><a href="causal-component-analysis.html#pca-principle-component-analysis"><i class="fa fa-check"></i><b>29.1</b> PCA (Principle Component Analysis)</a></li>
<li class="chapter" data-level="29.2" data-path="causal-component-analysis.html"><a href="causal-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.2</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.1</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.2</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a></li>
<li class="chapter" data-level="32" data-path="text-based-causal-inference.html"><a href="text-based-causal-inference.html"><i class="fa fa-check"></i><b>32</b> Text-based Causal Inference</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.1</b> Regression splines</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.2</b> MARS</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.3</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuksel/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Causal MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="parametric-estimations---basics" class="section level1 hasAnchor" number="9">
<h1><span class="header-section-number">Chapter 9</span> Parametric Estimations - Basics<a href="parametric-estimations---basics.html#parametric-estimations---basics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="the-dichotomy-of-statistical-modeling-data-versus-algorithmic-approaches" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> The Dichotomy of Statistical Modeling: Data versus Algorithmic Approaches:<a href="parametric-estimations---basics.html#the-dichotomy-of-statistical-modeling-data-versus-algorithmic-approaches" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the realm of statistical modeling, a fundamental dichotomy exists, deeply rooted in the philosophies of data and algorithmic modeling. This section delves into these two distinct cultures, tracing their origins and exploring their implications in the field of statistics, reflecting on the critical insights provided by various experts in their review of Leo Breiman’s influential work in the Special Issue: Commentaries on Breimen’s Two Cultures paper (<a href="https://muse.jhu.edu/issue/45147" class="uri">https://muse.jhu.edu/issue/45147</a>).</p>
<p>The concept of ‘two cultures’ in statistical modeling can be traced back to CP Snow’s seminal work in 1959, which highlighted a divide between traditional and scientific cultures. This idea was further refined by Leo Breiman, a statistician from the University of California Berkeley, in his 2001 publication “Statistical modeling: The two cultures”. Breiman estimated that 98% of statisticians belonged to the data modeling culture, but he advocated strongly for the algorithmic modeling approach.</p>
<p><strong>Data Modeling Culture</strong></p>
<p>Data modeling (Classical Statistical Modeling) assumes a stochastic approach, relying on methodologies like linear regression, logistic regression, or the Cox model. The central focus here is estimating the model’s parameters from the data, with validation achieved through goodness-of-fit tests. This culture values the simplicity and understandability of the relationship between input variables and responses, often leading to the creation of multiple models from which the most suitable is chosen. However, its reliance on parametric models can be a limitation, as these models often struggle to accurately represent complex or high-dimensional data where the underlying model is not well understood or too complicated for simple models.</p>
<p><strong>Algorithmic Modeling Culture</strong></p>
<p>Contrastingly, algorithmic modeling (Machine Learning) views the model as a ‘black box’, emphasizing function estimation and techniques like neural networks and decision trees. This culture prioritizes predictive accuracy over the interpretability of the model. It generally performs better in situations where the relationship between input and output is complex or not well understood, or in handling large datasets. The takeaway here is a focus on finding robust solutions, exploring data thoroughly before modeling, and relying heavily on computers as partners in the modeling process. A significant limitation of this approach is the lack of interpretability; it’s often challenging to understand the reasons behind a model’s predictions, which can be problematic in fields where the rationale is as crucial as the outcome.</p>
<p><strong>Breiman’s Critique</strong></p>
<p>Breiman’s critique of the data modeling culture within the statistical community is multifaceted. He argues that an overemphasis on this approach has not only led to the development of irrelevant theory and, at times, questionable scientific conclusions but has also kept statisticians from adopting more suitable algorithmic models. This adherence to traditional data models, according to Breiman, has constrained statisticians to a limited scope of problems, often resulting in oversimplified solutions that fail to capture the complexity of real-world data. He points out that lessons from algorithmic modeling suggest a preference for multiplicity, where multiple models yield good predictions and where there’s a balance between simplicity and accuracy, especially when dealing with high-dimensional input. Contrary to the data modeling advice that cautions against high-dimensional scenarios, algorithmic modeling leans into this complexity, developing techniques to mitigate the risks of overfitting. Breiman underscores that the primary goal should be the acquisition of accurate information over interpretability, as exemplified in cases like survival data analysis where algorithmic models have demonstrated superior performance to traditional methods. Additionally, he laments that the field’s fixation on data models has precluded statisticians from engaging with exciting new problems that could benefit from algorithmic approaches.</p>
<p><strong>The Contemporary Landscape:</strong></p>
<p>In econometrics, the initial response to Breiman’s paper was one of bafflement, as the field was predominantly focused on causal effects. This causality-based focus led to a de-emphasis on R2 values and a drive to build credible cases for causal effect estimation.</p>
<p>Notable figures like Sir David Cox and Brad Efron contributed their perspectives, with Cox advocating for methodological diversity and cautioning against the mechanical application of models. Efron, while admiring Breiman’s openness to new ideas, questioned the overstated importance of prediction in Breiman’s approach. Breiman responded by clarifying that algorithmic modeling supplements rather than replaces data modeling, and that it has shown significant breakthroughs in complex prediction tasks.</p>
<p>Andrew Gelman provides a critical perspective on Breiman’s work. Gelman critiques Breiman’s dismissal of Bayesian methods, suggesting that despite Breiman’s misconceptions, his broader perspectives and contributions remain valuable. Gelman proposes applying Breiman’s algorithmic perspective to current statistical problems, emphasizing hierarchical modeling, model checking, and black-box computing. He agrees with Breiman on the importance of data usage and notes the effectiveness of modern big-data approaches, such as regularization. Gelman also identifies a division in statistical thinking between complex modeling and reduced-form approaches, observing that both of Breiman’s cultures fall within the complex modeling approach. Gelman proposes applying Breiman’s algorithmic perspective to current statistical problems in areas such as generalizing inference to new predictive problems, model checking, and addressing the challenges of black-box computing. These applications highlight the need to respect the unknown nature of data mechanisms and the importance of continuously improving models based on their fit to the data.</p>
<p>In recent years, there has been a significant convergence of econometrics and machine learning, as explored in the work of Guido Imbens and Susan Athey. Initially, these disciplines seemed to operate in silos, focusing on data analysis without significant interaction. However, recent developments have seen a growing synergy between the two, particularly in the realm of econometrics. Econometricians, who traditionally prioritized causal effect estimation over prediction, have increasingly adopted machine learning algorithms as valuable tools for economic data analysis.</p>
<p>Econometrics has historically emphasized the identification and estimation of causal effects. This focus manifested in methodologies that prioritized building credible cases for causal effect estimation over mere predictive accuracy. In their paper, they presented Two key examples illustrate this approach. First one is Estimating Supply and Demand Functions, and second one is Returns to Education in detail. Imbens and Athey conclude as the integration of machine learning methods into econometric causal modeling has been a game-changer. The adoption of algorithmic machine learning methods has enabled more effective estimation of average treatment effects and treatment effects conditional on observable characteristics. This development has enhanced the accuracy and depth of econometric analyses.</p>
<p>The integration of machine learning in econometrics has been a game-changer, especially in areas such as economics, healthcare and public policy. In healthcare, machine learning methods are being used to estimate treatment effects, particularly in situations where randomized controlled trials are not feasible. The use of machine learning in evaluating the effectiveness of new drugs or medical procedures. By applying advanced algorithms to large healthcare datasets, researchers can control for a wide range of confounding variables, thereby isolating the causal impact of a treatment. Another area where the fusion of econometrics and machine learning has been impactful is in analyzing the effectiveness of environmental policies. For instance, assessing the impact of carbon taxes or cap-and-trade policies on reducing greenhouse gas emissions requires a nuanced understanding of causal relationships in complex economic and environmental systems. Machine learning models, equipped with advanced prediction capabilities, are being integrated with econometric models to analyze the causal effects of these policies. By processing large datasets, including satellite data and emissions records, machine learning algorithms can uncover patterns and relationships that traditional econometric models might miss.</p>
<p>Reflecting on the developments over the past two decades, it is evident that Breiman’s perspective has gained substantial traction. The rise of deep learning and machine learning, particularly in prediction and classification tasks, highlights the increasing prominence of the algorithmic approach. This shift has led to algorithmic modeling contributing insights into various scientific fields and establishing predictive accuracy as a primary tool for model evaluation. there has been considerable progress in merging the machine learning algorithms with econometric methods. Developers of these algorithms have expanded their focus to include causal objectives and restrictions. This expansion has opened new avenues, such as causal discovery, demonstrating a fruitful convergence and mutual enrichment between the two cultures.</p>
<p>In conclusion, the dichotomy between data and algorithmic modeling cultures represents a fundamental aspect of statistical modeling, each with its strengths and limitations. The evolution of these cultures underscores the dynamic nature of statistical methodologies and their impact on our understanding of data in the modern world. This continuous dialogue between different perspectives enriches the field, paving the way for more robust and versatile statistical practices.</p>
<p>** include and clarify citations and quatations in the thext above</p>
<p><a href="http://www2.math.uu.se/~thulin/mm/breiman.pdf" class="uri">http://www2.math.uu.se/~thulin/mm/breiman.pdf</a></p>
<p><a href="https://www.quora.com/What-was-Leo-Breiman-trying-to-convey-in-his-research-paper-Statistical-Modeling-The-Two-Cultures-Was-he-trying-to-say-that-the-field-is-odd-Did-he-mean-that-algorithmic-modelling-machine-learning-is-superior-to-traditional-data-modelling" class="uri">https://www.quora.com/What-was-Leo-Breiman-trying-to-convey-in-his-research-paper-Statistical-Modeling-The-Two-Cultures-Was-he-trying-to-say-that-the-field-is-odd-Did-he-mean-that-algorithmic-modelling-machine-learning-is-superior-to-traditional-data-modelling</a></p>
<p><a href="https://www.uio.no/studier/emner/matnat/math/STK9200/h21/two_cultures_pdf.pdf" class="uri">https://www.uio.no/studier/emner/matnat/math/STK9200/h21/two_cultures_pdf.pdf</a></p>
<p><a href="http://www.stat.columbia.edu/~gelman/research/published/gelman_breiman.pdf" class="uri">http://www.stat.columbia.edu/~gelman/research/published/gelman_breiman.pdf</a></p>
<p><a href="https://muse.jhu.edu/article/799731/pdf" class="uri">https://muse.jhu.edu/article/799731/pdf</a></p>
<p>Leo Breiman <span class="citation">(<a href="#ref-Breiman_2001"><strong>Breiman_2001?</strong></a>)</span>: <a href="https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726">Statistical Modeling: The Two Cultures</a>:</p>
<p><strong>The Dichotomy of Statistical Modeling: Parametric vs Nonparametric Models</strong></p>
<p>Leo Breiman’s two cultures in statistical modeling, the data model approach and the algorithmic model approach, have a complex relationship with parametric and nonparametric models. The data model approach often leans towards parametric models, which operate under specific assumptions about the data’s underlying distribution, like normal or binomial distributions. These models, including linear regression, logistic regression, ANOVA, polynomial regression, and Poisson regression, are pivotal in estimating distribution parameters such as mean and standard deviation. They are known for their interpretability and efficiency in making predictions or inferences about the population, especially when the data closely adheres to their underlying assumptions. However, their effectiveness diminishes when these assumptions are violated, potentially leading to biased or inaccurate results.</p>
<p>On the other hand, the algorithmic model approach aligns more closely with nonparametric models, which do not assume a specific distribution for the data. This approach includes flexible and robust techniques such as k-Nearest Neighbors, Spearman rank correlation, kernel density estimation, and Decision Trees like CART. Nonparametric models are particularly useful when the data distribution is unknown or when it does not conform to the assumptions required by parametric models. They are often the preferred choice for handling ordinal or categorical data, or in situations where the assumptions of parametric models are not met. While nonparametric models are celebrated for their adaptability to a wide range of data structures, they can be less efficient and might possess lower statistical power compared to parametric models. These models, which include decision trees and neural networks, are adept at modeling complex, nonlinear relationships that parametric models might struggle with. However, their flexibility comes with the need for larger datasets for effective learning and a general trade-off in interpretability compared to parametric models</p>
<p>It’s important to recognize that the alignments between Leo Breiman’s two cultures in statistical modeling and parametric or nonparametric models are not rigid. The data model approach, which traditionally focuses on inference and understanding the underlying model, can incorporate nonparametric models. Conversely, the algorithmic approach, known for prioritizing predictive accuracy and often treating the model as a black box, can also utilize parametric models, depending on the specific context. This indicates that the decision between parametric and nonparametric models transcends mere preference, evolving into a strategic choice influenced by data characteristics and research objectives. While parametric models excel in estimating parameters within a known probability distribution, offering clarity and efficiency in many scenarios, nonparametric models provide a versatile and flexible alternative, especially valuable when dealing with data that does not conform to specific distributional assumptions or when the distribution is unknown.</p>
</div>
<div id="parametric-estimations" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Parametric Estimations<a href="parametric-estimations---basics.html#parametric-estimations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In <strong>parametric econometrics</strong> we assume that the data come from a generating process that takes the following form:</p>
<p><span class="math display">\[
y=X \beta+\varepsilon
\]</span></p>
<p>Model (<span class="math inline">\(X\)</span>’s) are determined by the researcher and probability theory is a foundation of econometrics</p>
<p>In <strong>Machine learning</strong> we do not make any assumption on how the data have been generated:</p>
<p><span class="math display">\[
y \approx m(X)
\]</span></p>
<p>Model (<span class="math inline">\(X\)</span>’s) is not selected by the researcher and probability theory is not required</p>
<p>Nonparametric econometrics makes the link between the two: <strong>Machine Learning: an extension of nonparametric econometrics</strong></p>
<p>To see the difference between two “cultures”, we start with parametric modeling in classification problems.</p>
<p>So far we have only considered models for numeric response variables. What happens if the response variable is categorical? Can we use linear models in these situations? Yes, we can. To understand how, let’s look at the model that we have been using, ordinary least-square (OLS) regression, which is actually a specific case of the more general, generalized linear model (GLM). So, in general, GLMs relate the mean of the response to a linear combination of the predictors, <span class="math inline">\(\eta(x)\)</span>, through the use of a link function, <span class="math inline">\(g()\)</span>. That is,</p>
<p><span class="math display" id="eq:5-1">\[\begin{equation}
\eta(\mathbf{x})=g(\mathrm{E}[Y | \mathbf{X}=\mathbf{x}]),
  \tag{9.1}
\end{equation}\]</span></p>
<p>Or,</p>
<p><span class="math display" id="eq:5-2">\[\begin{equation}
\eta(\mathbf{x})=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\ldots+\beta_{p-1} x_{p-1} = g(\mathrm{E}[Y | \mathbf{X}=\mathbf{x}])
  \tag{9.2}
\end{equation}\]</span></p>
<p>In the case of a OLS,</p>
<p><span class="math display">\[
g(\mathrm{E}[Y | \mathbf{X}=\mathbf{x}]) = E[Y | \mathbf{X}=\mathbf{x}],
\]</span></p>
<p>To illustrate the use of a GLM we’ll focus on the case of binary responses variable coded using 0 and 1. In practice, these 0 and 1s will code for two classes such as yes/no, committed-crime/not, sick/healthy, etc.</p>
<p><span class="math display">\[
Y=\left\{\begin{array}{ll}{1} &amp; {\text { yes }} \\ {0} &amp; {\text { no }}\end{array}\right.
\]</span></p>
</div>
<div id="lpm" class="section level2 hasAnchor" number="9.3">
<h2><span class="header-section-number">9.3</span> LPM<a href="parametric-estimations---basics.html#lpm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Estimate the regression function, which under squared error loss is the conditional mean of <span class="math inline">\(Y\)</span> , the response, given <span class="math inline">\(X\)</span>, the features. These goal are essentially the same. We want to fit a model that “generalizes” well, that is, works well on new data that was not used to fit the model. To do this, we want to use a model of appropriate flexibility so as not to overfit to the training data.</p>
<p>Linear models are a family of parametric models which assume that the regression function (outcome is known and continuous) is a linear combination of the features. The <span class="math inline">\(\beta\)</span> coefficients are model parameters that are learned from the data via least squares or maximum likelihood.</p>
<p>A linear classifier (like LPM and Logistic) is one where a “hyperplane” is formed by taking a linear combination of the features, such that one side of the hyperplane predicts one class and the other side predicts the other.
nonparametric classifier such as knn</p>
<p>Let’s use the same dataset, <strong>Vehicles</strong>, that we used in the lab sections and create a new variable, <code>mpg</code>:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="parametric-estimations---basics.html#cb100-1" tabindex="-1"></a><span class="co">#Inspect the data before doing anything</span></span>
<span id="cb100-2"><a href="parametric-estimations---basics.html#cb100-2" tabindex="-1"></a><span class="fu">library</span>(fueleconomy)  <span class="co">#install.packages(&quot;fueleconomy&quot;)</span></span>
<span id="cb100-3"><a href="parametric-estimations---basics.html#cb100-3" tabindex="-1"></a><span class="fu">data</span>(vehicles)</span>
<span id="cb100-4"><a href="parametric-estimations---basics.html#cb100-4" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(vehicles)</span>
<span id="cb100-5"><a href="parametric-estimations---basics.html#cb100-5" tabindex="-1"></a></span>
<span id="cb100-6"><a href="parametric-estimations---basics.html#cb100-6" tabindex="-1"></a><span class="co">#Keep only observations without NA</span></span>
<span id="cb100-7"><a href="parametric-estimations---basics.html#cb100-7" tabindex="-1"></a><span class="fu">dim</span>(df)</span></code></pre></div>
<pre><code>## [1] 33442    12</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="parametric-estimations---basics.html#cb102-1" tabindex="-1"></a>data <span class="ot">&lt;-</span> df[<span class="fu">complete.cases</span>(df), ]</span>
<span id="cb102-2"><a href="parametric-estimations---basics.html#cb102-2" tabindex="-1"></a><span class="fu">dim</span>(data)</span></code></pre></div>
<pre><code>## [1] 33382    12</code></pre>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="parametric-estimations---basics.html#cb104-1" tabindex="-1"></a><span class="co">#Let&#39;s create a binary variable, mpg = 1 if hwy &gt; mean(hwy), 0 otherwise</span></span>
<span id="cb104-2"><a href="parametric-estimations---basics.html#cb104-2" tabindex="-1"></a>mpg <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">nrow</span>(data))) <span class="co">#Create vector mpg</span></span>
<span id="cb104-3"><a href="parametric-estimations---basics.html#cb104-3" tabindex="-1"></a>data2 <span class="ot">&lt;-</span> <span class="fu">cbind</span>(data, mpg) <span class="co"># add it to data</span></span>
<span id="cb104-4"><a href="parametric-estimations---basics.html#cb104-4" tabindex="-1"></a>data2<span class="sc">$</span>mpg[data2<span class="sc">$</span>hwy <span class="sc">&gt;</span> <span class="fu">mean</span>(data2<span class="sc">$</span>hwy)] <span class="ot">&lt;-</span> <span class="dv">1</span></span></code></pre></div>
<p>We are going to have a model that will predict whether the vehicle is a high mpg (i.e. <code>mpg</code> = 1) or low mpg (<code>mpg</code> = 0) car. As you notice, we have lots of character variables. Our model cannot accept character variables, but we can convert them into factor variables that give each unique character variable a number. This allows our model to accept our data. Let’s convert them to factor variables now:</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="parametric-estimations---basics.html#cb105-1" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(data2)) {</span>
<span id="cb105-2"><a href="parametric-estimations---basics.html#cb105-2" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">is.character</span>(data2[,i])) data2[,i] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(data2[,i])</span>
<span id="cb105-3"><a href="parametric-estimations---basics.html#cb105-3" tabindex="-1"></a>}</span>
<span id="cb105-4"><a href="parametric-estimations---basics.html#cb105-4" tabindex="-1"></a><span class="fu">str</span>(data2)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    33382 obs. of  13 variables:
##  $ id   : num  13309 13310 13311 14038 14039 ...
##  $ make : Factor w/ 124 levels &quot;Acura&quot;,&quot;Alfa Romeo&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ model: Factor w/ 3174 levels &quot;1-Ton Truck 2WD&quot;,..: 28 28 28 29 29 29 29 29 29 30 ...
##  $ year : num  1997 1997 1997 1998 1998 ...
##  $ class: Factor w/ 34 levels &quot;Compact Cars&quot;,..: 29 29 29 29 29 29 29 29 29 1 ...
##  $ trans: Factor w/ 46 levels &quot;Auto (AV-S6)&quot;,..: 32 43 32 32 43 32 32 43 32 32 ...
##  $ drive: Factor w/ 7 levels &quot;2-Wheel Drive&quot;,..: 5 5 5 5 5 5 5 5 5 5 ...
##  $ cyl  : num  4 4 6 4 4 6 4 4 6 5 ...
##  $ displ: num  2.2 2.2 3 2.3 2.3 3 2.3 2.3 3 2.5 ...
##  $ fuel : Factor w/ 12 levels &quot;CNG&quot;,&quot;Diesel&quot;,..: 11 11 11 11 11 11 11 11 11 7 ...
##  $ hwy  : num  26 28 26 27 29 26 27 29 26 23 ...
##  $ cty  : num  20 22 18 19 21 17 20 21 17 18 ...
##  $ mpg  : num  1 1 1 1 1 1 1 1 1 0 ...</code></pre>
<p>Done! We are ready to have a model to predict <code>mpg</code>. For now, we’ll use only <code>fuel</code>.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="parametric-estimations---basics.html#cb107-1" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> fuel <span class="sc">+</span> <span class="dv">0</span>, <span class="at">data =</span> data2) <span class="co">#No intercept</span></span>
<span id="cb107-2"><a href="parametric-estimations---basics.html#cb107-2" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ fuel + 0, data = data2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.8571 -0.4832 -0.2694  0.5168  0.7306 
## 
## Coefficients:
##                                 Estimate Std. Error t value Pr(&gt;|t|)    
## fuelCNG                         0.362069   0.065383   5.538 3.09e-08 ***
## fuelDiesel                      0.479405   0.016843  28.463  &lt; 2e-16 ***
## fuelGasoline or E85             0.269415   0.015418  17.474  &lt; 2e-16 ***
## fuelGasoline or natural gas     0.277778   0.117366   2.367   0.0180 *  
## fuelGasoline or propane         0.000000   0.176049   0.000   1.0000    
## fuelMidgrade                    0.302326   0.075935   3.981 6.87e-05 ***
## fuelPremium                     0.507717   0.005364  94.650  &lt; 2e-16 ***
## fuelPremium and Electricity     1.000000   0.497942   2.008   0.0446 *  
## fuelPremium Gas or Electricity  0.857143   0.188205   4.554 5.27e-06 ***
## fuelPremium or E85              0.500000   0.053081   9.420  &lt; 2e-16 ***
## fuelRegular                     0.483221   0.003311 145.943  &lt; 2e-16 ***
## fuelRegular Gas and Electricity 1.000000   0.176049   5.680 1.36e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4979 on 33370 degrees of freedom
## Multiple R-squared:  0.4862, Adjusted R-squared:  0.486 
## F-statistic:  2631 on 12 and 33370 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>What we estimated is LPM. Since <span class="math inline">\(Y\)</span> is 1 or 0,</p>
<p><span class="math display">\[
E[Y | \mathbf{X}=\mathbf{Regular}]) = Probability(Y|X = \mathbf{Regular}),
\]</span></p>
<p>In this context, the link function is called “identity” because it directly “links” the probability to the linear function of the predictor variables. Let’s see if we can verify this:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="parametric-estimations---basics.html#cb109-1" tabindex="-1"></a>tab <span class="ot">&lt;-</span> <span class="fu">table</span>(data2<span class="sc">$</span>fuel, data2<span class="sc">$</span>mpg)</span>
<span id="cb109-2"><a href="parametric-estimations---basics.html#cb109-2" tabindex="-1"></a><span class="fu">ftable</span>(<span class="fu">addmargins</span>(tab))</span></code></pre></div>
<pre><code>##                                  0     1   Sum
##                                               
## CNG                             37    21    58
## Diesel                         455   419   874
## Gasoline or E85                762   281  1043
## Gasoline or natural gas         13     5    18
## Gasoline or propane              8     0     8
## Midgrade                        30    13    43
## Premium                       4242  4375  8617
## Premium and Electricity          0     1     1
## Premium Gas or Electricity       1     6     7
## Premium or E85                  44    44    88
## Regular                      11688 10929 22617
## Regular Gas and Electricity      0     8     8
## Sum                          17280 16102 33382</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="parametric-estimations---basics.html#cb111-1" tabindex="-1"></a><span class="fu">prop.table</span>(tab, <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##                              
##                                       0         1
##   CNG                         0.6379310 0.3620690
##   Diesel                      0.5205950 0.4794050
##   Gasoline or E85             0.7305849 0.2694151
##   Gasoline or natural gas     0.7222222 0.2777778
##   Gasoline or propane         1.0000000 0.0000000
##   Midgrade                    0.6976744 0.3023256
##   Premium                     0.4922827 0.5077173
##   Premium and Electricity     0.0000000 1.0000000
##   Premium Gas or Electricity  0.1428571 0.8571429
##   Premium or E85              0.5000000 0.5000000
##   Regular                     0.5167794 0.4832206
##   Regular Gas and Electricity 0.0000000 1.0000000</code></pre>
<p>Yes! That’s why OLS with a binary <span class="math inline">\(Y\)</span> is actually LPM. That is,</p>
<p><span class="math display">\[
Pr[Y = 1 | x=\mathbf{Regular}]) = \beta_{0}+\beta_{1} x_{i}.
\]</span></p>
<p>A more formal explanation is related to how <span class="math inline">\(Y\)</span> is distributed. Since <span class="math inline">\(Y\)</span> has only 2 possible outcomes (1 and 0), it has a specific probability distribution. First, let’s refresh our memories about Binomial and Bernoulli distributions. In general, if a random variable, <span class="math inline">\(X\)</span>, follows the <strong>binomial distribution</strong> with parameters <span class="math inline">\(n \in \mathbb{N}\)</span> and <span class="math inline">\(p \in [0,1]\)</span>, we write <span class="math inline">\(X \sim B(n, p)\)</span>. The probability of getting exactly <span class="math inline">\(k\)</span> successes in <span class="math inline">\(n\)</span> trials is given by the probability mass function:</p>
<p><span class="math display" id="eq:5-3">\[\begin{equation}
\operatorname{Pr}(X=k)=\left(\begin{array}{l}{n} \\ {k}\end{array}\right) p^{k}(1-p)^{n-k}
  \tag{9.3}
\end{equation}\]</span>
for <span class="math inline">\(k = 0, 1, 2, ..., n\)</span>, where</p>
<p><span class="math display">\[
\left(\begin{array}{l}{n} \\ {k}\end{array}\right)=\frac{n !}{k !(n-k) !}
\]</span></p>
<p>Formula 5.3 can be understood as follows: <span class="math inline">\(k\)</span> successes occur with probability <span class="math inline">\(p^k\)</span> and <span class="math inline">\(n-k\)</span> failures occur with probability <span class="math inline">\((1-p)^{n−k}\)</span>. However, the <span class="math inline">\(k\)</span> successes can occur anywhere among the <span class="math inline">\(n\)</span> trials, and there are <span class="math inline">\(n!/k!(n!-k!)\)</span> different ways of distributing <span class="math inline">\(k\)</span> successes in a sequence of <span class="math inline">\(n\)</span> trials. Suppose a <em>biased coin</em> comes up heads with probability 0.3 when tossed. What is the probability of achieving 4 heads after 6 tosses?</p>
<p><span class="math display">\[
\operatorname{Pr}(4 \text { heads})=f(4)=\operatorname{Pr}(X=4)=\left(\begin{array}{l}{6} \\ {4}\end{array}\right) 0.3^{4}(1-0.3)^{6-4}=0.059535
\]</span></p>
<p>The <strong>Bernoulli distribution</strong> on the other hand, is a discrete probability distribution of a random variable which takes the value 1 with probability <span class="math inline">\(p\)</span> and the value 0 with probability <span class="math inline">\(q = (1 - p)\)</span>, that is, the probability distribution of any single experiment that asks a yes–no question. The <strong>Bernoulli distribution</strong> is a special case of the <strong>binomial distribution</strong>, where <span class="math inline">\(n = 1\)</span>. Symbolically, <span class="math inline">\(X \sim B(1, p)\)</span> has the same meaning as <span class="math inline">\(X \sim Bernoulli(p)\)</span>. Conversely, any binomial distribution, <span class="math inline">\(B(n, p)\)</span>, is the distribution of the sum of <span class="math inline">\(n\)</span> Bernoulli trials, <span class="math inline">\(Bernoulli(p)\)</span>, each with the same probability <span class="math inline">\(p\)</span>.</p>
<p><span class="math display">\[
\operatorname{Pr}(X=k) =p^{k}(1-p)^{1-k} \quad \text { for } k \in\{0,1\}
\]</span></p>
<p>Formally, the outcomes <span class="math inline">\(Y_i\)</span> are described as being Bernoulli-distributed data, where each outcome is determined by an unobserved probability <span class="math inline">\(p_i\)</span> that is specific to the outcome at hand, but related to the explanatory variables. This can be expressed in any of the following equivalent forms:</p>
<p><span class="math display" id="eq:5-4">\[\begin{equation}
\operatorname{Pr}\left(Y_{i}=y | x_{1, i}, \ldots, x_{m, i}\right)=\left\{\begin{array}{ll}{p_{i}} &amp; {\text { if } y=1} \\ {1-p_{i}} &amp; {\text { if } y=0}\end{array}\right.
  \tag{9.4}
\end{equation}\]</span></p>
<p>The expression 5.4 is the probability mass function of the Bernoulli distribution, specifying the probability of seeing each of the two possible outcomes. Similarly, this can be written as follows, which avoids having to write separate cases and is more convenient for certain types of calculations. This relies on the fact that <span class="math inline">\(Y_{i}\)</span> can take only the value 0 or 1. In each case, one of the exponents will be 1, which will make the outcome either <span class="math inline">\(p_{i}\)</span> or 1−<span class="math inline">\(p_{i}\)</span>, as in 5.4.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><span class="math display">\[
\operatorname{Pr}\left(Y_{i}=y | x_{1, i}, \ldots, x_{m, i}\right)=p_{i}^{y}\left(1-p_{i}\right)^{(1-y)}
\]</span></p>
<p>Hence this shows that</p>
<p><span class="math display">\[
\operatorname{Pr}\left(Y_{i}=1 | x_{1, i}, \ldots, x_{m, i}\right)=p_{i}=E[Y_{i}  | \mathbf{X}=\mathbf{x}])
\]</span></p>
<p>Let’s have a more complex model:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="parametric-estimations---basics.html#cb113-1" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> fuel <span class="sc">+</span> drive <span class="sc">+</span> cyl, <span class="at">data =</span> data2)</span>
<span id="cb113-2"><a href="parametric-estimations---basics.html#cb113-2" tabindex="-1"></a><span class="fu">summary</span>(model2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ fuel + drive + cyl, data = data2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.09668 -0.21869  0.01541  0.12750  0.97032 
## 
## Coefficients:
##                                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                      0.858047   0.049540  17.320  &lt; 2e-16 ***
## fuelDiesel                       0.194540   0.047511   4.095 4.24e-05 ***
## fuelGasoline or E85              0.030228   0.047277   0.639  0.52258    
## fuelGasoline or natural gas      0.031187   0.094466   0.330  0.74129    
## fuelGasoline or propane          0.031018   0.132069   0.235  0.81432    
## fuelMidgrade                     0.214471   0.070592   3.038  0.00238 ** 
## fuelPremium                      0.189008   0.046143   4.096 4.21e-05 ***
## fuelPremium and Electricity      0.746139   0.353119   2.113  0.03461 *  
## fuelPremium Gas or Electricity   0.098336   0.140113   0.702  0.48279    
## fuelPremium or E85               0.307425   0.059412   5.174 2.30e-07 ***
## fuelRegular                      0.006088   0.046062   0.132  0.89485    
## fuelRegular Gas and Electricity  0.092330   0.132082   0.699  0.48454    
## drive4-Wheel Drive               0.125323   0.020832   6.016 1.81e-09 ***
## drive4-Wheel or All-Wheel Drive -0.053057   0.016456  -3.224  0.00126 ** 
## driveAll-Wheel Drive             0.333921   0.018879  17.687  &lt; 2e-16 ***
## driveFront-Wheel Drive           0.497978   0.016327  30.499  &lt; 2e-16 ***
## drivePart-time 4-Wheel Drive    -0.078447   0.039258  -1.998  0.04570 *  
## driveRear-Wheel Drive            0.068346   0.016265   4.202 2.65e-05 ***
## cyl                             -0.112089   0.001311 -85.488  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3501 on 33363 degrees of freedom
## Multiple R-squared:  0.5094, Adjusted R-squared:  0.5091 
## F-statistic:  1924 on 18 and 33363 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Since OLS is a “Gaussian” member of GLS family, we can also estimate it as GLS. We use glm() and define the family as “gaussian”.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="parametric-estimations---basics.html#cb115-1" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> fuel <span class="sc">+</span> drive <span class="sc">+</span> cyl, <span class="at">family =</span> gaussian, <span class="at">data =</span> data2)</span>
<span id="cb115-2"><a href="parametric-estimations---basics.html#cb115-2" tabindex="-1"></a><span class="co">#You can check it by comparing model2 above to summary(model3)</span></span>
<span id="cb115-3"><a href="parametric-estimations---basics.html#cb115-3" tabindex="-1"></a><span class="co">#Let&#39;s check only the coefficients  </span></span>
<span id="cb115-4"><a href="parametric-estimations---basics.html#cb115-4" tabindex="-1"></a><span class="fu">identical</span>(<span class="fu">round</span>(<span class="fu">coef</span>(model2),<span class="dv">2</span>), <span class="fu">round</span>(<span class="fu">coef</span>(model3),<span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>With this LPM model, we can now predict the classification of future cars in term of high (<code>mpg</code> = 1) or low (<code>mpg</code> = 0), which was our objective. Let’s see how successful we are in identifying cars with <code>mpg</code> = 1 in our own sample.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="parametric-estimations---basics.html#cb117-1" tabindex="-1"></a><span class="co">#How many cars we have with mpg = 1 and mpg = 0 in our data</span></span>
<span id="cb117-2"><a href="parametric-estimations---basics.html#cb117-2" tabindex="-1"></a><span class="fu">table</span>(data2<span class="sc">$</span>mpg) </span></code></pre></div>
<pre><code>## 
##     0     1 
## 17280 16102</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="parametric-estimations---basics.html#cb119-1" tabindex="-1"></a><span class="co">#In-sample fitted values or predicted probabilities for mpg = 1</span></span>
<span id="cb119-2"><a href="parametric-estimations---basics.html#cb119-2" tabindex="-1"></a><span class="co">#Remember our E(Y|X) is Pr(Y=1|X)</span></span>
<span id="cb119-3"><a href="parametric-estimations---basics.html#cb119-3" tabindex="-1"></a>mpg_hat <span class="ot">&lt;-</span> <span class="fu">fitted</span>(model2)</span>
<span id="cb119-4"><a href="parametric-estimations---basics.html#cb119-4" tabindex="-1"></a></span>
<span id="cb119-5"><a href="parametric-estimations---basics.html#cb119-5" tabindex="-1"></a><span class="co">#If you think that any predicted mpg above 0.5 should be consider 1 then</span></span>
<span id="cb119-6"><a href="parametric-estimations---basics.html#cb119-6" tabindex="-1"></a><span class="fu">length</span>(mpg_hat[mpg_hat <span class="sc">&gt;</span> <span class="fl">0.5</span>]) </span></code></pre></div>
<pre><code>## [1] 14079</code></pre>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="parametric-estimations---basics.html#cb121-1" tabindex="-1"></a><span class="fu">length</span>(mpg_hat[mpg_hat <span class="sc">&lt;=</span> <span class="fl">0.5</span>])</span></code></pre></div>
<pre><code>## [1] 19303</code></pre>
<p>This is Problem 1: we are using 0.5 as our threshold (discriminating) probability to convert predicted probabilities to predicted “labels”. When we use 0.5 as our threshold probability though, our prediction is significantly off: we predict many cars with <code>mpg</code> = 0 as having <code>mpg</code> = 1.</p>
<p>And here is Problem 2:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="parametric-estimations---basics.html#cb123-1" tabindex="-1"></a><span class="fu">summary</span>(mpg_hat)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -0.7994  0.2187  0.4429  0.4824  0.9138  1.2088</code></pre>
<p>The predicted probabilities (of <code>mpg</code> = 1) are not bounded between 1 and 0. We will talk about these issues later. First let’s see our next classification model or GLM.</p>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="9.4">
<h2><span class="header-section-number">9.4</span> Logistic Regression<a href="parametric-estimations---basics.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Logistic Regression
Linear vs. Logistic Probability Models, which is better and when? We will briefly talk about it here. You can find a nice summary by Paul Von Hippel here as well (<a href="https://statisticalhorizons.com/linear-vs-logistic" class="uri">https://statisticalhorizons.com/linear-vs-logistic</a>) <span class="citation">(<a href="#ref-Hippel_2015"><strong>Hippel_2015?</strong></a>)</span>.</p>
<p>First, let’s define some notation that we will use throughout. (Note that many machine learning texts use <span class="math inline">\(p\)</span> as the number of parameters. Here we use as a notation for probability. You should be aware of it.)</p>
<p><span class="math display">\[
p(\mathbf{x})=P[Y=1 | \mathbf{X}=\mathbf{x}]
\]</span></p>
<p>With a binary (Bernoulli) response, we will mostly focus on the case when <span class="math inline">\(Y = 1\)</span>, since, with only two possibilities, it is trivial to obtain probabilities when <span class="math inline">\(Y = 0\)</span>.</p>
<p><span class="math display">\[
\begin{array}{c}{P[Y=0 | \mathbf{X}=\mathbf{x}]+P[Y=1 | \mathbf{X}=\mathbf{x}]=1} \\\\ {P[Y=0 | \mathbf{X}=\mathbf{x}]=1-p(\mathbf{x})}\end{array}
\]</span></p>
<p>An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a sigmoid function, which takes any real input <span class="math inline">\(z\)</span>, and outputs a value between zero and one. The standard logistic function is defined as follows:</p>
<p><span class="math display" id="eq:5-5">\[\begin{equation}
\sigma(z)=\frac{e^{z}}{e^{z}+1}=\frac{1}{1+e^{-z}}
  \tag{9.5}
\end{equation}\]</span></p>
<p>Let’s see possible <span class="math inline">\(\sigma(z)\)</span> values and plot them against <span class="math inline">\(z\)</span>.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="parametric-estimations---basics.html#cb125-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb125-2"><a href="parametric-estimations---basics.html#cb125-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb125-3"><a href="parametric-estimations---basics.html#cb125-3" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>,<span class="dv">2</span>)</span>
<span id="cb125-4"><a href="parametric-estimations---basics.html#cb125-4" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>x))</span>
<span id="cb125-5"><a href="parametric-estimations---basics.html#cb125-5" tabindex="-1"></a><span class="fu">plot</span>(sigma <span class="sc">~</span> x, <span class="at">col =</span><span class="st">&quot;blue&quot;</span>, <span class="at">cex.axis =</span> <span class="fl">0.7</span>)</span></code></pre></div>
<p><img src="09-ParametricEst_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>This logistic function is nice because: (1) whatever the <span class="math inline">\(x\)</span>’s are <span class="math inline">\(\sigma(z)\)</span> is always between 0 and 1, (2) The effect of <span class="math inline">\(x\)</span> on <span class="math inline">\(\sigma(z)\)</span> is not linear. That is, there is lower and upper thresholds in <span class="math inline">\(x\)</span> that before and after those values (around -2 and 2 here) the marginal effect of <span class="math inline">\(x\)</span> on <span class="math inline">\(\sigma(z)\)</span> is very low. Therefore, it seems that if we use a logistic function and replace <span class="math inline">\(\sigma(z)\)</span> with <span class="math inline">\(p(x)\)</span>, we can solve issues related to these 2 major drawbacks of LPM.</p>
<p>Let us assume that <span class="math inline">\(z = y = \beta_{0}+\beta_{1} x_{1}\)</span>, the general logistic function can now be written as:</p>
<p><span class="math display" id="eq:5-6">\[\begin{equation}
p(x)=P[Y=1|\mathbf{X}=\mathbf{x}]=\frac{1}{1+e^{-\left(\beta_{0}+\beta_{1} x\right)}}
  \tag{9.6}
\end{equation}\]</span></p>
<p>To understand why nonlinearity would be a desirable future in some probability predictions, let’s imagine we try to predict the effect of saving (<span class="math inline">\(x\)</span>) on home ownership (<span class="math inline">\(p(x)\)</span>). If you have no saving now (<span class="math inline">\(x=0\)</span>), additional $10K saving would not make a significant difference in your decision to buy a house (<span class="math inline">\(P(Y=1|x)\)</span>). Similarly, when you have $500K (<span class="math inline">\(x\)</span>) saving but without having house, additional $10K (<span class="math inline">\(dx\)</span>) saving should not make you buy a house (with $500K, you could’ve bought a house already, had you wanted one). That is why flat lower and upper tails of <span class="math inline">\(\sigma(z)\)</span> are nice futures reflecting very low marginal effects of <span class="math inline">\(x\)</span> on the probability of having a house in this case.</p>
<p>After a simple algebra, we can also write the same function as follows,</p>
<p><span class="math display" id="eq:5-7">\[\begin{equation}
\ln \left(\frac{p(x)}{1-p(x)}\right)=\beta_{0}+\beta_{1} x,
  \tag{9.7}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(p(x)/[1-p(x)]\)</span> is called <em>odds</em>, a ratio of success over failure. The natural log (ln) of this ratio is called, <strong>log odds</strong>, or <strong>Logit</strong>, usually denoted as <span class="math inline">\(\mathbf(L)\)</span>. Let’s see if this expression is really linear.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="parametric-estimations---basics.html#cb126-1" tabindex="-1"></a>p_x <span class="ot">&lt;-</span> sigma</span>
<span id="cb126-2"><a href="parametric-estimations---basics.html#cb126-2" tabindex="-1"></a>Logit <span class="ot">&lt;-</span> <span class="fu">log</span>(p_x<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>p_x)) <span class="co">#By defult log() calculates natural logarithms</span></span>
<span id="cb126-3"><a href="parametric-estimations---basics.html#cb126-3" tabindex="-1"></a><span class="fu">plot</span>(Logit <span class="sc">~</span> x, <span class="at">col =</span><span class="st">&quot;red&quot;</span>, <span class="at">cex.axis =</span> <span class="fl">0.7</span>)</span></code></pre></div>
<p><img src="09-ParametricEst_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>In many cases, researchers use a logistic function, when the outcome variable in a regression is dichotomous. Although there are situations where the linear model is clearly problematic (as described above), there are many common situations where the linear model is just fine, and even has advantages. Let’s start by comparing the two models explicitly. If the outcome <span class="math inline">\(Y\)</span> is dichotomous with values 1 and 0, we define <span class="math inline">\(P[Y=1|X] = E(Y|X)\)</span> as proved earlier, which is just the probability that <span class="math inline">\(Y\)</span> is 1, given some value of the regressors <span class="math inline">\(X\)</span>. Then the linear and logistic probability models are:</p>
<p><span class="math display">\[
P[Y = 1|\mathbf{X}=\mathbf{x}]=E[Y | \mathbf{X}=\mathbf{x}] = \beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\ldots+\beta_{k} x_{k},
\]</span>
<span class="math inline">\(~\)</span></p>
<p><span class="math display">\[
\ln \left(\frac{P[Y=1|\mathbf{X}]}{1-P[Y=1|\mathbf{X}]}\right)=\beta_{0}+\beta_{1} x_{1}+\ldots+\beta_{k} x_{k}
\]</span></p>
<p><span class="math inline">\(~\)</span></p>
<p>The linear model assumes that the probability <span class="math inline">\(P\)</span> is a linear function of the regressors, while the logistic model assumes that the natural log of the odds <span class="math inline">\(P/(1-P)\)</span> is a linear function of the regressors. Note that applying the inverse logit transformation allows us to obtain an expression for <span class="math inline">\(P(x)\)</span>. With LPM you don’t need that transformation to have <span class="math inline">\(P(x)\)</span>. While LPM can be estimated easily with OLS, the Logistic model needs MLE.</p>
<p><span class="math display">\[
p(\mathbf{x})=E[Y | \mathbf{X}=\mathbf{x}]=P[Y=1 | \mathbf{X}=\mathbf{x}]=\frac{1}{1+e^{-(\beta_{0}+\beta_{1} x_{1}+\cdots+\beta_{p-1} x_{(p-1)})}}
\]</span>
<span class="math inline">\(~\)</span></p>
<p>The major advantage of LPM is its interpretability. In the linear model, if <span class="math inline">\(\beta_{2}\)</span> is (say) 0.05, that means that a one-unit increase in <span class="math inline">\(x_{2}\)</span> is associated with a 5-percentage point increase in the probability that <span class="math inline">\(Y\)</span> is 1. Just about everyone has some understanding of what it would mean to increase by 5 percentage points their probability of, say, voting, or dying, or becoming obese. The logistic model is less interpretable. In the logistic model, if <span class="math inline">\(\beta_{1}\)</span> is 0.05, that means that a one-unit increase in <span class="math inline">\(x_{1}\)</span> is associated with a 0.05 “unit” increase in the log odds, <span class="math inline">\(\text{log}(P/{(1-P)})\)</span>, that <span class="math inline">\(Y\)</span> is 1. And what does that mean? I’ve never had any intuition for log odds. So you have to convert it to the odd ratio (OR) or use the above equation to calculate fitted (predicted) probabilities. Not a problem with R, Stata, etc.</p>
<p>But the main question is when we should use the logistic model? The logistic model is unavoidable if it fits the data much better than the linear model. And sometimes it does. But in many situations the linear model fits just as well, or almost as well, as the logistic model. In fact, in many situations, the linear and logistic model give results that are practically indistinguishable except that the logistic estimates are harder to interpret. Here is the difference: <strong>For the logistic model to fit better than the linear model, it must be the case that the log odds are a linear function of X, but the probability is not.</strong></p>
<p>Lets review these concepts in a simulation exercise:</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="parametric-estimations---basics.html#cb127-1" tabindex="-1"></a><span class="co">#Creating random data</span></span>
<span id="cb127-2"><a href="parametric-estimations---basics.html#cb127-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>) <span class="co"># In order to get the same data everytime </span></span>
<span id="cb127-3"><a href="parametric-estimations---basics.html#cb127-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">500</span> <span class="co"># number of observation</span></span>
<span id="cb127-4"><a href="parametric-estimations---basics.html#cb127-4" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">rnorm</span>(n) <span class="co"># this is our x</span></span>
<span id="cb127-5"><a href="parametric-estimations---basics.html#cb127-5" tabindex="-1"></a>z <span class="ot">=</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> x</span>
<span id="cb127-6"><a href="parametric-estimations---basics.html#cb127-6" tabindex="-1"></a></span>
<span id="cb127-7"><a href="parametric-estimations---basics.html#cb127-7" tabindex="-1"></a><span class="co">#Probablity is defined by a logistic function</span></span>
<span id="cb127-8"><a href="parametric-estimations---basics.html#cb127-8" tabindex="-1"></a><span class="co">#Therefore it is not a linear function of x!</span></span>
<span id="cb127-9"><a href="parametric-estimations---basics.html#cb127-9" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>z))</span>
<span id="cb127-10"><a href="parametric-estimations---basics.html#cb127-10" tabindex="-1"></a></span>
<span id="cb127-11"><a href="parametric-estimations---basics.html#cb127-11" tabindex="-1"></a><span class="co">#Remember Bernoulli distribution defines Y as 1 or 0 </span></span>
<span id="cb127-12"><a href="parametric-estimations---basics.html#cb127-12" tabindex="-1"></a><span class="co">#Bernoulli is the special case of the binomial distribution with size = 1</span></span>
<span id="cb127-13"><a href="parametric-estimations---basics.html#cb127-13" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> p)</span>
<span id="cb127-14"><a href="parametric-estimations---basics.html#cb127-14" tabindex="-1"></a></span>
<span id="cb127-15"><a href="parametric-estimations---basics.html#cb127-15" tabindex="-1"></a><span class="co">#And we create our data</span></span>
<span id="cb127-16"><a href="parametric-estimations---basics.html#cb127-16" tabindex="-1"></a>data <span class="ot">&lt;-</span>  <span class="fu">data.frame</span>(y, x)</span>
<span id="cb127-17"><a href="parametric-estimations---basics.html#cb127-17" tabindex="-1"></a><span class="fu">head</span>(data)</span></code></pre></div>
<pre><code>##   y          x
## 1 0 -0.6264538
## 2 0  0.1836433
## 3 0 -0.8356286
## 4 0  1.5952808
## 5 0  0.3295078
## 6 0 -0.8204684</code></pre>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="parametric-estimations---basics.html#cb129-1" tabindex="-1"></a><span class="fu">table</span>(y)</span></code></pre></div>
<pre><code>## y
##   0   1 
## 353 147</code></pre>
<p>We know that probablity is defined by a logistic function (see above). What happens if we fit it as LPM, which is <span class="math inline">\(Pr[Y = 1 | x=\mathbf{x}]) = \beta_{0}+\beta_{1} x_{i}\)</span>?</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="parametric-estimations---basics.html#cb131-1" tabindex="-1"></a>lpm <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data)</span>
<span id="cb131-2"><a href="parametric-estimations---basics.html#cb131-2" tabindex="-1"></a><span class="fu">summary</span>(lpm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.76537 -0.25866 -0.08228  0.28686  0.82338 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.28746    0.01567   18.34   &lt;2e-16 ***
## x            0.28892    0.01550   18.64   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3504 on 498 degrees of freedom
## Multiple R-squared:  0.411,  Adjusted R-squared:  0.4098 
## F-statistic: 347.5 on 1 and 498 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="parametric-estimations---basics.html#cb133-1" tabindex="-1"></a><span class="co">#Here is the plot Probabilities (fitted and DGM) vs x. </span></span>
<span id="cb133-2"><a href="parametric-estimations---basics.html#cb133-2" tabindex="-1"></a><span class="fu">plot</span>(x, p, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">0.7</span>, <span class="at">cex.axis =</span> <span class="fl">0.8</span>)</span>
<span id="cb133-3"><a href="parametric-estimations---basics.html#cb133-3" tabindex="-1"></a><span class="fu">abline</span>(lpm, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb133-4"><a href="parametric-estimations---basics.html#cb133-4" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Estimated Probability by LPM&quot;</span>, <span class="st">&quot;Probability&quot;</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb133-5"><a href="parametric-estimations---basics.html#cb133-5" tabindex="-1"></a><span class="at">pch =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;green&quot;</span>), <span class="at">cex =</span> <span class="fl">0.7</span>)</span></code></pre></div>
<p><img src="09-ParametricEst_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>How about a logistic regression?</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="parametric-estimations---basics.html#cb134-1" tabindex="-1"></a>logis <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data, <span class="at">family =</span> binomial)</span>
<span id="cb134-2"><a href="parametric-estimations---basics.html#cb134-2" tabindex="-1"></a><span class="fu">summary</span>(logis)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x, family = binomial, data = data)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -1.8253     0.1867  -9.776   &lt;2e-16 ***
## x             2.7809     0.2615  10.635   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 605.69  on 499  degrees of freedom
## Residual deviance: 328.13  on 498  degrees of freedom
## AIC: 332.13
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="parametric-estimations---basics.html#cb136-1" tabindex="-1"></a><span class="co">#Here is the plot Probabilities (fitted and DGM) vs x. </span></span>
<span id="cb136-2"><a href="parametric-estimations---basics.html#cb136-2" tabindex="-1"></a></span>
<span id="cb136-3"><a href="parametric-estimations---basics.html#cb136-3" tabindex="-1"></a><span class="fu">plot</span>(x, p, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">cex.lab =</span> <span class="fl">0.8</span>, <span class="at">cex.axis =</span> <span class="fl">0.8</span>)</span>
<span id="cb136-4"><a href="parametric-estimations---basics.html#cb136-4" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">predict</span>(logis, <span class="fu">data.frame</span>(x), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>), <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb136-5"><a href="parametric-estimations---basics.html#cb136-5" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;Estimated Probability by GLM&quot;</span>, <span class="st">&quot;Probability&quot;</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb136-6"><a href="parametric-estimations---basics.html#cb136-6" tabindex="-1"></a><span class="at">pch =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;green&quot;</span>), <span class="at">cex =</span> <span class="fl">0.7</span>)</span></code></pre></div>
<p><img src="09-ParametricEst_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>As you can see, the estimated logistic regression coefficients are in line with our DGM coefficients (-2, 3).</p>
<p><span class="math display">\[
\log \left(\frac{\hat{p}(\mathbf{x})}{1-\hat{p}(\mathbf{x})}\right)=-1.8253+2.7809 x
\]</span></p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Intuitively, when <span class="math inline">\(n=1\)</span>, achieving head once (<span class="math inline">\(k=1\)</span>) is <span class="math inline">\(P(head)= p^{k}(1-p)^{1-k}=p\)</span> or <span class="math inline">\(P(tail)= p^{k}(1-p)^{1-k}=1-p.\)</span><a href="parametric-estimations---basics.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-v.s.-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nonparametric-estimations---basics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuksel/machinemetrics/edit/master/09-ParametricEst.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
