
# Ensemble Methods

BTBA

## Bagging

TBA
## Boosting

TBA
### AdaBoost

TBA
  
### XGBoost

TBA

## Ensemble Applications

To conclude this section we will cover classification and regression applications using bagging, random forest and, boosting. First we will start with a classification problem.  In comparing different ensemble methods, we must look not only at their accuracy, but evaluate their stability as well.


## Classification
TBA
## Regression

TBA

## Exploration

TBA
  
## Boosting Applications

TBA

### Random search with parallel processing
  
Now, we will apply a random grid search introduced by Bergstra and Bengio in [Random Search for Hyper-Parameter Optimization](https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)) [@Bergs_2012].  This paper shows that randomly chosen trials are more efficient for hyperparameter optimization than trials on a grid.  Random search is a slight variation on grid search. Instead of searching over the entire grid, random search evaluates randomly selected parts on the grid. 

TBA

### Boosting vs. Others

TBA
  
### Classification with XGBoost

TBA

  
