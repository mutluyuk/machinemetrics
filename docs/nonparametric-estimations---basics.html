<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Nonparametric Estimations - Basics | MachineMetrics</title>
  <meta name="description" content="Chapter 10 Nonparametric Estimations - Basics | MachineMetrics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Nonparametric Estimations - Basics | MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Nonparametric Estimations - Basics | MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="parametric-estimations---basics.html"/>
<link rel="next" href="hyperparameter-tuning.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html"><i class="fa fa-check"></i><b>2</b> Statistical Models and Simulations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#distinguishing-between-statistical-modeling-and-machine-learning-in-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#goals-and-objectives"><i class="fa fa-check"></i><b>2.1.1</b> Goals and Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#prediction-vs.-inference"><i class="fa fa-check"></i><b>2.1.2</b> Prediction vs. Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#conclusion"><i class="fa fa-check"></i><b>2.1.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#model-selection-and-approaches-in-data-modeling"><i class="fa fa-check"></i><b>2.4</b> Model Selection and Approaches in Data Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#mle"><i class="fa fa-check"></i><b>3.5.3</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error---mse"><i class="fa fa-check"></i><b>5.1</b> Estimation error - MSE</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.2</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#technical-points-about-mse-and-mspe"><i class="fa fa-check"></i><b>5.3</b> Technical points about MSE and MSPE</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.1</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.1</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.2</b> LPM</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
<li class="chapter" data-level="10.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#knn"><i class="fa fa-check"></i><b>10.3</b> Knn</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#adult-dataset"><i class="fa fa-check"></i><b>10.3.1</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="11.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#grid-search"><i class="fa fa-check"></i><b>11.4</b> Grid Search</a></li>
<li class="chapter" data-level="11.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>11.5</b> Cross-validated grid search</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#one-variable"><i class="fa fa-check"></i><b>12.4.1</b> One-variable</a></li>
<li class="chapter" data-level="12.4.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>12.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="12.4.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#multivariable"><i class="fa fa-check"></i><b>12.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>16.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.3</b> Pruning</a></li>
<li class="chapter" data-level="16.4" data-path="regression-trees.html"><a href="regression-trees.html#classification-with-titanic"><i class="fa fa-check"></i><b>16.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="16.5" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>17.2</b> Random Forest</a></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>17.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="17.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="17.3.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.3.3</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.4</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.5</b> Classification</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.6</b> Regression</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.7</b> Exploration</a></li>
<li class="chapter" data-level="17.8" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.8</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression-1"><i class="fa fa-check"></i><b>17.8.1</b> Regression</a></li>
<li class="chapter" data-level="17.8.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.8.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.8.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.8.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.8.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-1"><i class="fa fa-check"></i><b>17.8.4</b> Classification</a></li>
<li class="chapter" data-level="17.8.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost.m1"><i class="fa fa-check"></i><b>17.8.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="17.8.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.8.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#out-sample-prediction-accuracy"><i class="fa fa-check"></i><b>20.3</b> out-sample prediction accuracy</a></li>
<li class="chapter" data-level="20.4" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-2.html"><a href="classification-2.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-2.html"><a href="classification-2.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-2.html"><a href="classification-2.html#mnist-dataset"><i class="fa fa-check"></i><b>21.2</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="21.3" data-path="classification-2.html"><a href="classification-2.html#linear-classifiers-again"><i class="fa fa-check"></i><b>21.3</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="21.4" data-path="classification-2.html"><a href="classification-2.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.4</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.5" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret"><i class="fa fa-check"></i><b>21.5</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="classification-2.html"><a href="classification-2.html#mnist_27"><i class="fa fa-check"></i><b>21.5.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="21.5.2" data-path="classification-2.html"><a href="classification-2.html#adult-dataset-1"><i class="fa fa-check"></i><b>21.5.2</b> Adult dataset</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="classification-2.html"><a href="classification-2.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.6</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.7" data-path="classification-2.html"><a href="classification-2.html#confusion-matrix"><i class="fa fa-check"></i><b>21.7</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.8" data-path="classification-2.html"><a href="classification-2.html#performance-measures"><i class="fa fa-check"></i><b>21.8</b> Performance measures</a></li>
<li class="chapter" data-level="21.9" data-path="classification-2.html"><a href="classification-2.html#roc-curve"><i class="fa fa-check"></i><b>21.9</b> ROC Curve</a></li>
<li class="chapter" data-level="21.10" data-path="classification-2.html"><a href="classification-2.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.10</b> AUC - Area Under the Curve</a></li>
<li class="chapter" data-level="21.11" data-path="classification-2.html"><a href="classification-2.html#classification-example"><i class="fa fa-check"></i><b>21.11</b> Classification Example</a></li>
<li class="chapter" data-level="21.12" data-path="classification-2.html"><a href="classification-2.html#lpm-1"><i class="fa fa-check"></i><b>21.12</b> LPM</a></li>
<li class="chapter" data-level="21.13" data-path="classification-2.html"><a href="classification-2.html#logistic-regression-1"><i class="fa fa-check"></i><b>21.13</b> Logistic Regression</a></li>
<li class="chapter" data-level="21.14" data-path="classification-2.html"><a href="classification-2.html#knn-1"><i class="fa fa-check"></i><b>21.14</b> kNN</a>
<ul>
<li class="chapter" data-level="21.14.1" data-path="classification-2.html"><a href="classification-2.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>21.14.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="21.14.2" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret-1"><i class="fa fa-check"></i><b>21.14.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>22</b> Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="time-series.html"><a href="time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="time-series.html"><a href="time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="time-series.html"><a href="time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="time-series.html"><a href="time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="time-series.html"><a href="time-series.html#stationarity"><i class="fa fa-check"></i><b>22.5</b> Stationarity</a></li>
<li class="chapter" data-level="22.6" data-path="time-series.html"><a href="time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.6</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="time-series.html"><a href="time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.7</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.8" data-path="time-series.html"><a href="time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.8</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.9" data-path="time-series.html"><a href="time-series.html#speed"><i class="fa fa-check"></i><b>22.9</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="forecast.html"><a href="forecast.html"><i class="fa fa-check"></i><b>23</b> Forecast</a>
<ul>
<li class="chapter" data-level="23.1" data-path="forecast.html"><a href="forecast.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="forecast.html"><a href="forecast.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="forecast.html"><a href="forecast.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="forecast.html"><a href="forecast.html#random-forest-1"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="forecast.html"><a href="forecast.html#univariate"><i class="fa fa-check"></i><b>23.5</b> Univariate</a></li>
<li class="chapter" data-level="23.6" data-path="forecast.html"><a href="forecast.html#multivariate"><i class="fa fa-check"></i><b>23.6</b> Multivariate</a></li>
<li class="chapter" data-level="23.7" data-path="forecast.html"><a href="forecast.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.7</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>24.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-margin"><i class="fa fa-check"></i><b>24.1.1</b> The Margin</a></li>
<li class="chapter" data-level="24.1.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-non-separable-case"><i class="fa fa-check"></i><b>24.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>24.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="24.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#application-with-svm"><i class="fa fa-check"></i><b>24.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#precision-matrix"><i class="fa fa-check"></i><b>27.4</b> Precision Matrix</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.5</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.6" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.6</b> Regularized Covariance Matrix</a></li>
<li class="chapter" data-level="27.7" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>27.7</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="27.8" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#high-dimensional-data"><i class="fa fa-check"></i><b>27.8</b> High-dimensional data</a></li>
<li class="chapter" data-level="27.9" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>27.9</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#rankr-approximations"><i class="fa fa-check"></i><b>28.4</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="28.5" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.5</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html"><i class="fa fa-check"></i><b>29</b> PCA (Principle Component Analysis)</a>
<ul>
<li class="chapter" data-level="29.1" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.1</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>30.1</b> Using bins</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>30.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="30.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>30.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="30.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="31.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>31.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="31.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>31.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="text-analysis.html"><a href="text-analysis.html"><i class="fa fa-check"></i><b>32</b> Text Analysis</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#other-nonparametric-estimation-methods-1"><i class="fa fa-check"></i><b>33.1</b> Other Nonparametric Estimation methods</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.2</b> Regression splines</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.3</b> MARS</a></li>
<li class="chapter" data-level="33.4" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.4</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuk/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nonparametric-estimations---basics" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> Nonparametric Estimations - Basics<a href="nonparametric-estimations---basics.html#nonparametric-estimations---basics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The models we see in the previous chapters are parametric, which means that they have to assume a certain structure on the regression function <span class="math inline">\(m\)</span> controlled by parameters before the estimations. Therefore, the results from parametric models are the best for estimating <span class="math inline">\(m\)</span>, if and only if their model specifications are correct. Avoiding this assumption is the strongest point of nonparametric methods, which do not require any hard-to-satisfy pre-determined regression functions.</p>
<p>Before talking about a nonparametric estimator for the regression function <span class="math inline">\(m\)</span>, we should first look at a simple nonparametric density estimation of the predictor <span class="math inline">\(X\)</span>. This estimator is aimed to estimate <span class="math inline">\(f(x)\)</span>, the density of <span class="math inline">\(X\)</span>, from a sample and without assuming any specific form for <span class="math inline">\(f\)</span>. That is, without assuming, for example, that the data is normally distributed. Therefore, we first start with nonparametric density estimations.</p>
<div id="density-estimations" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Density estimations<a href="nonparametric-estimations---basics.html#density-estimations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We are only going to look at one-variable Kernel density estimation. Let’s assume that a sample of <span class="math inline">\(n\)</span> observations, <span class="math inline">\(y_1,...,y_n\)</span>, is drawn from a parametric distribution <span class="math inline">\(f(y,\theta)\)</span>. If the data are i.i.d., the joint density function is:</p>
<p><span class="math display" id="eq:6-1">\[\begin{equation}
f(y;\theta)=\prod_{i=1}^{n} f\left(y_{i} ; \theta\right)
  \tag{10.1}
\end{equation}\]</span></p>
<p>To estimate the parameters that maximize this density function (“likelihood”), or more easily, its logarithmic transformation:</p>
<p><span class="math display" id="eq:6-2">\[\begin{equation}
\ell(y ; \theta)=\log f(y ; \theta)=\sum_{i=1}^{n} \log f\left(y_{i} ; \theta\right)
  \tag{10.2}
\end{equation}\]</span></p>
<p>We use MLE to estimate <span class="math inline">\(\theta\)</span>. This is called <strong>parametric estimation</strong> and if our pre-determined density model is not right, that is, if <span class="math inline">\(f\)</span> is misspecified, we will have biased estimators on <span class="math inline">\(\theta\)</span>. To avoid this problem, we can use <strong>nonparametric estimation</strong>. The starting point for a density estimation is a histogram. We define the intervals by choosing a number of bins and a starting value for the first interval. Here, we use 0 as a starting value and 10 bins:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="nonparametric-estimations---basics.html#cb103-1" tabindex="-1"></a><span class="co">#Random integers from 1 to 100</span></span>
<span id="cb103-2"><a href="nonparametric-estimations---basics.html#cb103-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb103-3"><a href="nonparametric-estimations---basics.html#cb103-3" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="dv">100</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb103-4"><a href="nonparametric-estimations---basics.html#cb103-4" tabindex="-1"></a><span class="fu">stem</span>(data)</span></code></pre></div>
<pre><code>## 
##   The decimal point is 1 digit(s) to the right of the |
## 
##   0 | 46777999
##   1 | 23344456
##   2 | 12335555677
##   3 | 00111224456889
##   4 | 01122337
##   5 | 0012337
##   6 | 003477999
##   7 | 1222466899
##   8 | 112366799
##   9 | 0011123334566799</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="nonparametric-estimations---basics.html#cb105-1" tabindex="-1"></a>foo <span class="ot">&lt;-</span> <span class="fu">hist</span>(data, <span class="at">nclass =</span> <span class="dv">10</span>,</span>
<span id="cb105-2"><a href="nonparametric-estimations---basics.html#cb105-2" tabindex="-1"></a>            <span class="at">col =</span> <span class="st">&quot;lightblue&quot;</span>,</span>
<span id="cb105-3"><a href="nonparametric-estimations---basics.html#cb105-3" tabindex="-1"></a>            <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>)</span></code></pre></div>
<p><img src="10-NonParametricBasics_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="nonparametric-estimations---basics.html#cb106-1" tabindex="-1"></a>foo<span class="sc">$</span>counts</span></code></pre></div>
<pre><code>##  [1]  8  8 13 13  9  7  7 10 11 14</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="nonparametric-estimations---basics.html#cb108-1" tabindex="-1"></a>foo<span class="sc">$</span>density</span></code></pre></div>
<pre><code>##  [1] 0.008 0.008 0.013 0.013 0.009 0.007 0.007 0.010 0.011 0.014</code></pre>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="nonparametric-estimations---basics.html#cb110-1" tabindex="-1"></a><span class="fu">sum</span>(foo<span class="sc">$</span>density)</span></code></pre></div>
<pre><code>## [1] 0.1</code></pre>
<p>Not that the sum of these densities is not one. The vertical scale of a ‘frequency histogram’ shows the number of observations in each bin. From above, we know that the tallest bar has 14 observations, so this bar accounts for relative frequency 14/100=0.14 of the observations. As the relative frequency indicate probability their total would be 1. We are looking for a density function which gives the “height” of each observation. Since the width of this bar is 10, the density of each observation in the bin is 0.014.</p>
<p>Can we have a formula that we calculate the density for each bin?</p>
<p><span class="math display" id="eq:6-3">\[\begin{equation}
\hat{f}(y)=\frac{1}{n} \times \frac{\text{ number of observations in the interval of } y}{\text { width of the interval }}
  \tag{10.3}
\end{equation}\]</span></p>
<p>Here is the pdf on the same data with binwidth = 4 for our example:</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="nonparametric-estimations---basics.html#cb112-1" tabindex="-1"></a><span class="co"># to put pdf and X&#39;s on the same graph, we scale the data</span></span>
<span id="cb112-2"><a href="nonparametric-estimations---basics.html#cb112-2" tabindex="-1"></a>foo <span class="ot">&lt;-</span> <span class="fu">hist</span>(data<span class="sc">/</span>(<span class="dv">10</span><span class="sc">*</span><span class="fu">mean</span>(data)), <span class="at">nclass =</span> <span class="dv">25</span>,</span>
<span id="cb112-3"><a href="nonparametric-estimations---basics.html#cb112-3" tabindex="-1"></a>            <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>, <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.2</span>))</span>
<span id="cb112-4"><a href="nonparametric-estimations---basics.html#cb112-4" tabindex="-1"></a><span class="fu">lines</span>(foo<span class="sc">$</span>mids, foo<span class="sc">$</span>density, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>) <span class="co">#Naive</span></span></code></pre></div>
<p><img src="10-NonParametricBasics_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>The number of bins defines the degree of smoothness of the histogram. We can have the following general expression for a nonparametric density estimation:</p>
<p><span class="math display" id="eq:6-4">\[\begin{equation}
f(x) \cong \frac{k}{n h} \text { where }\left\{\begin{array}{ll}{h} &amp; {\text { binwidth }} \\ {n} &amp; {\text { total } \text { number of observation points }} \\ {k} &amp; {\text { number of observations inside } h}\end{array}\right.
  \tag{10.4}
\end{equation}\]</span></p>
<p>Note that, in practical density estimation problems, two basic approaches can be adopted: (1) we can fix <span class="math inline">\(h\)</span> (width of the interval) and determine <span class="math inline">\(k\)</span> in each bin from the data, which is the subject of this chapter and called <strong>kernel density estimation (KDE)</strong>; or (2) we can fix <span class="math inline">\(k\)</span> in each bin and determine <span class="math inline">\(h\)</span> from the data. This gives rise to the <strong>k-nearest-neighbors (kNN)</strong> approach, which we cover in the next chapters.</p>
<p>The global density is obtained with a <em>moving window</em> (intervals with intersections). This also called as <strong>Naive estimator</strong> (a.k.a. Parzen windows), which is not sensitive to the position of bins, but it is not smooth either. We can rewrite it as</p>
<p><span class="math display" id="eq:6-5">\[\begin{equation}
\hat{f}(y)=\frac{1}{n h} \sum_{i=1}^{n} I\left(y-\frac{h}{2}&lt;y_{i}&lt;y+\frac{h}{2}\right),
  \tag{10.5}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(I(.)\)</span> is an indicator function, which results in value of 1 if the expression inside of the function is satisfied (0 otherwise). Hence it counts the number of observations in a given window. The binwidth (<span class="math inline">\(h\)</span>) defines the bin range by adding and subtracting <span class="math inline">\(h/2\)</span> from <span class="math inline">\(y\)</span>. Let’s rearrange 6.5 differently:</p>
<p><span class="math display">\[
\hat{f}(y)=\frac{1}{2n h} \sum_{i=1}^{n} I\left(y-h&lt;y_{i}&lt;y+h\right)
\]</span></p>
<p>If we rewrite the inequality by subtracting <span class="math inline">\(y\)</span> and divide it by <span class="math inline">\(h\)</span>:</p>
<p><span class="math display">\[
\hat{f}(y)=\frac{1}{2n h} \sum_{i=1}^{n} I\left(-1&lt;\frac{y-y_{i}}{h}&lt;1\right)
\]</span>
which can be written more compactly,</p>
<p><span class="math display" id="eq:6-6">\[\begin{equation}
\hat{f}(y)=\frac{1}{2nh} \sum_{i=1}^{n} w\left(\frac{y-y_{i}}{h}\right) \quad \text { where } \quad w(x)=\left\{\begin{array}{ll}{1} &amp; {\text { if }|x|&lt;1} \\ {0} &amp; {\text { otherwise }}\end{array}\right.
  \tag{10.6}
\end{equation}\]</span></p>
<p>Consider a sample <span class="math inline">\(\left\{X_{i}\right\}_{i=1}^{10}\)</span>, which is 4, 5, 5, 6, 12, 14, 15, 15, 16, 17. And the bin width is <span class="math inline">\(h=4\)</span>. What’s the density of 3, i.e, <span class="math inline">\(\hat{f}(3)\)</span>? Note that there is not 3 in the data.</p>
<p><span class="math display">\[
\begin{aligned} \hat{f}(3) &amp;=\frac{1}{2*10*4}\left\{w\left(\frac{3-4}{4}\right)+w\left(\frac{3-5}{4}\right)+\ldots+w\left(\frac{3-17}{4}\right)\right\} \\ &amp;=\frac{1}{80}\left\{1+1+1+1+0+\ldots+0\right\} \\ &amp;=\frac{1}{20} \end{aligned}
\]</span></p>
<p>This “naive” estimator yields density estimates that have discontinuities and weights equal at all points <span class="math inline">\(x_i\)</span> regardless of their distance to the estimation point <span class="math inline">\(x\)</span>. In other words, in any given bin, <span class="math inline">\(x\)</span>’s have a uniform distribution. That’s why, <span class="math inline">\(w(x)\)</span> is commonly replaced with a smooth kernel function <span class="math inline">\(K(x)\)</span>. Kernel replaces it with usually, but not always, with a radially symmetric and unimodal pdf, such as the Gaussian. You can choose <strong>“gaussian”, “epanechnikov”, “rectangular”, “triangular”, “biweight”, “cosine”, “optcosine”</strong> distributions in the R’s <code>density()</code> function. With the Kernel density estimator replacing <span class="math inline">\(w\)</span> by a kernel function <span class="math inline">\(K\)</span>:</p>
<p><span class="math display" id="eq:6-7">\[\begin{equation}
\hat{f}(y)=\frac{1}{2n h} \sum_{i=1}^{n} K\left(\frac{y-y_{i}}{h}\right),
  \tag{10.7}
\end{equation}\]</span></p>
<p>Here are the samples of kernels, <span class="math inline">\(K(x)\)</span>:</p>
<p><span class="math display">\[
\text { Rectangular (Uniform): } ~~ K(x)=\left\{\begin{array}{ll}{\frac{1}{2}} &amp; {|x|&lt;1} \\ {0} &amp; {\text { otherwise }}\end{array}\right.
\]</span>
<span class="math display">\[
\text { Epanechnikov: } ~~ K(x)=\left\{\begin{array}{cc}{\frac{3}{4}\left(1-\frac{1}{5} x^{2}\right) / \sqrt{5}} &amp; {|x|&lt;\sqrt{5}} \\ {0} &amp; {\text { otherwise }}\end{array}\right.
\]</span>
<span class="math display">\[
\text { Gaussian: } ~~ K(x)=\frac{1}{\sqrt{2 \pi}} e^{(-1 / 2) x^{2}}
\]</span></p>
<p>Although the kernel density estimator depends on the choices of the kernel function <span class="math inline">\(K\)</span>, it is very sensitive to <span class="math inline">\(h\)</span>, not to <span class="math inline">\(K\)</span>.</p>
<p>In R, the standard kernel density estimation is obtained by <code>density()</code>, which uses <strong>Silverman rule-of-thumb</strong> to select the optimal bandwidth, <span class="math inline">\(h\)</span>, and the <strong>Gaussian kernel</strong>. Here is an example with our artificial data:</p>
<p>#```{r, warning=FALSE, message=FALSE}
#X &lt;- readRDS(“fes73.rds”)
#X &lt;- X/mean(X)
#hist(X[X&lt;3.5], nclass = 130 , probability = TRUE, col = “white”,
# cex.axis = 0.75, cex.main = 0.8)
#lines(density(X, adjust = 1/4), col = “red”) # bw/4
#lines(density(X, adjust = 1), col = “blue”) # bw
#lines(density(X, adjust = 4), col = “green”) # bw * 4
#lines(density(X, kernel = “rectangular”, adjust = 1/4), col = “black”) # bw * 4</p>
<p>#Here is the details of the last one
#density(X, adjust = 4)
#```</p>
<p>Bigger the bandwidth <span class="math inline">\(h\)</span> smoother the pdf. Which one is better? Here, we again have a <em>bias-variance tradeoff</em>. As we will see later, <span class="math inline">\(h\)</span> can be found by cross-validation methods. An example of cross-validation methods on KDE can be found <a href="https://bookdown.org/egarpor/PM-UC3M/npreg-npdens.html#npreg-npdens-kde">here</a> <span class="citation">(<a href="#ref-Garcia-Portugues2022"><strong>Garcia-Portugues2022?</strong></a>)</span>.</p>
<p>Why do we estimate pdf with KDE? Note that, when you explore our density object by <code>str()</code>, you’ll see that <span class="math inline">\(Y\)</span> will get you the pdf values of density for each value of <span class="math inline">\(X\)</span> you have in our data. Of course pdf is a function: the values of pdf are <span class="math inline">\(Y\)</span> and the input values are <span class="math inline">\(X\)</span>. Hence, given a new data point on <span class="math inline">\(X\)</span>, we may want to find the outcome of <span class="math inline">\(Y\)</span> (the value of pdf for that data point) based on the function, the kernel density estimator that we have from the <code>density()</code> function result. How can we do that? Lets look at an example.</p>
<p>#```{r, warning=FALSE, message=FALSE}
#poo &lt;- density(X, adjust = 4)</p>
<p>#dens &lt;- with(poo, approxfun(x, y, rule=1))
# Here y is the pdf of a specified x.
#dens(1.832)
#```</p>
<p>This is a predicted value of pdf when <span class="math inline">\(x=1.832\)</span> estimated by KDE without specifying the model apriori, which is like a magic! Based on the sample we have, we just predicted <span class="math inline">\(Y\)</span> without explicitly modeling it. Keep in mind that our objective here is not to estimate probabilities. We can do it if we want. But then, of course we have to remember that values of a density curve are not the same thing as probabilities. As with any continuous distribution, the probability that <span class="math inline">\(X\)</span> is exactly 1.832, for example, is 0. Taking the integral of the desired section in the estimated pdf would give us the corresponding probability.</p>
</div>
<div id="kernel-regression" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Kernel regression<a href="nonparametric-estimations---basics.html#kernel-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Theoretically, nonparametric density estimation can be easily extended to several dimensions (multivariate distributions). For instance, suppose you are interested in the relationship between one endogenous variable <span class="math inline">\(Y\)</span> and a few exogenous variables of <span class="math inline">\(X\)</span>. The ultimate goal is forecasting new realizations of <span class="math inline">\(Y\)</span> given new realizations of <span class="math inline">\(X\)</span>’s. You have little clue what functional form the relationship could take. Suppose you have a sufficiently large sample, so that you may obtain a reasonably accurate estimate of the joint probability density (by kernel density estimation or similar) of <span class="math inline">\(Y\)</span> and the <span class="math inline">\(X\)</span>’s.</p>
<p>In practice, however, you rarely actually have a good enough sample to perform highly accurate density estimation. As the dimension increases, KDE rapidly needs many more samples. Hence, KDE is rarely useful beyond the order of 10 dimensions. Even in low dimensions, a density estimation-based model has essentially no ability to generalize; if your test set has any examples outside the support of your training distribution, you’re likely in trouble.</p>
<p>In regression functions, the outcome is the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>’s. Unlike linear regression, nonparametric regression is agnostic about the functional form between the outcome and the covariates and is <em>therefore not subject to misspecification error</em>. In nonparametric regression, you do not specify the functional form. You specify the outcome variable and the covariates. In traditional parametric regression models, the functional form of the model is specified before the model is fit to data, and the objective is to estimate the parameters of the model. In nonparametric regression, in contrast, the objective is to estimate the regression function directly without specifying its form explicitly <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>.</p>
<p>The traditional regression model fits the model:</p>
<p><span class="math display" id="eq:6-8">\[\begin{equation}
y=m(\mathbf{x}, \boldsymbol{\theta})+\varepsilon
  \tag{10.8}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is a vector of parameters to be estimated, and <strong>x</strong> is a vector of predictors. The errors, <span class="math inline">\(\varepsilon\)</span> are assumed to be normally and independently distributed with mean 0 and constant variance <span class="math inline">\(\sigma^2\)</span>. The function <span class="math inline">\(m(\mathbf{x},\theta)\)</span>, relating the average value of the response <span class="math inline">\(y\)</span> to the predictors, is specified in advance, as it is in a linear regression model. The general nonparametric regression model is written in a similar manner, but the function <span class="math inline">\(m\)</span> is left unspecified for the <span class="math inline">\(p\)</span> predictors:</p>
<p><span class="math display">\[
\begin{aligned} y &amp;=m(\mathbf{x})+\varepsilon \\ &amp;=m\left(x_{1}, x_{2}, \ldots, x_{p}\right)+\varepsilon \end{aligned}
\]</span>
Moreover, the objective of nonparametric regression is to estimate the regression function <span class="math inline">\(m(\mathbf{x})\)</span> directly, rather than to estimate parameters. Most methods of nonparametric regression implicitly assume that <span class="math inline">\(m\)</span> is a smooth, continuous function. As in nonlinear regression, it is standard to assume that error is normally and identically distributed <span class="math inline">\(\varepsilon \sim NID(0, \sigma^2)\)</span>.</p>
<p>An important special case of the general model is nonparametric simple regression, where there is only one predictor:</p>
<p><span class="math display">\[
y=m(x)+\varepsilon
\]</span></p>
<p>Because it is difficult to fit the general nonparametric regression model when there are many predictors, and because it is difficult to display the fitted model when there are more than two or three predictors, more restrictive models have been developed. One such model is the additive regression model,</p>
<p><span class="math display" id="eq:6-9">\[\begin{equation}
y=\beta_{0}+m_{1}\left(x_{1}\right)+m_{2}\left(x_{2}\right)+\cdots+m_{p}\left(x_{p}\right)+\varepsilon
  \tag{10.9}
\end{equation}\]</span></p>
<p>Variations on the additive regression model include semiparametric models, in which some of the predictors enter linearly or interactively.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<p>Let’s consider the simplest situation with one continuous predictor, <span class="math inline">\(X\)</span>. Due to its definition, we can rewrite <span class="math inline">\(m\)</span> as</p>
<p><span class="math display" id="eq:6-10">\[\begin{equation}
\begin{split}
\begin{aligned} m(x) &amp;=\mathbb{E}[Y | X=x] \\ &amp;=\int y f_{Y | X=x}(y) \mathrm{d} y \\ &amp;=\frac{\int y f(x, y) \mathrm{d} y}{f_{X}(x)} \end{aligned}
  \end{split}
  \tag{10.10}
\end{equation}\]</span></p>
<p>This shows that the regression function can be computed from the joint density <span class="math inline">\(f(x,y)\)</span> and the marginal <span class="math inline">\(f(x)\)</span>. Therefore, given a sample <span class="math inline">\(\left\{\left(X_{i}, Y_{i}\right)\right\}_{i=1}^{n}\)</span>, a nonparametric estimate of <span class="math inline">\(m\)</span> may follow by replacing the previous densities by their kernel density estimators, which we’ve just seen in the previous section.</p>
<p>A limitation of the bin smoothing approach in kernel density estimations is that we need small windows for the approximately constant assumptions to hold. As a result, we end up with a small number of data points to average and obtain imprecise estimates of <span class="math inline">\(f(x)\)</span>. Local weighted regression (<code>loess</code>) permits us to consider larger window sizes. <code>loess()</code> is a nonparametric approach that fits multiple regressions in local neighborhood. It is called local regression because, instead of assuming the function is approximately constant in a window, it fits a local regression at the “neighborhood” of <span class="math inline">\(x_0\)</span>. The distance from <span class="math inline">\(x_0\)</span> is controlled by the span setting, which determines the width of the moving (sliding) window when smoothing the data. <code>span</code> (also defined as alpha) represents the proportion of the data (size of the sliding window) that is considered to be neighboring <span class="math inline">\(x_0\)</span> <a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. Moreover, the weighting in the regression is proportional to <span class="math inline">\(1-(\text{distance}/\text{maximum distance})^3)^3\)</span>, which is called the Tukey tri-weight. Different than the Gaussian kernel, the Tukey tri-weight covers more points closer to the center point.</p>
<p>We will not see the theoretical derivations of kernel regressions but an illustration of local polynomial of order 0, 1 and 2, below. (Examples from Ahamada and Flachaire) <span class="citation">(<a href="#ref-Ibrahim_2011"><strong>Ibrahim_2011?</strong></a>)</span>. The Nadaraya–Watson estimator is a local polynomial of order 0, which estimates a local mean of <span class="math inline">\(Y_1...Y_n\)</span> around <span class="math inline">\(X=x_0\)</span>.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="nonparametric-estimations---basics.html#cb113-1" tabindex="-1"></a><span class="co">#Simulating our data</span></span>
<span id="cb113-2"><a href="nonparametric-estimations---basics.html#cb113-2" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">300</span></span>
<span id="cb113-3"><a href="nonparametric-estimations---basics.html#cb113-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb113-4"><a href="nonparametric-estimations---basics.html#cb113-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb113-5"><a href="nonparametric-estimations---basics.html#cb113-5" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n)<span class="sc">/</span><span class="dv">4</span></span>
<span id="cb113-6"><a href="nonparametric-estimations---basics.html#cb113-6" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span></code></pre></div>
<p><img src="10-NonParametricBasics_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="nonparametric-estimations---basics.html#cb114-1" tabindex="-1"></a><span class="co">#Estimation</span></span>
<span id="cb114-2"><a href="nonparametric-estimations---basics.html#cb114-2" tabindex="-1"></a>loe0 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">0</span>, <span class="at">span =</span> <span class="fl">0.5</span>) <span class="co">#Nadaraya-Watson</span></span>
<span id="cb114-3"><a href="nonparametric-estimations---basics.html#cb114-3" tabindex="-1"></a>loe1 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">1</span>, <span class="at">span =</span> <span class="fl">0.5</span>) <span class="co">#Local linear</span></span>
<span id="cb114-4"><a href="nonparametric-estimations---basics.html#cb114-4" tabindex="-1"></a>loe2 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="fl">0.5</span>) <span class="co">#Locally quadratic</span></span>
<span id="cb114-5"><a href="nonparametric-estimations---basics.html#cb114-5" tabindex="-1"></a></span>
<span id="cb114-6"><a href="nonparametric-estimations---basics.html#cb114-6" tabindex="-1"></a><span class="co">#To have a plot, we first calculate the fitted values on a grid,</span></span>
<span id="cb114-7"><a href="nonparametric-estimations---basics.html#cb114-7" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb114-8"><a href="nonparametric-estimations---basics.html#cb114-8" tabindex="-1"></a>fit0 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe0, t)</span>
<span id="cb114-9"><a href="nonparametric-estimations---basics.html#cb114-9" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe1, t)</span>
<span id="cb114-10"><a href="nonparametric-estimations---basics.html#cb114-10" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe2, t)</span>
<span id="cb114-11"><a href="nonparametric-estimations---basics.html#cb114-11" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>)</span>
<span id="cb114-12"><a href="nonparametric-estimations---basics.html#cb114-12" tabindex="-1"></a><span class="fu">lines</span>(t, fit0, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb114-13"><a href="nonparametric-estimations---basics.html#cb114-13" tabindex="-1"></a><span class="fu">lines</span>(t, fit1, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb114-14"><a href="nonparametric-estimations---basics.html#cb114-14" tabindex="-1"></a><span class="fu">lines</span>(t, fit2, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="10-NonParametricBasics_files/figure-html/unnamed-chunk-3-2.png" width="672" /></p>
<p>Let’s see sensitivity of local quadratic to the bandwidth:</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="nonparametric-estimations---basics.html#cb115-1" tabindex="-1"></a>fit0 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="fl">0.05</span>)) <span class="co">#minimum, 5%*300 = 14 obs. </span></span>
<span id="cb115-2"><a href="nonparametric-estimations---basics.html#cb115-2" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="fl">0.75</span>)) <span class="co">#default</span></span>
<span id="cb115-3"><a href="nonparametric-estimations---basics.html#cb115-3" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="dv">2</span>)) </span>
<span id="cb115-4"><a href="nonparametric-estimations---basics.html#cb115-4" tabindex="-1"></a></span>
<span id="cb115-5"><a href="nonparametric-estimations---basics.html#cb115-5" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>)</span>
<span id="cb115-6"><a href="nonparametric-estimations---basics.html#cb115-6" tabindex="-1"></a><span class="fu">lines</span>(x, fit0, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>)</span>
<span id="cb115-7"><a href="nonparametric-estimations---basics.html#cb115-7" tabindex="-1"></a><span class="fu">lines</span>(x, fit1, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb115-8"><a href="nonparametric-estimations---basics.html#cb115-8" tabindex="-1"></a><span class="fu">lines</span>(x, fit2, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="10-NonParametricBasics_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>As we have seen in the concept of bias-variance trade off before, which bandwidth we choose will be determined by the prediction accuracy. This subject is related to <strong>cross-validation</strong>, which we will see later as a whole chapter.</p>
</div>
<div id="knn" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Knn<a href="nonparametric-estimations---basics.html#knn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>With k-nearest neighbors (kNN) we estimate <span class="math inline">\(p(x_1, x_2)\)</span> by using a method similar to <em>bin smoothing</em>. kNN is a nonparametric method used for classification or regression. That’s why sometimes it is called as a <em>black-box</em> method, as it does not return an explicit model but the direct classified outcome. In <em>kNN classification</em>, the output is a class membership. An object is assigned to the class most common among its k nearest neighbors. In <em>kNN regression</em>, the output is the property value for the object. This value is the average of the values of k nearest neighbors, which we’ve seen in bin smoothing applications. Here are the steps to understand it:</p>
<p><img src="png/kNN1.png" /><!-- --></p>
<p>Suppose we have to classify (identify) the red dot as 7 or 2. Since it’s a nonparametric approach, we have to define bins. If the number of observations in bins set to 1 (<span class="math inline">\(k = 1\)</span>), then we need to find one observation that is nearest to the red dot. How? Since we know to coordinates (<span class="math inline">\(x_1, x_2\)</span>) of that red dot, we can calculate find its nearest neighbors by some distance functions for all points (observations) in the data. A popular choice is the Euclidean distance given by</p>
<p><span class="math display">\[
d\left(x, x^{\prime}\right)=\sqrt{\left(x_{1}-x_{1}^{\prime}\right)^{2}+\ldots+\left(x_{n}-x_{n}^{\prime}\right)^{2}}.
\]</span></p>
<p>Other measures are also available and can be more suitable in different settings including the Manhattan, Chebyshev and Hamming distance. The last one is used if the features are binary. In our case the features are continuous so we can use the Euclidean distance. We now have to calculate this measure for every point (observation) in our data. In our graph we have 10 points, and we have to have 10 distance measures from the red dot. Usually, in practice, we calculate all distance measures between each point, which becomes a symmetric matrix with <span class="math inline">\(n\)</span>x<span class="math inline">\(n\)</span> dimensions. When <span class="math inline">\(k=1\)</span>, the observation that has the shortest distance is going to be the one to predict what the red dot could be. This is shown in the figure below:</p>
<p><img src="png/kNN2.png" /><!-- --></p>
<p>If we define the bin as <span class="math inline">\(k=3\)</span>, we look for the 3 nearest points to the red dot and then take an average of the 1s (7s) and 0s (2s) associated with these points. Here is an example:</p>
<p><img src="png/kNN3.png" /><!-- --></p>
<p>Using <span class="math inline">\(k\)</span> neighbors to estimate the probability of <span class="math inline">\(Y=1\)</span> (the dot is 7), that is</p>
<p><span class="math display" id="eq:8-2">\[\begin{equation}
\hat{P}_{k}(Y=1 | X=x)=\frac{1}{k} \sum_{i \in \mathcal{N}_{k}(x, D)} I\left(y_{i}=1\right)
  \tag{10.11}
\end{equation}\]</span></p>
<p>With this predicted probability, we classify the red dot to the class with the most observations in the <span class="math inline">\(k\)</span> nearest neighbors (we assign a class at random to one of the classes tied for highest). Here is the rule in our case:</p>
<p><span class="math display">\[
\hat{C}_{k}(x)=\left\{\begin{array}{ll}{1} &amp; {\hat{p}_{k 0}(x)&gt;0.5} \\ {0} &amp; {\hat{p}_{k 1}(x)&lt;0.5}\end{array}\right.
\]</span></p>
<p>Suppose our red dot has <span class="math inline">\(x=(x_1,x_2)=(4,3)\)</span></p>
<p><span class="math display">\[
\begin{aligned} \hat{P}\left(Y=\text { Seven } | X_{1}=4, X_{2}=3\right)=\frac{2}{3} \\ \hat{P}\left(Y=\text { Two} | X_{1}=4, X_{2}=3\right)=\frac{1}{3} \end{aligned}
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\hat{C}_{k=4}\left(x_{1}=4, x_{2}=3\right)=\text { Seven }
\]</span></p>
<p>As it’s clear from this application, <span class="math inline">\(k\)</span> is our hyperparameter and we need to tune it as to have the best predictive kNN algorithm. The following section will show its application. But before that, let’s see the hyperplane to understand its nonparametric structure. We will use <code>knn3()</code> from the <em>Caret</em> package. We will not train a model but only see how the separation between classes will be nonlinear and different for different <span class="math inline">\(k\)</span>.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="nonparametric-estimations---basics.html#cb116-1" tabindex="-1"></a><span class="co">#With k = 5</span></span></code></pre></div>
<p>One with <span class="math inline">\(k=2\)</span> shows signs for overfitting, the other one with <span class="math inline">\(k=400\)</span> indicates oversmoothing or underfitting. We need to tune <span class="math inline">\(k\)</span> such a way that it will be best in terms of prediction accuracy.</p>
<div id="adult-dataset" class="section level3 hasAnchor" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> Adult dataset<a href="nonparametric-estimations---basics.html#adult-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This dataset provides information on income and attributes that may effect it. Information on the dataset is give at its <a href="https://archive.ics.uci.edu/ml/datasets/Adult">website</a> <span class="citation">(<a href="#ref-Kohavi_1996"><strong>Kohavi_1996?</strong></a>)</span>:</p>
<blockquote>
<p>Extraction from 1994 US. Census database. A set of reasonably clean records was extracted using the following conditions: ((<code>AAGE</code>&gt;16) &amp;&amp; (<code>AGI</code>&gt;100) &amp;&amp; (<code>AFNLWGT</code>&gt;1)&amp;&amp; (<code>HRSWK</code>&gt;0)).</p>
</blockquote>
<p>The prediction task is to determine whether a person makes over 50K a year.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="nonparametric-estimations---basics.html#cb117-1" tabindex="-1"></a><span class="co"># Download adult income data</span></span>
<span id="cb117-2"><a href="nonparametric-estimations---basics.html#cb117-2" tabindex="-1"></a><span class="co"># SET YOUR WORKING DIRECTORY FIRST</span></span>
<span id="cb117-3"><a href="nonparametric-estimations---basics.html#cb117-3" tabindex="-1"></a></span>
<span id="cb117-4"><a href="nonparametric-estimations---basics.html#cb117-4" tabindex="-1"></a><span class="co"># url.train &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&quot;</span></span>
<span id="cb117-5"><a href="nonparametric-estimations---basics.html#cb117-5" tabindex="-1"></a><span class="co"># url.test &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test&quot;</span></span>
<span id="cb117-6"><a href="nonparametric-estimations---basics.html#cb117-6" tabindex="-1"></a><span class="co"># url.names &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&quot;</span></span>
<span id="cb117-7"><a href="nonparametric-estimations---basics.html#cb117-7" tabindex="-1"></a><span class="co"># download.file(url.train, destfile = &quot;adult_train.csv&quot;)</span></span>
<span id="cb117-8"><a href="nonparametric-estimations---basics.html#cb117-8" tabindex="-1"></a><span class="co"># download.file(url.test, destfile = &quot;adult_test.csv&quot;)</span></span>
<span id="cb117-9"><a href="nonparametric-estimations---basics.html#cb117-9" tabindex="-1"></a><span class="co"># download.file(url.names, destfile = &quot;adult_names.txt&quot;)</span></span>
<span id="cb117-10"><a href="nonparametric-estimations---basics.html#cb117-10" tabindex="-1"></a></span>
<span id="cb117-11"><a href="nonparametric-estimations---basics.html#cb117-11" tabindex="-1"></a><span class="co"># Read the training set into memory</span></span>
<span id="cb117-12"><a href="nonparametric-estimations---basics.html#cb117-12" tabindex="-1"></a><span class="co">#train &lt;- read.csv(&quot;adult_train.csv&quot;, header = FALSE)</span></span>
<span id="cb117-13"><a href="nonparametric-estimations---basics.html#cb117-13" tabindex="-1"></a><span class="co">#str(train)</span></span>
<span id="cb117-14"><a href="nonparametric-estimations---basics.html#cb117-14" tabindex="-1"></a></span>
<span id="cb117-15"><a href="nonparametric-estimations---basics.html#cb117-15" tabindex="-1"></a><span class="co"># Read the test set into memory</span></span>
<span id="cb117-16"><a href="nonparametric-estimations---basics.html#cb117-16" tabindex="-1"></a><span class="co">#test &lt;- read.csv(&quot;adult_test.csv&quot;, header = FALSE)</span></span></code></pre></div>
<p>The data doesn’t have the variable names. That’s bad because we don’t know which one is which. Check the <strong>adult_names.txt</strong> file. The list of variables is given in that file. Thanks to <a href="https://rpubs.com/mbaumer/knn">Matthew Baumer</a> <span class="citation">(<a href="#ref-Baumer_2015"><strong>Baumer_2015?</strong></a>)</span>, we can write them manually:</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="nonparametric-estimations---basics.html#cb118-1" tabindex="-1"></a><span class="co">#varNames &lt;- c(&quot;Age&quot;, </span></span>
<span id="cb118-2"><a href="nonparametric-estimations---basics.html#cb118-2" tabindex="-1"></a><span class="co">#              &quot;WorkClass&quot;,</span></span>
<span id="cb118-3"><a href="nonparametric-estimations---basics.html#cb118-3" tabindex="-1"></a> <span class="co">#             &quot;fnlwgt&quot;,</span></span>
<span id="cb118-4"><a href="nonparametric-estimations---basics.html#cb118-4" tabindex="-1"></a>  <span class="co">#            &quot;Education&quot;,</span></span>
<span id="cb118-5"><a href="nonparametric-estimations---basics.html#cb118-5" tabindex="-1"></a>   <span class="co">#           &quot;EducationNum&quot;,</span></span>
<span id="cb118-6"><a href="nonparametric-estimations---basics.html#cb118-6" tabindex="-1"></a>    <span class="co">#          &quot;MaritalStatus&quot;,</span></span>
<span id="cb118-7"><a href="nonparametric-estimations---basics.html#cb118-7" tabindex="-1"></a>      <span class="co">#        &quot;Occupation&quot;,</span></span>
<span id="cb118-8"><a href="nonparametric-estimations---basics.html#cb118-8" tabindex="-1"></a>     <span class="co">#         &quot;Relationship&quot;,</span></span>
<span id="cb118-9"><a href="nonparametric-estimations---basics.html#cb118-9" tabindex="-1"></a><span class="co">#              &quot;Race&quot;,</span></span>
<span id="cb118-10"><a href="nonparametric-estimations---basics.html#cb118-10" tabindex="-1"></a> <span class="co">#             &quot;Sex&quot;,</span></span>
<span id="cb118-11"><a href="nonparametric-estimations---basics.html#cb118-11" tabindex="-1"></a>  <span class="co">#            &quot;CapitalGain&quot;,</span></span>
<span id="cb118-12"><a href="nonparametric-estimations---basics.html#cb118-12" tabindex="-1"></a>   <span class="co">#           &quot;CapitalLoss&quot;,</span></span>
<span id="cb118-13"><a href="nonparametric-estimations---basics.html#cb118-13" tabindex="-1"></a>    <span class="co">#          &quot;HoursPerWeek&quot;,</span></span>
<span id="cb118-14"><a href="nonparametric-estimations---basics.html#cb118-14" tabindex="-1"></a>     <span class="co">#         &quot;NativeCountry&quot;,</span></span>
<span id="cb118-15"><a href="nonparametric-estimations---basics.html#cb118-15" tabindex="-1"></a>      <span class="co">#        &quot;IncomeLevel&quot;)</span></span>
<span id="cb118-16"><a href="nonparametric-estimations---basics.html#cb118-16" tabindex="-1"></a><span class="co">#names(train) &lt;- varNames</span></span>
<span id="cb118-17"><a href="nonparametric-estimations---basics.html#cb118-17" tabindex="-1"></a><span class="co">#names(test) &lt;- varNames</span></span>
<span id="cb118-18"><a href="nonparametric-estimations---basics.html#cb118-18" tabindex="-1"></a><span class="co">#str(train)</span></span></code></pre></div>
<p>Since the dataset is large we are not going to use the test set but split the train set into our own test and train sets. Note that, however, if we had used the original test set, we would have had to make some adjustments/cleaning before using it. For example, if you look at <code>Age</code> variable, it seems as a factor variable. It’s an integer in the training set. We have to change it first. Moreover, our <span class="math inline">\(Y\)</span> has two levels in the train set, it has 3 levels in the test set. We have to go over each variable and make sure that the test and train sets have the same features and class types. This task is left to you if you want to use the original train and test sets. A final tip: remove the first row in the original test set!</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="nonparametric-estimations---basics.html#cb119-1" tabindex="-1"></a><span class="co">#Caret needs some preparations!</span></span>
<span id="cb119-2"><a href="nonparametric-estimations---basics.html#cb119-2" tabindex="-1"></a><span class="co">#table(train$IncomeLevel)</span></span>
<span id="cb119-3"><a href="nonparametric-estimations---basics.html#cb119-3" tabindex="-1"></a><span class="co"># this is b/c we will use it the same data for LPM later in class examples</span></span>
<span id="cb119-4"><a href="nonparametric-estimations---basics.html#cb119-4" tabindex="-1"></a><span class="co">#train$Y &lt;- ifelse(train$IncomeLevel==&quot; &lt;=50K&quot;, 0, 1) </span></span>
<span id="cb119-5"><a href="nonparametric-estimations---basics.html#cb119-5" tabindex="-1"></a><span class="co">#train &lt;- train[, -15]</span></span>
<span id="cb119-6"><a href="nonparametric-estimations---basics.html#cb119-6" tabindex="-1"></a><span class="co"># kNN needs Y to be a factor variable</span></span>
<span id="cb119-7"><a href="nonparametric-estimations---basics.html#cb119-7" tabindex="-1"></a><span class="co">#train$Y &lt;- as.factor(train$Y)</span></span>
<span id="cb119-8"><a href="nonparametric-estimations---basics.html#cb119-8" tabindex="-1"></a><span class="co">#levels(train$Y)[levels(train$Y)==&quot;0&quot;] &lt;- &quot;Less&quot;</span></span>
<span id="cb119-9"><a href="nonparametric-estimations---basics.html#cb119-9" tabindex="-1"></a><span class="co">#levels(train$Y)[levels(train$Y)==&quot;1&quot;] &lt;- &quot;More&quot;</span></span>
<span id="cb119-10"><a href="nonparametric-estimations---basics.html#cb119-10" tabindex="-1"></a><span class="co">#levels(train$Y)</span></span>
<span id="cb119-11"><a href="nonparametric-estimations---basics.html#cb119-11" tabindex="-1"></a></span>
<span id="cb119-12"><a href="nonparametric-estimations---basics.html#cb119-12" tabindex="-1"></a><span class="co">#kNN</span></span>
<span id="cb119-13"><a href="nonparametric-estimations---basics.html#cb119-13" tabindex="-1"></a><span class="co">#set.seed(3033)</span></span>
<span id="cb119-14"><a href="nonparametric-estimations---basics.html#cb119-14" tabindex="-1"></a><span class="co">#train_df &lt;- caret::createDataPartition(y = train$Y, p= 0.7, list = FALSE)</span></span>
<span id="cb119-15"><a href="nonparametric-estimations---basics.html#cb119-15" tabindex="-1"></a><span class="co">#training &lt;- train[train_df,]</span></span>
<span id="cb119-16"><a href="nonparametric-estimations---basics.html#cb119-16" tabindex="-1"></a><span class="co">#testing &lt;- train[-train_df,]</span></span>
<span id="cb119-17"><a href="nonparametric-estimations---basics.html#cb119-17" tabindex="-1"></a></span>
<span id="cb119-18"><a href="nonparametric-estimations---basics.html#cb119-18" tabindex="-1"></a><span class="co">#Training/Model building with 10-k cross validation</span></span>
<span id="cb119-19"><a href="nonparametric-estimations---basics.html#cb119-19" tabindex="-1"></a><span class="co">#It will take a long time.  If you want to run it</span></span>
<span id="cb119-20"><a href="nonparametric-estimations---basics.html#cb119-20" tabindex="-1"></a><span class="co">#make sure that you have something to read:-)</span></span>
<span id="cb119-21"><a href="nonparametric-estimations---basics.html#cb119-21" tabindex="-1"></a><span class="co">#cv &lt;- caret::trainControl(method = &quot;cv&quot;, number = 10, p = 0.9)</span></span>
<span id="cb119-22"><a href="nonparametric-estimations---basics.html#cb119-22" tabindex="-1"></a><span class="co">#model_knn3 &lt;- caret::train(Y ~ ., method = &quot;knn&quot;, data = training,</span></span>
<span id="cb119-23"><a href="nonparametric-estimations---basics.html#cb119-23" tabindex="-1"></a><span class="co">#                   tuneGrid = data.frame(k=seq(9, 41 ,2)),</span></span>
<span id="cb119-24"><a href="nonparametric-estimations---basics.html#cb119-24" tabindex="-1"></a><span class="co">#                   trControl = cv)</span></span>
<span id="cb119-25"><a href="nonparametric-estimations---basics.html#cb119-25" tabindex="-1"></a><span class="co">#ggplot(model_knn3, highlight = TRUE)</span></span></code></pre></div>
<p>Now we are going to use the test set to see the model’s performance.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="nonparametric-estimations---basics.html#cb120-1" tabindex="-1"></a><span class="co">#caret::confusionMatrix(predict(model_knn3, testing, type = &quot;raw&quot;),</span></span>
<span id="cb120-2"><a href="nonparametric-estimations---basics.html#cb120-2" tabindex="-1"></a> <span class="co">#               testing$Y)</span></span></code></pre></div>
<p>Next, as you can guess, we will delve into these performance measures. But before that, let’s ask some questions.</p>
<p>Why would you go with kNN? LPM may be as good as kNN. How can you see the individual effect of each feature on this classification? It seems that there is no way that we can interpret the results by looking at each feature. A learning algorithm may not be evaluated only by its predictive capacity. We may want to interpret the results by identifying the important predictors and their importance in predicting the outcome. <strong>There is always a trade-off between interpretability and predictive accuracy</strong>. Here is a an illustration. We will talk about this later in the book.</p>
<p><img src="png/tradeoff.png" width="130%" height="130%" /></p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>For more theoretical explanations, see the link <a href="https://socialsciences.mcmaster.ca/jfox/Books/Companion/appendices/Appendix-Nonparametric-Regression.pdf">here</a> to one of the good sources) <span class="citation">(<a href="#ref-Fox_2018"><strong>Fox_2018?</strong></a>)</span><a href="nonparametric-estimations---basics.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Without theoretical background, which can be found at <span class="citation">(<a href="#ref-Fox_2018"><strong>Fox_2018?</strong></a>)</span> and <span class="citation">(<a href="#ref-Garcia-Portugues2022"><strong>Garcia-Portugues2022?</strong></a>)</span>. We will see several important applications later.<a href="nonparametric-estimations---basics.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>if N is the number of data points and span= 0.5, then for a given x, loess will use the 0.5*N closest points to x for the fit. Usually span should be between 0 and 1. When its larger than 1, then the regression will be over-smoothed<a href="nonparametric-estimations---basics.html#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="parametric-estimations---basics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hyperparameter-tuning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuk/machinemetrics/edit/master/10-NonParametricBasics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
