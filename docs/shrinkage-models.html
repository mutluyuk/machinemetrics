<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Shrinkage Models | MachineMetrics</title>
  <meta name="description" content="Chapter 15 Shrinkage Models | MachineMetrics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Shrinkage Models | MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Shrinkage Models | MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="interpretability.html"/>
<link rel="next" href="regression-trees.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> INTRODUCTION:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html"><i class="fa fa-check"></i><b>2</b> Statistical Models and Simulations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#distinguishing-between-statistical-modeling-and-machine-learning-in-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#goals-and-objectives"><i class="fa fa-check"></i><b>2.1.1</b> Goals and Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#prediction-vs.-inference"><i class="fa fa-check"></i><b>2.1.2</b> Prediction vs. Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#conclusion"><i class="fa fa-check"></i><b>2.1.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#model-selection-and-approaches-in-data-modeling"><i class="fa fa-check"></i><b>2.4</b> Model Selection and Approaches in Data Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#mle"><i class="fa fa-check"></i><b>3.5.3</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error---mse"><i class="fa fa-check"></i><b>5.1</b> Estimation error - MSE</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.2</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#technical-points-about-mse-and-mspe"><i class="fa fa-check"></i><b>5.3</b> Technical points about MSE and MSPE</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.1</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.1</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.2</b> LPM</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
<li class="chapter" data-level="10.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#knn"><i class="fa fa-check"></i><b>10.3</b> Knn</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#adult-dataset"><i class="fa fa-check"></i><b>10.3.1</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="11.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#grid-search"><i class="fa fa-check"></i><b>11.4</b> Grid Search</a></li>
<li class="chapter" data-level="11.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>11.5</b> Cross-validated grid search</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#one-variable"><i class="fa fa-check"></i><b>12.4.1</b> One-variable</a></li>
<li class="chapter" data-level="12.4.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>12.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="12.4.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#multivariable"><i class="fa fa-check"></i><b>12.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>16.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.3</b> Pruning</a></li>
<li class="chapter" data-level="16.4" data-path="regression-trees.html"><a href="regression-trees.html#classification-with-titanic"><i class="fa fa-check"></i><b>16.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="16.5" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>17.2</b> Random Forest</a></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>17.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="17.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="17.3.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.3.3</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.4</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.5</b> Classification</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.6</b> Regression</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.7</b> Exploration</a></li>
<li class="chapter" data-level="17.8" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.8</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression-1"><i class="fa fa-check"></i><b>17.8.1</b> Regression</a></li>
<li class="chapter" data-level="17.8.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.8.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.8.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.8.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.8.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-1"><i class="fa fa-check"></i><b>17.8.4</b> Classification</a></li>
<li class="chapter" data-level="17.8.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost.m1"><i class="fa fa-check"></i><b>17.8.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="17.8.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.8.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#out-sample-prediction-accuracy"><i class="fa fa-check"></i><b>20.3</b> out-sample prediction accuracy</a></li>
<li class="chapter" data-level="20.4" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-2.html"><a href="classification-2.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-2.html"><a href="classification-2.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-2.html"><a href="classification-2.html#mnist-dataset"><i class="fa fa-check"></i><b>21.2</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="21.3" data-path="classification-2.html"><a href="classification-2.html#linear-classifiers-again"><i class="fa fa-check"></i><b>21.3</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="21.4" data-path="classification-2.html"><a href="classification-2.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.4</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.5" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret"><i class="fa fa-check"></i><b>21.5</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="classification-2.html"><a href="classification-2.html#mnist_27"><i class="fa fa-check"></i><b>21.5.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="21.5.2" data-path="classification-2.html"><a href="classification-2.html#adult-dataset-1"><i class="fa fa-check"></i><b>21.5.2</b> Adult dataset</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="classification-2.html"><a href="classification-2.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.6</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.7" data-path="classification-2.html"><a href="classification-2.html#confusion-matrix"><i class="fa fa-check"></i><b>21.7</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.8" data-path="classification-2.html"><a href="classification-2.html#performance-measures"><i class="fa fa-check"></i><b>21.8</b> Performance measures</a></li>
<li class="chapter" data-level="21.9" data-path="classification-2.html"><a href="classification-2.html#roc-curve"><i class="fa fa-check"></i><b>21.9</b> ROC Curve</a></li>
<li class="chapter" data-level="21.10" data-path="classification-2.html"><a href="classification-2.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.10</b> AUC - Area Under the Curve</a></li>
<li class="chapter" data-level="21.11" data-path="classification-2.html"><a href="classification-2.html#classification-example"><i class="fa fa-check"></i><b>21.11</b> Classification Example</a></li>
<li class="chapter" data-level="21.12" data-path="classification-2.html"><a href="classification-2.html#lpm-1"><i class="fa fa-check"></i><b>21.12</b> LPM</a></li>
<li class="chapter" data-level="21.13" data-path="classification-2.html"><a href="classification-2.html#logistic-regression-1"><i class="fa fa-check"></i><b>21.13</b> Logistic Regression</a></li>
<li class="chapter" data-level="21.14" data-path="classification-2.html"><a href="classification-2.html#knn-1"><i class="fa fa-check"></i><b>21.14</b> kNN</a>
<ul>
<li class="chapter" data-level="21.14.1" data-path="classification-2.html"><a href="classification-2.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>21.14.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="21.14.2" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret-1"><i class="fa fa-check"></i><b>21.14.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>22</b> Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="time-series.html"><a href="time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="time-series.html"><a href="time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="time-series.html"><a href="time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="time-series.html"><a href="time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="time-series.html"><a href="time-series.html#stationarity"><i class="fa fa-check"></i><b>22.5</b> Stationarity</a></li>
<li class="chapter" data-level="22.6" data-path="time-series.html"><a href="time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.6</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="time-series.html"><a href="time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.7</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.8" data-path="time-series.html"><a href="time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.8</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.9" data-path="time-series.html"><a href="time-series.html#speed"><i class="fa fa-check"></i><b>22.9</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="forecast.html"><a href="forecast.html"><i class="fa fa-check"></i><b>23</b> Forecast</a>
<ul>
<li class="chapter" data-level="23.1" data-path="forecast.html"><a href="forecast.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="forecast.html"><a href="forecast.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="forecast.html"><a href="forecast.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="forecast.html"><a href="forecast.html#random-forest-1"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="forecast.html"><a href="forecast.html#univariate"><i class="fa fa-check"></i><b>23.5</b> Univariate</a></li>
<li class="chapter" data-level="23.6" data-path="forecast.html"><a href="forecast.html#multivariate"><i class="fa fa-check"></i><b>23.6</b> Multivariate</a></li>
<li class="chapter" data-level="23.7" data-path="forecast.html"><a href="forecast.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.7</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>24.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-margin"><i class="fa fa-check"></i><b>24.1.1</b> The Margin</a></li>
<li class="chapter" data-level="24.1.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-non-separable-case"><i class="fa fa-check"></i><b>24.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>24.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="24.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#application-with-svm"><i class="fa fa-check"></i><b>24.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#precision-matrix"><i class="fa fa-check"></i><b>27.4</b> Precision Matrix</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.5</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.6" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.6</b> Regularized Covariance Matrix</a></li>
<li class="chapter" data-level="27.7" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>27.7</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="27.8" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#high-dimensional-data"><i class="fa fa-check"></i><b>27.8</b> High-dimensional data</a></li>
<li class="chapter" data-level="27.9" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>27.9</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#rankr-approximations"><i class="fa fa-check"></i><b>28.4</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="28.5" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.5</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html"><i class="fa fa-check"></i><b>29</b> PCA (Principle Component Analysis)</a>
<ul>
<li class="chapter" data-level="29.1" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.1</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>30.1</b> Using bins</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>30.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="30.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>30.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="30.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="31.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>31.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="31.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>31.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="text-analysis.html"><a href="text-analysis.html"><i class="fa fa-check"></i><b>32</b> Text Analysis</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#other-nonparametric-estimation-methods-1"><i class="fa fa-check"></i><b>33.1</b> Other Nonparametric Estimation methods</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.2</b> Regression splines</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.3</b> MARS</a></li>
<li class="chapter" data-level="33.4" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.4</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuk/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shrinkage-models" class="section level1 hasAnchor" number="15">
<h1><span class="header-section-number">Chapter 15</span> Shrinkage Models<a href="shrinkage-models.html#shrinkage-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In simple regression or classification problems, we cannot train a parametric model in a way that the fitted model minimizes the out-of-sample prediction error. We could (and did) fit the parametric models <strong>manually</strong> by adding or removing predictors and their interactions and polynomials. As we have seen in earlier chapters, by dropping a variable in a regression, for example, it is possible to reduce the variance at the cost of a negligible increase in bias.</p>
<p>In fitting the predictive model, some of the variables used in a regression may not be well associated with the response. Keeping those “irrelevant” variables often leads to unnecessary complexity in the resulting model. Regularization or penalization is an alternative and automated fitting procedure that refers to a process that removes irrelevant variables or shrinks the magnitude of their parameters, which can yield better prediction accuracy and model interpretability by preventing overfitting.</p>
<p>There are several types of regularization techniques that can be used in parametric models. Each of these techniques adds a different type of penalty term to the objective function and can be used in different situations depending on the characteristics of the data and the desired properties of the model. Two methods, Ridge and Lasso, are two of well-known benchmark techniques that reduce the model complexity and prevent overfitting resulting from simple linear regression.</p>
<p>The general principle in penalization can be shown as</p>
<p><span class="math display">\[
\widehat{m}_\lambda(\boldsymbol{x})=\operatorname{argmin}\left\{\sum_{i=1}^n \underbrace{\mathcal{L}\left(y_i, m(\boldsymbol{x})\right)}_{\text {loss function }}+\underbrace{\lambda\|m\|_{\ell_q}}_{\text {penalization }}\right\}
\]</span></p>
<p>where <span class="math inline">\(\mathcal{L}\)</span> could be conditional mean, quantiles, expectiles, <span class="math inline">\(m\)</span> could be linear, logit, splines, tree-based models, neural networks. The penalization, <span class="math inline">\(\ell_q\)</span>, could be lasso (<span class="math inline">\(\ell_1\)</span>) or ridge (<span class="math inline">\(\ell_2\)</span>). And, <span class="math inline">\(\lambda\)</span> regulates overfitting that can be determined by cross-validation or other methods. It puts a price to pay for a having more flexible model:</p>
<ul>
<li><span class="math inline">\(\lambda\rightarrow0\)</span>: it interpolates data, low bias, high variance</li>
<li><span class="math inline">\(\lambda\rightarrow\infty\)</span>: linear model high bias, low variance</li>
</ul>
<p>There are two fundamental goals in statistical learning: achieving a high prediction accuracy and identifying relevant predictors. The second objective, variable selection, is particularly important when there is a true sparsity in the underlying model. By their nature, penalized parametric models are not well-performing tools for prediction. But, they provide important tools for model selection specially when <span class="math inline">\(p&gt;N\)</span> and the true model is sparse. This section starts with two major models in regularized regressions, Ridge and Lasso, and develops an idea on sparse statistical modelling with Adaptive Lasso.</p>
<p>Although there are many sources on the subject, perhaps the most fundamental one is <a href="https://hastie.su.domains/StatLearnSparsity/">Statistical Learning with Sparsity</a> by Hastie et al. (2015).</p>
<div id="ridge" class="section level2 hasAnchor" number="15.1">
<h2><span class="header-section-number">15.1</span> Ridge<a href="shrinkage-models.html#ridge" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The least squares fitting procedure is that one estimates <span class="math inline">\(\beta_{0}, \beta_{1}, \ldots, \beta_{p}\)</span> that minimize the residual sum of squares:</p>
<p><span class="math display">\[
\mathrm{RSS}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}
\]</span>
Ridge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity.</p>
<p><span class="math display">\[
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2} =\mathrm{RSS}+\lambda \sum_{j=1}^{p} \beta_{j}^{2},
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the hyperparameter that can be tuned by cross-validation and grid search. The last term, <span class="math inline">\(\lambda \sum_{j} \beta_{j}^{2}\)</span>, is a constraint, which is also called shrinkage penalty. This type of penalty is called as <span class="math inline">\(\ell_{2}\)</span> (L-2 penalty). As with Ordinary Least Squares (OLS), this cost function tries to minimize RSS but also penalizes the size of the coefficients.</p>
<p>More specifically,</p>
<p><span class="math display">\[
\hat{\beta}_\lambda^{\text {ridge }}=\operatorname{argmin}\left\{\left\|\mathbf{y}-\left(\beta_0+\mathbf{X} \beta\right)\right\|_{\ell_2}^2+\lambda\|\beta\|_{\ell_2}^2\right\},
\]</span></p>
<p>which has the solution:</p>
<p><span class="math display">\[
\hat{\beta}_\lambda=\left(\mathbf{X}^{\top} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{\top} \mathbf{y}
\]</span></p>
<p>where,</p>
<ul>
<li>If <span class="math inline">\(\lambda \rightarrow 0, \quad \hat{\beta}_0^{\text {ridge }}=\hat{\beta}^{\text {ols }}\)</span>,</li>
<li>If <span class="math inline">\(\lambda \rightarrow \infty, \quad \hat{\beta}_{\infty}^{\text {ridge }}=\mathbf{0}\)</span>.</li>
</ul>
<p>The hyperparameter <span class="math inline">\(\lambda\)</span> controls the relative impact of the penalization on the regression coefficient estimates. When <span class="math inline">\(\lambda = 0\)</span>, the cost function becomes RSS (residual sum of squares), that is the cost function of OLS and the estimations, produce the least squares estimates. However, as <span class="math inline">\(\lambda\)</span> gets higher, the impact of the shrinkage penalty grows, and the coefficients of the ridge regression will approach zero. Note that, the shrinkage penalty is applied to slope coefficients not to the intercept, which is simply the mean of the response, when all features are zero.</p>
<p>Let’s apply this to the same data we used earlier, <code>Hitters</code> from the <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">ISLR</a> <span class="citation">(<a href="#ref-ISLR_2021"><strong>ISLR_2021?</strong></a>)</span> package:</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="shrinkage-models.html#cb258-1" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb258-2"><a href="shrinkage-models.html#cb258-2" tabindex="-1"></a></span>
<span id="cb258-3"><a href="shrinkage-models.html#cb258-3" tabindex="-1"></a><span class="fu">remove</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span>
<span id="cb258-4"><a href="shrinkage-models.html#cb258-4" tabindex="-1"></a></span>
<span id="cb258-5"><a href="shrinkage-models.html#cb258-5" tabindex="-1"></a><span class="fu">data</span>(Hitters)</span>
<span id="cb258-6"><a href="shrinkage-models.html#cb258-6" tabindex="-1"></a>df <span class="ot">&lt;-</span> Hitters[<span class="fu">complete.cases</span>(Hitters<span class="sc">$</span>Salary), ]</span></code></pre></div>
<p>We will use the <code>glmnet</code> package to fit a ridge regression. The generic function in <code>glmnet</code> is defined by</p>
<p><span class="math display">\[
\min _{\beta_0, \beta} \frac{1}{N} \sum_{i=1}^N w_i l\left(y_i, \beta_0+\beta^T x_i\right)+\lambda\left[(1-\alpha)\|\beta\|_2^2 / 2+\alpha\|\beta\|_1\right] \text {, }
\]</span>
where <span class="math inline">\(l\left(y_i, \eta_i\right)\)</span> is the negative log-likelihood contribution for observation <span class="math inline">\(i\)</span> and <span class="math inline">\(\alpha\)</span> is the elastic net penalty. When <span class="math inline">\(\alpha=1\)</span> ( the default), the penalty term becomes <span class="math inline">\(\ell_{1}\)</span> and the resulting model is called lasso regression (least absolute shrinkage and selection operator). When <span class="math inline">\(\alpha=1\)</span>, the penalty term becomes <span class="math inline">\(\ell_{2}\)</span> and the resulting model is called ridge regression (some authors use the term Tikhonov–Phillips regularization). As before, the tuning parameter <span class="math inline">\(\lambda\)</span> controls the overall strength of the penalty. Since the penalty shrinks the coefficients of correlated variables (in Ridge) or pick one of them and discard the others (in Lasso), the variables are supposed to be standardized, which is done by <code>glmnet</code>.</p>
<p>The <code>glmnet</code> function has a slightly different syntax from other model-fitting functions that we have used so far in this book (<code>y ~ X</code>). Therefore, before we execute the syntax, we have the prepare the model so that <code>X</code> will be a matrix and <code>y</code> will be a vector. The matrix <code>X</code> has to be prepared before we proceed, which must be free of <code>NA</code>s.</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="shrinkage-models.html#cb259-1" tabindex="-1"></a>X  <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., df)[, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb259-2"><a href="shrinkage-models.html#cb259-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> df<span class="sc">$</span>Salary</span></code></pre></div>
<p>The <code>glmnet</code> package is maintained by Trevor Hastie who provides a friendly <a href="https://glmnet.stanford.edu/articles/glmnet.html">vignette</a> <span class="citation">(<a href="#ref-Hastie_glmnet"><strong>Hastie_glmnet?</strong></a>)</span>. They describe the importance of <code>model.matrix()</code> in <code>glmnet</code> as follows:</p>
<blockquote>
<p>(…)particularly useful for creating <span class="math inline">\(x\)</span>; not only does it produce a matrix corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because <code>glmnet()</code> can only take numerical, quantitative inputs.</p>
</blockquote>
<p>Here is the example for a ridge regression:</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="shrinkage-models.html#cb260-1" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb260-2"><a href="shrinkage-models.html#cb260-2" tabindex="-1"></a>grid <span class="ot">=</span> <span class="dv">10</span> <span class="sc">^</span> <span class="fu">seq</span>(<span class="dv">10</span>,<span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb260-3"><a href="shrinkage-models.html#cb260-3" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> grid)</span></code></pre></div>
<p>Although we defined the grid, we did not do a grid search explicitly by cross validation. Moreover, we do not need to select a grid. By default, the <code>glmnet()</code> function performs ridge regression for an automatically selected range of <span class="math inline">\(\lambda\)</span> values. It ranges from the null model - only intercept when <span class="math inline">\(\lambda\)</span> is at the upper bound and the least squares fit when the <span class="math inline">\(\lambda\)</span> is at lower bound.</p>
<p>The application above is to show that we can also choose to implement the function over a grid of values. Further, the <code>glmnet()</code> function standardizes the variables so that they are on the same scale. To turn off this default setting, we use the argument <code>standardize=FALSE</code>.</p>
<p>The methods here, ridge and lasso, are parametric models. Unlike non-parametric methods, each model is defined by a set of parameters or, as in our case, coefficients. Therefore, when we do a grid search, each value of the hyperparameter (<span class="math inline">\(\lambda\)</span>) is associated with one model defined by a set of coefficients. In order to see the coefficients we need to apply another function, <code>coef()</code>. Remember, we have 100 <span class="math inline">\(\lambda&#39;s\)</span>. Hence, <code>coef()</code> produces a 20 x 100 matrix, with 20 rows (one for each predictor, plus an intercept) and 100 columns (one for each value of <span class="math inline">\(\lambda\)</span>).</p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb261-1"><a href="shrinkage-models.html#cb261-1" tabindex="-1"></a><span class="fu">dim</span>(<span class="fu">coef</span>(model))</span></code></pre></div>
<pre><code>## [1]  20 100</code></pre>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="shrinkage-models.html#cb263-1" tabindex="-1"></a>model<span class="sc">$</span>lambda[<span class="fu">c</span>(<span class="dv">20</span>, <span class="dv">80</span>)]</span></code></pre></div>
<pre><code>## [1] 4.977024e+07 2.656088e+00</code></pre>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="shrinkage-models.html#cb265-1" tabindex="-1"></a><span class="fu">coef</span>(model)[, <span class="fu">c</span>(<span class="dv">20</span>, <span class="dv">80</span>)]</span></code></pre></div>
<pre><code>## 20 x 2 sparse Matrix of class &quot;dgCMatrix&quot;
##                       s19          s79
## (Intercept)  5.358880e+02  156.6073700
## AtBat        1.093664e-05   -1.7526436
## Hits         3.967221e-05    6.1739859
## HmRun        1.598556e-04    1.3285278
## Runs         6.708833e-05   -0.7689372
## RBI          7.086606e-05   -0.1297830
## Walks        8.340541e-05    5.5357165
## Years        3.410894e-04   -9.2923000
## CAtBat       9.390097e-07   -0.0792321
## CHits        3.455823e-06    0.2132942
## CHmRun       2.606160e-05    0.6557328
## CRuns        6.933126e-06    0.8349167
## CRBI         7.155123e-06    0.4090719
## CWalks       7.570013e-06   -0.6623253
## LeagueN     -1.164983e-04   62.0427219
## DivisionW   -1.568625e-03 -121.5286522
## PutOuts      4.380543e-06    0.2809457
## Assists      7.154972e-07    0.3124435
## Errors      -3.336588e-06   -3.6852362
## NewLeagueN  -2.312257e-05  -27.9849755</code></pre>
<p>As we see, the coefficient estimates are much smaller when a large value of <span class="math inline">\(\lambda\)</span> is used.</p>
<p>We generally use the <code>predict()</code> function as before. But, here we can also use it to estimate the ridge regression coefficients for a new value of <span class="math inline">\(\lambda\)</span>. Hence, if we don’t want to rely on the internal grid search provided by <code>glmnet()</code>, we can do our own grid search by <code>predict()</code>. This is an example when <span class="math inline">\(\lambda = 50\)</span>, which wasn’t in the grid.</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="shrinkage-models.html#cb267-1" tabindex="-1"></a><span class="fu">predict</span>(model, <span class="at">s =</span> <span class="dv">50</span>, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>)</span></code></pre></div>
<pre><code>## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        s1
## (Intercept)  4.876610e+01
## AtBat       -3.580999e-01
## Hits         1.969359e+00
## HmRun       -1.278248e+00
## Runs         1.145892e+00
## RBI          8.038292e-01
## Walks        2.716186e+00
## Years       -6.218319e+00
## CAtBat       5.447837e-03
## CHits        1.064895e-01
## CHmRun       6.244860e-01
## CRuns        2.214985e-01
## CRBI         2.186914e-01
## CWalks      -1.500245e-01
## LeagueN      4.592589e+01
## DivisionW   -1.182011e+02
## PutOuts      2.502322e-01
## Assists      1.215665e-01
## Errors      -3.278600e+00
## NewLeagueN  -9.496680e+00</code></pre>
<p>There are two ways that we can train ridge (and Lasso):</p>
<ul>
<li>We use our own training algorithm;</li>
<li>Or, we rely on <code>'glmnet</code> internal cross-validation process.</li>
</ul>
<p>Here is an example for our own algorithm for training ridge regression:</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="shrinkage-models.html#cb269-1" tabindex="-1"></a>grid <span class="ot">=</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb269-2"><a href="shrinkage-models.html#cb269-2" tabindex="-1"></a></span>
<span id="cb269-3"><a href="shrinkage-models.html#cb269-3" tabindex="-1"></a>MSPE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb269-4"><a href="shrinkage-models.html#cb269-4" tabindex="-1"></a>MMSPE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb269-5"><a href="shrinkage-models.html#cb269-5" tabindex="-1"></a></span>
<span id="cb269-6"><a href="shrinkage-models.html#cb269-6" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(grid)){</span>
<span id="cb269-7"><a href="shrinkage-models.html#cb269-7" tabindex="-1"></a>  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>){</span>
<span id="cb269-8"><a href="shrinkage-models.html#cb269-8" tabindex="-1"></a>    <span class="fu">set.seed</span>(j)</span>
<span id="cb269-9"><a href="shrinkage-models.html#cb269-9" tabindex="-1"></a>    ind <span class="ot">&lt;-</span> <span class="fu">unique</span>(<span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>))</span>
<span id="cb269-10"><a href="shrinkage-models.html#cb269-10" tabindex="-1"></a>    </span>
<span id="cb269-11"><a href="shrinkage-models.html#cb269-11" tabindex="-1"></a>    train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb269-12"><a href="shrinkage-models.html#cb269-12" tabindex="-1"></a>    xtrain <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary<span class="sc">~</span>., train)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb269-13"><a href="shrinkage-models.html#cb269-13" tabindex="-1"></a>    ytrain <span class="ot">&lt;-</span> df[ind, <span class="st">&quot;Salary&quot;</span>]</span>
<span id="cb269-14"><a href="shrinkage-models.html#cb269-14" tabindex="-1"></a>    </span>
<span id="cb269-15"><a href="shrinkage-models.html#cb269-15" tabindex="-1"></a>    test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb269-16"><a href="shrinkage-models.html#cb269-16" tabindex="-1"></a>    xtest <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary<span class="sc">~</span>., test)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb269-17"><a href="shrinkage-models.html#cb269-17" tabindex="-1"></a>    ytest <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, <span class="st">&quot;Salary&quot;</span>]</span>
<span id="cb269-18"><a href="shrinkage-models.html#cb269-18" tabindex="-1"></a>  </span>
<span id="cb269-19"><a href="shrinkage-models.html#cb269-19" tabindex="-1"></a>    model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(xtrain, ytrain, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> grid[i], <span class="at">thresh =</span> <span class="fl">1e-12</span>)</span>
<span id="cb269-20"><a href="shrinkage-models.html#cb269-20" tabindex="-1"></a>    yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, <span class="at">s =</span> grid[i], <span class="at">newx =</span> xtest)</span>
<span id="cb269-21"><a href="shrinkage-models.html#cb269-21" tabindex="-1"></a>    MSPE[j] <span class="ot">&lt;-</span> <span class="fu">mean</span>((yhat <span class="sc">-</span> ytest)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb269-22"><a href="shrinkage-models.html#cb269-22" tabindex="-1"></a>    }</span>
<span id="cb269-23"><a href="shrinkage-models.html#cb269-23" tabindex="-1"></a>  MMSPE[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(MSPE)</span>
<span id="cb269-24"><a href="shrinkage-models.html#cb269-24" tabindex="-1"></a>}</span>
<span id="cb269-25"><a href="shrinkage-models.html#cb269-25" tabindex="-1"></a></span>
<span id="cb269-26"><a href="shrinkage-models.html#cb269-26" tabindex="-1"></a><span class="fu">min</span>(MMSPE)</span></code></pre></div>
<pre><code>## [1] 119058.3</code></pre>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="shrinkage-models.html#cb271-1" tabindex="-1"></a>grid[<span class="fu">which.min</span>(MMSPE)]</span></code></pre></div>
<pre><code>## [1] 14.17474</code></pre>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="shrinkage-models.html#cb273-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">log</span>(grid), MMSPE, <span class="at">type =</span> <span class="st">&quot;o&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="15-ShrinkageMethods_files/figure-html/ridge6-1.png" width="672" /></p>
<p>What is the tuned model using the last training set with this <span class="math inline">\(\lambda\)</span>?</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="shrinkage-models.html#cb274-1" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> grid[<span class="fu">which.min</span>(MMSPE)]</span>
<span id="cb274-2"><a href="shrinkage-models.html#cb274-2" tabindex="-1"></a>coeff <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, <span class="at">s =</span> lambda , <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>, <span class="at">newx =</span> xtrain)</span>
<span id="cb274-3"><a href="shrinkage-models.html#cb274-3" tabindex="-1"></a>coeff</span></code></pre></div>
<pre><code>## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        s1
## (Intercept)  285.78834247
## AtBat         -1.27240085
## Hits           2.06931134
## HmRun          0.04319066
## Runs           2.75588969
## RBI            0.45631590
## Walks          3.46189297
## Years         -8.82528502
## CAtBat        -0.26127780
## CHits          1.28540111
## CHmRun         1.31904979
## CRuns          0.05880843
## CRBI          -0.05103190
## CWalks        -0.34003983
## LeagueN      131.98795986
## DivisionW   -119.25402540
## PutOuts        0.19785230
## Assists        0.64820842
## Errors        -6.97397640
## NewLeagueN   -54.55149894</code></pre>
<p>We may want to compare the ridge with a simple OLS:</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="shrinkage-models.html#cb276-1" tabindex="-1"></a>MSPE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb276-2"><a href="shrinkage-models.html#cb276-2" tabindex="-1"></a></span>
<span id="cb276-3"><a href="shrinkage-models.html#cb276-3" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb276-4"><a href="shrinkage-models.html#cb276-4" tabindex="-1"></a>  <span class="fu">set.seed</span>(j)</span>
<span id="cb276-5"><a href="shrinkage-models.html#cb276-5" tabindex="-1"></a>  ind <span class="ot">&lt;-</span> <span class="fu">unique</span>(<span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>))</span>
<span id="cb276-6"><a href="shrinkage-models.html#cb276-6" tabindex="-1"></a>  train <span class="ot">&lt;-</span> df[ind,]</span>
<span id="cb276-7"><a href="shrinkage-models.html#cb276-7" tabindex="-1"></a>  test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb276-8"><a href="shrinkage-models.html#cb276-8" tabindex="-1"></a>  </span>
<span id="cb276-9"><a href="shrinkage-models.html#cb276-9" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">lm</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> train)</span>
<span id="cb276-10"><a href="shrinkage-models.html#cb276-10" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, <span class="at">newdata =</span> test)</span>
<span id="cb276-11"><a href="shrinkage-models.html#cb276-11" tabindex="-1"></a>  MSPE[j] <span class="ot">&lt;-</span> <span class="fu">mean</span>((yhat <span class="sc">-</span> test<span class="sc">$</span>Salary) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb276-12"><a href="shrinkage-models.html#cb276-12" tabindex="-1"></a>}</span>
<span id="cb276-13"><a href="shrinkage-models.html#cb276-13" tabindex="-1"></a><span class="fu">mean</span>(MSPE)</span></code></pre></div>
<pre><code>## [1] 124217.3</code></pre>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="shrinkage-models.html#cb278-1" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Salary ~ ., data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -715.51 -187.40  -32.85  148.29 1686.38 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)  285.95478  126.06479   2.268   0.0248 *
## AtBat         -1.26497    0.94674  -1.336   0.1837  
## Hits           2.02174    3.61275   0.560   0.5766  
## HmRun         -0.01383    8.03787  -0.002   0.9986  
## Runs           2.79786    4.23051   0.661   0.5095  
## RBI            0.47768    3.56888   0.134   0.8937  
## Walks          3.44099    2.57671   1.335   0.1839  
## Years         -8.76533   17.25334  -0.508   0.6122  
## CAtBat        -0.26610    0.20435  -1.302   0.1950  
## CHits          1.31361    1.09982   1.194   0.2343  
## CHmRun         1.35851    2.30018   0.591   0.5557  
## CRuns          0.04142    1.02393   0.040   0.9678  
## CRBI          -0.06982    1.08722  -0.064   0.9489  
## CWalks        -0.33312    0.45479  -0.732   0.4651  
## LeagueN      132.36961  113.39037   1.167   0.2450  
## DivisionW   -119.16837   56.96453  -2.092   0.0382 *
## PutOuts        0.19795    0.10911   1.814   0.0718 .
## Assists        0.64902    0.29986   2.164   0.0321 *
## Errors        -6.97871    5.97011  -1.169   0.2444  
## NewLeagueN   -54.96821  111.81338  -0.492   0.6238  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 335.6 on 140 degrees of freedom
## Multiple R-squared:  0.4428, Adjusted R-squared:  0.3672 
## F-statistic: 5.856 on 19 and 140 DF,  p-value: 1.346e-10</code></pre>
<p>The second way is to rely on the <code>glmnet</code> internal training process, <code>cv.glmnet</code>, which is the main function to do cross-validation along with various supporting methods such as plotting and prediction. A part of the following scripts follows the same algorithm as the one in the book (<a href="https://www.statlearning.com">Introduction to Statistical Learning</a> - ISLR p.254). This approach uses a specific grid on <span class="math inline">\(\lambda\)</span>. We also run the same grid search 100 times to see the associated uncertainty.</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="shrinkage-models.html#cb280-1" tabindex="-1"></a><span class="co"># With a defined grid on lambda</span></span>
<span id="cb280-2"><a href="shrinkage-models.html#cb280-2" tabindex="-1"></a>bestlam <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb280-3"><a href="shrinkage-models.html#cb280-3" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb280-4"><a href="shrinkage-models.html#cb280-4" tabindex="-1"></a>grid <span class="ot">=</span> <span class="dv">10</span> <span class="sc">^</span> <span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb280-5"><a href="shrinkage-models.html#cb280-5" tabindex="-1"></a></span>
<span id="cb280-6"><a href="shrinkage-models.html#cb280-6" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>){</span>
<span id="cb280-7"><a href="shrinkage-models.html#cb280-7" tabindex="-1"></a>  <span class="fu">set.seed</span>(i)</span>
<span id="cb280-8"><a href="shrinkage-models.html#cb280-8" tabindex="-1"></a>  train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(X), <span class="fu">nrow</span>(X) <span class="sc">*</span> <span class="fl">0.5</span>) <span class="co"># 50% split</span></span>
<span id="cb280-9"><a href="shrinkage-models.html#cb280-9" tabindex="-1"></a>  test <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span>train)</span>
<span id="cb280-10"><a href="shrinkage-models.html#cb280-10" tabindex="-1"></a>  ytest <span class="ot">&lt;-</span> y[test]</span>
<span id="cb280-11"><a href="shrinkage-models.html#cb280-11" tabindex="-1"></a></span>
<span id="cb280-12"><a href="shrinkage-models.html#cb280-12" tabindex="-1"></a>  <span class="co">#finding lambda</span></span>
<span id="cb280-13"><a href="shrinkage-models.html#cb280-13" tabindex="-1"></a>  cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X[train,], y[train], <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb280-14"><a href="shrinkage-models.html#cb280-14" tabindex="-1"></a>  bestlam[i] <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>lambda.min</span>
<span id="cb280-15"><a href="shrinkage-models.html#cb280-15" tabindex="-1"></a></span>
<span id="cb280-16"><a href="shrinkage-models.html#cb280-16" tabindex="-1"></a>  <span class="co">#Predicting with that lambda</span></span>
<span id="cb280-17"><a href="shrinkage-models.html#cb280-17" tabindex="-1"></a>  ridge.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X[train,], y[train], <span class="at">alpha =</span> <span class="dv">0</span>,</span>
<span id="cb280-18"><a href="shrinkage-models.html#cb280-18" tabindex="-1"></a>                      <span class="at">lambda =</span> grid, <span class="at">thresh =</span> <span class="fl">1e-12</span>)</span>
<span id="cb280-19"><a href="shrinkage-models.html#cb280-19" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> bestlam[i], <span class="at">newx =</span> X[test,])</span>
<span id="cb280-20"><a href="shrinkage-models.html#cb280-20" tabindex="-1"></a>  mse[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((yhat <span class="sc">-</span> ytest)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb280-21"><a href="shrinkage-models.html#cb280-21" tabindex="-1"></a>}</span>
<span id="cb280-22"><a href="shrinkage-models.html#cb280-22" tabindex="-1"></a></span>
<span id="cb280-23"><a href="shrinkage-models.html#cb280-23" tabindex="-1"></a><span class="fu">mean</span>(bestlam)</span></code></pre></div>
<pre><code>## [1] 290.227</code></pre>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="shrinkage-models.html#cb282-1" tabindex="-1"></a><span class="fu">mean</span>(mse)</span></code></pre></div>
<pre><code>## [1] 127472.6</code></pre>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="shrinkage-models.html#cb284-1" tabindex="-1"></a><span class="fu">plot</span>(bestlam, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="15-ShrinkageMethods_files/figure-html/ridge9-1.png" width="672" /></p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="shrinkage-models.html#cb285-1" tabindex="-1"></a><span class="fu">plot</span>(mse, <span class="at">col =</span> <span class="st">&quot;pink&quot;</span>)</span></code></pre></div>
<p><img src="15-ShrinkageMethods_files/figure-html/ridge9-2.png" width="672" /></p>
<p>Now the same application without a specific grid:</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="shrinkage-models.html#cb286-1" tabindex="-1"></a>bestlam <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb286-2"><a href="shrinkage-models.html#cb286-2" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb286-3"><a href="shrinkage-models.html#cb286-3" tabindex="-1"></a></span>
<span id="cb286-4"><a href="shrinkage-models.html#cb286-4" tabindex="-1"></a><span class="co"># Without a pre-defined grid on lambda</span></span>
<span id="cb286-5"><a href="shrinkage-models.html#cb286-5" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>){</span>
<span id="cb286-6"><a href="shrinkage-models.html#cb286-6" tabindex="-1"></a>  <span class="fu">set.seed</span>(i)</span>
<span id="cb286-7"><a href="shrinkage-models.html#cb286-7" tabindex="-1"></a>  train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(X), <span class="fu">nrow</span>(X) <span class="sc">*</span> <span class="fl">0.5</span>) <span class="co"># arbitrary split</span></span>
<span id="cb286-8"><a href="shrinkage-models.html#cb286-8" tabindex="-1"></a>  test <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span>train)</span>
<span id="cb286-9"><a href="shrinkage-models.html#cb286-9" tabindex="-1"></a>  ytest <span class="ot">&lt;-</span> y[test]</span>
<span id="cb286-10"><a href="shrinkage-models.html#cb286-10" tabindex="-1"></a>  </span>
<span id="cb286-11"><a href="shrinkage-models.html#cb286-11" tabindex="-1"></a>  cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X[train,], y[train], <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb286-12"><a href="shrinkage-models.html#cb286-12" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(cv.out, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>, <span class="at">newx =</span> X[test,])</span>
<span id="cb286-13"><a href="shrinkage-models.html#cb286-13" tabindex="-1"></a>  mse[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((yhat <span class="sc">-</span> ytest) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb286-14"><a href="shrinkage-models.html#cb286-14" tabindex="-1"></a>}</span>
<span id="cb286-15"><a href="shrinkage-models.html#cb286-15" tabindex="-1"></a></span>
<span id="cb286-16"><a href="shrinkage-models.html#cb286-16" tabindex="-1"></a><span class="fu">mean</span>(mse)</span></code></pre></div>
<pre><code>## [1] 127481.6</code></pre>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="shrinkage-models.html#cb288-1" tabindex="-1"></a><span class="fu">plot</span>(mse, <span class="at">col =</span> <span class="st">&quot;pink&quot;</span>)</span></code></pre></div>
<p><img src="15-ShrinkageMethods_files/figure-html/ridge10-1.png" width="672" /></p>
<p>Ridge regression adds a penalty term that is the sum of the squares of the coefficients of the features in the model. This results in a penalty that is continuous and differentiable, which makes Ridge regression easy to optimize using gradient descent. Ridge regression can be useful when we have a large number of features but we still want to keep all of the features in the model. Ridge regression works best in situations where the least squares estimates have high variance.</p>
<p>On the other hand, Lasso (Least Absolute Shrinkage and Selection Operator) adds a penalty term that is the sum of the absolute values of the coefficients in the model. This results in a penalty that is non-differentiable, which makes it more difficult to optimize using gradient descent. However, Lasso has the advantage of being able to set the coefficients of some features to exactly zero, effectively eliminating those features from the model. This can be useful when we have a large number of features, and we want to select a subset of the most important features to include in the model.</p>
</div>
<div id="lasso" class="section level2 hasAnchor" number="15.2">
<h2><span class="header-section-number">15.2</span> Lasso<a href="shrinkage-models.html#lasso" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The penalty in ridge regression, <span class="math inline">\(\lambda \sum_{j} \beta_{j}^{2}\)</span>, will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero. This may present a problem in model interpretation when the number of variables is quite large. One of the key advantages of Lasso is that it can set the coefficients of some features to exactly zero, effectively eliminating those features from the model.</p>
<p>By eliminating unnecessary or redundant features from the model, Lasso can help to improve the interpretability and simplicity of the model. This can be particularly useful when you have a large number of features and you want to identify the most important ones for predicting the target variable.</p>
<p>The lasso, a relatively recent alternative to ridge regression, minimizes the following quantity:</p>
<p><span class="math display" id="eq:16-1">\[\begin{equation}
\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} \beta_{j} x_{i j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|=\operatorname{RSS}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|
  \tag{15.1}
\end{equation}\]</span></p>
<p>The lasso also shrinks the coefficient estimates towards zero. However, the <span class="math inline">\(\ell_{1}\)</span> penalty, the second term of equation 18.1, has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter <span class="math inline">\(\lambda\)</span> is sufficiently large. Hence, the lasso performs variable selection. As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression.</p>
<p>In general, one might expect lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients and the remaining predictors have no significant effect on the outcome. This property is known as “sparsity”, because it results in a model with a relatively small number of non-zero coefficients. In some cases, Lasso can find a true sparsity pattern in the data by identifying a small subset of the most important features that are sufficient to accurately predict the target variable.</p>
<p>Now, we apply lasso to the same data, <code>Hitters</code>. Again, we will follow a similar way to compare ridge and lasso as in <a href="https://www.statlearning.com">Introduction to Statistical Learning</a> (ISLR).</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="shrinkage-models.html#cb289-1" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb289-2"><a href="shrinkage-models.html#cb289-2" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb289-3"><a href="shrinkage-models.html#cb289-3" tabindex="-1"></a><span class="fu">remove</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span>
<span id="cb289-4"><a href="shrinkage-models.html#cb289-4" tabindex="-1"></a></span>
<span id="cb289-5"><a href="shrinkage-models.html#cb289-5" tabindex="-1"></a><span class="fu">data</span>(Hitters)</span>
<span id="cb289-6"><a href="shrinkage-models.html#cb289-6" tabindex="-1"></a>df <span class="ot">&lt;-</span> Hitters[<span class="fu">complete.cases</span>(Hitters<span class="sc">$</span>Salary), ]</span>
<span id="cb289-7"><a href="shrinkage-models.html#cb289-7" tabindex="-1"></a>X  <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., df)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb289-8"><a href="shrinkage-models.html#cb289-8" tabindex="-1"></a>y <span class="ot">&lt;-</span> df<span class="sc">$</span>Salary</span>
<span id="cb289-9"><a href="shrinkage-models.html#cb289-9" tabindex="-1"></a></span>
<span id="cb289-10"><a href="shrinkage-models.html#cb289-10" tabindex="-1"></a><span class="co"># Without a specific grid on lambda</span></span>
<span id="cb289-11"><a href="shrinkage-models.html#cb289-11" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb289-12"><a href="shrinkage-models.html#cb289-12" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(X), <span class="fu">nrow</span>(X) <span class="sc">*</span> <span class="fl">0.5</span>)</span>
<span id="cb289-13"><a href="shrinkage-models.html#cb289-13" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span>train)</span>
<span id="cb289-14"><a href="shrinkage-models.html#cb289-14" tabindex="-1"></a>ytest <span class="ot">&lt;-</span> y[test]</span>
<span id="cb289-15"><a href="shrinkage-models.html#cb289-15" tabindex="-1"></a></span>
<span id="cb289-16"><a href="shrinkage-models.html#cb289-16" tabindex="-1"></a><span class="co"># Ridge</span></span>
<span id="cb289-17"><a href="shrinkage-models.html#cb289-17" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb289-18"><a href="shrinkage-models.html#cb289-18" tabindex="-1"></a>ridge.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X[train,], y[train], <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb289-19"><a href="shrinkage-models.html#cb289-19" tabindex="-1"></a>yhatR <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge.out, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>, <span class="at">newx =</span> X[test,])</span>
<span id="cb289-20"><a href="shrinkage-models.html#cb289-20" tabindex="-1"></a>mse_r <span class="ot">&lt;-</span> <span class="fu">mean</span>((yhatR <span class="sc">-</span> ytest)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb289-21"><a href="shrinkage-models.html#cb289-21" tabindex="-1"></a></span>
<span id="cb289-22"><a href="shrinkage-models.html#cb289-22" tabindex="-1"></a><span class="co"># Lasso</span></span>
<span id="cb289-23"><a href="shrinkage-models.html#cb289-23" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb289-24"><a href="shrinkage-models.html#cb289-24" tabindex="-1"></a>lasso.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X[train,], y[train], <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb289-25"><a href="shrinkage-models.html#cb289-25" tabindex="-1"></a>yhatL <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso.out, <span class="at">s =</span> <span class="st">&quot;lambda.min&quot;</span>, <span class="at">newx =</span> X[test,])</span>
<span id="cb289-26"><a href="shrinkage-models.html#cb289-26" tabindex="-1"></a>mse_l <span class="ot">&lt;-</span> <span class="fu">mean</span>((yhatL <span class="sc">-</span> ytest) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb289-27"><a href="shrinkage-models.html#cb289-27" tabindex="-1"></a></span>
<span id="cb289-28"><a href="shrinkage-models.html#cb289-28" tabindex="-1"></a>mse_r</span></code></pre></div>
<pre><code>## [1] 139863.2</code></pre>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="shrinkage-models.html#cb291-1" tabindex="-1"></a>mse_l</span></code></pre></div>
<pre><code>## [1] 143668.8</code></pre>
<p>Now, we will define our own grid search:</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="shrinkage-models.html#cb293-1" tabindex="-1"></a><span class="co"># With a specific grid on lambda + lm()</span></span>
<span id="cb293-2"><a href="shrinkage-models.html#cb293-2" tabindex="-1"></a>grid <span class="ot">=</span> <span class="dv">10</span> <span class="sc">^</span> <span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb293-3"><a href="shrinkage-models.html#cb293-3" tabindex="-1"></a></span>
<span id="cb293-4"><a href="shrinkage-models.html#cb293-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb293-5"><a href="shrinkage-models.html#cb293-5" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(X), <span class="fu">nrow</span>(X)<span class="sc">*</span><span class="fl">0.5</span>)</span>
<span id="cb293-6"><a href="shrinkage-models.html#cb293-6" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="sc">-</span>train)</span>
<span id="cb293-7"><a href="shrinkage-models.html#cb293-7" tabindex="-1"></a>ytest <span class="ot">&lt;-</span> y[test]</span>
<span id="cb293-8"><a href="shrinkage-models.html#cb293-8" tabindex="-1"></a></span>
<span id="cb293-9"><a href="shrinkage-models.html#cb293-9" tabindex="-1"></a><span class="co">#Ridge</span></span>
<span id="cb293-10"><a href="shrinkage-models.html#cb293-10" tabindex="-1"></a>ridge.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X[train,], y[train], <span class="at">alpha =</span> <span class="dv">0</span>,</span>
<span id="cb293-11"><a href="shrinkage-models.html#cb293-11" tabindex="-1"></a>                    <span class="at">lambda =</span> grid, <span class="at">thresh =</span> <span class="fl">1e-12</span>)</span>
<span id="cb293-12"><a href="shrinkage-models.html#cb293-12" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb293-13"><a href="shrinkage-models.html#cb293-13" tabindex="-1"></a>cv.outR <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X[train,], y[train], <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb293-14"><a href="shrinkage-models.html#cb293-14" tabindex="-1"></a>bestlamR <span class="ot">&lt;-</span> cv.outR<span class="sc">$</span>lambda.min</span>
<span id="cb293-15"><a href="shrinkage-models.html#cb293-15" tabindex="-1"></a>yhatR <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> bestlamR, <span class="at">newx =</span> X[test,])</span>
<span id="cb293-16"><a href="shrinkage-models.html#cb293-16" tabindex="-1"></a>mse_R <span class="ot">&lt;-</span> <span class="fu">mean</span>((yhatR <span class="sc">-</span> ytest) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb293-17"><a href="shrinkage-models.html#cb293-17" tabindex="-1"></a></span>
<span id="cb293-18"><a href="shrinkage-models.html#cb293-18" tabindex="-1"></a><span class="co"># Lasso</span></span>
<span id="cb293-19"><a href="shrinkage-models.html#cb293-19" tabindex="-1"></a>lasso.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(X[train,], y[train], <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb293-20"><a href="shrinkage-models.html#cb293-20" tabindex="-1"></a>                    <span class="at">lambda =</span> grid, <span class="at">thresh =</span> <span class="fl">1e-12</span>)</span>
<span id="cb293-21"><a href="shrinkage-models.html#cb293-21" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb293-22"><a href="shrinkage-models.html#cb293-22" tabindex="-1"></a>cv.outL <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X[train,], y[train], <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb293-23"><a href="shrinkage-models.html#cb293-23" tabindex="-1"></a>bestlamL <span class="ot">&lt;-</span> cv.outL<span class="sc">$</span>lambda.min</span>
<span id="cb293-24"><a href="shrinkage-models.html#cb293-24" tabindex="-1"></a>yhatL <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso.mod, <span class="at">s =</span> bestlamL, <span class="at">newx =</span> X[test,])</span>
<span id="cb293-25"><a href="shrinkage-models.html#cb293-25" tabindex="-1"></a>mse_L <span class="ot">&lt;-</span> <span class="fu">mean</span>((yhatL <span class="sc">-</span> ytest) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb293-26"><a href="shrinkage-models.html#cb293-26" tabindex="-1"></a></span>
<span id="cb293-27"><a href="shrinkage-models.html#cb293-27" tabindex="-1"></a>mse_R</span></code></pre></div>
<pre><code>## [1] 139856.6</code></pre>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="shrinkage-models.html#cb295-1" tabindex="-1"></a>mse_L</span></code></pre></div>
<pre><code>## [1] 143572.1</code></pre>
<p>Now, we apply our own algorithm:</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="shrinkage-models.html#cb297-1" tabindex="-1"></a>grid <span class="ot">=</span> <span class="dv">10</span> <span class="sc">^</span> <span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb297-2"><a href="shrinkage-models.html#cb297-2" tabindex="-1"></a>MSPE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb297-3"><a href="shrinkage-models.html#cb297-3" tabindex="-1"></a>MMSPE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb297-4"><a href="shrinkage-models.html#cb297-4" tabindex="-1"></a></span>
<span id="cb297-5"><a href="shrinkage-models.html#cb297-5" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(grid)){</span>
<span id="cb297-6"><a href="shrinkage-models.html#cb297-6" tabindex="-1"></a>  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>){</span>
<span id="cb297-7"><a href="shrinkage-models.html#cb297-7" tabindex="-1"></a>    <span class="fu">set.seed</span>(j)</span>
<span id="cb297-8"><a href="shrinkage-models.html#cb297-8" tabindex="-1"></a>    ind <span class="ot">&lt;-</span> <span class="fu">unique</span>(<span class="fu">sample</span>(<span class="fu">nrow</span>(df), <span class="fu">nrow</span>(df), <span class="at">replace =</span> <span class="cn">TRUE</span>))</span>
<span id="cb297-9"><a href="shrinkage-models.html#cb297-9" tabindex="-1"></a>    </span>
<span id="cb297-10"><a href="shrinkage-models.html#cb297-10" tabindex="-1"></a>    train <span class="ot">&lt;-</span> df[ind, ]</span>
<span id="cb297-11"><a href="shrinkage-models.html#cb297-11" tabindex="-1"></a>    xtrain <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., train)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb297-12"><a href="shrinkage-models.html#cb297-12" tabindex="-1"></a>    ytrain <span class="ot">&lt;-</span> df[ind, <span class="dv">19</span>]</span>
<span id="cb297-13"><a href="shrinkage-models.html#cb297-13" tabindex="-1"></a>    </span>
<span id="cb297-14"><a href="shrinkage-models.html#cb297-14" tabindex="-1"></a>    test <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, ]</span>
<span id="cb297-15"><a href="shrinkage-models.html#cb297-15" tabindex="-1"></a>    xtest <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary<span class="sc">~</span>., test)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb297-16"><a href="shrinkage-models.html#cb297-16" tabindex="-1"></a>    ytest <span class="ot">&lt;-</span> df[<span class="sc">-</span>ind, <span class="dv">19</span>]</span>
<span id="cb297-17"><a href="shrinkage-models.html#cb297-17" tabindex="-1"></a>  </span>
<span id="cb297-18"><a href="shrinkage-models.html#cb297-18" tabindex="-1"></a>    model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(xtrain, ytrain, <span class="at">alpha =</span> <span class="dv">1</span>,</span>
<span id="cb297-19"><a href="shrinkage-models.html#cb297-19" tabindex="-1"></a>                    <span class="at">lambda =</span> grid[i], <span class="at">thresh =</span> <span class="fl">1e-12</span>)</span>
<span id="cb297-20"><a href="shrinkage-models.html#cb297-20" tabindex="-1"></a>    yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, <span class="at">s =</span> grid[i], <span class="at">newx =</span> xtest)</span>
<span id="cb297-21"><a href="shrinkage-models.html#cb297-21" tabindex="-1"></a>    MSPE[j] <span class="ot">&lt;-</span> <span class="fu">mean</span>((yhat <span class="sc">-</span> ytest) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb297-22"><a href="shrinkage-models.html#cb297-22" tabindex="-1"></a>    }</span>
<span id="cb297-23"><a href="shrinkage-models.html#cb297-23" tabindex="-1"></a>  MMSPE[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(MSPE)</span>
<span id="cb297-24"><a href="shrinkage-models.html#cb297-24" tabindex="-1"></a>}</span>
<span id="cb297-25"><a href="shrinkage-models.html#cb297-25" tabindex="-1"></a></span>
<span id="cb297-26"><a href="shrinkage-models.html#cb297-26" tabindex="-1"></a><span class="fu">min</span>(MMSPE)</span></code></pre></div>
<pre><code>## [1] 119855.1</code></pre>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="shrinkage-models.html#cb299-1" tabindex="-1"></a>grid[<span class="fu">which.min</span>(MMSPE)]</span></code></pre></div>
<pre><code>## [1] 2.656088</code></pre>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="shrinkage-models.html#cb301-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">log</span>(grid), MMSPE, <span class="at">type=</span><span class="st">&quot;o&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="15-ShrinkageMethods_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>What are the coefficients?</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="shrinkage-models.html#cb302-1" tabindex="-1"></a>coef_lasso <span class="ot">&lt;-</span> <span class="fu">coef</span>(model, <span class="at">s=</span>grid[<span class="fu">which.min</span>(MMSPE)], <span class="at">nonzero =</span> T)</span>
<span id="cb302-2"><a href="shrinkage-models.html#cb302-2" tabindex="-1"></a>coef_lasso</span></code></pre></div>
<pre><code>## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        s1
## (Intercept)  285.73172897
## AtBat         -1.26603002
## Hits           2.04074005
## HmRun          0.02355750
## Runs           2.77938363
## RBI            0.45867292
## Walks          3.44852914
## Years         -8.78839869
## CAtBat        -0.26343169
## CHits          1.29690477
## CHmRun         1.32913790
## CRuns          0.05007662
## CRBI          -0.05515544
## CWalks        -0.33624685
## LeagueN      132.06438132
## DivisionW   -119.26618910
## PutOuts        0.19772257
## Assists        0.64809649
## Errors        -6.97381705
## NewLeagueN   -54.62728800</code></pre>
<p>We can also try a classification problem with LPM or Logistic regression when the response is categorical. If there are two possible outcomes, we use the binomial distribution, else we use the multinomial.</p>
</div>
<div id="adaptive-lasso" class="section level2 hasAnchor" number="15.3">
<h2><span class="header-section-number">15.3</span> Adaptive Lasso<a href="shrinkage-models.html#adaptive-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Adaptive lasso is a method for regularization and variable selection in regression analysis that was introduced by Zou (2006) in <a href="http://users.stat.umn.edu/~zouxx019/Papers/adalasso.pdf">The Adaptive Lasso and Its Oracle Properties</a>. In this paper, the author proposed the use of a weighted <span class="math inline">\(\ell_{1}\)</span> penalty in the objective function, with the weights chosen to adapt to the correlation structure of the data. He showed that this method can result in a more stable model with fewer coefficients being exactly zero, compared to the standard lasso method which uses a simple <span class="math inline">\(\ell_{1}\)</span> penalty.</p>
<p>Since its introduction, adaptive lasso has been widely used in a variety of applications in statistical modeling and machine learning. It has been applied to problems such as feature selections in genomic data, high-dimensional regressions, and model selections with generalized linear models. Adaptive lasso is useful in situations where the predictors are correlated and there is a need to select a small subset of important variables to include in the model. It has been shown that adaptive lasso is an oracle efficient estimator (consistency in variable selection and asymptotic normality in coefficient estimation), while the plain lasso is not.</p>
<p>Consider the linear regression model:</p>
<p><span class="math display">\[
y_i=x_i^{\prime} \beta+\epsilon_i, ~~~~i=1, \ldots, n ~~~~\text{and} ~~~~\beta \text { is } (p \times 1)
\]</span>
The adaptive Lasso estimates <span class="math inline">\(\beta\)</span> by minimizing</p>
<p><span class="math display">\[
L(\beta)=\sum_{i=1}^n\left(y_i-x_i^{\prime} \beta\right)^2+\lambda_n \sum_{j=1}^p \frac{1}{w_j}\left|\beta_j\right|
\]</span></p>
<p>where, typically <span class="math inline">\(w_j=(\left|\hat{\beta}_{O L S_j}\right|)^{\gamma}\)</span> or <span class="math inline">\(w_j=(\left|\hat{\beta}_{Ridge_j}\right|)^{\gamma}\)</span>, where <span class="math inline">\(\gamma\)</span> is a positive constant for adjustment of the Adaptive Weights vector, and suggested to be the possible values of 0.5, 1, and 2.</p>
<p>The weights in adaptive lasso (AL) provides a prior “intelligence” about variables such that,while the plain Lasso penalizes all parameters equally, the adaptive Lasso is likely to penalize non-zero coefficients less than the zero ones. This is because the weights can be obtained from the consistent least squares estimator. If <span class="math inline">\(\beta_{AL, j}=0\)</span>, then <span class="math inline">\(\hat{\beta}_{O L S, j}\)</span> is likely to be close to zero leading to a very small <span class="math inline">\(w_j\)</span>. Hence, truly zero coefficients are penalized a lot. Calculating the weights in adaptive lasso requires a two-step procedure</p>
<p>Here is an example where we use the ridge weight in adaptive lasso:</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="shrinkage-models.html#cb304-1" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb304-2"><a href="shrinkage-models.html#cb304-2" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb304-3"><a href="shrinkage-models.html#cb304-3" tabindex="-1"></a></span>
<span id="cb304-4"><a href="shrinkage-models.html#cb304-4" tabindex="-1"></a><span class="fu">remove</span>(<span class="at">list =</span> <span class="fu">ls</span>())</span>
<span id="cb304-5"><a href="shrinkage-models.html#cb304-5" tabindex="-1"></a></span>
<span id="cb304-6"><a href="shrinkage-models.html#cb304-6" tabindex="-1"></a><span class="fu">data</span>(Hitters)</span>
<span id="cb304-7"><a href="shrinkage-models.html#cb304-7" tabindex="-1"></a>df <span class="ot">&lt;-</span> Hitters[<span class="fu">complete.cases</span>(Hitters<span class="sc">$</span>Salary), ]</span>
<span id="cb304-8"><a href="shrinkage-models.html#cb304-8" tabindex="-1"></a>X  <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary<span class="sc">~</span>., df)[,<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb304-9"><a href="shrinkage-models.html#cb304-9" tabindex="-1"></a>y <span class="ot">&lt;-</span> df<span class="sc">$</span>Salary</span>
<span id="cb304-10"><a href="shrinkage-models.html#cb304-10" tabindex="-1"></a></span>
<span id="cb304-11"><a href="shrinkage-models.html#cb304-11" tabindex="-1"></a><span class="co"># Ridge weights with gamma = 1</span></span>
<span id="cb304-12"><a href="shrinkage-models.html#cb304-12" tabindex="-1"></a>g <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb304-13"><a href="shrinkage-models.html#cb304-13" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb304-14"><a href="shrinkage-models.html#cb304-14" tabindex="-1"></a>modelr <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb304-15"><a href="shrinkage-models.html#cb304-15" tabindex="-1"></a>coefr <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">coef</span>(modelr, <span class="at">s =</span> modelr<span class="sc">$</span>lambda.min))</span>
<span id="cb304-16"><a href="shrinkage-models.html#cb304-16" tabindex="-1"></a>w.r <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">/</span>(<span class="fu">abs</span>(coefr[<span class="sc">-</span><span class="dv">1</span>,]))<span class="sc">^</span>g</span>
<span id="cb304-17"><a href="shrinkage-models.html#cb304-17" tabindex="-1"></a></span>
<span id="cb304-18"><a href="shrinkage-models.html#cb304-18" tabindex="-1"></a><span class="do">## Adaptive Lasso</span></span>
<span id="cb304-19"><a href="shrinkage-models.html#cb304-19" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb304-20"><a href="shrinkage-models.html#cb304-20" tabindex="-1"></a>alasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha=</span><span class="dv">1</span>, <span class="at">penalty.factor =</span> w.r)</span>
<span id="cb304-21"><a href="shrinkage-models.html#cb304-21" tabindex="-1"></a></span>
<span id="cb304-22"><a href="shrinkage-models.html#cb304-22" tabindex="-1"></a><span class="do">## Lasso</span></span>
<span id="cb304-23"><a href="shrinkage-models.html#cb304-23" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb304-24"><a href="shrinkage-models.html#cb304-24" tabindex="-1"></a>lasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(X, y, <span class="at">alpha=</span><span class="dv">1</span>)</span>
<span id="cb304-25"><a href="shrinkage-models.html#cb304-25" tabindex="-1"></a></span>
<span id="cb304-26"><a href="shrinkage-models.html#cb304-26" tabindex="-1"></a><span class="co"># Sparsity</span></span>
<span id="cb304-27"><a href="shrinkage-models.html#cb304-27" tabindex="-1"></a><span class="fu">cbind</span>(<span class="at">LASSO =</span> <span class="fu">coef</span>(lasso, <span class="at">s=</span><span class="st">&quot;lambda.1se&quot;</span>),</span>
<span id="cb304-28"><a href="shrinkage-models.html#cb304-28" tabindex="-1"></a>           <span class="at">ALASSO =</span> <span class="fu">coef</span>(alasso, <span class="at">s=</span><span class="st">&quot;lambda.1se&quot;</span>))</span></code></pre></div>
<pre><code>## 20 x 2 sparse Matrix of class &quot;dgCMatrix&quot;
##                       s1          s1
## (Intercept) 127.95694754   -7.109481
## AtBat         .             .       
## Hits          1.42342566    2.054867
## HmRun         .             .       
## Runs          .             .       
## RBI           .             .       
## Walks         1.58214111    3.573120
## Years         .            31.573334
## CAtBat        .             .       
## CHits         .             .       
## CHmRun        .             .       
## CRuns         0.16027975    .       
## CRBI          0.33667715    .       
## CWalks        .             .       
## LeagueN       .            29.811080
## DivisionW    -8.06171262 -138.088953
## PutOuts       0.08393604    .       
## Assists       .             .       
## Errors        .             .       
## NewLeagueN    .             .</code></pre>
<p>We can see the difference between lasso and adaptive lasso in this example: <code>PutOuts</code>, <code>CRuns</code>, and <code>CRBI</code> picked by lasso are not selected by adaptive lasso. There are only three common features in both methods: <code>Hits</code>, <code>Walks</code>, and <code>DivisionW</code>. To understand which model is better in terms of catching the true sparsity, we will have a simulation to illustrate some of the properties of the Lasso and the adaptive Lasso.</p>
</div>
<div id="sparsity" class="section level2 hasAnchor" number="15.4">
<h2><span class="header-section-number">15.4</span> Sparsity<a href="shrinkage-models.html#sparsity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This is a simulation to illustrate some of the properties of Lasso-type estimations. There are two objectives in using these penalized regressions: model selection (identifying “correct” sparsity) and prediction accuracy. These two objectives require different optimization approaches and usually are not compatible. In model selection, the objective is to shrink the dimension of the model to the “true” sparsity. This is usually evaluated by checking whether the Oracle properties are satisfied. These asymptotic properties look at (1) if the model identified by the penalized regression converges to the “true” sparsity, (2) if the coefficients are consistent.</p>
<p>The literature suggests that Lasso is not an oracle estimator. Adaptive Lasso was developed (Zou 2006) to fill this gap.</p>
<p>Let’s specify a data generating process with a linear regression model:</p>
<p><span class="math display">\[
y_i=x_i^{\prime} \beta+u_i, ~~~~~i=1, \ldots, n
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is <span class="math inline">\(p \times 1\)</span>. First, we consider the case where <span class="math inline">\(p&lt;n\)</span> then move to the case where <span class="math inline">\(p \geq n\)</span>. We define <span class="math inline">\(\beta=(1,1,0,0)^{\prime}\)</span> and <span class="math inline">\(n=100\)</span>.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="shrinkage-models.html#cb306-1" tabindex="-1"></a><span class="co">#This function generates the data</span></span>
<span id="cb306-2"><a href="shrinkage-models.html#cb306-2" tabindex="-1"></a>dgp <span class="ot">&lt;-</span> <span class="cf">function</span>(N, Beta) {</span>
<span id="cb306-3"><a href="shrinkage-models.html#cb306-3" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">length</span>(Beta)</span>
<span id="cb306-4"><a href="shrinkage-models.html#cb306-4" tabindex="-1"></a>  </span>
<span id="cb306-5"><a href="shrinkage-models.html#cb306-5" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(N <span class="sc">*</span> p), <span class="at">ncol =</span> p)</span>
<span id="cb306-6"><a href="shrinkage-models.html#cb306-6" tabindex="-1"></a>  u <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(N), <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb306-7"><a href="shrinkage-models.html#cb306-7" tabindex="-1"></a>  dgm <span class="ot">&lt;-</span> X <span class="sc">%*%</span> Beta</span>
<span id="cb306-8"><a href="shrinkage-models.html#cb306-8" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> Beta <span class="sc">+</span> u</span>
<span id="cb306-9"><a href="shrinkage-models.html#cb306-9" tabindex="-1"></a>  </span>
<span id="cb306-10"><a href="shrinkage-models.html#cb306-10" tabindex="-1"></a>  return <span class="ot">&lt;-</span> <span class="fu">list</span>(y, X)</span>
<span id="cb306-11"><a href="shrinkage-models.html#cb306-11" tabindex="-1"></a>}</span>
<span id="cb306-12"><a href="shrinkage-models.html#cb306-12" tabindex="-1"></a></span>
<span id="cb306-13"><a href="shrinkage-models.html#cb306-13" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb306-14"><a href="shrinkage-models.html#cb306-14" tabindex="-1"></a>Beta <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb306-15"><a href="shrinkage-models.html#cb306-15" tabindex="-1"></a></span>
<span id="cb306-16"><a href="shrinkage-models.html#cb306-16" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">148</span>)</span>
<span id="cb306-17"><a href="shrinkage-models.html#cb306-17" tabindex="-1"></a>Output <span class="ot">&lt;-</span> <span class="fu">dgp</span>(N, Beta)</span>
<span id="cb306-18"><a href="shrinkage-models.html#cb306-18" tabindex="-1"></a>y <span class="ot">&lt;-</span> Output[[<span class="dv">1</span>]]</span>
<span id="cb306-19"><a href="shrinkage-models.html#cb306-19" tabindex="-1"></a>X <span class="ot">&lt;-</span> Output[[<span class="dv">2</span>]]</span></code></pre></div>
<p>First, we apply lasso</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="shrinkage-models.html#cb307-1" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb307-2"><a href="shrinkage-models.html#cb307-2" tabindex="-1"></a></span>
<span id="cb307-3"><a href="shrinkage-models.html#cb307-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">432</span>)</span>
<span id="cb307-4"><a href="shrinkage-models.html#cb307-4" tabindex="-1"></a>lasso <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>)</span>
<span id="cb307-5"><a href="shrinkage-models.html#cb307-5" tabindex="-1"></a></span>
<span id="cb307-6"><a href="shrinkage-models.html#cb307-6" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> lasso<span class="sc">$</span>beta</span>
<span id="cb307-7"><a href="shrinkage-models.html#cb307-7" tabindex="-1"></a>S_matrix <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">t</span>(beta_hat), <span class="st">&quot;lambda&quot;</span> <span class="ot">=</span> lasso<span class="sc">$</span>lambda)</span>
<span id="cb307-8"><a href="shrinkage-models.html#cb307-8" tabindex="-1"></a>S_matrix[<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>, <span class="dv">25</span><span class="sc">:</span><span class="dv">30</span>, <span class="dv">55</span><span class="sc">:</span><span class="dv">60</span>), ] <span class="co"># selected rows</span></span></code></pre></div>
<pre><code>## 20 x 5 sparse Matrix of class &quot;dgCMatrix&quot;
##             V1        V2          V3         V4      lambda
## s0  .          .          .          .          1.083220708
## s1  0.09439841 0.0283513  .          .          0.986990366
## s2  0.17344129 0.1097255  .          .          0.899308862
## s3  0.24546220 0.1838706  .          .          0.819416741
## s4  0.31108496 0.2514289  .          .          0.746622016
## s5  0.37087798 0.3129855  .          .          0.680294174
## s6  0.42535915 0.3690736  .          .          0.619858715
## s7  0.47500037 0.4201789  .          .          0.564792175
## s24 0.87944075 0.8365481  .          .          0.116150206
## s25 0.88874261 0.8461243  .          .          0.105831742
## s26 0.89685610 0.8542117 -0.00686322 .          0.096429941
## s27 0.90418482 0.8614679 -0.01432988 .          0.087863371
## s28 0.91086250 0.8680794 -0.02113323 .          0.080057832
## s29 0.91694695 0.8741036 -0.02733218 .          0.072945714
## s54 0.98352129 0.9289175 -0.09282009 0.05192379 0.007126869
## s55 0.98423271 0.9294382 -0.09350608 0.05278151 0.006493738
## s56 0.98488092 0.9299126 -0.09413113 0.05356303 0.005916852
## s57 0.98547155 0.9303449 -0.09470066 0.05427512 0.005391215
## s58 0.98600972 0.9307388 -0.09521958 0.05492395 0.004912274
## s59 0.98650007 0.9310977 -0.09569241 0.05551515 0.004475881</code></pre>
<p>Which set of beta_hat should we select? To answer this question we need to find the lambda. We need <span class="math inline">\(\lambda_n \rightarrow \infty\)</span> in order to shrink the truly zero coefficients to zero. This requires <span class="math inline">\(\lambda_n\)</span> to be sufficiently large. This would introduce asymptotic bias to the non-zero coefficients.</p>
<p>In practice, choosing <span class="math inline">\(\lambda_n\)</span> by <span class="math inline">\(\mathrm{BIC}\)</span> (Bayesian Information Criterion) results in a consistent model selection in the fixed <span class="math inline">\(p\)</span> setting. That is, let <span class="math inline">\(\mathcal{A}=\left\{j: \beta_{0, j} \neq 0\right\}\)</span>, active set or relevant variables,</p>
<p><span class="math display">\[
P\left(\hat{\mathcal{A}}_{\lambda_{BIC}}=\mathcal{A}\right) \rightarrow 1
\]</span></p>
<p>Thus, let <span class="math inline">\(S S E_\lambda\)</span> be the sum of squared error terms for a given value of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(n z_\lambda\)</span> be the number of non-zero coefficients. Then, it can be shown that</p>
<p><span class="math display">\[
B I C_\lambda=\log \left(S S E_\lambda\right)+\frac{\log (n)}{n} n z_\lambda
\]</span></p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="shrinkage-models.html#cb309-1" tabindex="-1"></a><span class="co"># Predict yhat for each of 61 lambda (s)</span></span>
<span id="cb309-2"><a href="shrinkage-models.html#cb309-2" tabindex="-1"></a>y_hat <span class="ot">=</span> <span class="fu">predict</span>(lasso, <span class="at">newx =</span> X)</span>
<span id="cb309-3"><a href="shrinkage-models.html#cb309-3" tabindex="-1"></a><span class="fu">dim</span>(y_hat)</span></code></pre></div>
<pre><code>## [1] 100  60</code></pre>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="shrinkage-models.html#cb311-1" tabindex="-1"></a><span class="co"># SSE for each lambda (s)</span></span>
<span id="cb311-2"><a href="shrinkage-models.html#cb311-2" tabindex="-1"></a>SSE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb311-3"><a href="shrinkage-models.html#cb311-3" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(y_hat)) {</span>
<span id="cb311-4"><a href="shrinkage-models.html#cb311-4" tabindex="-1"></a>  SSE_each <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_hat[, i] <span class="sc">-</span> y[, <span class="dv">1</span>]) <span class="sc">^</span> (<span class="dv">2</span>))</span>
<span id="cb311-5"><a href="shrinkage-models.html#cb311-5" tabindex="-1"></a>  SSE <span class="ot">&lt;-</span> <span class="fu">c</span>(SSE, SSE_each)</span>
<span id="cb311-6"><a href="shrinkage-models.html#cb311-6" tabindex="-1"></a>}</span>
<span id="cb311-7"><a href="shrinkage-models.html#cb311-7" tabindex="-1"></a></span>
<span id="cb311-8"><a href="shrinkage-models.html#cb311-8" tabindex="-1"></a><span class="co"># BIC</span></span>
<span id="cb311-9"><a href="shrinkage-models.html#cb311-9" tabindex="-1"></a>nz <span class="ot">&lt;-</span> <span class="fu">colSums</span>(beta_hat <span class="sc">!=</span> <span class="dv">0</span>) <span class="co"># Number of non-zero coefficients for each lambda</span></span>
<span id="cb311-10"><a href="shrinkage-models.html#cb311-10" tabindex="-1"></a>BIC <span class="ot">&lt;-</span> <span class="fu">log</span>(SSE) <span class="sc">+</span> (<span class="fu">log</span>(N) <span class="sc">/</span> N) <span class="sc">*</span> nz <span class="co"># BIC</span></span>
<span id="cb311-11"><a href="shrinkage-models.html#cb311-11" tabindex="-1"></a>BIC</span></code></pre></div>
<pre><code>##       s0       s1       s2       s3       s4       s5       s6       s7 
## 5.598919 5.595359 5.468287 5.348947 5.237755 5.135013 5.040883 4.955387 
##       s8       s9      s10      s11      s12      s13      s14      s15 
## 4.878394 4.809638 4.748729 4.695181 4.648437 4.607898 4.572946 4.542971 
##      s16      s17      s18      s19      s20      s21      s22      s23 
## 4.517383 4.495631 4.477205 4.461646 4.448541 4.437530 4.428295 4.420563 
##      s24      s25      s26      s27      s28      s29      s30      s31 
## 4.414098 4.408698 4.448661 4.443309 4.438844 4.435121 4.432021 4.429439 
##      s32      s33      s34      s35      s36      s37      s38      s39 
## 4.427290 4.425503 4.424017 4.468004 4.466218 4.464732 4.463498 4.462471 
##      s40      s41      s42      s43      s44      s45      s46      s47 
## 4.461618 4.460910 4.460321 4.459832 4.459426 4.459088 4.458808 4.458575 
##      s48      s49      s50      s51      s52      s53      s54      s55 
## 4.458382 4.458222 4.458088 4.457978 4.457886 4.457810 4.457746 4.457694 
##      s56      s57      s58      s59 
## 4.457650 4.457614 4.457584 4.457559</code></pre>
<p>And, the selected model that has the minimum BIC</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="shrinkage-models.html#cb313-1" tabindex="-1"></a>beta_lasso <span class="ot">&lt;-</span> beta_hat[, <span class="fu">which</span>(BIC <span class="sc">==</span> <span class="fu">min</span>(BIC))]</span>
<span id="cb313-2"><a href="shrinkage-models.html#cb313-2" tabindex="-1"></a>beta_lasso</span></code></pre></div>
<pre><code>##        V1        V2        V3        V4 
## 0.8887426 0.8461243 0.0000000 0.0000000</code></pre>
<p>This is the <code>beta_hat</code> that identifies the true sparsity. And, the second Oracle property, the <span class="math inline">\(\ell_2\)</span> error:</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="shrinkage-models.html#cb315-1" tabindex="-1"></a>l_2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((beta_lasso <span class="sc">-</span> Beta) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb315-2"><a href="shrinkage-models.html#cb315-2" tabindex="-1"></a>l_2</span></code></pre></div>
<pre><code>## [1] 0.189884</code></pre>
<p>Here we will create a simulation that will report two Oracle Properties for Lasso and Adaptive Lasso:</p>
<ul>
<li>True sparsity,<br />
</li>
<li><span class="math inline">\(\ell_2\)</span> error.</li>
</ul>
<p><strong>Lasso</strong></p>
<p>We first have a function, <code>msc()</code>, that executes a simulation with all the steps shown before:</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="shrinkage-models.html#cb317-1" tabindex="-1"></a>mcs <span class="ot">&lt;-</span> <span class="cf">function</span>(mc, N, Beta) {</span>
<span id="cb317-2"><a href="shrinkage-models.html#cb317-2" tabindex="-1"></a>  mcmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> mc, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb317-3"><a href="shrinkage-models.html#cb317-3" tabindex="-1"></a>  beta_lasso_mat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nr =</span> mc, <span class="at">nc =</span> <span class="fu">length</span>(Beta))</span>
<span id="cb317-4"><a href="shrinkage-models.html#cb317-4" tabindex="-1"></a>  </span>
<span id="cb317-5"><a href="shrinkage-models.html#cb317-5" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>mc) {</span>
<span id="cb317-6"><a href="shrinkage-models.html#cb317-6" tabindex="-1"></a>    <span class="fu">set.seed</span>(i)</span>
<span id="cb317-7"><a href="shrinkage-models.html#cb317-7" tabindex="-1"></a>    data <span class="ot">&lt;-</span> <span class="fu">dgp</span>(N, Beta)</span>
<span id="cb317-8"><a href="shrinkage-models.html#cb317-8" tabindex="-1"></a>    y <span class="ot">&lt;-</span> data[[<span class="dv">1</span>]]</span>
<span id="cb317-9"><a href="shrinkage-models.html#cb317-9" tabindex="-1"></a>    X <span class="ot">&lt;-</span> data[[<span class="dv">2</span>]]</span>
<span id="cb317-10"><a href="shrinkage-models.html#cb317-10" tabindex="-1"></a>    </span>
<span id="cb317-11"><a href="shrinkage-models.html#cb317-11" tabindex="-1"></a>    <span class="fu">set.seed</span>(i)</span>
<span id="cb317-12"><a href="shrinkage-models.html#cb317-12" tabindex="-1"></a>    lasso <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>)</span>
<span id="cb317-13"><a href="shrinkage-models.html#cb317-13" tabindex="-1"></a>    beta_hat <span class="ot">&lt;-</span> lasso<span class="sc">$</span>beta    <span class="co"># beta_hat is a matrix</span></span>
<span id="cb317-14"><a href="shrinkage-models.html#cb317-14" tabindex="-1"></a>    y_hat <span class="ot">=</span> <span class="fu">predict</span>(lasso, <span class="at">newx =</span> X)</span>
<span id="cb317-15"><a href="shrinkage-models.html#cb317-15" tabindex="-1"></a>    </span>
<span id="cb317-16"><a href="shrinkage-models.html#cb317-16" tabindex="-1"></a>    SSE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb317-17"><a href="shrinkage-models.html#cb317-17" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(y_hat)) {</span>
<span id="cb317-18"><a href="shrinkage-models.html#cb317-18" tabindex="-1"></a>      SSE_each <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_hat[, j] <span class="sc">-</span> y[, <span class="dv">1</span>]) <span class="sc">^</span> (<span class="dv">2</span>))</span>
<span id="cb317-19"><a href="shrinkage-models.html#cb317-19" tabindex="-1"></a>      SSE <span class="ot">&lt;-</span> <span class="fu">c</span>(SSE, SSE_each)</span>
<span id="cb317-20"><a href="shrinkage-models.html#cb317-20" tabindex="-1"></a>    }</span>
<span id="cb317-21"><a href="shrinkage-models.html#cb317-21" tabindex="-1"></a>    </span>
<span id="cb317-22"><a href="shrinkage-models.html#cb317-22" tabindex="-1"></a>    nz <span class="ot">&lt;-</span> <span class="fu">colSums</span>(beta_hat <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb317-23"><a href="shrinkage-models.html#cb317-23" tabindex="-1"></a>    BIC <span class="ot">&lt;-</span> <span class="fu">log</span>(SSE) <span class="sc">+</span> (<span class="fu">log</span>(N) <span class="sc">/</span> N) <span class="sc">*</span> nz</span>
<span id="cb317-24"><a href="shrinkage-models.html#cb317-24" tabindex="-1"></a>    beta_lasso <span class="ot">&lt;-</span> beta_hat[, <span class="fu">which</span>(BIC <span class="sc">==</span> <span class="fu">min</span>(BIC))]</span>
<span id="cb317-25"><a href="shrinkage-models.html#cb317-25" tabindex="-1"></a>    nonz_beta <span class="ot">=</span> <span class="fu">length</span>(Beta[Beta <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb317-26"><a href="shrinkage-models.html#cb317-26" tabindex="-1"></a>    nonz_beta_hat <span class="ot">=</span> <span class="fu">length</span>(beta_lasso[beta_lasso <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb317-27"><a href="shrinkage-models.html#cb317-27" tabindex="-1"></a>    </span>
<span id="cb317-28"><a href="shrinkage-models.html#cb317-28" tabindex="-1"></a>    mcmat[i, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((beta_lasso <span class="sc">-</span> Beta) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb317-29"><a href="shrinkage-models.html#cb317-29" tabindex="-1"></a>    mcmat[i, <span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(nonz_beta <span class="sc">!=</span> nonz_beta_hat, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb317-30"><a href="shrinkage-models.html#cb317-30" tabindex="-1"></a>    mcmat[i, <span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">sum</span>(beta_lasso <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb317-31"><a href="shrinkage-models.html#cb317-31" tabindex="-1"></a>    beta_lasso_mat[i, ] <span class="ot">&lt;-</span> beta_lasso</span>
<span id="cb317-32"><a href="shrinkage-models.html#cb317-32" tabindex="-1"></a>  }</span>
<span id="cb317-33"><a href="shrinkage-models.html#cb317-33" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(mcmat, beta_lasso_mat))</span>
<span id="cb317-34"><a href="shrinkage-models.html#cb317-34" tabindex="-1"></a>}</span></code></pre></div>
<p>We are ready for simulation:</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="shrinkage-models.html#cb318-1" tabindex="-1"></a>mc <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb318-2"><a href="shrinkage-models.html#cb318-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb318-3"><a href="shrinkage-models.html#cb318-3" tabindex="-1"></a>Beta <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="at">nc =</span> <span class="dv">1</span>)</span>
<span id="cb318-4"><a href="shrinkage-models.html#cb318-4" tabindex="-1"></a>output <span class="ot">&lt;-</span> <span class="fu">mcs</span>(mc, N, Beta) <span class="co">#see the function</span></span>
<span id="cb318-5"><a href="shrinkage-models.html#cb318-5" tabindex="-1"></a></span>
<span id="cb318-6"><a href="shrinkage-models.html#cb318-6" tabindex="-1"></a>MC_betas <span class="ot">=</span> output[[<span class="dv">2</span>]]</span>
<span id="cb318-7"><a href="shrinkage-models.html#cb318-7" tabindex="-1"></a>MC_performance <span class="ot">=</span> output[[<span class="dv">1</span>]]</span>
<span id="cb318-8"><a href="shrinkage-models.html#cb318-8" tabindex="-1"></a></span>
<span id="cb318-9"><a href="shrinkage-models.html#cb318-9" tabindex="-1"></a><span class="fu">sum</span>(MC_performance[, <span class="dv">2</span>]) <span class="co">#how many times lasso finds true sparsity</span></span></code></pre></div>
<pre><code>## [1] 400</code></pre>
<p>This is the first property: lasso identifies the true sparsity <span class="math inline">\(400/500 = 80\%\)</span> of cases. And the second property, <span class="math inline">\(\ell_2\)</span> error, in the simulation is (in total):</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="shrinkage-models.html#cb320-1" tabindex="-1"></a><span class="fu">sum</span>(MC_performance[, <span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 29.41841</code></pre>
<p><strong>Adaptive Lasso</strong></p>
<p>This time we let our adaptive lasso use lasso coefficients as penalty weights in <code>glmnet()</code>. Let’s have the same function with Adaptive Lasso for the simulation:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="shrinkage-models.html#cb322-1" tabindex="-1"></a><span class="co"># Adaptive LASSO</span></span>
<span id="cb322-2"><a href="shrinkage-models.html#cb322-2" tabindex="-1"></a>mcsA <span class="ot">&lt;-</span> <span class="cf">function</span>(mc, N, Beta) {</span>
<span id="cb322-3"><a href="shrinkage-models.html#cb322-3" tabindex="-1"></a>  mcmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nr =</span> mc, <span class="at">nc =</span> <span class="dv">3</span>)</span>
<span id="cb322-4"><a href="shrinkage-models.html#cb322-4" tabindex="-1"></a>  beta_lasso_mat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nr =</span> mc, <span class="at">nc =</span> <span class="fu">length</span>(Beta))</span>
<span id="cb322-5"><a href="shrinkage-models.html#cb322-5" tabindex="-1"></a>  </span>
<span id="cb322-6"><a href="shrinkage-models.html#cb322-6" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>mc) {</span>
<span id="cb322-7"><a href="shrinkage-models.html#cb322-7" tabindex="-1"></a>    data <span class="ot">&lt;-</span> <span class="fu">dgp</span>(N, Beta)</span>
<span id="cb322-8"><a href="shrinkage-models.html#cb322-8" tabindex="-1"></a>    y <span class="ot">&lt;-</span> data[[<span class="dv">1</span>]]</span>
<span id="cb322-9"><a href="shrinkage-models.html#cb322-9" tabindex="-1"></a>    X <span class="ot">&lt;-</span> data[[<span class="dv">2</span>]]</span>
<span id="cb322-10"><a href="shrinkage-models.html#cb322-10" tabindex="-1"></a>    </span>
<span id="cb322-11"><a href="shrinkage-models.html#cb322-11" tabindex="-1"></a>    lasso <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>)</span>
<span id="cb322-12"><a href="shrinkage-models.html#cb322-12" tabindex="-1"></a>    beta_hat <span class="ot">&lt;-</span> lasso<span class="sc">$</span>beta</span>
<span id="cb322-13"><a href="shrinkage-models.html#cb322-13" tabindex="-1"></a>    </span>
<span id="cb322-14"><a href="shrinkage-models.html#cb322-14" tabindex="-1"></a>    y_hat <span class="ot">=</span> <span class="fu">predict</span>(lasso, <span class="at">newx =</span> X)</span>
<span id="cb322-15"><a href="shrinkage-models.html#cb322-15" tabindex="-1"></a>    </span>
<span id="cb322-16"><a href="shrinkage-models.html#cb322-16" tabindex="-1"></a>    SSE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb322-17"><a href="shrinkage-models.html#cb322-17" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(y_hat)) {</span>
<span id="cb322-18"><a href="shrinkage-models.html#cb322-18" tabindex="-1"></a>      SSE_each <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_hat[, j] <span class="sc">-</span> y[, <span class="dv">1</span>]) <span class="sc">^</span> (<span class="dv">2</span>))</span>
<span id="cb322-19"><a href="shrinkage-models.html#cb322-19" tabindex="-1"></a>      SSE <span class="ot">&lt;-</span> <span class="fu">c</span>(SSE, SSE_each)</span>
<span id="cb322-20"><a href="shrinkage-models.html#cb322-20" tabindex="-1"></a>    }</span>
<span id="cb322-21"><a href="shrinkage-models.html#cb322-21" tabindex="-1"></a>    </span>
<span id="cb322-22"><a href="shrinkage-models.html#cb322-22" tabindex="-1"></a>    nz <span class="ot">&lt;-</span> <span class="fu">colSums</span>(beta_hat <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb322-23"><a href="shrinkage-models.html#cb322-23" tabindex="-1"></a>    BIC <span class="ot">&lt;-</span> <span class="fu">log</span>(SSE) <span class="sc">+</span> (<span class="fu">log</span>(N) <span class="sc">/</span> N) <span class="sc">*</span> nz</span>
<span id="cb322-24"><a href="shrinkage-models.html#cb322-24" tabindex="-1"></a>    beta_lasso <span class="ot">&lt;-</span> beta_hat[, <span class="fu">which</span>(BIC <span class="sc">==</span> <span class="fu">min</span>(BIC))]</span>
<span id="cb322-25"><a href="shrinkage-models.html#cb322-25" tabindex="-1"></a>    </span>
<span id="cb322-26"><a href="shrinkage-models.html#cb322-26" tabindex="-1"></a>    weights <span class="ot">=</span> <span class="fu">abs</span>(beta_lasso) <span class="sc">^</span> (<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb322-27"><a href="shrinkage-models.html#cb322-27" tabindex="-1"></a>    weights[beta_lasso <span class="sc">==</span> <span class="dv">0</span>] <span class="ot">=</span> <span class="dv">10</span> <span class="sc">^</span> <span class="dv">10</span> <span class="co"># to handle inf&#39;s</span></span>
<span id="cb322-28"><a href="shrinkage-models.html#cb322-28" tabindex="-1"></a>    </span>
<span id="cb322-29"><a href="shrinkage-models.html#cb322-29" tabindex="-1"></a>    <span class="co">#Now Adaptive Lasso</span></span>
<span id="cb322-30"><a href="shrinkage-models.html#cb322-30" tabindex="-1"></a>    lasso <span class="ot">&lt;-</span></span>
<span id="cb322-31"><a href="shrinkage-models.html#cb322-31" tabindex="-1"></a>      <span class="fu">glmnet</span>(</span>
<span id="cb322-32"><a href="shrinkage-models.html#cb322-32" tabindex="-1"></a>        <span class="at">x =</span> X,</span>
<span id="cb322-33"><a href="shrinkage-models.html#cb322-33" tabindex="-1"></a>        <span class="at">y =</span> y,</span>
<span id="cb322-34"><a href="shrinkage-models.html#cb322-34" tabindex="-1"></a>        <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>,</span>
<span id="cb322-35"><a href="shrinkage-models.html#cb322-35" tabindex="-1"></a>        <span class="at">penalty.factor =</span> weights</span>
<span id="cb322-36"><a href="shrinkage-models.html#cb322-36" tabindex="-1"></a>      )</span>
<span id="cb322-37"><a href="shrinkage-models.html#cb322-37" tabindex="-1"></a>    beta_hat <span class="ot">&lt;-</span> lasso<span class="sc">$</span>beta</span>
<span id="cb322-38"><a href="shrinkage-models.html#cb322-38" tabindex="-1"></a>    </span>
<span id="cb322-39"><a href="shrinkage-models.html#cb322-39" tabindex="-1"></a>    y_hat <span class="ot">=</span> <span class="fu">predict</span>(lasso, <span class="at">newx =</span> X)</span>
<span id="cb322-40"><a href="shrinkage-models.html#cb322-40" tabindex="-1"></a>    </span>
<span id="cb322-41"><a href="shrinkage-models.html#cb322-41" tabindex="-1"></a>    SSE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb322-42"><a href="shrinkage-models.html#cb322-42" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(y_hat)) {</span>
<span id="cb322-43"><a href="shrinkage-models.html#cb322-43" tabindex="-1"></a>      SSE_each <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_hat[, j] <span class="sc">-</span> y[, <span class="dv">1</span>]) <span class="sc">^</span> (<span class="dv">2</span>))</span>
<span id="cb322-44"><a href="shrinkage-models.html#cb322-44" tabindex="-1"></a>      SSE <span class="ot">&lt;-</span> <span class="fu">c</span>(SSE, SSE_each)</span>
<span id="cb322-45"><a href="shrinkage-models.html#cb322-45" tabindex="-1"></a>    }</span>
<span id="cb322-46"><a href="shrinkage-models.html#cb322-46" tabindex="-1"></a>    </span>
<span id="cb322-47"><a href="shrinkage-models.html#cb322-47" tabindex="-1"></a>    nz <span class="ot">&lt;-</span> <span class="fu">colSums</span>(beta_hat <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb322-48"><a href="shrinkage-models.html#cb322-48" tabindex="-1"></a>    BIC <span class="ot">&lt;-</span> <span class="fu">log</span>(SSE) <span class="sc">+</span> (<span class="fu">log</span>(N) <span class="sc">/</span> N) <span class="sc">*</span> nz</span>
<span id="cb322-49"><a href="shrinkage-models.html#cb322-49" tabindex="-1"></a>    beta_lasso <span class="ot">&lt;-</span> beta_hat[, <span class="fu">which</span>(BIC <span class="sc">==</span> <span class="fu">min</span>(BIC))]</span>
<span id="cb322-50"><a href="shrinkage-models.html#cb322-50" tabindex="-1"></a>    nonz_beta <span class="ot">=</span> <span class="fu">length</span>(Beta[Beta <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb322-51"><a href="shrinkage-models.html#cb322-51" tabindex="-1"></a>    nonz_beta_hat <span class="ot">=</span> <span class="fu">length</span>(beta_lasso[beta_lasso <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb322-52"><a href="shrinkage-models.html#cb322-52" tabindex="-1"></a>    </span>
<span id="cb322-53"><a href="shrinkage-models.html#cb322-53" tabindex="-1"></a>    mcmat[i, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((beta_lasso <span class="sc">-</span> Beta) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb322-54"><a href="shrinkage-models.html#cb322-54" tabindex="-1"></a>    mcmat[i, <span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(nonz_beta <span class="sc">!=</span> nonz_beta_hat, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb322-55"><a href="shrinkage-models.html#cb322-55" tabindex="-1"></a>    mcmat[i, <span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">sum</span>(beta_lasso <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb322-56"><a href="shrinkage-models.html#cb322-56" tabindex="-1"></a>    beta_lasso_mat[i, ] <span class="ot">&lt;-</span> beta_lasso</span>
<span id="cb322-57"><a href="shrinkage-models.html#cb322-57" tabindex="-1"></a>  }</span>
<span id="cb322-58"><a href="shrinkage-models.html#cb322-58" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(mcmat, beta_lasso_mat))</span>
<span id="cb322-59"><a href="shrinkage-models.html#cb322-59" tabindex="-1"></a>}</span></code></pre></div>
<p>Here are the results for adaptive lasso:</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="shrinkage-models.html#cb323-1" tabindex="-1"></a>mc <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb323-2"><a href="shrinkage-models.html#cb323-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb323-3"><a href="shrinkage-models.html#cb323-3" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="at">nc =</span> <span class="dv">1</span>)</span>
<span id="cb323-4"><a href="shrinkage-models.html#cb323-4" tabindex="-1"></a>output <span class="ot">&lt;-</span> <span class="fu">mcsA</span>(mc, N, beta) <span class="co">#see the function</span></span>
<span id="cb323-5"><a href="shrinkage-models.html#cb323-5" tabindex="-1"></a></span>
<span id="cb323-6"><a href="shrinkage-models.html#cb323-6" tabindex="-1"></a>MC_betas <span class="ot">=</span> output[[<span class="dv">2</span>]]</span>
<span id="cb323-7"><a href="shrinkage-models.html#cb323-7" tabindex="-1"></a>MC_performance <span class="ot">=</span> output[[<span class="dv">1</span>]]</span>
<span id="cb323-8"><a href="shrinkage-models.html#cb323-8" tabindex="-1"></a></span>
<span id="cb323-9"><a href="shrinkage-models.html#cb323-9" tabindex="-1"></a><span class="fu">sum</span>(MC_performance[, <span class="dv">2</span>])</span></code></pre></div>
<pre><code>## [1] 492</code></pre>
<p>And,</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="shrinkage-models.html#cb325-1" tabindex="-1"></a><span class="fu">sum</span>(MC_performance[,<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 20.21311</code></pre>
<p>The simulation results clearly show that Adaptive Lasso is an Oracle estimator and a better choice for sparsity applications.</p>
<p>We saw here a basic application of adaptive lasso, which has several different variations in practice, such as Thresholded Lasso and Rigorous Lasso. Model selections with lasso has been an active research area. One of the well-known applications is the double-selection lasso linear regression method that can be used for variable selections. Moreover, lasso type applications are also used in time-series forecasting and graphical network analysis for dimension reductions.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interpretability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-trees.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuk/machinemetrics/edit/master/15-ShrinkageMethods.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
