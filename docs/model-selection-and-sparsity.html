<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 20 Model selection and Sparsity | MachineMetrics</title>
  <meta name="description" content="Chapter 20 Model selection and Sparsity | MachineMetrics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 20 Model selection and Sparsity | MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 20 Model selection and Sparsity | MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="heterogeneous-treatment-effects.html"/>
<link rel="next" href="classification-2.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html"><i class="fa fa-check"></i><b>2</b> Statistical Models and Simulations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#distinguishing-between-statistical-modeling-and-machine-learning-in-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#goals-and-objectives"><i class="fa fa-check"></i><b>2.1.1</b> Goals and Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#prediction-vs.-inference"><i class="fa fa-check"></i><b>2.1.2</b> Prediction vs. Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#conclusion"><i class="fa fa-check"></i><b>2.1.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#model-selection-and-approaches-in-data-modeling"><i class="fa fa-check"></i><b>2.4</b> Model Selection and Approaches in Data Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#mle"><i class="fa fa-check"></i><b>3.5.3</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error---mse"><i class="fa fa-check"></i><b>5.1</b> Estimation error - MSE</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.2</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#technical-points-about-mse-and-mspe"><i class="fa fa-check"></i><b>5.3</b> Technical points about MSE and MSPE</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.1</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.1</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.2</b> LPM</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
<li class="chapter" data-level="10.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#knn"><i class="fa fa-check"></i><b>10.3</b> Knn</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#adult-dataset"><i class="fa fa-check"></i><b>10.3.1</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="11.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#grid-search"><i class="fa fa-check"></i><b>11.4</b> Grid Search</a></li>
<li class="chapter" data-level="11.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>11.5</b> Cross-validated grid search</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#one-variable"><i class="fa fa-check"></i><b>12.4.1</b> One-variable</a></li>
<li class="chapter" data-level="12.4.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>12.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="12.4.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#multivariable"><i class="fa fa-check"></i><b>12.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>16.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.3</b> Pruning</a></li>
<li class="chapter" data-level="16.4" data-path="regression-trees.html"><a href="regression-trees.html#classification-with-titanic"><i class="fa fa-check"></i><b>16.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="16.5" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>17.2</b> Random Forest</a></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>17.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="17.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="17.3.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.3.3</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.4</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.5</b> Classification</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.6</b> Regression</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.7</b> Exploration</a></li>
<li class="chapter" data-level="17.8" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.8</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression-1"><i class="fa fa-check"></i><b>17.8.1</b> Regression</a></li>
<li class="chapter" data-level="17.8.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.8.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.8.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.8.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.8.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-1"><i class="fa fa-check"></i><b>17.8.4</b> Classification</a></li>
<li class="chapter" data-level="17.8.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost.m1"><i class="fa fa-check"></i><b>17.8.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="17.8.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.8.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#out-sample-prediction-accuracy"><i class="fa fa-check"></i><b>20.3</b> out-sample prediction accuracy</a></li>
<li class="chapter" data-level="20.4" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-2.html"><a href="classification-2.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-2.html"><a href="classification-2.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-2.html"><a href="classification-2.html#mnist-dataset"><i class="fa fa-check"></i><b>21.2</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="21.3" data-path="classification-2.html"><a href="classification-2.html#linear-classifiers-again"><i class="fa fa-check"></i><b>21.3</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="21.4" data-path="classification-2.html"><a href="classification-2.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.4</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.5" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret"><i class="fa fa-check"></i><b>21.5</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="classification-2.html"><a href="classification-2.html#mnist_27"><i class="fa fa-check"></i><b>21.5.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="21.5.2" data-path="classification-2.html"><a href="classification-2.html#adult-dataset-1"><i class="fa fa-check"></i><b>21.5.2</b> Adult dataset</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="classification-2.html"><a href="classification-2.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.6</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.7" data-path="classification-2.html"><a href="classification-2.html#confusion-matrix"><i class="fa fa-check"></i><b>21.7</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.8" data-path="classification-2.html"><a href="classification-2.html#performance-measures"><i class="fa fa-check"></i><b>21.8</b> Performance measures</a></li>
<li class="chapter" data-level="21.9" data-path="classification-2.html"><a href="classification-2.html#roc-curve"><i class="fa fa-check"></i><b>21.9</b> ROC Curve</a></li>
<li class="chapter" data-level="21.10" data-path="classification-2.html"><a href="classification-2.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.10</b> AUC - Area Under the Curve</a></li>
<li class="chapter" data-level="21.11" data-path="classification-2.html"><a href="classification-2.html#classification-example"><i class="fa fa-check"></i><b>21.11</b> Classification Example</a></li>
<li class="chapter" data-level="21.12" data-path="classification-2.html"><a href="classification-2.html#lpm-1"><i class="fa fa-check"></i><b>21.12</b> LPM</a></li>
<li class="chapter" data-level="21.13" data-path="classification-2.html"><a href="classification-2.html#logistic-regression-1"><i class="fa fa-check"></i><b>21.13</b> Logistic Regression</a></li>
<li class="chapter" data-level="21.14" data-path="classification-2.html"><a href="classification-2.html#knn-1"><i class="fa fa-check"></i><b>21.14</b> kNN</a>
<ul>
<li class="chapter" data-level="21.14.1" data-path="classification-2.html"><a href="classification-2.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>21.14.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="21.14.2" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret-1"><i class="fa fa-check"></i><b>21.14.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>22</b> Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="time-series.html"><a href="time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="time-series.html"><a href="time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="time-series.html"><a href="time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="time-series.html"><a href="time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="time-series.html"><a href="time-series.html#stationarity"><i class="fa fa-check"></i><b>22.5</b> Stationarity</a></li>
<li class="chapter" data-level="22.6" data-path="time-series.html"><a href="time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.6</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="time-series.html"><a href="time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.7</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.8" data-path="time-series.html"><a href="time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.8</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.9" data-path="time-series.html"><a href="time-series.html#speed"><i class="fa fa-check"></i><b>22.9</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="forecast.html"><a href="forecast.html"><i class="fa fa-check"></i><b>23</b> Forecast</a>
<ul>
<li class="chapter" data-level="23.1" data-path="forecast.html"><a href="forecast.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="forecast.html"><a href="forecast.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="forecast.html"><a href="forecast.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="forecast.html"><a href="forecast.html#random-forest-1"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="forecast.html"><a href="forecast.html#univariate"><i class="fa fa-check"></i><b>23.5</b> Univariate</a></li>
<li class="chapter" data-level="23.6" data-path="forecast.html"><a href="forecast.html#multivariate"><i class="fa fa-check"></i><b>23.6</b> Multivariate</a></li>
<li class="chapter" data-level="23.7" data-path="forecast.html"><a href="forecast.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.7</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>24.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-margin"><i class="fa fa-check"></i><b>24.1.1</b> The Margin</a></li>
<li class="chapter" data-level="24.1.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-non-separable-case"><i class="fa fa-check"></i><b>24.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>24.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="24.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#application-with-svm"><i class="fa fa-check"></i><b>24.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#precision-matrix"><i class="fa fa-check"></i><b>27.4</b> Precision Matrix</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.5</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.6" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.6</b> Regularized Covariance Matrix</a></li>
<li class="chapter" data-level="27.7" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>27.7</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="27.8" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#high-dimensional-data"><i class="fa fa-check"></i><b>27.8</b> High-dimensional data</a></li>
<li class="chapter" data-level="27.9" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>27.9</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#rankr-approximations"><i class="fa fa-check"></i><b>28.4</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="28.5" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.5</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html"><i class="fa fa-check"></i><b>29</b> PCA (Principle Component Analysis)</a>
<ul>
<li class="chapter" data-level="29.1" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.1</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>30.1</b> Using bins</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>30.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="30.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>30.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="30.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="31.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>31.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="31.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>31.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="text-analysis.html"><a href="text-analysis.html"><i class="fa fa-check"></i><b>32</b> Text Analysis</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#other-nonparametric-estimation-methods-1"><i class="fa fa-check"></i><b>33.1</b> Other Nonparametric Estimation methods</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.2</b> Regression splines</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.3</b> MARS</a></li>
<li class="chapter" data-level="33.4" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.4</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuk/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-selection-and-sparsity" class="section level1 hasAnchor" number="20">
<h1><span class="header-section-number">Chapter 20</span> Model selection and Sparsity<a href="model-selection-and-sparsity.html#model-selection-and-sparsity" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="model-selection" class="section level2 hasAnchor" number="20.1">
<h2><span class="header-section-number">20.1</span> Model selection<a href="model-selection-and-sparsity.html#model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="dropping-a-variable-in-a-regression" class="section level2 hasAnchor" number="20.2">
<h2><span class="header-section-number">20.2</span> Dropping a variable in a regression<a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can assume that the outcome <span class="math inline">\(y_i\)</span> is determined by the following function:</p>
<p><span class="math display">\[
y_{i}=\beta_0+\beta_1 x_{i}+\varepsilon_{i}, ~~~~ i=1, \ldots, n
\]</span>
where <span class="math inline">\(\varepsilon_{i} \sim N\left(0, \sigma^{2}\right)\)</span>, <span class="math inline">\(\mathbf{Cov}\left(\varepsilon_{i}, \varepsilon_{j}\right)=0\)</span> for <span class="math inline">\(i\neq j.\)</span> Although unrealistic, for now we assume that <span class="math inline">\(x_i\)</span> is <strong>fixed</strong> (non-stochastic) for simplicity in notations. That means in each sample we have same <span class="math inline">\(x_i\)</span>. We can write this function as</p>
<p><span class="math display">\[
y_{i}=f(x_i)+\varepsilon_{i}, ~~~~ i=1, \ldots, n
\]</span></p>
<p>As usual, <span class="math inline">\(f(x_i)\)</span> is the deterministic part (DGM) and <span class="math inline">\(\varepsilon_i\)</span> is the random part in the function that together determine the value of <span class="math inline">\(y_i\)</span>. Again, we are living in two universes: the population and a sample. Since none of the elements in population is known to us, we can only <strong>assume</strong> what <span class="math inline">\(f(x)\)</span> would be. Based on a sample and the assumption about DGM, we choose an estimator of <span class="math inline">\(f(x)\)</span>,</p>
<p><span class="math display">\[
\hat{f}(x) = \hat{\beta}_0+\hat{\beta}_1 x_{i},
\]</span></p>
<p>which is BLUE of <span class="math inline">\(f(x)\)</span>, when it is estimated with OLS given the assumptions about <span class="math inline">\(\varepsilon_i\)</span> stated above. Since the task of this estimation is to satisfy the <strong>unbiasedness</strong> condition, i.e. <span class="math inline">\(\mathbf{E}[\hat{f}(x)]=f(x)\)</span>, it can be achieved only if <span class="math inline">\(\mathbf{E}[\hat{\beta_0}]=\beta_0\)</span> and <span class="math inline">\(\mathbf{E}[\hat{\beta_1}]=\beta_1\)</span>. At the end of this process, we can understand the effect of <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span>, signified by the unbiased slope coefficient <span class="math inline">\(\hat{\beta_1}\)</span>. This is not as an easy job as it sounds in this simple example. Finding an unbiased estimator of <span class="math inline">\(\beta\)</span> is the main challenge in the field of econometrics.</p>
<p>In <strong>prediction</strong>, on the other hand, our main task is <strong>not</strong> to find unbiased estimator of <span class="math inline">\(f(x)\)</span>. We just want to <strong>predict</strong> <span class="math inline">\(y_0\)</span> given <span class="math inline">\(x_0\)</span>. The subscript <span class="math inline">\(0\)</span> tells us that we want to predict <span class="math inline">\(y\)</span> for a specific value of <span class="math inline">\(x\)</span>. Hence we can write it as,</p>
<p><span class="math display">\[
y_{0}=\beta_0+\beta_1 x_{0}+\varepsilon_{0},
\]</span></p>
<p>In other words, when <span class="math inline">\(x_0=5\)</span>, for example, <span class="math inline">\(y_0\)</span> will be determined by <span class="math inline">\(f(x_0)\)</span> and the random error, <span class="math inline">\(\varepsilon_0\)</span>, which has the same variance, <span class="math inline">\(\sigma^2\)</span>, as <span class="math inline">\(\varepsilon_i\)</span>. Hence, when <span class="math inline">\(x_0=5\)</span>, although <span class="math inline">\(f(x_0)\)</span> is fixed, <span class="math inline">\(y_0\)</span> will vary because of its random part, <span class="math inline">\(\varepsilon_0\)</span>. This in an irreducible uncertainty in predicting <span class="math inline">\(y_0\)</span> given <span class="math inline">\(f(x_0)\)</span>. We do not know about the population. Therefore, we do not know what <span class="math inline">\(f(x_0)\)</span> is. We can have a sample from the population and build a model <span class="math inline">\(\hat{f}(x)\)</span> so that <span class="math inline">\(\hat{f}(x_0)\)</span> would be as close to <span class="math inline">\(f(x_0)\)</span> as possible. But this introduces another layer of uncertainty in predicting <span class="math inline">\(y_0\)</span>. Since each sample is random and different, <span class="math inline">\(\hat{f}(x_0)\)</span> will be a function of the sample: <span class="math inline">\(\hat{f}(x_0, S_m)\)</span>. Of course, we will have one sample in practice. However, if this variation is high, it would be highly likely that our predictions, <span class="math inline">\(\hat{f}(x_0, S_m)\)</span>, would be far off from <span class="math inline">\(f(x_0)\)</span>.</p>
<p>We can use an <strong>unbiased</strong> estimator for prediction, but as we have seen before, we may be able to improve MSPE if we allow some <strong>bias</strong> in <span class="math inline">\(\hat{f}(x)\)</span>. To see this potential trade-off, we look at the decomposition of MSPE with a simplified notation:</p>
<p><span class="math display">\[
\mathbf{MSPE}=\mathbf{E}\left[(y_0-\hat{f})^{2}\right]=\mathbf{E}\left[(f+\varepsilon-\hat{f})^{2}\right]
\]</span>
<span class="math display">\[
\mathbf{MSPE}=\mathbf{E}\left[(f+\varepsilon-\hat{f}+\mathbf{E}[\hat{f}]-\mathbf{E}[\hat{f}])^{2}\right]
\]</span></p>
<p>We have seen this before. Since we calculate MSPE for <span class="math inline">\(x_i = x_0\)</span>, we call it the conditional MSPE, which can be expressed as <span class="math inline">\(\mathbf{MSPE}=\mathbf{E}\left[(y_0-\hat{f})^{2}|x=x_0\right]\)</span>. We will see unconditional MSPE, which is the average of all possible data points later in last two sections. The simplification will follow the same steps, and we will have:</p>
<p><span class="math display">\[
\mathbf{MSPE}=(f-\mathbf{E}[\hat{f}])^{2}+\mathbf{E}\left[(\mathbf{E}[\hat{f}]-\hat{f})^{2}\right]+\mathbf{E}\left[\varepsilon^{2}\right]
\]</span></p>
<p>Let’s look at the first term first:</p>
<p><span class="math display">\[
\left(f-\mathbf{E}[\hat{f}]\right)^{2}=\left(\beta_0+\beta_1 x_{0}-\mathbf{E}[\hat{\beta}_0]-x_{0}\mathbf{E}[\hat{\beta}_1]\right)^2=\left((\beta_0-\mathbf{E}[\hat{\beta}_0])+x_{0}(\beta_1-\mathbf{E}[\hat{\beta}_1])\right)^2.
\]</span></p>
<p>Hence it shows the bias (squared) in parameters. The second term is the variance of <span class="math inline">\(\hat{f}(x)\)</span>:</p>
<p><span class="math display">\[
\mathbf{E}\left[(\mathbf{E}[\hat{f}]-\hat{f})^{2}\right]=\mathbf{Var}[\hat{f}(x)]=\mathbf{Var}[\hat{\beta}_0+\hat{\beta}_1 x_{0}]=\mathbf{Var}[\hat{\beta}_0]+x_{0}^2\mathbf{Var}[\hat{\beta}_1]+2x_{0}\mathbf{Cov}[\hat{\beta}_0,\hat{\beta}_1]
\]</span></p>
<p>As expected, the model’s variance is the sum of the variances of estimators and their covariance. Again, the variance can be thought of variation of <span class="math inline">\(\hat{f}(x)\)</span> from sample to sample.</p>
<p>With the irreducible prediction error <span class="math inline">\(\mathbf{E}[\varepsilon^{2}]=\sigma^2\)</span>,</p>
<p><span class="math display">\[
\mathbf{MSPE}=(\mathbf{bias})^{2}+\mathbf{Var}(\hat{f})+\sigma^2.
\]</span></p>
<p>Suppose that our OLS estimators are <strong>unbiased</strong> and that <span class="math inline">\(\mathbf{Cov}[\hat{\beta}_0,\hat{\beta}_1]=0\)</span>. In that case,</p>
<p><span class="math display">\[
\mathbf{MSPE}_{OLS}  =\mathbf{Var}(\hat{\beta}_{0})+x_{0}^2\mathbf{Var}(\hat{\beta}_{1})+\sigma^2
\]</span></p>
<p>Before going further, let’s summarize the meaning of this measure. The mean squared prediction error of unbiased <span class="math inline">\(\hat{f}(x_0)\)</span>, or how much <span class="math inline">\(\hat{f}(x_0)\)</span> deviates from <span class="math inline">\(y_0\)</span> is defined by two factors: First, <span class="math inline">\(y_0\)</span> itself varies around <span class="math inline">\(f(x_0)\)</span> by <span class="math inline">\(\sigma^2\)</span>. This is irreducible. Second, <span class="math inline">\(\hat{f}(x_0)\)</span> varies from sample to sample. The model’s variance is the sum of variations in estimated coefficients from sample to sample, which can be reducible.</p>
<p>Suppose that <span class="math inline">\(\hat{\beta}_{1}\)</span> has a large variance. Hence, we can ask what would happen if we dropped the variable:</p>
<p><span class="math display">\[
\mathbf{MSPE}_{Biased~OLS}  = \mathbf{Bias}^2+\mathbf{Var}(\hat{\beta}_{0})+\sigma^2
\]</span></p>
<p>When we take the difference:</p>
<p><span class="math display">\[
\mathbf{MSPE}_{OLS} -\mathbf{MSPE}_{Biased~OLS} =x_{0}^2\mathbf{Var}(\hat{\beta}_{1}) - \mathbf{Bias}^2
\]</span></p>
<p>This expression shows that dropping a variable would decrease the expected prediction error if:</p>
<p><span class="math display">\[
x_{0}^2\mathbf{Var}(\hat{\beta}_{1}) &gt; \mathbf{Bias}^2 ~~\Rightarrow~~  \mathbf{MSPE}_{Biased~OLS} &lt; \mathbf{MSPE}_{OLS}
\]</span></p>
<p><strong>This option, omitting a variable, is unthinkable if our task is to obtain an unbiased estimator</strong> of <span class="math inline">\({f}(x)\)</span>, but improves the prediction accuracy if the condition above is satisfied. Let’s expand this example into a two-variable case:</p>
<p><span class="math display">\[
y_{i}=\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}+\varepsilon_{i}, ~~~~ i=1, \ldots, n.
\]</span></p>
<p>Thus, the bias term becomes</p>
<p><span class="math display">\[
\left(f-\mathbf{E}[\hat{f}]\right)^{2}=\left((\beta_0-\mathbf{E}[\hat{\beta}_0])+x_{10}(\beta_1-\mathbf{E}[\hat{\beta}_1])+x_{20}(\beta_2-\mathbf{E}[\hat{\beta}_2])\right)^2.
\]</span></p>
<p>And let’s assume that <span class="math inline">\(\mathbf{Cov}[\hat{\beta}_0,\hat{\beta}_1]=\mathbf{Cov}[\hat{\beta}_0,\hat{\beta}_2]=0\)</span>, but <span class="math inline">\(\mathbf{Cov}[\hat{\beta}_1,\hat{\beta}_2] \neq 0\)</span>. Hence, the variance of <span class="math inline">\(\hat{f}(x)\)</span>:</p>
<p><span class="math display">\[
\mathbf{Var}[\hat{f}(x)]=\mathbf{Var}[\hat{\beta}_0+\hat{\beta}_1 x_{10}+\hat{\beta}_2 x_{20}]=\mathbf{Var}[\hat{\beta}_0]+x_{10}^2\mathbf{Var}[\hat{\beta}_1]+x_{20}^2\mathbf{Var}[\hat{\beta}_2]+\\2x_{10}x_{20}\mathbf{Cov}[\hat{\beta}_1,\hat{\beta}_2].
\]</span></p>
<p>This two-variable example shows that as the number of variables rises, the covariance between variables inflates the model’s variance further. This fact captured by Variance Inflation Factor (<a href="https://en.wikipedia.org/wiki/Variance_inflation_factor">VIF</a>) in econometrics is a key point in high-dimensional models for two reasons: First, dropping a variable highly correlated with other variables would reduce the model’s variance substantially. Second, a highly correlated variable also has limited new information among other variables. Hence dropping a highly correlated variable (with a high variance) would have a less significant effect on the prediction accuracy while reducing the model’s variance substantially.</p>
<p>Suppose that we want to predict <span class="math inline">\(y_0\)</span> for <span class="math inline">\(\left[x_{10},~ x_{20}\right]\)</span> and <span class="math inline">\(\mathbf{Var}[\hat{\beta}_2] \approx 10~\text{x}~\mathbf{Var}[\hat{\beta}_1]\)</span>. Hence, we consider dropping <span class="math inline">\(x_2\)</span>. To evaluate the effect of this decision on MSPE, we take the difference between two MSPE’s:</p>
<p><span class="math display">\[
\mathbf{MSPE}_{OLS} -\mathbf{MSPE}_{Biased~OLS} =x_{20}^2\mathbf{Var}(\hat{\beta}_{2}) + 2x_{10}x_{20}\mathbf{Cov}[\hat{\beta}_1,\hat{\beta}_2] - \mathbf{Bias}^2
\]</span></p>
<p>Thus, dropping <span class="math inline">\(x_2\)</span> would decrease the prediction error if</p>
<p><span class="math display">\[
x_{20}^2\mathbf{Var}(\hat{\beta}_{2}) + 2x_{10}x_{20}\mathbf{Cov}[\hat{\beta}_1,\hat{\beta}_2]&gt; \mathbf{Bias}^2 ~~\Rightarrow~~  \mathbf{MSPE}_{Biased~OLS} &lt; \mathbf{MSPE}_{OLS}
\]</span></p>
<p>We know from Elementary Econometrics that <span class="math inline">\(\mathbf{Var}(\hat{\beta}_j)\)</span> increases by <span class="math inline">\(\sigma^2\)</span>, decreases by the <span class="math inline">\(\mathbf{Var}(x_j)\)</span>, and rises by the correlation between <span class="math inline">\(x_j\)</span> and other <span class="math inline">\(x\)</span>’s. Let’s look at <span class="math inline">\(\mathbf{Var}(\hat{\beta}_j)\)</span> closer:</p>
<p>{% raw %}
<span class="math display">\[
\mathbf{Var}(\hat{\beta}_{j}) = \frac{\sigma^{2}}{\mathbf{Var}(x_{j})} \cdot \frac{1}{1-R_{j}^{2}}
\]</span>
{% endraw %}</p>
<p>where <span class="math inline">\(R_j^2\)</span> is <span class="math inline">\(R^2\)</span> in the regression on <span class="math inline">\(x_j\)</span> on the remaining <span class="math inline">\((k-2)\)</span> regressors (<span class="math inline">\(x\)</span>’s). The second term is called the variance-inflating factor (VIF). As usual, a higher variability in a particular <span class="math inline">\(x\)</span> leads to proportionately less variance in the corresponding coefficient estimate. Note that, however, as <span class="math inline">\(R_j^2\)</span> get closer to one, that is, as the correlation between <span class="math inline">\(x_j\)</span> with other regressors approaches to unity, <span class="math inline">\(\mathbf{Var}(\hat{\beta}_j)\)</span> goes to infinity.</p>
<p>The variance of <span class="math inline">\(\varepsilon_i\)</span>, <span class="math inline">\(\sigma^2\)</span>, indicates how much <span class="math inline">\(y_i\)</span>’s deviate from the <span class="math inline">\(f(x)\)</span>. Since <span class="math inline">\(\sigma^2\)</span> is typically unknown, we estimate it from <strong>the sample</strong> as</p>
<p><span class="math display">\[
\widehat{\sigma}^{2}=\frac{1}{(n-k+1)} \sum_{i=1}^{n}\left(y_{i}-\hat{f}(x)\right)^{2}
\]</span></p>
<p>Remember that we have multiple samples, hence if our estimator is <strong>unbiased</strong>, we can prove that <span class="math inline">\(\mathbf{E}(\hat{\sigma}^2)=\sigma^2\)</span>. The proof is not important now. However, <span class="math inline">\(\mathbf{Var}(\hat{\beta}_j)\)</span> becomes</p>
<p><span class="math display">\[
\mathbf{Var}\left(\hat{\beta}_{j}\right)=\frac{\sum_{i=1}^{n}\left(y_{i}-\hat{f}(x)\right)^{2}}{(n-k+1)\mathbf{Var}\left(x_{j}\right)} \cdot \frac{1}{1-R_{j}^{2}},
\]</span></p>
<p>It is clear now that a greater sample size, <span class="math inline">\(n\)</span>, results in a proportionately less variance in the coefficient estimates. On the other hand, as the number of regressors, <span class="math inline">\(k\)</span>, goes up, the variance goes up. In large <span class="math inline">\(n\)</span> and small <span class="math inline">\(k\)</span>, the trade-off by dropping a variable would be insignificant, but as <span class="math inline">\(k/n\)</span> rises, the trade-off becomes more important.</p>
<p>Let’s have a simulation example to conclude this section. Here are the steps for our simulation:</p>
<ol style="list-style-type: decimal">
<li>There is a random variable, <span class="math inline">\(y\)</span>, that we want to predict.</li>
<li><span class="math inline">\(y_{i}=f(x_i)+\varepsilon_{i}\)</span>.</li>
<li>DGM is <span class="math inline">\(f(x_i)=\beta_0+\beta_1 x_{1i}+\beta_2 x_{2i}\)</span></li>
<li><span class="math inline">\(\varepsilon_{i} \sim N(0, \sigma^2)\)</span>.</li>
<li>The steps above define the <strong>population</strong>. We will withdraw <span class="math inline">\(M\)</span> number of <strong>samples</strong> from this population.</li>
<li>Using each sample (<span class="math inline">\(S_m\)</span>, where <span class="math inline">\(m=1, \ldots, M\)</span>), we will estimate two models: <strong>unbiased</strong> <span class="math inline">\(\hat{f}(x)_{OLS}\)</span> and <strong>biased</strong> <span class="math inline">\(\hat{f}(x)_{Biased~OLS}\)</span></li>
<li>Using these models we will predict <span class="math inline">\(y&#39;_i\)</span> from a different sample (<span class="math inline">\(T\)</span>) drawn from the same population. We can call it the “unseen” dataset or the “test” dataset, which contains out-of-sample data points, <span class="math inline">\((y&#39;_i, x_{1i}, x_{2i})\)</span>., where <span class="math inline">\(i=1, \ldots, n\)</span>.</li>
</ol>
<p>Before we start, we need to be clear how we define MSPE in our simulation. Since we will predict every <span class="math inline">\(y&#39;_i\)</span> with corresponding predictors <span class="math inline">\((x_{1i}, x_{2i})\)</span> in test set <span class="math inline">\(T\)</span> by each <span class="math inline">\(\hat{f}(x_{1i}, x_{2i}, S_m))\)</span> estimated by each sample, we calculate the following <strong>unconditional</strong> MSPE:</p>
<p><span class="math display">\[
\mathbf{MSPE}=\mathbf{E}_{S}\mathbf{E}_{S_{m}}\left[(y&#39;_i-\hat{f}(x_{1i}, x_{2i}, S_m))^{2}\right]=\\\mathbf{E}_S\left[\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}^{\prime}-\hat{f}(x_{1i}, x_{2i}, S_m)\right)^{2}\right],~~~~~~~~~ m=1, \ldots, M
\]</span></p>
<p>We first calculate MSPE for all data points in the test set using <span class="math inline">\(\hat{f}(x_{1T}, x_{2T}, S_m)\)</span>, and then take the average of <span class="math inline">\(M\)</span> samples.</p>
<p>Below, we will show the sensitivity of trade-off by the size of irreducible error. The simulation below plots <span class="math inline">\(diff= \mathbf{MSPE}_{OLS}-\mathbf{MSPE}_{Biased~OLS}\)</span> against <span class="math inline">\(\sigma\)</span>.</p>
<div class="sourceCode" id="cb539"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb539-1"><a href="model-selection-and-sparsity.html#cb539-1" tabindex="-1"></a><span class="co"># Function for X - fixed at repeated samples</span></span>
<span id="cb539-2"><a href="model-selection-and-sparsity.html#cb539-2" tabindex="-1"></a><span class="co"># Argument l is used for correlation and with 0.01</span></span>
<span id="cb539-3"><a href="model-selection-and-sparsity.html#cb539-3" tabindex="-1"></a><span class="co"># Correlation between x_1 and x_2 is 0.7494</span></span>
<span id="cb539-4"><a href="model-selection-and-sparsity.html#cb539-4" tabindex="-1"></a>xfunc <span class="ot">&lt;-</span> <span class="cf">function</span>(n, l){</span>
<span id="cb539-5"><a href="model-selection-and-sparsity.html#cb539-5" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb539-6"><a href="model-selection-and-sparsity.html#cb539-6" tabindex="-1"></a>  x_1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">25</span>) </span>
<span id="cb539-7"><a href="model-selection-and-sparsity.html#cb539-7" tabindex="-1"></a>  x_2 <span class="ot">&lt;-</span> l<span class="sc">*</span>x_1<span class="sc">+</span><span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="fl">0.2</span>)</span>
<span id="cb539-8"><a href="model-selection-and-sparsity.html#cb539-8" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;x_1&quot;</span> <span class="ot">=</span> x_1, <span class="st">&quot;x_2&quot;</span> <span class="ot">=</span> x_2)</span>
<span id="cb539-9"><a href="model-selection-and-sparsity.html#cb539-9" tabindex="-1"></a>  <span class="fu">return</span>(X)</span>
<span id="cb539-10"><a href="model-selection-and-sparsity.html#cb539-10" tabindex="-1"></a>}</span>
<span id="cb539-11"><a href="model-selection-and-sparsity.html#cb539-11" tabindex="-1"></a></span>
<span id="cb539-12"><a href="model-selection-and-sparsity.html#cb539-12" tabindex="-1"></a><span class="co"># Note that we can model dependencies with copulas in R</span></span>
<span id="cb539-13"><a href="model-selection-and-sparsity.html#cb539-13" tabindex="-1"></a><span class="co"># More specifically by using mvrnorn() function.  However, here</span></span>
<span id="cb539-14"><a href="model-selection-and-sparsity.html#cb539-14" tabindex="-1"></a><span class="co"># We want one variable with a higher variance. which is easier to do manaully</span></span>
<span id="cb539-15"><a href="model-selection-and-sparsity.html#cb539-15" tabindex="-1"></a><span class="co"># More: https://datascienceplus.com/modelling-dependence-with-copulas/ </span></span>
<span id="cb539-16"><a href="model-selection-and-sparsity.html#cb539-16" tabindex="-1"></a></span>
<span id="cb539-17"><a href="model-selection-and-sparsity.html#cb539-17" tabindex="-1"></a><span class="co"># Function for test set - with different X&#39;s but same dist.</span></span>
<span id="cb539-18"><a href="model-selection-and-sparsity.html#cb539-18" tabindex="-1"></a>unseen <span class="ot">&lt;-</span> <span class="cf">function</span>(n, sigma, l){</span>
<span id="cb539-19"><a href="model-selection-and-sparsity.html#cb539-19" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb539-20"><a href="model-selection-and-sparsity.html#cb539-20" tabindex="-1"></a>  x_11 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">25</span>) </span>
<span id="cb539-21"><a href="model-selection-and-sparsity.html#cb539-21" tabindex="-1"></a>  x_22 <span class="ot">&lt;-</span> l<span class="sc">*</span>x_11<span class="sc">+</span><span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="fl">0.2</span>)</span>
<span id="cb539-22"><a href="model-selection-and-sparsity.html#cb539-22" tabindex="-1"></a>  f <span class="ot">&lt;-</span> <span class="dv">0</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x_11 <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x_22   </span>
<span id="cb539-23"><a href="model-selection-and-sparsity.html#cb539-23" tabindex="-1"></a>  y_u <span class="ot">&lt;-</span> f <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma) </span>
<span id="cb539-24"><a href="model-selection-and-sparsity.html#cb539-24" tabindex="-1"></a>  un <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;y&quot;</span> <span class="ot">=</span> y_u, <span class="st">&quot;x_1&quot;</span> <span class="ot">=</span> x_11, <span class="st">&quot;x_2&quot;</span> <span class="ot">=</span> x_22)</span>
<span id="cb539-25"><a href="model-selection-and-sparsity.html#cb539-25" tabindex="-1"></a>  <span class="fu">return</span>(un)</span>
<span id="cb539-26"><a href="model-selection-and-sparsity.html#cb539-26" tabindex="-1"></a>}</span>
<span id="cb539-27"><a href="model-selection-and-sparsity.html#cb539-27" tabindex="-1"></a></span>
<span id="cb539-28"><a href="model-selection-and-sparsity.html#cb539-28" tabindex="-1"></a><span class="co"># Function for simulation (M - number of samples)</span></span>
<span id="cb539-29"><a href="model-selection-and-sparsity.html#cb539-29" tabindex="-1"></a>sim <span class="ot">&lt;-</span> <span class="cf">function</span>(M, n, sigma, l){</span>
<span id="cb539-30"><a href="model-selection-and-sparsity.html#cb539-30" tabindex="-1"></a>  </span>
<span id="cb539-31"><a href="model-selection-and-sparsity.html#cb539-31" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">xfunc</span>(n, l) <span class="co"># Repeated X&#39;s in each sample</span></span>
<span id="cb539-32"><a href="model-selection-and-sparsity.html#cb539-32" tabindex="-1"></a>  un <span class="ot">&lt;-</span> <span class="fu">unseen</span>(n, sigma, l) <span class="co"># Out-of sample (y, x_1, x_2)</span></span>
<span id="cb539-33"><a href="model-selection-and-sparsity.html#cb539-33" tabindex="-1"></a></span>
<span id="cb539-34"><a href="model-selection-and-sparsity.html#cb539-34" tabindex="-1"></a>  <span class="co"># containers</span></span>
<span id="cb539-35"><a href="model-selection-and-sparsity.html#cb539-35" tabindex="-1"></a>  MSPE_ols <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, M)</span>
<span id="cb539-36"><a href="model-selection-and-sparsity.html#cb539-36" tabindex="-1"></a>  MSPE_b <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, M)</span>
<span id="cb539-37"><a href="model-selection-and-sparsity.html#cb539-37" tabindex="-1"></a>  coeff <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, M, <span class="dv">3</span>)</span>
<span id="cb539-38"><a href="model-selection-and-sparsity.html#cb539-38" tabindex="-1"></a>  coeff_b <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, M, <span class="dv">2</span>)</span>
<span id="cb539-39"><a href="model-selection-and-sparsity.html#cb539-39" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, M, n)</span>
<span id="cb539-40"><a href="model-selection-and-sparsity.html#cb539-40" tabindex="-1"></a>  yhat_b <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, M, n)</span>
<span id="cb539-41"><a href="model-selection-and-sparsity.html#cb539-41" tabindex="-1"></a>  </span>
<span id="cb539-42"><a href="model-selection-and-sparsity.html#cb539-42" tabindex="-1"></a>  <span class="co"># loop for samples</span></span>
<span id="cb539-43"><a href="model-selection-and-sparsity.html#cb539-43" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>M) {</span>
<span id="cb539-44"><a href="model-selection-and-sparsity.html#cb539-44" tabindex="-1"></a>    f <span class="ot">&lt;-</span> <span class="dv">0</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>X<span class="sc">$</span>x_1 <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>X<span class="sc">$</span>x_2   <span class="co"># DGM</span></span>
<span id="cb539-45"><a href="model-selection-and-sparsity.html#cb539-45" tabindex="-1"></a>    y <span class="ot">&lt;-</span> f <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb539-46"><a href="model-selection-and-sparsity.html#cb539-46" tabindex="-1"></a>    samp <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;y&quot;</span> <span class="ot">=</span> y, X)</span>
<span id="cb539-47"><a href="model-selection-and-sparsity.html#cb539-47" tabindex="-1"></a>    ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>., samp) <span class="co"># Unbaised OLS</span></span>
<span id="cb539-48"><a href="model-selection-and-sparsity.html#cb539-48" tabindex="-1"></a>    ols_b <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span>x_1, samp) <span class="co">#Biased OLS</span></span>
<span id="cb539-49"><a href="model-selection-and-sparsity.html#cb539-49" tabindex="-1"></a>    coeff[i,] <span class="ot">&lt;-</span> ols<span class="sc">$</span>coefficients</span>
<span id="cb539-50"><a href="model-selection-and-sparsity.html#cb539-50" tabindex="-1"></a>    coeff_b[i,] <span class="ot">&lt;-</span> ols_b<span class="sc">$</span>coefficients</span>
<span id="cb539-51"><a href="model-selection-and-sparsity.html#cb539-51" tabindex="-1"></a>    yhat[i,] <span class="ot">&lt;-</span> <span class="fu">predict</span>(ols, un)</span>
<span id="cb539-52"><a href="model-selection-and-sparsity.html#cb539-52" tabindex="-1"></a>    yhat_b[i,] <span class="ot">&lt;-</span> <span class="fu">predict</span>(ols_b, un)</span>
<span id="cb539-53"><a href="model-selection-and-sparsity.html#cb539-53" tabindex="-1"></a>    MSPE_ols[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((un<span class="sc">$</span>y<span class="sc">-</span>yhat[i])<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb539-54"><a href="model-selection-and-sparsity.html#cb539-54" tabindex="-1"></a>    MSPE_b[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((un<span class="sc">$</span>y<span class="sc">-</span>yhat_b[i])<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb539-55"><a href="model-selection-and-sparsity.html#cb539-55" tabindex="-1"></a>  }</span>
<span id="cb539-56"><a href="model-selection-and-sparsity.html#cb539-56" tabindex="-1"></a>  d <span class="ot">=</span> <span class="fu">mean</span>(MSPE_ols)<span class="sc">-</span><span class="fu">mean</span>(MSPE_b)</span>
<span id="cb539-57"><a href="model-selection-and-sparsity.html#cb539-57" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">list</span>(d, MSPE_b, MSPE_ols, coeff, coeff_b, yhat, yhat_b)</span>
<span id="cb539-58"><a href="model-selection-and-sparsity.html#cb539-58" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb539-59"><a href="model-selection-and-sparsity.html#cb539-59" tabindex="-1"></a>}</span>
<span id="cb539-60"><a href="model-selection-and-sparsity.html#cb539-60" tabindex="-1"></a></span>
<span id="cb539-61"><a href="model-selection-and-sparsity.html#cb539-61" tabindex="-1"></a><span class="co"># Sensitivity of (MSPE_biased)-(MSPE_ols)</span></span>
<span id="cb539-62"><a href="model-selection-and-sparsity.html#cb539-62" tabindex="-1"></a><span class="co"># different sigma for the irreducible error</span></span>
<span id="cb539-63"><a href="model-selection-and-sparsity.html#cb539-63" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">1</span>, <span class="dv">20</span>, <span class="dv">1</span>)</span>
<span id="cb539-64"><a href="model-selection-and-sparsity.html#cb539-64" tabindex="-1"></a>MSPE_dif <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(sigma))</span>
<span id="cb539-65"><a href="model-selection-and-sparsity.html#cb539-65" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span> <span class="fu">length</span>(sigma)) {</span>
<span id="cb539-66"><a href="model-selection-and-sparsity.html#cb539-66" tabindex="-1"></a>  MSPE_dif[i] <span class="ot">&lt;-</span> <span class="fu">sim</span>(<span class="dv">1000</span>, <span class="dv">100</span>, sigma[i], <span class="fl">0.01</span>)[[<span class="dv">1</span>]]</span>
<span id="cb539-67"><a href="model-selection-and-sparsity.html#cb539-67" tabindex="-1"></a>}</span>
<span id="cb539-68"><a href="model-selection-and-sparsity.html#cb539-68" tabindex="-1"></a></span>
<span id="cb539-69"><a href="model-selection-and-sparsity.html#cb539-69" tabindex="-1"></a><span class="fu">plot</span>(sigma, MSPE_dif, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Difference in MSPE vs. sigma&quot;</span>,</span>
<span id="cb539-70"><a href="model-selection-and-sparsity.html#cb539-70" tabindex="-1"></a>     <span class="at">cex =</span> <span class="fl">0.9</span>, <span class="at">cex.main=</span> <span class="fl">0.8</span>, <span class="at">cex.lab =</span> <span class="fl">0.7</span>, <span class="at">cex.axis =</span> <span class="fl">0.8</span>)</span></code></pre></div>
<p><img src="20-ModelSelection_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>The simulation shows that the <strong>biased</strong> <span class="math inline">\(\hat{f}(x)\)</span> is getting a better precision in prediction as the “noise” in the data gets higher. The reason can be understood if we look at <span class="math inline">\(\mathbf{Var}(\hat{\beta}_{2}) + 2\mathbf{Cov}[\hat{\beta}_1,\hat{\beta}_2]\)</span> closer:</p>
<p><span class="math display">\[
\mathbf{Var}\left(\hat{\beta}_{2}\right)=\frac{\sigma^{2}}{\mathbf{Var}\left(x_{2}\right)} \cdot \frac{1}{1-r_{1,2}^{2}},
\]</span></p>
<p>where <span class="math inline">\(r_{1,2}^{2}\)</span> is the coefficient of correlation between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. And,</p>
<p><span class="math display">\[
\mathbf{Cov}\left(\hat{\beta}_{1},\hat{\beta}_{2}\right)=\frac{-r_{1,2}^{2}\sigma^{2}}{\sqrt{\mathbf{Var}\left(x_{1}\right)\mathbf{Var}\left(x_{2}\right)}} \cdot \frac{1}{1-r_{1,2}^{2}},
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{MSPE}_{O L S}-&amp; \mathbf{MSPE}_{Biased~OLS}=\mathbf{V} \mathbf{a r}\left(\hat{\beta}_{2}\right)+2 \mathbf{C} \mathbf{o} \mathbf{v}\left[\hat{\beta}_{1}, \hat{\beta}_{2}\right]-\mathbf{Bias}^{2}=\\
&amp; \frac{\sigma^{2}}{1-r_{1,2}^{2}}\left(\frac{1}{\mathbf{V a r}\left(x_{2}\right)}+\frac{-2 r_{1,2}^{2}}{\sqrt{\mathbf{V a r}\left(x_{1}\right) \mathbf{V a r}\left(x_{2}\right)}}\right)-\mathbf{Bias}^{2}
\end{aligned}
\]</span></p>
<p>Given the bias due to the omitted variable <span class="math inline">\(x_2\)</span>, this expression shows the difference as a function of <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(r_{1,2}^{2}\)</span> and explains why the biased-OLS estimator have increasingly better predictions.</p>
<p>As a final experiment, let’s have the same simulation that shows the relationship between correlation and trade-off. To create different correlations between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, we use the <code>xfunc()</code> we created earlier. The argument <span class="math inline">\(l\)</span> is used to change the the correlation and can be seen below. In our case, when <span class="math inline">\(l=0.01\)</span> <span class="math inline">\(r_{1,2}^{2}=0.7494\)</span>.</p>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="model-selection-and-sparsity.html#cb540-1" tabindex="-1"></a><span class="co"># Function for X for correlation</span></span>
<span id="cb540-2"><a href="model-selection-and-sparsity.html#cb540-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">xfunc</span>(<span class="dv">100</span>, <span class="fl">0.001</span>)</span>
<span id="cb540-3"><a href="model-selection-and-sparsity.html#cb540-3" tabindex="-1"></a><span class="fu">cor</span>(X)</span></code></pre></div>
<pre><code>##            x_1        x_2
## x_1 1.00000000 0.06838898
## x_2 0.06838898 1.00000000</code></pre>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="model-selection-and-sparsity.html#cb542-1" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">xfunc</span>(<span class="dv">100</span>, <span class="fl">0.0011</span>)</span>
<span id="cb542-2"><a href="model-selection-and-sparsity.html#cb542-2" tabindex="-1"></a><span class="fu">cor</span>(X)</span></code></pre></div>
<pre><code>##            x_1        x_2
## x_1 1.00000000 0.08010547
## x_2 0.08010547 1.00000000</code></pre>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="model-selection-and-sparsity.html#cb544-1" tabindex="-1"></a><span class="co"># We use this in our simulation</span></span>
<span id="cb544-2"><a href="model-selection-and-sparsity.html#cb544-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">xfunc</span>(<span class="dv">100</span>, <span class="fl">0.01</span>)</span>
<span id="cb544-3"><a href="model-selection-and-sparsity.html#cb544-3" tabindex="-1"></a><span class="fu">cor</span>(X)</span></code></pre></div>
<pre><code>##           x_1       x_2
## x_1 1.0000000 0.7494025
## x_2 0.7494025 1.0000000</code></pre>
<p>Now the simulation with different levels of correlation:</p>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="model-selection-and-sparsity.html#cb546-1" tabindex="-1"></a><span class="co"># Sensitivity of (MSPE_biased)-(MSPE_ols)</span></span>
<span id="cb546-2"><a href="model-selection-and-sparsity.html#cb546-2" tabindex="-1"></a><span class="co"># different levels of correlation when sigma^2=7</span></span>
<span id="cb546-3"><a href="model-selection-and-sparsity.html#cb546-3" tabindex="-1"></a>l <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.001</span>, <span class="fl">0.011</span>, <span class="fl">0.0001</span>)</span>
<span id="cb546-4"><a href="model-selection-and-sparsity.html#cb546-4" tabindex="-1"></a>MSPE_dif <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(l))</span>
<span id="cb546-5"><a href="model-selection-and-sparsity.html#cb546-5" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span> <span class="fu">length</span>(l)) {</span>
<span id="cb546-6"><a href="model-selection-and-sparsity.html#cb546-6" tabindex="-1"></a>  MSPE_dif[i] <span class="ot">&lt;-</span> <span class="fu">sim</span>(<span class="dv">1000</span>, <span class="dv">100</span>, <span class="dv">7</span>, l[i])[[<span class="dv">1</span>]]</span>
<span id="cb546-7"><a href="model-selection-and-sparsity.html#cb546-7" tabindex="-1"></a>}</span>
<span id="cb546-8"><a href="model-selection-and-sparsity.html#cb546-8" tabindex="-1"></a></span>
<span id="cb546-9"><a href="model-selection-and-sparsity.html#cb546-9" tabindex="-1"></a><span class="fu">plot</span>(l, MSPE_dif, <span class="at">col=</span><span class="st">&quot;red&quot;</span>, <span class="at">main=</span> <span class="st">&quot;Difference in MSPE vs Correlation b/w X&#39;s&quot;</span>,</span>
<span id="cb546-10"><a href="model-selection-and-sparsity.html#cb546-10" tabindex="-1"></a>     <span class="at">cex=</span><span class="fl">0.9</span>, <span class="at">cex.main=</span> <span class="fl">0.8</span>, <span class="at">cex.lab =</span> <span class="fl">0.7</span>, <span class="at">cex.axis =</span> <span class="fl">0.8</span>)</span></code></pre></div>
<p><img src="20-ModelSelection_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>As the correlation between <span class="math inline">\(x\)</span>’s goes up, <span class="math inline">\(\mathbf{MSPE}_{OLS}-\mathbf{MSPE}_{Biased~OLS}\)</span> rises. Later we will have a high-dimensional dataset (large <span class="math inline">\(k\)</span>) to show the importance of correlation. We will leave the calculation of bias and how the sample size affects trade-off to labs.</p>
</div>
<div id="out-sample-prediction-accuracy" class="section level2 hasAnchor" number="20.3">
<h2><span class="header-section-number">20.3</span> out-sample prediction accuracy<a href="model-selection-and-sparsity.html#out-sample-prediction-accuracy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As a side note: when we compare the models in terms their out-sample prediction accuracy, we usually use the root MSPE (RMSPE), which gives us the prediction error in original units.</p>
<p>When we calculate empirical in-sample MSPE with one sample, we can asses its out-of-sample prediction performance by the Mallows <span class="math inline">\(C_P\)</span> statistics, which just substitutes the feasible estimator of <span class="math inline">\(\sigma^2\)</span> into the overfitting penalty. That is, for a linear model with <span class="math inline">\(p + 1\)</span> coefficients fit by OLS,</p>
<p><span class="math display">\[
C_{p}=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{f}(x_i)\right)^{2}+\frac{2 \widehat{\sigma}^{2}}{n}(p+1),
\]</span></p>
<p>which becomes a good proxy for the our-of-sample error. That is, a small value of <span class="math inline">\(C_p\)</span> means that the model is relatively precise. For comparing models, we really care about differences in empirical out-sample MSPE’s:</p>
<p><span class="math display">\[
\Delta C_{p}=\mathbf{MSPE}_{1}-\mathbf{MSPE}_{2}+\frac{2}{n} \widehat{\sigma}^{2}\left(p_{1}-p_{2}\right),
\]</span></p>
<p>where we use <span class="math inline">\(\hat{\sigma}^2\)</span> from the largest model.</p>
<p><a href="https://online.stat.psu.edu/stat462/node/197/" class="uri">https://online.stat.psu.edu/stat462/node/197/</a></p>
<p>How are we going to find the best predictor? In addition to <span class="math inline">\(C_p\)</span>, we can also use <strong>Akaike Information Criterion (AIC)</strong>, which also has the form of “in-sample performance plus penalty”. AIC can be applied whenever we have a likelihood function, whereas <span class="math inline">\(C_p\)</span> can be used when we use squared errors. We will see later AIC and BIC (Bayesian Information Criteria) in this book. With these measures, we can indirectly estimate the test (out-of-sample) error by making an adjustment to the training (in-sample) error to account for the bias due to overfitting. Therefore, these methods are <strong>ex-post</strong> tools to <strong>penalize</strong> the overfitting.</p>
<p>On the other hand, we can directly estimate the test error (out-sample) and choose the model that minimizes it. We can do it by directly validating the model using a cross-validation approach. Therefore, cross-validation methods provide <strong>ex-ante</strong> penalization for overfitting and are the main tools in selecting predictive models in machine learning applications as they have almost no assumptions.</p>
</div>
<div id="sparsity-1" class="section level2 hasAnchor" number="20.4">
<h2><span class="header-section-number">20.4</span> Sparsity<a href="model-selection-and-sparsity.html#sparsity-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="heterogeneous-treatment-effects.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="classification-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuk/machinemetrics/edit/master/20-ModelSelection.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
