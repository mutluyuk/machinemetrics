[["index.html", "MachineMetrics for Economists, Social and Health Scientists Preface Why this book is different? Structure of Manuscript: Who Can Use This Book? Acknowledgements License", " MachineMetrics for Economists, Social and Health Scientists Yigit Aydede and Mutlu Yuksel This version: 2023-10-06 Preface Why this book is different? The uniqueness of this book lies in its approach to presenting topics and examples relevant to the fields of economics, social sciences, and related disciplines. Key features that set this book apart include: Accessible language and concepts: Throughout the book, we consciously avoid using excessively technical terminology or concepts exclusive to computer science. Instead, we strive to present explanations in a clear and straightforward manner, employing terms and ideas that economists and social scientists are already familiar with. This approach ensures that the content is both accessible and relevant to the target audience Practical application: The topics covered in this book are illustrated using simulations or real-world data sets, enabling readers to connect the theoretical concepts with practical examples. Abundance of examples: We provide numerous examples throughout the book, ensuring that readers can thoroughly comprehend the concepts and methods presented. Use of R programming language: Rather than depending on specialized packages, we emphasize the use of the core R language for all demonstrations and examples. This method allows readers to develop a more profound comprehension of the techniques and fosters the acquisition of crucial programming abilities. Additionally, we incorporate commonly used R packages for data analytics in specific sections to acquaint our readers with these tools. Tailored content: The book is specifically designed for researchers in economics, social sciences, and related fields, addressing topics and methods that are most relevant to their work. Cutting-edge research: In nearly all chapters, we include sections that showcase the most recent research papers in economics and social sciences. This feature keeps readers informed about the latest advancements in their respective fields, and it underscores the practical relevance of the methods discussed in the book. By incorporating these features, we have created a resource that not only teaches essential concepts and techniques, but also demonstrates their real-world applicability and value for researchers in economics, social sciences, and related disciplines. Structure of Manuscript: In this book, we delve into an extensive range of subjects aimed at equipping readers with a comprehensive understanding of various aspects of data analysis, modeling, and machine learning. We commence with an introduction that highlights the differences between prediction and estimation, the relevance of the discussed topics in economics, health, and social sciences, the interpretation of concepts and terminology, and a comparison between machine learning and traditional econometric approaches. The initial section progresses to cover comprison of statistical and machine learning models, simulations, and discussions on prediction and estimation, correlation and counterfactual causal models. To grasp the key methodology sections, we first explore the concept of learning, error types, bias-variance trade-offs, and overfitting in seperate chapters. We explain both fundamental parametric and nonparametric estimation techniques in order to familiarize our readers with these concepts. These initial chapters provide a seamless transition from inferential statistics and the “parametric world” to predictive models by including a section on nonparametric methods. In many cases, even at the graduate level, nonparametric methods are not commonly taught, as they are less frequently employed in inferential statistics. However, nonparametric econometrics serves as a bridge between the two domains of data modeling and algorithmic modeling, given that machine learning is essentially an extension of nonparametric econometrics. Subsequently, we present methods for hyperparameter tuning and a range of optimization algorithms, such as cross-validation and gradient descent. We present these topics using concepts that are well-known to economists, social scientists, and health researchers. Throughout the shrinkage sections, we discuss ridge, lasso, and elastic net methods. Subsequently, readers will encounter topics such as regression trees and ensemble learning techniques like bagging, boosting, and random forest models. We also delve into causality and machine learning, examining the implementation of counterfactual causal methods in health, economics, and social sciences, in addition to model selection, sparsity, and the application of machine learning tools. The diverse topics covered in these chapters include random experiment, instrumental variables, difference-in-differences, regression discontinuity, synthetic control, double/debiased lasso methods, and recently developed heterogeneous treatment effects, such as causal tree and causal forest. We dedicate a comprehensive and standalone chapter to a thorough exploration of classification. In this chapter, we cover a wide range of topics, including: Introduction to Classification, Linear Probability and Logistic Regression, Goodness of Fit, Confusion Table, Performance Measures, Receiver Operating Characteristic (ROC), Area Under the Curve (AUC), and real-world Applications using the Caret package. Following this, we delve into time series analysis combined with machine learning approaches. The topics covered in time series and forecast chapters include: ARIMA Models, the Hyndman-Khandakar Algorithm, Grid Search for ARIMA, Time Series Embedding, Vector Autoregression for Recursive Forecasting, Embedding for Direct Forecast, Random Forest, Univariate and Multivariate Analysis, and Rolling and Expanding Windows. Next, we cover Support Vector Machines, neural networks, back propagation, and deep learning techniques. Both Support Vector Machines and Neural Networks utilize specific data transformations that project the data into a higher-dimensional space. In this section, we elucidate these topics in a step-by-step manner, employing simulations and concepts that are easy to understand. This approach distinguishes our book from many others in the fields of machine learning and data analytics, as we refrain from relying solely on pre-built R functions and instead focus on providing clear explanations and using hands-on simulation explained step-by-step. Following that, we introduce the fundamentals of covariance, correlation, semi-partial correlation, regularized covariance matrix, and graphical ridge in the graphical network analysis section. We also cover matrix decomposition and singular decomposition techniques. In the final section, we discuss principal component analysis, factor analysis, smoothing techniques, and address handling imbalanced data and fraud detection, as well as other nonparametric estimation methods. This provides readers with valuable insights into these specialized topics. By covering this extensive range of topics, we aim to equip readers with the necessary knowledge and tools to effectively analyze, model, and make predictions using a wide array of methods and techniques in their fields. Who Can Use This Book? This book has been carefully crafted to cater to a diverse audience of motivated students and researchers who have a foundational understanding of inferential statistics using parametric models. The book’s focus is on applied concepts, prioritizing practical application over extensive theoretical proofs and justifications. As such, it serves as an invaluable resource for those who wish to delve into real-world examples and case studies. While no prior experience with the R programming language is assumed, having some familiarity with coding concepts will prove beneficial for readers. The book’s content and structure have been designed to accommodate individuals with varying levels of coding expertise, ensuring that everyone can benefit from the material presented. The target audience for this book includes, but is not limited to: Graduate and advanced undergraduate students in economics, social and health sciences, and related disciplines who are keen to expand their knowledge of data analysis techniques and applications. Researchers and practitioners in the fields of economics, social sciences, and beyond, who wish to acquire practical skills in data analytics and gain insights into the latest methodologies. Educators and instructors who seek a comprehensive, application-focused resource for teaching data analysis methods to students in economics, social sciences, and related areas. In summary, this book is an essential resource for anyone who is eager to learn and apply advanced data analysis techniques in the context of economics, social sciences, and related disciplines. With its clear explanations, practical examples, and accessible language, this book will enable readers to develop their skills and knowledge, regardless of their prior experience with R or coding in general. Acknowledgements We would like to extend our heartfelt gratitude to our loved ones for their constant support during the creation of this book.Their unwavering belief in our abilities and vision has been invaluable, and we could not have reached this milestone without them. Yigit is grateful for the sacrifices Isik has made and for her steadfast encouragement in the pursuit of this dream. Yigit is also grateful for the opportunity to share his passion for learning with Ege, his son. Mutlu would like to extend his heartfelt thanks to his wife, Mevlude, whose love, patience, and understanding have been a constant source of strength and inspiration. Mutlu also extends his heartfelt gratitude to his sons, Eren and Kaan, whose laughter, curiosity, and boundless energy have been a driving force behind his determination to work harder and establish a lasting legacy. License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["introduction.html", "Chapter 1 Introduction: 1.1 Prediction vs. Estimation: 1.2 Where can you use the covered topics in Social Sciences?: 1.3 Translation of Concepts: Different Terminology 1.4 Is Machine Learning Better?", " Chapter 1 Introduction: Let’s start the book with an ancient parable. A group of blind men heard that a strange animal had been brought to the town, but none of them were aware of its shape and form. Out of curiosity, they said: “We must inspect and know it by touch, of which we are capable”. So, they sought it out, and when they found it, they groped about it. They say the following one by one: An animal such as a tree! … said while holding his leg An animal, such as walls! … said while touching its broad and sturdy side An animal like a snake! … said while holding the squirming trunk within his hands An animal, such as rope! … said while seizing on the swinging tail An animal like a fan! … said while touching the ear An animal such as a spear! … said while touching his tusk What is this animal? This animal is elephant! Like in this parable, think someone, who just wanted to start to learn econometrics, came to you and ask for some advice. They’re asking you the following question. How can I learn Econometrics? Your answer will depend on yours and that persons interest. You can tell that person to start with cross section, time-series, or panel data. Or you can start with frequentist approach or Bayesian approach. You can start with Econometrics theory then applied or vice versa. You can start with Applied Micro Econometrics or Macro Econometrics. You can start with parametric econometrics or nonparametric econometrics. You can provide any sources and courses and lectures with any of these topics and all of them are rightfully starting point to learn Econometrics. But people within this field even within the subfields don’t agree totally which is the real econometrics and which is not. Why are we telling this? Because everyone is talking about what machine learning is. Everyone explains this depending on their own field or from their own subfield or own perspective or what they’re doing. Even finding a good definition for Machine Learning has become a subtle job as “machine learning” seems increasingly an overloaded term implying that a robot-like machine predicts the things by learning itself without being explicitly programmed. Ethem Alpaydin, defines machine learning as follows: Machine learning is programming computers to optimize a performance criterion using example data or past experience. We have a model defined up to some parameters, and learning is the execution of a computer program to optimize the parameters of the model using the training data of past experience. (…) Machine learning uses the theory of statistics in building mathematical models, because the core task is making inference from sample. The role of computer science is twofold: First, in training, we need efficient algorithms to solve the optimization problem, as well as to store and process the massive amount of data we generally have. Second, once the model is learned, its representation and algorithmic solution for inference needs to be efficient as well. Hence, there are no “mysterious” machines that are learning and acting alone, but well-defined statistical/econometrics models for predictions that are optimized by efficient algorithms and executed by powerful machines, as known as computers. Thus, the name of the book: **MachineMetrics The following machine learning visualization shows separate fields in machine learning. INSERT PICTURE We designed this book for researchers who deal with data and aimed to provide toolkit which is easily accessible. When we started to learn these topics years ago, we did not know where to start and poke in each topic in that graph. We tried to learn one topic or how we can use it in our own research. We tried to identify the topics we can use as an applied microeconomics and in general as an economist as we are not dealing in robot navigation or game AI or similar topics in our research. Thus, we are only interested in subtopics which are relevant for our research such as dimension reduction and the structure discovery, the regression part and forecasting mostly for finance and so on (these topics from figure). We can use topics related to predictions and optimizations and some new insights. As a social scientist the part we can employ in our research is just some part of the topics in that figure. Because of that, when you discuss with someone who is in machine learning from computer science and robotics or AI sector and so on, they may try to explain where ML can be used as recommending songs, pictures, the fraud detection, computer vision, speech recognition, document classification, automated driving, but it may not be interesting or relevant for us directly. However, nearly all these subfields start and build on the statistical learning methods we will cover in this book. Imagine guiding a child to distinguish between animals, particularly identifying what a dog is and what a cat is. By the end of this exercise, the child will learn to recognize these animals. Instead of detailing every nuance of what differentiates a dog from a cat, you might show the child various pictures of dogs and cats. Over time, the child will start to notice patterns and be able to differentiate between the two animals. Machine learning works in a similar way. You feed a computer lots of data (like pictures of cats and dogs), and over time, the computer learns to recognize the patterns in the data. Put simply, machine learning teaches computers to recognize patterns much as we teach children—though the former relies on data and algorithms, while the latter uses tangible examples and verbal explanations. Distinguishing between cats and dogs in images is just one facet of machine learning. Similarly, these techniques power our email filters to sort spam from important messages, enable virtual assistants like Siri or Alexa to understand voice commands, and help streaming platforms like Netflix or Spotify suggest movies or songs tailored to our preferences. These instances, alongside text analysis and speech recognition, underscore the pervasive role of machine learning in modern life. Yet, our primary emphasis will be on the techniques and uses of machine learning in data analysis, which is used for estimation procedures, data exploration, and causal inference. A more fitting example for this book’s content would be the following: In a community with rising concerns about food allergies, Alex, a young individual, grappled with recurring allergic reactions. Driven by a pressing need to pinpoint their root cause, he embarked on a personal mission. Eager to understand the extent of his susceptibility, Alex meticulously documented each instance he consumed food as well as various nuts, leading to a preliminary data collection effort. In a bid to learn the reason of his sporadic sickness after eating, Alex adopted an approach resembling statistical and machine learning. By systematically recording his meals and subsequent health reactions, he discerned a pattern linking his discomfort to garlic consumption. Testing and validating his hypothesis through various food experiments, Alex refined his understanding, confirming garlic as the culprit while ruling out other foods. His methodical process of data collection, pattern recognition, hypothesis testing, and model refining mirrors the foundational steps in machine learning, showcasing how both humans and machines learn from data and adjust based on outcomes. Visualization of this gathered data unveiled stark patterns, underscoring the correlation between his various nuts and garlic consumption and the allergic reactions, with garlic standing out prominently. Motivated to transition from mere correlation to concrete causation, Alex conducted controlled experiments, eventually confirming that garlic was the definitive cause of his allergic symptoms. Realizing the potential broader implications of his discovery, especially for those who might share similar susceptibilities, Alex’s endeavor caught the attention of researchers. Researchers embarked on a study to gauge its prevalence among those similar to Alex in the larger community. They selected a representative sample and found a significant proportion exhibited allergic reactions. Through their findings, they estimated a certain percentage of the broader group might have this allergy. Using statistical tools, they provided a confidence interval to show the likely range of this percentage. They then extrapolated their findings to the broader community, highlighting potential allergen prevalence. Yet, they acknowledged that their conclusions depend on the sample’s accuracy and potential biases. This exercise underscores the principles of statistical estimation, with a focus on sampling, confidence intervals, and extrapolation. Delving into statistical methods, they journeyed from basic correlations to deep causative insights, unraveling the true triggers behind such allergies. The research eventually progressed to other methodologies like time series forecasting of allergy intensities. Through using time series analysis and forecasting, researchers not only confirm the relationship between garlic consumption and allergic reactions in a larger sample but also provide valuable predictive insights for individuals similar to Alex. Additionally, researchers employed graphical network analysis for the spread of allergy awareness in the population. By leveraging this analysis, they could grasp the intricacies of the community’s social dynamics and the paths information took. This empowers them to deploy targeted interventions, ensuring that knowledge about garlic allergies is disseminated effectively. Researchers also use of classification and regression models to ascertain risk categories and predict allergic reaction severity. While classification assists in grouping individuals as either high-risk or not, regression quantifies the anticipated intensity of their reactions. Starting with Alex’s personal exploration into his garlic allergy, mirroring the steps of machine learning, the scope expanded into a wider research project. This broader study harnessed statistical learning methodologies, using samples to gauge the prevalence of such allergies in the community. Both machine and statistical learning techniques can be instrumental in addressing varied research questions, demonstrating the multifaceted nature of learning from data. In general, there are four different starting points and approaches to Machine Learning Theory. Bias- variance trade-off approach, Vapnik-Chervonenkis theory, Computational Complexity of Machine Learning, and Bayesian Learning. In this book, we will focus on bias-variance trade-off approach. You can think this like in micro theory, when I teach, we will talk about preference relation or choice-based relation. Then I connect these concepts to utility maximization, and then continue from there. It is a similar idea but in economics especially as we will almost always deal with sampling issues and so on. We will start with bias- variance trade-off. We will also mainly follow frequentist approach not Bayesian approach. Again, as a side note, you can learn everything by starting from Vapnik-Chervonenkis Theory, the Perceptron Algorithm, especially if you are interested in deep learning and neural networks and you will reach to the same point. However, we will also cover these topics (deep learning and neural networks) in our book as well. Machine Learning has three main paradigms build on the aforementioned theories. Supervised learning, Unsupervised learning, and Reinforcement learning. The main paradigm we will cover in this book is supervised learning. Simply put, when we know the outcome, it is called supervised learning, and when we do not it is called unsupervised learning. The main topics we will cover under these paradigms are classification, regression, clustering, and dimensionality reduction. We will also cover some additional topics such as …… Some of these uninterpretable ML methods’ goal and aim are different as we will cover in this book. 1.1 Prediction vs. Estimation: As a researcher our aim is finding associations and predictions using different dataset. We want to clarify some concepts that are used interchangeably, which is a mistake, in different machine learning sources. Keep in mind, the main aspect of machine learning is to use one set of data to generalize the findings on new data not seen yet. We use the term Prediction to describe this process in Machine Learning. The other similar terms are extrapolation and forecasting. In social sciences, the most common term is estimation while analyzing main data. However, these terms have different connotations, and we think using them in the right place will help all of us to understand certain topics better. Let’s started by describing the term of prediction first. Prediction (Latin præ-, “before,” and dicere, “to say”), or forecast, is a statement about a future event. They are often, but not always, based upon experience or knowledge. There is no universal agreement about the exact difference from “estimation”; different authors and disciplines ascribe different connotations. Prediction in the non-economic social sciences differs from the natural sciences and includes multiple alternative methods such as trend projection, forecasting, scenario-building and surveys. You can read different definitions of prediction and its use from 12 different fields from science, sports, finance, and non-scientific context. Here is the link (https://en.wikipedia.org/wiki/Prediction). Extrapolation: the action of estimating or concluding something by assuming that existing trends will continue, or a current method will remain applicable. Extrapolation is estimating the value of a variable outside a known range of values by assuming that the estimated value follows some pattern from the known ones. In mathematics, extrapolation is a type of estimation, beyond the original observation range, the value of a variable is based on its relationship with another variable. Forecasting: the process of making predictions based on past and present data. Risk and uncertainty are central to forecasting and prediction; it is generally considered a good practice to indicate the degree of uncertainty attaching to forecasts. In any case, the data must be up to date in order for the forecast to be as accurate as possible. “Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.” (reference Hydman book ). This term is more commonly used in fields of Finance and Economics. Generally, people use time series data and methods for forecasting. Even though, we think extrapolation is better term, we use forecast in Economics, Finance, or prediction in Machine Learning. We should be aware that prediction in economics and social sciences differs from the natural sciences. If you discover a relationship in the natural sciences and explain the conditions for that relationship, then that relationship holds regardless of where and when it is found unlike social sciences. Hence, this relationship can be understood as both a prediction as well as an extrapolation. Consider the example of a chemical reaction occurring in a laboratory under well-defined conditions; the findings become immediately applicable to other locations with the same conditions. Moreover, in natural sciences, prediction incorporates multiple alternative methods such as trend projection, forecasting, and scenario building. The hardest part of prediction in the social sciences, “predictors are part of the social context about which they are trying to make a prediction and may influence that context in the process”. In another word, you predict something and implement the policy using this prediction, people change their behaviour based on this new policy, even before implementing the policy. In economics, this is well known as Lucas Critique. “Lucas summarized his critique: Given that the structure of an econometric model consists of optimal decision rules of economic agents, and that optimal decision rules vary systematically with changes in the structure of series relevant to the decision maker, it follows that any change in policy will systematically alter the structure of econometric models.” Estimation is various procedures to find an estimate using a sample drawn from the population. Estimate is the potential range of values of some property of a population and generated by projecting results from samples onto the entire population. Both effect and prediction research questions are inherently estimation questions, but they are distinct in their goals. In encapsulating Effect, the researcher is mainly interested in estimating the effect of exposure on outcome adjusted for covariates. On the other hand, when the focus is Prediction, the researcher is mainly interested in generating a function to input covariates and predict a value for the outcome. Mostly, economists and social and health scientists estimate an effect, and try to find Causal Explanation/Attribution. Even though, Economists prefer to use causal relation or explanation, statisticians studying these topics prefer to use the term of attribution. What about prediction? Most economist claim that economics research is not about prediction but estimation and finding a causal explanation. For instance, Ludwig von Mises, “[predicting the economic future is] beyond the power of mortal man”. Or Applied economists act as if unbiased estimation is prediction (on average). Even some claims prediction is not a good thing, and it is unscientific. We are interested in understanding things, not just predicting, or controlling them. However, “being able to predict something is a sign that you understand it, not being able to predict is a good sign that you don’t.” However, in real life, most people or firms are interested in prediction. For instance, individually you ask whether this education policy is good for my children, and you don’t care about the average estimates. You care about what will happen to your own kids education with this specific teacher or specific education policy. Similarly, crime forecasting in the law enforcement operations, or the change in specific treatment might cure the patient, or improve the prediction of one-year survival probability after sickness, treatment, etc. All in all, you want a specific individual prediction. You don’t care too much about the average estimated effect of the population. Here you can see two very good sources that you can read about this discussion. 1.2 Where can you use the covered topics in Social Sciences?: In addition to clarifying the terms and terminology above, we would like to emphasize that even though Machine Learning is used primarily for prediction, we, as economists,social and health scientists, can use machine learning and other statistical and econometric tools covered in this book for estimation procedures, exploration of data, causal inference, and more. To begin, we can transform text, images, historical documents, and similar unconventional information into new data. It may be possible to incorporate job descriptions, language and sentiment of financial documents, social media, court transcripts, and health reports into indexes or other types of variables. Researchers can implement these types of new tools in almost any program in a straightforward and reliable way with ready-to-use packages. The standard machine learning methods are correlational approaches; therefore, we can use some of these methods to visualize data in a more meaningful way, analyze it better, and identify patterns in the data. We have discussed several of these methods in this book, such as graphical modelling, semi-partial correlation, regularized covariance matrix, graphical ridge and lasso models. There are some statistical methods which assist researchers in reducing dimensions when they are attempting to identify patterns between variables and identify significant influences, i.e. correlation analysis. The correlation analysis is a statistical method used to determine the degree of relationship between two or more variables while accounting for both spatial and temporal dimensions of these static and dynamic relationships. Correlation analysis may become complicated when the model includes too many variables as well as ‘latent variables’. These latent variables are unobserved but influential factors that explain a significant proportion of the variation common among the input variables. In the social sciences, it is common to use either random or fixed effect indicators to account for these unobserved factors. However, dimension reduction methods should be used for settings which has too many latent variables, as well as dynamic spatial and temporal relationships. Some of these methods we cover are Principle Component Analysis, Factor Analysis, as well as Dynamic Mode Decomposition which can be thought of as an ideal combination of spatial dimensionality-reduction techniques. In economics as well as other health and social sciences, finding causal relationship is the ultimate goal for any researcher, policy maker or business. Researchers use some machine learning methods to improve some of the usual causal methods. Researchers work with Rubin causal framework, in which they compare the representative sample in actual and counterfactual situations to find treatment or causal effect. Thus, imputing missing counterfactual values are mainly a prediction problem. Hence, researchers implement various ML methods for Direct and conditional randomization, Indirect randomization: Instrumental variables, Local randomization: Regression discontinuity, Second-order randomization: Difference-in-difference, as well as Propensity Score matching, and Synthetic Control methods. Both Athey&amp;Imbens (2019) and Mullainathan &amp; Spiess (2017) highlight, “some substantive problems are naturally cast as prediction problems, and assessing their goodness of fit on a test set may be sufficient for the purposes of the analysis in such cases. In other cases, the output of a prediction problem is an input to the primary analysis of interest, and statistical analysis of the prediction component beyond convergence rates is not needed.” As an example , Variable Selection in $ y=D+X+$ as most coefficients besides treatment one are inconsequential! Another common example is first stage selection (since the first stage deals with prediction problems) in instrumental variable models. Debiased Machine Learning for Treatment is another recently developed method. [footnote: [https://doi.org/10.1146/annurev-economics-080217-053433 ]) ] (https://arxiv.org/pdf/1712.10024.pdf) Researchers are also interested in knowing how treatment affects certain sub-populations in addition to finding the average treatment effect. A given treatment may have different effects on different units. Heterogeneity of treatment effects refers to the study of these differences across subjects. There are several machine-learning methods that can help to improve heterogeneous treatments or causal effects, including Causal Forest. The casual forest approach splits the covariate sample and calculates predictions as a local average treatment effect. (footnote: https://arxiv.org/abs/1712.09988 ) Almost always, in social sciences, we assume our models are linear and parametric. Model selection, however, is the process of selecting one model out of numerous potentials for a predictive problem. We discuss in detail the importance and usefulness of model selection as well. Time series forecasting is a fundamental task at the core of many data-driven applications. Forecasting models were developed using a variety of advanced autoregressive methods, including ARIMA. Rather than using normal time series tests, we show how to grid search ARIMA model hyperparameters. Furthermore, methods based on deep learning have been explored for time series forecasting. Additionally, we cover an embedding method that enhances the performance of many deep learning models on time series data. As a result of the embedding layers, the model can simultaneously learn from several time series of different units. These categorical features (e.g., holidays, weather, geography) are embedded in a lower dimensional space to extract useful information. The dynamic discrete choice (DDC) models are used for modeling an agent’s choices among discrete options. As opposed to the assumption that observed choices are the result of static utility maximization, DDC models assume that observed choices are the result of maximization of the present value of utility. DDC methods aim to determine the structural parameters of the decision process of agents. By using these parameters, researchers can simulate how the agent would behave in a counterfactual setting. Recently developed machine learning methods have the potential to improve DDC models . [Dynamic Discrete Choice models (https://arxiv.org/abs/1808.02569) (https://sites.google.com/view/semenovavira/research?authuser=0)] Obtaining reliable estimates of demand is fundamental to a wide range of studies in Industrial Organization and other fields of economics. For every counterfactual question concerning a market, it is necessary to quantify the response of choices to ceteris paribus changes in prices and other characteristics of the market. The most common methods of estimating demand are linear, logit, nested logit, and DDC. Recently, ML methods have been incorporated into routine demand estimation techniques to improve the out-of-sample prediction accuracy. [Footnote: Recent very extensive review about demand estimation: https://cowles.yale.edu/sites/default/files/d2301.pdf] 1.3 Translation of Concepts: Different Terminology Raw data and labels; create features (covariates(x)) which is input data to train an algorithm: to run an estimation procedure Estimation sample; Training data Features – Predictive covariates, Regressors Labels- Outcome Training Data- Sample Prediction Rule (hypotheses): A function for prediction Hypothesis testing: testing whether prediction rule (function) is true or not (not a coefficient test) Learning algorithms: algorithm which map samples into predictor functions. a (categorical) dependent variable (y); Label a (continuous) dependent variable (y); Response Classification: predicting Discrete variables (-1, 1) Regression: predicting a continuous value Use training data, validation data and test data (in-sample and out-sample) Linear regression is one type of parametric algorithm Bias-Variance Tradeoff The main goal is to get good out-of-sample predictions. To do so, we must overcome a problem known as overfitting. Regularization ; hyperparameter by model tuning using cross-validation or penalty measure (like Akaike’s information criterion (AIC)) Minimize loss function and Learning parameters (minimize cost function using Lagrangian or gradient descent) Non-parametric supervised learning algorithms: k-NN and decision trees Unsupervised learning (no y): principal components analysis (PCA), k-means clustering, Hierarchical clustering (), Singular value decomposition (SVD) Support Vector Machine Ensemble methods; Bagging; Boosting Neural Networks; Deep Learning 1.4 Is Machine Learning Better? Machine learning, while a powerful tool, may not always be the most suitable approach for every research inquiry. A crucial aspect of conducting research is the formulation of a precise and answerable research question. The strengths and weaknesses of conventional research methods versus machine learning techniques differ, and it is vital to establish the research objective before embarking on any study. Traditional estimation methods still hold significant value and will continue to be employed in various research settings. However, machine learning techniques can be harnessed for their predictive capabilities. It is essential to understand that machine learning does not provide a solution to the core identification problem in social sciences, which revolves around the fact that counterfactual situations remain unobservable. Nonetheless, machine learning can enhance our ability to create certain counterfactual scenarios. Machine learning demonstrates exceptional performance in tasks involving the prediction of patterns or structures, such as letters, words, and images. For example, it can accurately predict handwritten letters with a remarkable 99.97 percent accuracy rate. Despite these impressive capabilities, machine learning cannot replace human intuition or theoretical understanding when addressing the question of “why?” Furthermore, it is limited in its ability to predict large, unforeseen events (also known as “black swans”), as its predictions are grounded in historical data, and predicting such exceptional occurrences proves to be challenging. In conclusion, researchers should carefully consider the purpose of their study and the specific research question at hand when deciding whether to employ machine learning techniques or rely on conventional statistical and econometrics methods. While machine learning offers notable advancements in predictive accuracy, it remains limited in addressing certain aspects of research, such as explaining causality and anticipating unexpected events. "],["statistical-models-and-simulations.html", "Chapter 2 Statistical Models and Simulations 2.1 Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis 2.2 Parametric and Nonparametric Models: 2.3 Predictive vs. Causal Models 2.4 Model Selection and Approaches in Data Modeling 2.5 Simulation", " Chapter 2 Statistical Models and Simulations 2.1 Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis In the age of data, understanding the tools and techniques available for data analysis is paramount. Machine learning and statistical modeling are two such techniques that frequently emerge in discussions. While they have similarities, they are distinct in their purposes and goals. Though machine learning models and statistical models are sometimes used interchangeably in data analysis, they are not identical. Each serves its own unique purpose and function. Recognizing the fundamental distinctions between these methodologies is essential for navigating the realm of data analysis effectively. Both machine learning and statistical modeling play crucial roles in data analysis, offering tools for predictions, model building, and making informed decisions. Statistical learning, often equated with machine learning, emphasizes the use of various methods for predictions, such as decision trees, neural networks, and support vector machines. On the other hand, main emphasis of Statistical learning, often connected to inferential statistics in social and health sciences, is developing statistical models that accurately represent the data and explaining and interpreting the relationships between variables. In this section, we will explore machine learning and statistical learning in more detail, discussing their key features, objectives, and differences. 2.1.1 Goals and Objectives At its core, the distinction between statistical models and machine learning is their primary objectives. Statistical models aim to investigate the relationships between variables, while machine learning models focus on delivering precise predictions. Machine learning, often termed “Algorithm-Based Learning,” offers a dynamic approach that allows algorithms to learn directly from data, eliminating the need for rule-based programming. As these algorithms process more data, they continually refine their performance, enhancing their prediction accuracy and decision-making capabilities. The overarching goal of machine learning is to utilize input data to generate accurate predictions. Through the application of mathematical and statistical techniques, these models identify patterns and relationships, setting the stage for predictions on new and unseen data. At its essence, machine learning revolves around creating models that predict future outcomes. What sets it apart is its ability to predict without being pre-programmed or having explicit assumptions for specific outcomes or functionals. The more data these models process, the sharper their predictive accuracy becomes. On the other hand, statistical models are designed to infer relationships between variables. Their main goal is to deeply analyze data, revealing intrinsic patterns or connections between variables. Such insights then become the foundation for well-informed decisions. Statistical learning, often described as “Learning from Data,” focuses on using data to determine its originating distribution. A typical task in statistical inference could be identifying the underlying distribution, F, from a sample set like X1,…,Xn ∼ F. Statistical modeling can be thought of as the formalization of relationships within data. It aims to define connections between variables through mathematical equations. At its heart, a statistical model is a hypothesis about how the observed data came to be, rooted in probability distributions. This encompasses a range of models, from regression and classification to non-parametric models. The main goal of statistical learning theory is to provide a framework for studying the problem of inference, which includes gaining knowledge, making predictions, making decisions, and constructing models from a set of data. This analysis is undertaken within a statistical paradigm, making certain assumptions about the nature of the underlying data generation process. 2.1.2 Prediction vs. Inference Statistical learning, often equated with machine learning, emphasizes the use of various methods for predictions, such as decision trees, neural networks, and support vector machines. The model is trained using the training set, aiming to optimize its prediction accuracy on the test set. Techniques like cross-validation and boosting are frequently employed to improve this accuracy. The overarching goal in statistical learning is to identify a useful approximation, fˆ(x), to the function f(x) that defines the predictive relationship between inputs and outputs. This approximation subsequently serves as a tool for making predictions or decisions based on the data at hand. Statistical learning, often connected to inferential statistics in social and health sciences do not involve a formal splitting of data into training and testing sets. The emphasis here is developing statistical models that accurately represent the data and explaining and interpreting the relationships between variables. Once developed, these models become instrumental for tasks like hypothesis testing, estimation, and other inference-related tasks. In inferential statistical modeling, models are assumed or derived through theory or information about the data generation process. Then these probabilistic models are used to interpret and identify the relationships between data and variables, such as the effects of predictor variables. These models establish the magnitude and significance of relationships between variables and their magnitudes. In contrast, machine learning or statistical learning models take a more empirical approach, focusing on making accurate predictions based on observed data patterns. 2.1.3 Conclusion In summary, while statistical learning and machine learning have overlapping areas, they are distinct in their core objectives. Machine learning models predominantly aim to make accurate predictions. In contrast, statistical models are tailored to infer and understand the relationships between variables. Statistical learning can be considered a subset of machine learning that applies regression and various methods for prediction. The primary difference between statistical learning and inferential statistics lies in the methods used and their focus on prediction versus inference. Both these approaches, statistical learning and inferential statistics, have the capability to make predictions and inferences. Yet, the primary focus of statistical learning is on prediction, with inference often playing a secondary role. In contrast, inferential statistics prioritize inference over prediction. It is crucial to emphasize the importance of Accuracy and Interpretability as well. Statistical models can sometimes be less accurate in capturing complex relationships between data, even if they can offer insights and predict outcomes. On the other hand, machine learning models tend to provide more accurate predictions. However, a trade-off exists, as these predictions, despite their accuracy, can often be complicated and less straightforward to interpret and explain. Statistical modeling and machine learning are distinct yet complementary techniques in data analysis, each with its unique features and applications. They allow for the development of algorithms that can learn from data and make predictions or decisions based on observed patterns. By understanding the key concepts, goals, and applications of these techniques, researchers and practitioners can harness their potential for a wide range of data-driven tasks and challenges. ====================== 2.2 Parametric and Nonparametric Models: Parametric and nonparametric models serve as foundational statistical tools for data analysis, predictions, and inferences about populations. Each type of model has its own strengths and limitations. The decision to employ a particular model depends on the nature of the data at hand and the specific research question being addressed. In this section, we will compare parametric and nonparametric models. Parametric models make assumptions regarding the underlying distribution of the data, such as a normal distribution or a binomial distribution. By making these assumptions, parametric models can estimate the parameters of the distribution, such as the mean and standard deviation, and use these estimates to make predictions or inferences about the population. Some examples of parametric models include linear regression, logistic regression, and ANOVA multiple regression, polynomial regression, and Poisson regression. Typically, these models are viewed as more efficient and robust compared to nonparametric models, provided the data aligns with the model’s assumptions. However, when these assumptions aren’t satisfied, the estimates from parametric models may be biased or inaccurate. Nonparametric models, on the other hand, do not make assumptions about the underlying distribution of the data and specify functional forms. These models are often used when the distribution of the data is unknown or when the assumptions of parametric models are not met. Nonparametric models are generally more flexible and robust than parametric models, but they may be less efficient and have lower statistical power. Examples of nonparametric models encompass k-Nearest Neighbors, the Spearman rank correlation, and kernel density estimation, Decision Trees like CART. Nonparametric models are generally used when the data is ordinal or categorical, or when the data does not meet the assumptions of parametric models. In summary, parametric and nonparametric models offer distinct approaches for analyzing data and making predictions or inferences about a population. Parametric models assume that the data follows a certain probability distribution and estimate the parameters of the distribution, while nonparametric models do not make any assumptions about the distribution of the data. Each type of model has its own strengths and limitations, and the choice of which model to use depends on the characteristics of the data and the research question being addressed. In Chapter 10, we will delve into the topic of nonparametric estimation, focusing on the conditional expectation function (CEF), denoted as E[Y | X = x] = m(x). Unlike parametric models, which impose a specific functional form on m(x), nonparametric models allow m(x) to take any nonlinear shape. This flexibility in form arises when an economic model does not restrict m(x) to a parametric function. As we progress through this chapter, we will discuss the various aspects of nonparametric estimation and its implications in modeling and analysis. 2.3 Predictive vs. Causal Models Predictive models and causal models are two types of statistical models used to analyze data and make predictions or inferences about a population. They differ in their goals and approaches: predictive models focus on forecasting future outcomes, while causal models aim to understand the underlying causes of a particular outcome. In this section, we will compare predictive and causal models. Predictive models are statistical models used to make predictions about future outcomes based on past data. Commonly employed in finance, marketing, and healthcare, researchers and analysts forecast trends or predict the likelihood of specific events. These models primarily rely on correlations between variables, using samples of data collected over time to construct a statistical model for future predictions. However, their focus on correlations limits their ability to identify causal relationships. Examples of predictive models include time series analysis, forecasting models, and machine learning algorithms for classification and regression tasks. These models are well-suited for predicting future events or trends but may not provide insights into the underlying causes of these outcomes. Causal models, in contrast, aim to understand the causal relationship between variables and identify factors that cause a particular outcome to occur. These models are often used in economics, sociology, and medicine to investigate the underlying causes of a phenomenon and to identify contributing factors. Causal models rely on the concept of causality, suggesting that one event or factor can cause another event or outcome. Researchers use experimental or quasi-experimental designs, manipulating or controlling variables of interest to establish causality. By doing so, they can isolate the effect of a specific variable on the outcome and determine its causal effect. Statistical techniques commonly used in building causal models include experimental design, observational studies, and instrumental variables analysis. These methods enable researchers to control for confounding variables and estimate the causal effect of a particular variable on the outcome of interest. In summary, predictive and causal models serve different purposes in the analysis of data and the generation of predictions or inferences about a population. Predictive models focus on forecasting future outcomes based on correlations between variables, while causal models seek to understand the underlying causes of outcomes by investigating causal relationships. 2.4 Model Selection and Approaches in Data Modeling Model Selection and Approaches in Data Modeling Introduction Data modeling, a cornerstone of modern analytics, necessitates a series of judicious decisions. This section offers an in-depth exploration of these choices, spotlighting the nuances between parametric and nonparametric models, and weaving in illustrative examples for clarity. Choosing the Model Family The journey of data modeling commences with the selection of an appropriate model family. Parametric models, delineated by specific parameters (βj), are refined by adjusting these parameters—a method exemplified by linear regression. Conversely, non-parametric models, a staple in machine learning, sidestep fixed parameter specifications, operating in a more fluid, algorithmic manner. For perspective, envision modeling housing prices: a parametric approach might rely on fixed factors like square footage and location, whereas a non-parametric method, such as a decision tree, might dynamically evaluate a myriad of factors. Linear vs. Polynomial Models The inherent traits of the data steer the choice between linear and polynomial models. When data hints at intricate relationships, the selection of variables and the degree of polynomial terms become paramount. For instance, a linear model might suggest a direct correlation between years of education and income. In contrast, a polynomial model might unveil nuances, such as the diminishing income benefits after a certain educational threshold. Importantly, in scenarios devoid of predictor interactions, a variable’s influence remains steadfast. This accentuates the importance of envisioning the “true” Data Generating Mechanism (DGM) during model selection. Model Fitting Techniques Upon settling on a model type, the focus shifts to the fitting technique. While methods like ordinary least squares (OLS) and maximum likelihood estimation (MLE) are widely acknowledged, a spectrum of alternatives beckons. The choice often mirrors the data’s characteristics and the desired estimate properties. For instance, when data showcases varying variances across observations, techniques like generalized least squares might emerge as more apt. Causal vs. Predictive Analyses The foundational decisions outlined above pave the way for discerning variable relationships, which can oscillate between causal and predictive analyses. Causal analyses unravel the intricate “why” behind relationships (e.g., discerning the health ramifications of certain diets), while predictive analyses are primed for forecasting future scenarios based on extant data (e.g., gauging a region’s rainfall for the upcoming month). Parametric vs. Nonparametric Models Parametric and nonparametric models serve as the bedrock of statistical modeling, guiding data analysis, predictions, and inferences. While parametric models predicate a defined relationship between variables, nonparametric models, lauded for their adaptability, can encapsulate more layered relationships. For instance, while a parametric model might linearly correlate age with fitness levels, a nonparametric model might discern unexpected fitness peaks or troughs at specific ages. Conclusion The art and science of data modeling hinge on astute model selection and approach. By meticulously evaluating the model family, its nature, and the fitting technique, one can craft models that resonate deeply with the inherent relationships between variables, fostering robust predictions and inferences. A profound grasp of the intricacies between parametric and nonparametric models is indispensable for tailored, effective model selection. 2.5 Simulation Simulation combines statistical and computational methods to model and analyze intricate systems and processes. By developing a mathematical or digital model of a system, researchers can produce synthetic data or forecast the system’s behavior. This technique is instrumental in fields like statistics, economics, and data science, providing deep insights into model characteristics and the effects of various factors on results. Why Use Simulation? There are three main reasons to employ simulation in modeling: Predictive Challenges: For some models, especially complex or nonlinear ones, predicting behavior can be tough. Example: Predicting stock market movements based on numerous unpredictable factors. Analytical Challenges: At times, the underlying mathematics of a model might be too complex or even unsolvable using standard methods, making simulation necessary. Example: Calculating the trajectory of a satellite in space with multiple gravitational influences. Change Impact Analysis: Simulation enables researchers to examine the effects of altering initial values, parameters, or assumptions, offering a glimpse into potential scenarios. Example: Testing the impact of different interest rates on an economic model. Applications of Simulation: Statistics: Simulation is frequently used in statistics to assess properties of statistical models, such as their reliability. It’s also beneficial for understanding how various factors influence statistical estimates and for validating and comparing model performance. Example: Bootstrapping techniques to estimate the accuracy of sample statistics. Economics: In economics, simulations help model and scrutinize intricate economic structures, from the global economic landscape to financial markets and supply chains. This aids in forecasting the repercussions of policy shifts or market dynamics, equipping policymakers and businesses with the knowledge to make informed choices. Example: Simulating the global economic impact of a sudden oil price surge. Data Science: Simulation is pivotal in data science for modeling vast, intricate datasets. It’s essential for forecasting data-driven system behaviors and for verifying the efficacy of machine learning and statistical models. Example: Using simulation to test the performance of a new recommendation algorithm before deploying it on a live platform. Simulation Techniques: Simulation techniques are essential when addressing challenges related to prediction, calculation, or adaptability in systems. Key techniques include Monte Carlo, discrete event, and agent-based simulations. Types of Simulation Techniques: Monte Carlo Simulation: This technique involves running a model multiple times with different random inputs to estimate potential outcomes. It’s widely used in finance, risk analysis, and for solving intricate problems in physics. Example: Estimating the risk of a financial portfolio over a given time period. Discrete Event Simulation: This approach represents systems as sequences of individual events, each with its own timestamp. Commonly used in manufacturing and healthcare, it helps in refining processes and evaluating performance. Example: Simulating patient flow in a hospital to optimize bed allocation and reduce waiting times. Agent-Based Simulation: In this method, systems are portrayed as groups of interacting agents. This is particularly useful in social sciences and economics to understand large-scale behaviors that arise from individual interactions. Example: Modeling the spread of opinions in a community based on individual interactions. Benefits of Simulation: Deep Insights: Simulations allow for an in-depth understanding of system behaviors and enable well-informed projections. Clarifying Complex Models: When models are intricate, simulations provide a clearer perspective. Alternative to Analytical Solutions: When direct analytical solutions are unavailable, simulations offer a method to understand and predict system behaviors. Sensitivity Analysis: Through simulation, researchers can determine how changes in variables affect the system, pinpointing key variables and predicting system responses to these changes. Conclusion Simulation techniques are invaluable in analyzing complex systems across various disciplines. They provide insights into system behaviors, simplify complexities, and offer solutions when analytical methods fall short. By employing simulations, researchers gain the ability to understand systems under varied conditions, leading to informed decisions, future predictions, and the development of optimized processes and strategies. "],["counterfactual.html", "Chapter 3 Counterfactual: 3.1 Qualitative and Quantitative research methods: 3.2 Quantitative - Research methods : 3.3 Data and visualization 3.4 Correlation 3.5 Effect of X on Y / Regression 3.6 Causal Effect", " Chapter 3 Counterfactual: Imagine a college student,Alex, who occasionally gets sick after eating but doesn’t know which specific food is the culprit. In a community increasingly concerned about food allergies, this young individual grappled with recurring allergic reactions. Driven by a pressing need to pinpoint the root cause, he embarked on a personal mission. Eager to understand the extent of his susceptibility, Alex meticulously documented every instance in which he consumed food, noting all the ingredients, leading to a comprehensive data collection effort. In his quest to uncover the reason for his sporadic sickness after eating, Alex adopted an approach that resembled common quantitative research methods: collecting data, utilizing descriptive statistics, visualizing the data, finding correlations, and ultimately using methods to determine the causation of the allergy. As Alex continues to gather more data about his dietary intake and subsequent health reactions, he starts by creating a simple table for each day. After visualizing the data, he begins to spot correlations between certain foods and his well-being. Data Visualization: Based on this recorded data, Alex can employ various visualization methods to better understand and identify patterns: For the Histogram of Reaction Intensity, this chart allows Alex to see the frequency of days with varying intensities of reactions. The X-axis represents the Reaction Intensity ranging from 1 to 10, while the Y-axis shows the Number of Days. An observation he might make is if he notices a high number of days with intensities around 8-10 after consuming garlic, providing an initial clue. In the Bar Chart of Reactions by Food, he can visually compare the average reaction intensities for various foods. The X-axis displays different foods or ingredients such as garlic, dairy, and wheat. The Y-axis represents the Average Reaction Intensity. If the bar for garlic consistently stands out in comparison to other foods, it further signals a potential issue. The Time Series Line Graph enables Alex to track the evolution of reactions over time. With the X-axis indicating the Date and the Y-axis highlighting the Reaction Intensity, a line tracing the intensity of reactions over time can help him pinpoint if certain clusters of high-intensity days align with the consumption of specific food. Recognizing correlations visually with these tools means Alex can discern if there’s a pronounced spike in the histogram every time garlic is consumed. The bar chart might indicate that garlic has a noticeably higher average reaction intensity than other foods. Similarly, the time series graph can demonstrate peaks in reaction intensities on specific dates, which Alex can then cross-reference with the food he consumed on those days. By visually plotting the data, Alex can more effectively recognize patterns and correlations, offering a foundational understanding before venturing into more intricate statistical analyses. Delving Into Correlations: Observing Correlations: Alex began to rank his reactions on a scale from 1 to 10, with 10 marking the most severe reaction. As days turned to weeks, he noticed that every time he consumed garlic, the intensity of his reaction consistently hovered between 8 to 10. Contrastingly, other foods such as dairy or wheat might occasionally result in a reaction intensity of 3 or 4, but not always. Potential Confounding Factors: On a particular day, Alex felt unwell after a meal without garlic, but recalled having a milkshake. Wondering if dairy might be another trigger, he started noting down dairy consumption alongside garlic. However, after several dairy-heavy days without any reaction, it becomes clear that the milkshake incident might have been a coincidence or caused by another factor. Strength of Correlation: As weeks go by, the association between garlic ingestion and feeling under the weather becomes more evident. The consistency and strength of this correlation are much higher than with any other food. In statistical terms, one might say that garlic have a strong positive correlation with Alex’s adverse reactions. Spurious Correlations: A pattern Alex took note of was his increased tendency to fall ill on weekends. However, after some contemplation, he discerned that weekends were when he often dined out, inadvertently upping the odds of ingesting garlic. his is an example of a spurious correlation: the actual problem wasn’t the weekend itself, but rather the increased exposure to the allergen. Drawing Conclusions: While correlation does not imply causation, the consistent and strong correlation between garlic consumption and adverse reactions, gives Alex confidence in the hypothesis that he is allergic to garlic. In this example, Alex’s observations and data tracking are analogous to the process of determining correlation in statistical or machine learning contexts. Correlations can highlight patterns, yet it’s crucial to ensure that confounding factors or spurious correlations aren’t misleading the conclusions. The Mystery of Mia’s Illness: On certain weekends, Mia, Alex’s girlfriend, also started feeling unwell. As she began to correlate her illness to the days she spent with Alex, she grew concerned. Was she allergic to something at Alex’s place? Or, even more alarmingly, was she developing an allergy to garlic, having shared many garlic-laden dishes with him? Mia decided to chart her symptoms alongside Alex’s diary of garlic consumption. To her surprise, she found that she felt sick on several occasions when Alex had garlic in his meals, even if she hadn’t consumed any garlic herself. Spurious Correlation Revealed:Further probing revealed an interesting detail. Whenever Alex prepared dishes with garlic at his place, he’d also light up a particular brand of aromatic candle to mask the strong garlic smell. Mia wasn’t reacting to the garlic, but to the scent of that specific candle. Her sickness wasn’t directly linked to the days Alex consumed garlic, but rather to the days the candle was lit. The correlation between her sickness and Alex’s garlic consumption was spurious, with the actual causative agent being the candle’s aroma. In this example, Mia’s conclusion, based on initial observations, would lead her down the wrong path, emphasizing the importance of not mistaking correlation for causation. It serves as a cautionary tale on the pitfalls of spurious correlations in both real-life and statistical contexts. Alex’s Deep Dive into the effect of his garlic consumption on his allergy severity: After discovering a strong correlation between his garlic consumption and allergic reactions, Alex decided to take his investigation a step further. While the correlation was evident, he wanted to quantitatively understand the exact impact of garlic consumption on his reactions. He suspected that while garlic was the primary association with his reactions, other variables might exacerbate or alleviate his symptoms. Beyond just the amount of garlic he consumed, could factors like his weight, the weather temperature, and even eating outside influence the severity of his reactions? Gathering Data: For several weeks, Alex meticulously documented the amount of garlic in his meals, his weight each day, the day’s peak weather temperature, whether he dined inside or outside. To understand the relationship better, Alex used an Ordinary Least Squares (OLS) regression. This approach would allow him to understand how each variable, when considered together, might predict the severity of his allergic reaction. He find that the coefficient for garlic quantity was positive, reaffirming that the more garlic he consumed, the stronger the allergic reaction. Interestingly, on days when he weighed more, the severity of his allergic reaction was slightly less, all else being equal. Perhaps his body was better equipped to handle allergens when he was at a slightly higher weight. On warmer days, Alex’s allergic reactions were milder than on colder days. Dining outside frequently correlated with more intense reactions. This puzzled Alex until he realized that eating outside often meant dining at restaurants or cafes where he had less control over ingredients, and the chance of consuming hidden garlic was higher. Alex remembered that his girlfriend once mentioned he seemed to react more during weekends. Reflecting on it, he saw that weekends were indeed when they often dined out, leading to more exposure to garlic-rich dishes. It wasn’t the fact that it was a weekend causing the reactions but the increased likelihood of eating garlic-containing food outside. This was a classic case of spurious correlation; the real culprit was the garlic, not the weekend! Equipped with these insights, Alex made some lifestyle changes. He became cautious about eating out, especially on weekends. He also kept an eye on the day’s temperature, preparing for potential reactions on colder days. Knowing that his weight had a buffering effect was an added insight, but he decided that a balanced diet and regular exercise were more crucial for his overall health. Investigating Causation Building on the previously identified correlation between garlic and adverse reactions, Alex feels the pressing need to ascertain whether garlic truly triggers his allergic responses. Although correlation had provided some preliminary insights, he recognized the limitations of correlation evidence in proving causation. He turned to Ordinary Least Squares (OLS) regression analysis, aiming to isolate the impact of garlic relative to other potential variables, like his weight, weather temperature, and the environment where he eats. He remembered a recent news article discussing certain foods that were structurally and chemically similar to garlic. The article suggested that these foods could also trigger allergic reactions in individuals sensitive to garlic. This revelation complicated his inquiry, as neither correlation nor regression methods could offer him a definitive answer. Could there be other foods amplifying his reactions? Or was garlic the sole offender? Determined to get to the bottom of this mystery, Alex decided to undertake a more rigorous approach: the experimental method. Often hailed as the gold standard for establishing causality, this method would allow Alex to control specific variables and thereby isolate the effects of garlic and other similar foods on his system. By methodically introducing and removing these foods from his diet in a controlled setting, he aimed to definitively ascertain the root cause of his adverse reactions. To unravel this mystery, Alex approached his friend Mia, who didn’t have any known food allergies, to participate in a controlled experiment. By having Mia as a control group, Alex could compare reactions between them, potentially teasing out the specific effects of garlic. They both embarked on a week-long experiment, where their diets were standardized, with the only variance being the consumption of garlic and its similar foods. Mia’s consistent lack of adverse reactions when consuming the same meals as Alex, especially those containing garlic, reinforced its role in Alex’s allergic symptoms. Meanwhile, Alex’s symptoms persisted, lending more weight to the hypothesis about garlic’s culpability. When Mia remained symptom-free even after consuming the foods similar to garlic that the news had warned about, it provided Alex with further clarity. It became evident that while those foods might be problematic for some, they weren’t the culprits in Alex’s case. By incorporating Mia into the experiment as a control group, Alex was not only able to more confidently ascertain the role of garlic in his reactions but also to rule out other potential allergens. Causation Established: With consistent results across multiple trials, combined with the knowledge that other potential causes have been ruled out, Alex concludes that garlic is not just correlated with, but is the actual cause of his allergic reactions. In scientific terms, Alex has moved from observing a correlation (a relationship between garlic consumption and allergic reactions) to establishing causation (determining that garlic directly causes the allergic reactions). This journey mirrors the scientific process where controlled experiments, repeated trials, and the isolation of variables are crucial for determining the true cause of an observed effect. 3.1 Qualitative and Quantitative research methods: In the realm of research, there are two primary methodologies: qualitative and quantitative. Qualitative research methods often involve focus groups, unstructured or in-depth interviews, and the review of documents to discern specific themes. For instance, in social sciences, a qualitative study might explore the lived experiences of individuals living in poverty, capturing their stories and challenges through in-depth interviews. In economics, qualitative research might delve into understanding the socio-economic factors influencing a community’s resistance to adopting digital currencies. On the other hand, quantitative research typically employs surveys, structured interviews, and measurements. It also involves reviewing records or documents to gather numeric or quantifiable data. Quantitative methods emphasize objective measurements and the statistical, mathematical, or numerical analysis of data collected through polls, questionnaires, and surveys. An example from economics might be a study analyzing the correlation between unemployment rates and economic recessions using historical data. Another economic example could be a quantitative analysis of the impact of interest rate changes on consumer spending patterns over a decade. Additionally, quantitative research can involve manipulating pre-existing statistical data using computational techniques. For instance, in economics, researchers might use computational models to predict the future growth rate of an economy based on various indicators. Quantitative research is not just a method but a way to learn about a specific group of people, known as a sample population. In the health sector, a quantitative study might examine the efficacy of a new drug on a sample population, measuring specific health outcomes and side effects. In economics, a study might evaluate the spending habits of millennials compared to baby boomers using structured surveys. Through scientific inquiry, it relies on data that are observed or measured to examine questions about this sample population. There are various designs under quantitative research, including Descriptive non-experimental, Quasi-experimental, and Experimental. The processes that underpin these two research types differ significantly. Qualitative research is characterized by its inductive approach, which aids in the formulation of theories or hypotheses. In contrast, quantitative research follows a deductive approach, aiming to test predefined concepts, constructs, and hypotheses that together form a theory. When considering the nature of the data, qualitative research is inherently subjective. It seeks to describe issues or conditions from the vantage point of those experiencing them. For example, in economics, a qualitative study might investigate the perceptions of small business owners towards global trade agreements. Quantitative research, however, is more objective. It focuses on observing the effects of a program on an issue or condition, with these observations subsequently interpreted by researchers. The type of data these methodologies yield is also distinct. Qualitative research is text-based, delving deep to provide rich information on a limited number of cases. Quantitative research, meanwhile, is number-based, offering a broader scope of information but spread across a larger number of cases, often sacrificing depth for breadth. In terms of response options, qualitative research tends to use unstructured or semi-structured options, allowing for more open-ended answers. Quantitative research, in contrast, relies on fixed response options, measurements, or observations. Furthermore, while qualitative research does not typically employ statistical tests in its analysis, quantitative research does, ensuring a more structured and numerical interpretation of data. Lastly, when it comes to generalizability, qualitative research findings are often less generalizable due to their in-depth focus on specific cases. Quantitative research, with its broader scope, tends to be more generalizable to larger populations. In summary, while both qualitative and quantitative research methodologies offer valuable insights, they differ in their methods, processes, nature of data, and generalizability, each serving unique purposes in the research landscape, as evidenced by their applications in fields like economics, social sciences, and health. In this book, we cover quantitative methods. https://libguides.usc.edu/writingguide/quantitative 3.2 Quantitative - Research methods : General perspective about research methods in health, economics, and social sciences: A researcher asks a good answerable question. It does not mean we can always find the answer right away with available data and methods. Yet we know that we can find an answer which will expand our knowledge of how the world works. A good answerable research question can be defined as a hypothesis which can be a phenomenon what we observe in the world. Also, hypothesis, that we want to prove, comes from theory as well. If the theory explains or predicts this is the specific hypothesis how the world works, then we should observe it with the data. To answer these questions, we collect or obtain data, then explore data. After making certain assumptions, we analyze the data. Then, we reach conclusions which can be associations, correlations, or causal relations. We use results to explain, extrapolate or predict! our hypothesis. We covered these concepts in detail in the introduction of this book. Different fields have dominant methods within their field to answer research questions. [The main source for health section is C. Manski, Patient Care under Uncertainty, Princeton University Press, 2019.] Health research use “Evidence Based Research!” Manski told in his seminal book “Research on treatment response and personalized risk assessment shares a common objective: Probabilistic prediction of some patient outcome conditional on specified patient attributes…Econometricians and Statisticians refer to conditional prediction as regression, a term in use since the nineteenth century. Some psychologists use the term actuarial prediction and statistical prediction. Computer scientists may refer to machine learning and artificial intelligence. Researchers in business school may speak of predictive analytics.” In most general way, after collecting and analyzing data, they present descriptive analysis seeks to understand associations. By various medical research methods, especially “Gold standard methods!”, when(if) they determine X causes Y. They propose treatment and surveillance. By using clinical trials, they want to determine treatment/surveillance and find its efficacy and effectiveness. Using Prescriptive analyses, they attempt to improve the performance of actual decision making. They try to find optimal solution between surveillance and aggressive treatment. Mainly, they use clinical judgment and evidence-based research. In general, statistical imprecision and identification problems affect empirical (evidence-based) research that uses sample data to predict population outcomes. There is a tension between the strength of assumptions and their credibility. The credibility of the inference decreases with the strength of the assumptions maintained. The most credible and acceptable method is The Gold Standard! In health. The “Gold Standard” Method for Health researchers is obtaining the effect of tested treatment comparing the results from trials and experiments which has treatment and control groups, and. In machine learning this is known as A/B testing, in economics it is random and field experiments. Even though this method is the most credible method in empirical research, it has problems like any other empirical methods. First, study/trial sample and actual patient population can be very different. Second, Due to small sample sizes, estimates and identification of treatment effects are imprecise. Third, it is wishful extrapolation to assume that treatment response in trials performed on volunteers is the same as what would occur in actual patient populations. Thus, predictions are fragile as they have limited data and do not handle uncertainty sufficiently. Most of health researcher give more value for the results obtained by a randomized trial with 200 observations than results from observational studies with 200,000 observations. Why do most of the medical and health researchers have this perspective? To justify trials performed on study populations that may differ substantially from patient populations, researchers in public health and the social sciences often cite Donald Campbell, who made a distinction between the internal and external validity of studies of the treatment response (Campbell and Stanley, 1963). A study has internal validity if it has credible findings for the study population (in-sample data in Machine Learning). It has external validity if an invariance assumption permits credible extrapolation (out-sample in ML). The appeal of randomized trials is their internal validity. Campbell argued that studies should be judged primarily by their internal validity and secondarily by their external validity. Since 1960s, this perspective has been used to argue for the primacy of experimental research over observational studies, whatever the study population may be. In contrast, observational studies which uses the representative sample of the population have more credibility in economics than randomized control trials with a small sample. The Campbell perspective has also been used to argue that the best observational studies are those that most closely approximate randomized experiments if they are done with representative samples. We should keep in mind that statistics and economics have the following perspective which is weird in other fields. For instance, “All models are wrong, but some are useful” (Box, 1976), and “Economists should not worry if their assumptions are wrong, as long as their conclusions are relatively accurate.” (Friedman, 1953) 3.3 Data and visualization Data and Visualization in Health, Economics, Business, and Social Sciences Researchers in fields such as health, economics, business, and social sciences primarily utilize three types of datasets: cross-sectional data, time series data, and panel data (also known as longitudinal data). Cross-sectional data is collected at a single point in time or over a short period from a sample of subjects, such as individuals, companies, or countries. This type of data is often used to describe the characteristics or attributes of a group at a specific point in time. Time series data, collected over an extended period at regular intervals, tracks changes in a particular variable over time. This data is useful for identifying trends and patterns over time and making predictions about future developments. Panel data, collected over an extended period from the same sample of subjects, tracks changes in a particular variable over time for each subject in the sample. This type of data is valuable for studying the relationship between different variables and identifying trends and patterns over time. When working with any type of data, it is crucial to follow the following steps: Examine the raw data: Understand the structure and content of the data. Clean and prepare the data: Check for errors, missing values, and outliers, and ensure that the data is in a usable format. Understand the data: Investigate the variables and their relationships through statistical analysis, visualizations, and other methods. Effective visualization can reveal data features that inform further analysis. Visualizing data is an essential step in the process of understanding and analyzing data in various fields. Effective visualization techniques, such as histograms, barplots, boxplots, scatterplots, and heatmaps, can provide valuable insights into patterns and trends within the data, guiding further analysis and decision-making. A histogram is a graphical representation that displays the frequency or distribution of a set of continuous or discrete data. It helps visualize the data and understand the underlying distribution. A histogram comprises a set of bins, which represent ranges of values, and the height of each bin indicates the frequency of data points within that range. For instance, a histogram displaying the weights of a group of animals might have bins representing weight ranges (e.g., 20-29 kg, 30-39 kg, etc.), and the height of each bin would indicate the number of animals within that weight range. A barplot is a graphical representation that illustrates the mean or median of a dataset. It serves to visualize the central tendency of the data and compare different groups or categories. A barplot comprises a set of bars, with the height of each bar representing the mean or median of the data for that group or category. For instance, a barplot might compare the average fuel efficiency of various car models or the median home prices in different neighborhoods. A boxplot is a graphical representation that displays the distribution of a dataset. It helps visualize the spread, skewness, and potential outliers in the data. A boxplot consists of a box representing the interquartile range (the middle 50% of the data), a line denoting the median, and “whiskers” extending from the box to the minimum and maximum values of the data. Boxplots are particularly useful for comparing the distributions of different groups or categories of data, such as the distribution of exam scores for students in different classes. A scatterplot is a graphical representation that exhibits the relationship between two variables. It enables the visualization of the relationship between the variables and identification of patterns and trends. A scatterplot consists of a set of points, with each point’s position representing the values of the two variables for that data point. For example, a scatterplot might demonstrate the relationship between advertising expenditures and sales revenue or between hours of study and test scores. A heatmap is a graphical representation that depicts the relationship between two or more variables. It aids in visualizing the relationship between the variables and identifying patterns and trends. A heatmap consists of a set of cells, with the color of each cell representing the value of the variables for that cell. Heatmaps are especially useful for visualizing data organized in a grid or matrix, such as the correlation between various stock prices or the frequency of crime incidents across different times and locations. 3.4 Correlation The phrase “correlation does not imply causation” is often used in discussions of statistical relationships, but what exactly does it mean? To answer this question, it is essential to understand the concept of correlation and its role in assessing the connection between two variables. Correlation is a statistical measure that quantifies the degree of association between two variables. It is often used to describe a linear relationship between the variables. The term “association” is broader than correlation, referring to any relationship between two variables, whether linear or not. In everyday life, we often observe correlations between events. Various industries, such as finance, healthcare, and marketing, frequently utilize correlation measurements to analyze data and make informed decisions. The word “correlation” can be broken down into “co,” meaning together, and “relation,” signifying a connection between two quantities. Correlation can be positive, negative, or uncorrelated. A positive correlation exists when two variables move in the same direction, i.e., an increase in one variable corresponds to an increase in the other. For example, if an increase in advertising expenditure leads to a rise in sales revenue, the variables are positively correlated. Conversely, a negative correlation occurs when two variables move in opposite directions, with an increase in one variable resulting in a decrease in the other. For instance, a negative correlation may exist between the number of hours spent watching television and academic performance, where increased television time leads to lower grades. Variables are considered uncorrelated when a change in one variable does not impact the other. Understanding how two variables are correlated enables us to predict future trends and discern the nature of their relationship or lack thereof. However, it is crucial to remember that correlation does not imply causation. A strong correlation between two variables does not necessarily mean that one causes the other. There may be confounding variables or other factors at play that influence the observed relationship. Therefore, it is essential to consider the context and perform additional analyses before drawing conclusions about causality from correlation alone. Correlation Analysis: Exploring Relationships Between Variables Correlation analysis is a statistical method used to study the association or absence of a relationship between two variables. By examining the correlation between variables, researchers can measure the strength and nature of their association. The primary goal of correlation analysis is to determine a numerical value that indicates the relationship between two variables and how they move together. When a change in one variable is accompanied by a change in another variable, whether direct or indirect, correlation analysis helps quantify the relationship between the two variables. One of the most popular correlation measures is the Pearson correlation coefficient, which quantifies the strength and direction of a linear relationship between two numerical variables (usually continuous, but not always). When people refer to “the correlation,” they usually mean the Pearson correlation coefficient. This coefficient ranges from -1 to 1, with -1 representing a strong negative relationship, 0 indicating no relationship, and 1 signifying a strong positive relationship. For instance, a Pearson correlation of 0.8 between two variables suggests a strong positive relationship, where an increase in one variable typically results in an increase in the other variable. Beyond the Pearson correlation, other correlation measures include partial correlation, conditional correlation, spatial correlation, and dynamic correlation. Partial correlation is a statistical method used to measure the relationship between two variables while controlling for the effects of one or more other variables. This technique isolates the relationship between the two variables of interest and examines how it is influenced by the other variables. For example, when examining the relationship between education level and income, partial correlation could be employed to control for the effects of variables such as age or work experience. Conditional correlation measures the relationship between two variables while accounting for the effects of one or more categorical variables. Similar to partial correlation, conditional correlation aims to understand the relationship between two variables of interest by considering the influence of other variables. Spatial correlation assesses the relationship between variables measured at different locations in space. This technique is particularly useful when analyzing data collected across geographical areas, such as climate, population, or economic indicators. Spatial correlation helps determine the extent to which observations close to one another in space are related. For instance, spatial correlation might be used to study the relationship between pollution levels and population density across various cities. Dynamic correlation measures the relationship between variables that change over time. This technique is especially valuable when analyzing data collected over extended periods, such as stock prices, economic indicators, or demographic trends. Dynamic correlation estimates the correlation between variables at different points in time and can help researchers understand how the relationship between variables evolves over time. In summary, correlation analysis is a powerful statistical tool for examining the relationships between variables, allowing researchers to measure the strength and nature of these associations. By employing various correlation measures such as Pearson correlation, partial correlation, conditional correlation, spatial correlation, and dynamic correlation, researchers can gain valuable insights into the complex relationships between variables in numerous fields. Correlation and Regression Correlation and regression are two statistical methods used to analyze the relationships between variables. While they share some similarities, their purposes and applications differ significantly. Similarities: Both correlation and regression are used to explore relationships between variables. They both assume a linear relationship between the variables in question. Both techniques involve the calculation of coefficients that quantify the relationships between variables. Key Differences: Correlation measures the strength and direction of the relationship between two variables, whereas regression models and predicts the value of one variable based on the value of another. Correlation is a descriptive statistical technique, providing a summary of the relationship between variables. In contrast, regression is a predictive statistical technique, employed to make predictions or explain the relationship between variables. Correlation typically analyzes relationships between two continuous variables, such as height and weight, or age and income. Regression can analyze relationships between two continuous variables but can also assess relationships between a continuous variable and a categorical variable, such as GPA and major. Correlation is quantified using a correlation coefficient, which ranges from -1 to 1. A coefficient of 0 indicates no relationship, while coefficients of 1 or -1 signify strong positive or negative relationships, respectively. Regression utilizes various statistics, including the coefficient of determination (R-squared), the standard error, and the p-value. In conclusion, while correlation and regression are related statistical techniques used to explore relationships between variables, they serve different purposes and applications. Correlation is a descriptive technique that measures the strength and direction of a relationship between two variables, while regression is a predictive technique that models and predicts the value of one variable based on another. Understanding these differences is essential when selecting the appropriate statistical method for data analysis. 3.5 Effect of X on Y / Regression Economists and social scientists predominantly utilize observational survey data to answer research questions or test hypotheses. By analyzing samples, they make inferences about the larger population. When conducting such analyses, it is crucial to address three key issues: How can factors other than x be allowed to influence y? What is the functional relationship between y and x? How can we ensure that we are capturing a ceteris paribus (all other things being equal) relationship between y and x? The simple linear regression model addresses these concerns effectively. While this model assumes a linear relationship between x and y, machine learning (ML) techniques do not assume a specific functional form and instead attempt to find the optimal functional form for the best predictive model. In contrast, economists and social scientists are more interested in interpreting the relationship between x and y rather than making predictions. Victor Chernozhukov’s presentation at the 2013 NBER Summer Institute demonstrated that Lasso, a linear method in ML, can better approximate the conditional expectation function (CEF) than ordinary least squares (OLS) for certain datasets. As Bruce Hansen notes in his recent book, “The OLS regression yields the best linear approximation of the conditional expectation function (CEF). This mathematical property makes regression a favorite tool among economists and social scientists, as it emphasizes the interpretation of an approximation of reality rather than complicated curve fitting.” The predictive power of x on y can be summarized using the conditional expectation function, E(y|xi). This function represents the expected value (population average) of y, given certain covariate x is held constant. It provides a useful representation of how y changes with x. The primary interest lies in the distribution of yi rather than predicting individual yi. The regression CEF theorem states that even if the CEF is nonlinear, regression provides the best linear approximation to it. OLS estimates the coefficient (β1) using a random sample of data to represent the population and makes assumptions about the relationships between the error term (u) and x. These assumptions include the expected value of the error term being zero in the population, mean independence of the error term, and the zero conditional mean assumption. In summary, simple linear regression and machine learning methods offer different approaches to understanding the relationship between x and y in observational survey data. While regression emphasizes interpretation and provides the best linear approximation to the CEF, machine learning techniques focus on finding the best functional form for predictive modeling. Both methods have their merits and can provide valuable insights for economists and social scientists. 3.5.1 How can we estimate the population parameters, \\(\\beta_{0}\\) and \\(\\beta_{1}\\)? We find a random sample, which represents the population. We plug observations into the population equation, and use two assumptions \\[ E(u)=0 , Cov(x,u)=0\\] We estimate \\[y_{i}=\\hat{\\beta_{0}}+\\Sigma_{1}^{k}\\hat{\\beta_{k}}x_{i}+u_{i}\\]. i.e we minimize sum of squared residuals by solving the following linear regression equation algebraically. This is also known as ordinary least squares(OLS). \\[ \\sum_{i=1}^{n}\\hat{u}_{i}^{2}=\\underset{\\hat{\\beta_{0}}, \\hat{\\beta_{1}}..\\hat{\\beta_{k}}}{\\operatorname{argmin}} \\sum_{i=1}^{n}\\left(y_{i}-\\left(\\hat{\\beta_{0}}+\\hat{\\beta_{1}} x_{i}+\\hat{\\beta_{2}} x_{i}+...+\\hat{\\beta_{k}} x_{i}\\right)\\right)^{2} \\] \\[y=X^{&#39;}\\beta+u\\], where \\[u_{i}\\sim(0, \\sigma^2)\\] then \\[\\hat{\\beta}= (X^{&#39;}X)^{-1}X^{&#39;}y\\] follows from \\[1/n \\sum [x_{i}(y_{i} - x_{i}^{&#39;}\\hat{\\beta})] = 0\\] 3.5.2 Predicting \\(y\\) For any candidates \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\), define a fitted value for each \\(i\\) as \\[ \\hat{y}_{i}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{i} \\] We have \\(n\\) of these. \\(\\hat{y}_i\\) is the value we predict for \\(y_{i}\\) given that \\(x=x_{i}\\) and \\(\\beta=\\hat{\\beta}\\). The “mistake” from our prediction is called the residual: \\[ \\hat{u}_{i}=y_{i}-\\hat{y}_{i} =y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i} \\] NOTE: Econometric applications typically are not interested in forecasting yi but rather in understanding relationship between \\(y_{i}\\) and some elements of \\(x_{i}\\) with other factors held fixed (ceteris paribus) We estimate the coefficients by minimizing sum of squared residuals in OLS. Different samples will generate different estimates \\((β^1)\\) for the true \\(β_{1}\\) which makes \\(β_{1}\\) a random variable. But never forget that β^1 is an estimator of that causal! parameter obtained with a specific sample from the population. We say the effect of x on y is β1 as long as it is unbiased. We also want our estimates to be consistent, and satisfy asymptotic normality. Unbiasedness is the idea that if we could take as many random samples on Y as we want from the population, and compute an estimate each time, the average of these estimates would be equal to β1. OLS is unbiased under the following assumptions. These assumptions are the population model can be Linear in Parameters, Random Sampling, Sample Variation in the Explanatory Variable, Zero Conditional Mean, Homoskedasticity - Constant Variance (or heteroskedasticity).This tells us that, on average, the estimates will be equal the population values. Consistency states that if one uses a larger sample size, one reduces the likelihood of obtaining an estimate \\(\\hat{\\beta}\\) that deviates substantially from the true effect \\(β\\). Thus, consistency implies that, as a sample size approaches the entire population, an estimate \\(\\hat{\\beta}\\) is more and more likely to reflect the true estimate \\(β\\). This holds true, for example, when \\(β\\) is unbiased and its variance decreases as the sample size increases. Assuming that many samples are drawn randomly, and \\(β\\) is estimated in each sample, asymptotic normality implies that, given the very large size of each sample, the pooled estimate \\(\\hat{\\beta}\\) obtained from the samples follows a normal distribution. Having this property makes it possible to approximate the distribution of an estimate across many samples well, even if we have only one sample with a sufficient number of observations rather than many samples. In statistical inference, it is very important to understand an estimate’s distribution. Be aware, most of the time, obtaining unbiased \\(β\\) is very hard (even impossible as population DGM is unknown!). Errors are not iid. For instance, maybe observations between units in a group are related to each other (clustered), non-representative sample, Exclusion or inclusion of variables, Measurement error, Endogeneity, Reverse causality, Missing observations. Thus, most of the time, we can only find associations, or correlations. In this situation, we can only say what is the relationship, association, or correlation between y and x. 3.5.3 MLE Maximum Likelihood Estimation (MLE) is a statistical method used to estimate parameters of a given statistical model based on the observed data. MLE aims to find the parameter values that maximize the likelihood of the observed data under the assumed probability distribution. Unlike Ordinary Least Squares (OLS), which focuses on minimizing the squared differences between the observed and predicted values, MLE seeks to find the parameter values that make the observed data most likely. In MLE, the main objective is to estimate the unknown parameters of a given statistical model by maximizing the likelihood function. The likelihood function measures the probability of observing the data given the parameters of the model. By maximizing the likelihood function, we find the parameter values that best explain the observed data under the assumed distribution. MLE has several advantages over other estimation methods, such as OLS, including: Consistency: As the sample size increases, MLE estimates tend to converge to the true parameter values, assuming that the model is correctly specified. This property ensures that MLE provides reliable estimates as more data becomes available. Asymptotic Normality: Under certain regularity conditions, the MLE estimates are asymptotically normally distributed. This property allows for the construction of confidence intervals and hypothesis testing using standard statistical techniques. Efficiency: MLE estimates are asymptotically efficient, meaning they achieve the lowest possible variance among all unbiased estimators. This property ensures that MLE provides the most precise estimates given the available data. Invariance: MLE estimates are invariant under transformations, which means that if we apply a transformation to the parameter space, the MLE estimate of the transformed parameter will be the same as the transformation of the MLE estimate of the original parameter. However, MLE also has some limitations, such as: Dependence on distribution assumptions: MLE requires the assumption of a specific probability distribution for the data. If the assumed distribution is incorrect, the MLE estimates may be biased or inconsistent. Sensitivity to outliers: MLE estimates can be sensitive to outliers in the data, as they aim to maximize the likelihood of the entire dataset. This sensitivity can lead to biased estimates if the data contains extreme values. Computational complexity: The maximization of the likelihood function can be computationally intensive, especially for large datasets and complex models. In summary, Maximum Likelihood Estimation (MLE) is a powerful statistical method for estimating the parameters of a given statistical model based on the observed data. It provides a flexible and efficient approach to parameter estimation under the assumption of a specific probability distribution. However, it is essential to consider the limitations of MLE when applying it to real-world data and carefully assess the distributional assumptions and the presence of outliers in the data. 3.6 Causal Effect You have most likely heard the term “Correlation is not causation”, which means, loosely that “just because two things happen together, doesn’t mean that one of them caused the other.” A better term is “correlation is not sufficient for causation.”There are some things we can do to make causal inference possible, but they happen before the sample is taken. This is a big change from most of the statistics you learn. Usually, you’re given numbers from some random sample, and you don’t have any control over that sample. You just have to take the numbers and make sense of them. In the design phase, you decide how treatments are going to be assigned to sample units/patients. If you can’t directly assign treatments, you need to collect data about covariates. “You may have heard the phrase”correlation does not equal causation,” which means that just because two things are related or happen together does not necessarily mean that one of them caused the other. A more accurate statement would be “correlation is not enough to establish causation.” To make causal inferences, there are certain steps that must be taken before collecting data, such as designing how treatments will be assigned to sample units or patients. In contrast, statistical analysis typically involves working with data that has already been collected, and the focus is on understanding and interpreting the relationships between variables within the given data set. It is important to keep in mind that correlation does not necessarily imply causation, and other factors may be at play when trying to understand the relationships between variables.” Most of the time though, we also want to find out not only the relationship or correlation between observations but also the reason behind it. Using this information, we are able to take action to alter the relationships and causes. Essentially, we want to know what the consequences are of not doing one thing or the other. Our goal is to understand the effects and consequences of specific actions, policies, and decisions. In order to develop a hypothesis about such causal effects, we rely upon previous observations, personal experiences, and other information. Researchers in economics, health, and social sciences analyze empirical data to evaluate policies, actions, and choices based on quantitative methods. When we want to say something about why? We work with potential outcomes Potential Outcomes (Neyman, 1923 - Rubin, 1975) framework, especially in applied economics, or directed acyclic graphs (DAG) which you can think presenting a chain of causal effects with a graph. We will review causal analysis as short as possible, and considering the approaches integrating the machine learning with causal analysis. The books we can recommend in this topic are Causal Inference-The Mixtape, The Effect, Mostly Harmless Econometrics,… At the center of interest is the causal effect of some intervention or treatment on an outcome of interest by inferring what the outcome would be in the presence and in the absence of a specific treatment. Causality is tied to a unit (person, firm, classroom, city) exposed to an action (manipulation, treatment or intervention) at a particular point in time. The Causal Effect is the comparison or difference of potential outcomes,\\(Y_{i}(1)\\) and \\(Y_{i}(0)\\), for the same unit, at the same moment in time post-treatment. where \\(Y_{i}(1)\\) is the outcome when unit \\(i\\) exposed the treatment (active treatment state), and \\(Y_{i}(0)\\) is the outcome when same unit \\(i\\) has not exposed the treatment (control state) (at the same point in time) Let’s say \\(D\\) is the treatment indicator (or intervention, or policy). When \\(D=1\\) , the unit receives the treatment or participates in the intervention, thus these units constitute “treatment group”. When \\(D=0\\) , the unit does not receive treatment or does not participate in the intervention, thus these units constitute “control group”. The causal effect of treatment,\\(D\\), on outcome,\\(Y\\), is the difference between the potential outcomes \\(Y_{i}(1)- Y_{i}(0)\\) for unit \\(i\\). However, we can not observe the unit in 2 different state at the same time. We can not observe “The Road Not Taken” (by Robert Frost). “The fundamental problem of causal inference” is therefore the problem that at most one of the potential outcomes can be realized and thus observed (Holland, 1986). Thus, Holland (1986, 2003) says “No causation without manipulation” Keep in mind that counter-factual state is and never will be observable. We can define counterfactual as what would have happened in the absence of a policy/treatment. Donald Rubin has been known to say that “causal inference is a missing data problem” (Ding and Li, 2018) Hence, there are several methods used to find causal effect. Experiments (Randomized Control Trials) and Quasi-natural Experiments such as Regression Discontinuity (Sharp, Fuzzy), Instrumental Variable, Difference-in-Difference(-in-Difference), Synthetic Cohort, Propensity Score, and Partial identification. We can never know the real! causal effect (of a unit) in social sciences. Using Rubin’s framework, we can only estimate the causal effect under certain assumptions. Overcoming the missing data problem arising from the fact that only one state of nature is realized is very difficult. To do so requires credible assumptions! Main implicit assumption in Rubin framework for all the aforementioned methods is the Stable Unit Treatment Value Assumption (SUTVA, Rubin 1978). SUTVA implies that potential outcomes of observation \\(i\\) are independent of the treatment assignment of all other units. In another word, the unit’s potential outcome are not affected by the spillover or interference effects by the treatment of other units. Thus, SUTVA rules out general equilibrium or indirect effects via spillovers. Moreover, SUTVA also implies that the treatment, \\(D\\), is identical for all observations and no variation in treatment intensity. Most of the current empirical work assumed this assumption satisfied. However, it may not be always plausible to assume no spillover effect when treated group is very large share of the population. We also need to reassess our policy proposal, which are based on the findings of randomized or natural experiments, for all population as they may violate SUTVA assumption when these studies are designed with small sample. 3.6.1 Average Treatment Effect(ATE) While causal effects cannot be observed and measured at a unit level, under certain statistical assumptions we may identify causal effect of treatment,D, on outcome,Y, at an aggregate level, while using treatment and control groups. Even though we cannot observe treatment effect for each individual/unit separately, we know that for a given population these individual causal effects-if exist- will generate a distribution of treatment effects. [footnote: A distribution is simply a collection of data, or scores, on a variable. Usually, these scores are arranged in order from smallest to largest and then they can be presented graphically. — Page 6, Statistics in Plain English, Third Edition, 2010.]Thus, we can estimate the mean, variance and other parameters related to this distribution. Most common parameter we want to estimate is the mean of treatment distribution. We can think the average treatment effect(ATE) is the population average of all individual treatment effects. We can formally write: \\[ \\delta^{ATE} = E[\\delta_{i}]= E[Y_{i}(1) - Y_{i}(0)] = E[Y_{i}(1)] - E[Y_{i}(0)] \\] As known, the expected value of a random variable X is often denoted by E(X). Assume government implement a policy, an agency act, or a doctor prescribe a pill. All in all, we can think 2 states. Treatment state in which every units in the population exposed a treatment, and control state in which every units in the population has not exposed the treatment . The equation above shows that the average of the outcome for everyone in treatment state and the average of the outcome for everyone in control state is called the average treatment effect for all population. Depends on the question, we may want to estimate different treatment effects as well. Some of them are: Average Treatment Effect on the Treated (ATT, or ATET): The average treatment effect on the treatment group is equal to the average treatment effect conditional on being a treatment group member (D=1). \\[ ATT= E[Y_{i}(1)|D=1] - E[Y_{i}(0) | D=1] \\] Average Treatment Effect on the Untreated (ATU): The average treatment effect on the control group is equal to the average treatment effect conditional on being untreated. \\[ ATU= E[Y_{i}(1)|D=0] - E[Y_{i}(0) | D=0] \\] However,we want to emphasize ATE (ATT, and ATU) is unknowable because of the fundamental problem of causal inference.i.e. we can only observe individuals either when they receive treatment state or when they do not, thus we cannot calculate the average treatment effect. However, we can estimate it. How? If we can find a population or split the population such as some of whom receive this treatment or act on it and some of whom has not. Then we can estimate “causal/treatment effect” as the difference between average outcome of the treated group and average outcome of control group. How does the population split into treatment and control group? Whether units have any choice to be eligible for the treatment or not? Whether splitting process has any direct effect on outcome or not? How large, how many and how similar similar these groups? Whether treatment level is equal or not? Whether everyone who are eligible for treatment receive treatment or not? Answers of all these questions require different identification strategies and leads all the different causal methods we use. 3.6.2 Additional Treatment Effects yesil pdf fileindan Local Average Treatment Effect (LATE): g = E[dijCompliers]The Local Average Treatment Effect (LATE) is a statistical measure that is used to estimate the effect of a treatment or intervention on a specific subgroup of the population. It is a measure of the average treatment effect for a group of individuals who would not have received the treatment if they had not been eligible for it. LATE is typically estimated using a technique called instrumental variables (IV) regression, which involves using a variable that is correlated with the treatment but is not directly affected by the outcome of interest. By using this “instrumental” variable, researchers can estimate the effect of the treatment on the subgroup of individuals who are eligible for the treatment based on their values of the instrumental variable. LATE is a useful measure when the treatment effect is not the same for all individuals in the population. For example, a study might estimate the LATE of a new medication on individuals with a specific type of disease, in order to understand how the medication affects this subgroup compared to the overall population. It is important to note that LATE is only applicable when the treatment is randomly assigned, meaning that individuals are assigned to receive the treatment or not receive the treatment based on chance rather than their characteristics or other factors. This is necessary in order to ensure that the treatment effect can be accurately estimated and is not confounded by other factors. Conditional Average Treatment Effect (CATE) d(x) = E[Yi(1)−Yi(0)jXi = x] = E[dijXi = x] • Xi exogenous pre-treatment covariates/features • Xi includes not only confounders but also other covariates which are potentially responsible for effect heterogeneity • CATEs are often called individualised or personalised treatment effects • CATEs can differ from CATET, r(x), and CLATE, g(x) The Conditional Average Treatment Effect (CATE) is a statistical measure that is used to estimate the effect of a treatment or intervention on a specific subgroup of the population. It is a measure of the average treatment effect for a group of individuals who are similar in some way, such as having the same level of a certain characteristic or risk factor. CATE is typically estimated using a technique called conditional mean regression, which involves estimating the expected value of the outcome variable for individuals with different values of the conditioning variable. By using this conditioning variable, researchers can estimate the effect of the treatment on the subgroup of individuals who are similar in terms of the conditioning variable, compared to the overall population. CATE is a useful measure when the treatment effect is not the same for all individuals in the population and when there is a characteristic or factor that may influence the treatment effect. For example, a study might estimate the CATE of a new medication on individuals with a specific type of disease, in order to understand how the medication affects this subgroup compared to individuals with a different type of disease. It is important to note that CATE is only applicable when the treatment is randomly assigned, meaning that individuals are assigned to receive the treatment or not receive the treatment based on chance rather than their characteristics or other factors. This is necessary in order to ensure that the treatment effect can be accurately estimated and is not confounded by other factors. Group Average Treatment Effects (GATEs): d(g) = E[d(x)jGi = g] where the groups g can be defined based on exogenous or endogenous variables Group Average Treatment Effects (GATE) is a statistical measure that is used to estimate the effect of a treatment or intervention on a specific subgroup of the population. It is a measure of the average treatment effect for a group of individuals who are similar in some way, such as having the same level of a certain characteristic or risk factor. GATE is typically estimated using a technique called group mean regression, which involves estimating the mean of the outcome variable for individuals with different values of the grouping variable. By using this grouping variable, researchers can estimate the effect of the treatment on the subgroup of individuals who are similar in terms of the grouping variable, compared to the overall population. GATE is a useful measure when the treatment effect is not the same for all individuals in the population and when there is a characteristic or factor that may influence the treatment effect. For example, a study might estimate the GATE of a new medication on individuals with a specific type of disease, in order to understand how the medication affects this subgroup compared to individuals with a different type of disease. It is important to note that GATE is only applicable when the treatment is randomly assigned, meaning that individuals are assigned to receive the treatment or not receive the treatment based on chance rather than their characteristics or other factors. This is necessary in order to ensure that the treatment effect can be accurately estimated and is not confounded by other factors. 3.6.3 Selection Bias and Heteregeneous Treatment Effect Bias: When a group of individuals receive a treatment and a group does not, most inclined to calculate the treatment effect just calculating the simple difference between average outcomes of treated group and control group. However, this is (nearly) always wrong. Can we find average treatment effect by calculating the simple difference between the average outcome for the treatment group and the average outcome for the control group? There may be already intrinsic differences between these 2 groups as some already decided to choose treatment, or there may be differences with some other characteristics that will already effect outcome directly or through another path. Hence, all of these will be included in the simple difference between average of outcome of these 2 groups. That “misassigned” effect is called as treatment selection bias. We can think the selection bias as the difference between a treatment group and a control group if there was no treatment at all. We want to emphasize that we may not observe this difference, we may not verify that. However, the main purpose of all causal inference methods is to eliminate as much as possible this bias by imposing different identifying assumptions. Heteregeneous treatment effect bias always exist if we want to calculate ATE. However, when we assume that treatment effects -dosage effect- are constant then this bias disappears. Even though this is a strong assumption, this is very common and plausible in social sciences and economics as we want to analyze average effects, not individual effect. That average treatment/causal effect is presented either treatment effect for average person or “homogeneous” average treatment effect for everyone. However, heterogeneous treatment effect and dealing its bias is one of the major topic in which machine learning methods are contributing recently. Decomposition of difference in means \\[ \\begin{eqnarray*} \\underbrace{E[Y_{i}(1) | D=1] - E[Y_{i}(0) | D=0]}_{{\\text{Simple Difference in Outcomes}}}&amp;=&amp; \\underbrace{E[Y_{i}(1)] - E[Y_{i}(0)]}_{{\\text{Average Treatment Effect}}} \\\\ &amp;&amp;+ \\underbrace{E[Y_{i}(0)|D=1] - E[Y_{i}(0) | D=0]}_{{\\text{Selection bias}}} \\\\ &amp;&amp; + \\underbrace{(1-\\pi)(ATT - ATU)}_{{\\text{Heterogenous treatment effect bias}}} \\end{eqnarray*} \\] where \\((1-\\pi)\\) is the share of the population in the control group.(Detailed derivation of this equation in Mixtape page 131-133) As we mentioned the simple difference between the average outcome for the treatment group and the average outcome for the control group can be assumed by most as an average treatment effect. It may be true only if we do not have selection and heterogeneous treatment bias. However, most of the time already the difference exist between a treatment group and a control group before treatment implemented. Thus selection bias exists. Most of the time the treatment effects individuals as well as groups differentially. Thus, the average effect of treatment for the group consist from treated individuals and for the group consist from untreated individuals differ. The multiplication of that difference and the share of the population in the control group is called as Heterogenous treatment effect bias. As previously noted, we are unable to directly observe individuals in both treatment and control states, making it impossible to explicitly calculate treatment effects and associated biases. Social scientists have been devising strategies to address these biases and estimate treatment effects, with machine learning methods contributing to these advancements in recent years. The various methodologies can be categorized as follows: Regression, penalized regression, and fixed effects Matching and propensity score methods Randomization inference Instrumental variables, difference-in-differences, regression discontinuity, and event studies Synthetic control method Causal forest method In the upcoming chapters, we will delve into these approaches and explore the relevant machine learning techniques that complement and enhance these methods in estimating treatment effects. "],["learning.html", "Chapter 4 Learning Learning Systems", " Chapter 4 Learning Imagine a child named Alex who occasionally gets sick after eating but doesn’t know which specific food is the culprit. He grappled with recurring allergic reactions. Driven by a pressing need to pinpoint their root cause, he embarked on a personal mission. Eager to understand the extent of his susceptibility, Alex sought patterns and clues. Data Collection: Each time Alex eats, he makes a list of everything he’s consumed that day. He also records how he felt afterward, specifically noting any days he felt ill. Pattern Recognition: After several weeks, Alex starts to see a pattern. Every time he ate dishes containing garlic, he felt sick within a few hours. However, on days he avoided garlic, he generally felt fine. Making Predictions: With this new insight, Alex hypothesizes that garlic might be the cause of his discomfort. To test this theory, he deliberately avoids garlic for a few days and notes his health. Conversely, on another day, with all other foods being consistent, he consumes garlic to see if the reaction recurs. Validation: During the days without garlic, Alex feels perfectly fine. However, after the day he reintroduces garlic, the familiar sickness returns. This strengthens Alex’s belief in the connection between garlic and his adverse reactions. Updating the Model: Wanting to be thorough, Alex decides to test other ingredients, wondering if the allergy might extend beyond garlic. After trying onions and shallots on different days and noticing no adverse reactions, Alex concludes that his allergy seems specific to garlic. In this example, Alex is acting like a basic machine learning model. Collect data (observations of the food). Recognize patterns in the data. Make informed predictions based on identified patterns. Validate predictions against actual occurrences. Adjust the predictive model considering new or contradictory data. While Alex’s learning process strongly suggests that garlic triggers his symptoms, it’s important to recognize the limitations of his informal “model.” Just as in any learning model, prediction errors could arise from multiple factors. For instance, there might be times when he consumes garlic but doesn’t get sick because of variations in the quantity consumed, or the form in which it’s ingested (raw versus cooked). There could also be external factors, like the combination of garlic with other foods, that influence his reaction. It’s also possible that, on some days, other confounding variables like stress or a different underlying illness might mask or exaggerate his garlic-induced symptoms. Thus, while Alex feels confident in his findings, he understands that real-life scenarios can introduce unpredictability, making it essential to continually refine and reassess his conclusions. This analogy showcases the parallels between Alex’s learning process and that of machines. In a manner mirroring machine learning’s approach to refining predictions with data, Alex has learned his very rare allergy. While he might not grasp the precise mechanisms or reasons behind his garlic allergy, and there could be some prediction errors, he has effectively employed a method reminiscent of machine learning. The insights he’s gained are invaluable, allowing him to make informed decisions about his diet in the future. Machine Learning comes more from the computer science field. While machine learning also wants to understand and interpret data similar to statistical learning as we talked in previous chapter, its main goal often leans more towards making accurate predictions or decisions. It might not always need to understand why something happens as long as it can predict it accurately. Think of it as an engineer who builds a tool that works efficiently, even if they don’t fully grasp the science behind every component. So, why do some people call machine learning “statistical learning”? Because many machine learning techniques are grounded in statistical methods. As both fields evolved, they started to overlap more and more. In many cases, the boundary between them has become blurry. For a layperson, you might think of statistical learning as a close cousin to machine learning. They both aim to understand and learn from data, but they might have slightly different priorities or approaches. At its heart, machine learning is a method of teaching computers to make decisions or predictions based on data, rather than being explicitly programmed to do so. In machine learning, the main aim is to create a model with specific settings or parameters. This model should be good at making predictions on new, unseen data. Later in this book, we’ll explore the three key steps in machine learning: prediction, training, and hyperparameter tuning. Everything in machine learning starts with data. This could be photos, texts, economic indicators, political polling data, public health records, or employment statistics. This data is then organized and used as training data for the machine learning model. Generally, the more data we have, the better the model will be. Once we have the data ready, the next steps are clear: First, there’s the Prediction or Inference Phase. This stage unfolds when a predictor, already trained, is employed on test data it hasn’t encountered before. By this time, the model and its settings are already decided, and we’re just using it to make predictions on new data points. Next is the Training or Parameter Estimation Phase. This is when we refine our predictive model using training data. Broadly, there are two strategies to pinpoint robust predictors given our data. One strategy is searching for the best predictor based on a designated measure of quality, often referred to as identifying a point estimate. The other is Bayesian inference outscope of this book. Irrespective of the strategy, the goal remains the same: using numerical methods to find parameters that align or “fit” our data. Last, we have the Hyperparameter Tuning or Model Selection Phase. Here, we’re trying to pick the optimal model and its corresponding hyperparameters on how well they do on a test or validation data. The main goal is to find a model that not only works well on our training data but will also make good predictions on new data it hasn’t seen before. After gathering and preparing the data, we select a model, train it, evaluate its performance, and fine-tune its parameters. Once optimized, the model is then employed to make predictions. In this section, we’ve touched upon the concepts of prediction, training, and hyperparameter tuning. However, rest assured, in future chapters, we will delve into these topics in comprehensive detail. Learning systems in machine learning refer to algorithms and models that improve their performance or adapt to new data over time without being explicitly programmed for each specific task. These systems “learn” from data, identifying patterns or making decisions based on input. In the realm of machine learning, systems are designed for various purposes and roles. Together, these roles underscore what machine learning models strive to accomplish, each offering insights and recommendations suited to particular situations and requirements. The Descriptive role is foundational. True to its name, it’s centered around understanding and articulating the information encapsulated within the data. This is particularly useful for understanding historical trends and patterns. Next is the Predictive role. This goes a step beyond by not just reflecting on past events but by forecasting the future one. By analyzing existing data, the system forms informed predictions about upcoming occurrences. Lastly, the Prescriptive role stands out. It’s arguably the most forward-looking of the trio. Rather than merely explaining or projecting, it escalates its function by recommending specific courses of action derived from its data analysis. For instance, industrial economists using machine learning might recommend optimal pricing strategies, suggest resource allocation, and guide firms on ideal hiring practices by analyzing consumer demand, competitor prices, product line performance, employment trends, and skill set data. Moreover, based on diverse data such as patient outcomes, resource availability, population health trends, drug efficacy, and claim histories, health economists can use machine learning to recommend cost-effective treatment paths, optimize resource allocation in hospitals, suggest preventive interventions for high-risk groups, advise on pharmaceutical pricing, and offer insights into health insurance premium setting. In the next subsection, we’ll explore step by step how to find and train models, and how these models generalize well to unseen data. Learning Systems A Machine Learning System is a dynamic framework often seen in various Machine Learning projects. This system generally unfolds in an iterative manner, encompassing several key phases. It all begins with Data Collection, where datasets are not only created but also maintained and updated. This phase is crucial as the quality and relevance of data significantly influence the outcomes. Following this, the system dives into the Experimentation phase. During this stage, the data is thoroughly explored, various hypotheses about the data and potential models are formulated, tested, and validated, resulting in the construction of both training and prediction pipelines. With a robust model at hand, the next step is Deployment, which entails integrating this model into a tangible, working product. But the journey doesn’t conclude here. The Operations phase ensures that the deployed model is under constant surveillance, ensuring it remains current and in tune with the ever-evolving environment. Together, these phases epitomize the core mechanics of a Machine Learning System, emphasizing continuous learning and adaptation. Machine learning systems are algorithms and models that adapt to new data over time, improving their performance without specific programming for every task. These systems “learn” from data, identifying patterns or making decisions based on given input. In this book, our primary focus will be on machine learning models, which are integral components of learning systems. A machine learning model combines programming code with data. How do we find, train, and make sure our machine learning models work well on unseen data? Here’s a step-by-step guide: Splitting the Data: We take our whole dataset and split it into two parts: one for training our model, training data, and one for testing its performance, testing data. Use Training Data Wisely: Most of the time, we use 80% of our total data for training. But even within this training data, we divide it further: estimation data and validation data. Keeping Testing Data Untouched: We don’t use the testing data to make any decision that lead to the selection of the model. It’s set aside only to see how well our finished model works. Choose Possible Models: Before we start training, we decide on a few potential models we might want to use. For parametric models: We assume a form of the model up to but not including the model parameters. For nonparametric models: We pick feature variables to be used and possible values of any tuning parameters. Training the Model: We take each potential model and train(fit) it using the estimation data which is usually bigger chunk of our training data. Check and Compare Models: Once trained, we see how well each model does using validation data, usually the smaller chunk of training data it hasn’t seen before. This performance is all about how good the model is at predicting on the validation data which was not used to train the models. Pick the Best Model: Based on how well they did on the validation performance, we choose the best model. Final Training: With our best model in hand, we then use all the training data, both estimation and validation data together, to give it one last thorough training. Test: Finally, we use our untouched test data to estimate model performance and see how well the best model does. Note that we are using the validation data to select a model, while the testing data is used to estimate model performance. Above, we outlined the machine learning process using plain language for clarity. Below, we’ll delve into these steps using more technical terms commonly accepted in the field. If certain concepts seem unfamiliar, rest assured, we’ll unpack each one in greater detail in the upcoming chapters. The learner has a sample of observations. This is an arbitrary (random) set of objects or instances each of which has a set of features (\\(\\mathbf{X}\\) - features vector) and labels/outcomes (\\(y\\)). We call this sequence of pairs as a training set: \\(S=\\left(\\left(\\mathbf{X}_{1}, y_{1}\\right) \\ldots\\left(\\mathbf{X}_{m}, y_{m}\\right)\\right)\\). We ask the learner to produce a prediction rule (a predictor or a classifier model), so that we can use it to predict the outcome of new domain points (observations/instances). We assume that the training dataset \\(S\\) is generated by a data-generating model (DGM) or some “correct” labeling function, \\(f(x)\\). The learner does not know about \\(f(x)\\). In fact, we ask the learner to discover it. The learner will come up with a prediction rule, \\(\\hat{f}(x)\\), by using \\(S\\), which will be different than \\(f(x)\\). Hence, we can measure the learning system’s performance by a loss function: \\(L_{(S, f)}(\\hat{f})\\), which is a kind of function that defines the difference between \\(\\hat{f}(x)\\) and \\(f(x)\\). This is also called as the generalization error or the risk. The goal of the algorithm is to find \\(\\hat{f}(x)\\) that minimizes the error with respect to the unknown \\(f(x)\\). The key point here is that, since the learner does not know \\(f(x)\\), it cannot calculate the loss function. However, it calculates the training error also called as the empirical error or the empirical risk, which is a function that defines the difference between \\(\\hat{f}(x)\\) and \\(y_i\\). Hence, the learning process can be defined as coming up with a predictor \\(\\hat{f}(x)\\) that minimizes the empirical error. This process is called Empirical Risk Minimization (ERM). Now the question becomes what sort of conditions would lead to bad or good ERM? If we use the training data (in-sample data points) to minimize the empirical risk, the process can lead to \\(L_{(S, f)}(\\hat{f}) = 0\\). This problem is called overfitting and the only way to rectify it is to restrict the number of features in the learning model. The common way to do this is to “train” the model over a subsection of the data (“seen” or in-sample data points) and apply ERM by using the test data (“unseen” or out-sample data points). Since this process restrict the learning model by limiting the number of features in it, this procedure is also called inductive bias in the process of learning. There are always two “universes” in a statistical analysis: the population and the sample. The population is usually unknown or inaccessible to us. We consider the sample as a random subset of the population. Whatever the statistical analysis we apply almost always uses that sample dataset, which could be very large or very small. Although the sample we have is randomly drawn from the population, it may not always be representative of the population. There is always some risk that the sampled data happens to be very unrepresentative of the population. Intuitively, the sample is a window through which we have partial information about the population. We use the sample to estimate an unknown parameter of the population, which is the main task of inferential statistics. Or, we use the sample to develop a prediction rule to predict unknown population outcomes. When we have a numeric outcome (non-binary), the lost function, which can be expressed as the mean squared error (MSE), assesses the quality of a predictor or an estimator. Note that we call \\(\\hat{f}(x)\\) as a predictor or estimator. Can we use an estimator as a predictor? Could a “good” estimator also be a “good” predictor. We had some simulations in the previous chapter showing that the best estimator could be the worst predictor. Why? In this section we will try to delve deeper into these questions to find answers. The starting point will be to define these two different but similar processes. "],["error.html", "Chapter 5 Error 5.1 Estimation error - MSE 5.2 Prediction error- MSPE 5.3 Technical points about MSE and MSPE", " Chapter 5 Error Lets assume you want to find an average years of schooling of all the people who reside in your city. In another words, you want to find mean of years of schooling of the population in your city. Most of the time gathering this information is very hard. One solution is waiting in main street and ask everyone who passes that day from there. You can say you collect data which is called the sample of the population. With this sample you can estimate this unknown parameter which is average years of schooling of the population. You need to come with a general rule about how to calculate the unknown parameter. This general rule is called estimator. You use your specific sample, i.e. realized data, to obtain specific number which is called estimate. You can collect different samples. In that case, the estimator will be same but the estimate will vary from sample to sample.Ideally you want the estimator from your sample will be equal to the real (unknown) parameter. As you will never know the real parameter, we use statistical properties to assume that your estimate is that parameter. Main assumption/requirement is you want a representative sample and to find unbiased estimator. However, you may have infinite number of unbiased estimators. Which one is the best unbiased estimator? There will be always some difference between the estimated value and the actual value of population characteristics. That difference is called as error. Specifically, this is called estimation error as this error is related with an estimation of a parameter. With the data in your hand, you may want to predict the specific individual or groups years of schooling. Using predictive methods, you will obtain a specific value. But that value will contain an error. In that case, this is called prediction error as this is related with a prediction of an outcome. With the advent of statistical/machine learning techniques, people are talking a lot about prediction error, while in classical statistics, one is focusing on parameter estimation error. suppose you have X , there is error when you want to find mean X parameter error, estimation error for coefficients prediction error is related with function irreducible and reducible error training error vs test data error (for prediction functions) measurement error ? different ways to find/min. errors. median, etc. error in classification, error in regression in ml below we will discuss estimation error and prediction error Reminder: Assuming a true linear model \\(y=X \\beta_0+\\varepsilon\\), estimate \\(\\hat{\\beta}\\) and prediction \\(\\hat{y}=X \\hat{\\beta}\\). One can define, with \\(\\|\\).\\(\\|\\) the mean square error norm for example: Estimation error: \\(\\|\\beta-\\hat{\\beta}\\|\\) Prediction error: \\(\\|y-\\hat{y}\\|=\\|X(\\beta-\\hat{\\beta})\\|\\) (note this definition omits the part related to the error term ) 5.1 Estimation error - MSE Let’s simulate the situation people report 9 years of education if the finished only compulsory schooling, or 12 years if the graduated from high school, or 16 years if they are college graduate. Assume, people report their years of schooling is any discrete year between 9 to 16. Lets assume each of our 10 different sample consist of 5000 individuals. The task is to estimate an unknown population parameter, say \\(\\theta\\), which could be a simple mean of \\(X\\), \\(\\mu_x\\), or more complex slope coefficient of an unknown DGM, \\(\\beta\\). Since we have only a random sample from the population, and because that sample could be unrepresentative of the population, or measurement error, we cannot say that \\(\\hat{\\theta}\\) is equal to \\(\\theta\\). Hence, we call \\(\\hat{\\theta}\\) as an estimator of \\(\\theta\\). We need to pick the best estimator to estimate \\(\\theta\\) among many possible estimators. In this simulation, we can use 3 different estimator if we want to estimate \\(\\mu_x\\). First, we could use the average of years of schooling for everyone who reported it, \\[ \\bar{X}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i} \\] or alternatively, we can just take the half of the first person’s and last person’s years of schooling, \\[ \\hat{X}=0.5 x_{1}+0.5x_{n} \\] or alternatively, we can just use weighted average of first person and the last person’s schooling. We can assign weight as 0.25 for the first person, and 1-0.25=0.75 for the last person, (you will find unbiased estimator when you assign any values as long as the sum is 1) \\[ \\tilde{X}=0.25 x_{1}+0.75x_{2} \\] Therefore, we need to define what makes an estimator the “best” among others. As we have seen before, the sampling distribution, which is the probability distribution of all possible estimates obtained from repeated sampling, would help us develop some principles. The first and the most important criteria should be that the expected mean of all estimates obtained from repeated samples should be equal to \\(\\mu_x\\). Any estimator satisfying this condition is called as an unbiased estimator. However, if \\(x\\)’s are independently and identically distributed (i.i.d), it can be shown that those two estimators, \\(\\bar{X}\\) and \\(\\hat{X}\\) are both unbiased. That is \\(\\mathbf{E}(\\bar{X})=\\mu_x\\) and \\(\\mathbf{E}(\\hat{X})=\\mu_x\\). Although, it would be easy to obtain the algebraic proof, a simulation exercise can help us visualize it. # Here is our population populationX &lt;- c(9,10,11,12,13,14,15,16) #Let&#39;s have a containers to have repeated samples (5000) samples &lt;- matrix(0, 5000, 10) colnames(samples) &lt;- c(&quot;X1&quot;, &quot;X2&quot;, &quot;X3&quot;, &quot;X4&quot;, &quot;X5&quot;, &quot;X6&quot;, &quot;X7&quot;, &quot;X8&quot;, &quot;X9&quot;, &quot;X10&quot;) # Let&#39;s have samples (with replacement always) set.seed(123) for (i in 1:nrow(samples)) { samples[i,] &lt;- sample(populationX, 10, replace = TRUE) } head(samples) ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 ## [1,] 15 15 11 14 11 10 10 14 11 13 ## [2,] 12 14 14 9 10 11 16 13 11 11 ## [3,] 9 12 9 9 13 11 16 10 15 10 ## [4,] 9 14 11 12 14 9 11 15 13 12 ## [5,] 15 16 10 13 15 9 9 10 15 11 ## [6,] 12 13 15 13 11 16 14 9 10 13 Each row below is displaying the first 6 results of 5000 random samples drawn from the sample of the population. Each column shows the order of random draws, that is \\(x_1, x_2, x_3\\). We know the population \\(\\mu_x\\) is 12.5, because this is the mean of our values (9…16) in the population. Knowing this, we can test the following points: Is \\(X\\) i.i.d? An identical distribution requires \\(\\mathbf{E}(x_1)=\\mathbf{E}(x_2)=\\mathbf{E}(x_3)\\) and \\(\\mathbf{Var}(x_1)=\\mathbf{Var}(x_2)=\\mathbf{Var}(x_3)\\). And an independent distribution requires \\(\\mathbf{Corr}(x_i,x_j)=0\\) where \\(i\\neq{j}\\). Are the three estimators unbiased. That is, whether \\(\\mathbf{E}(\\bar{X})= \\mathbf{E}(\\hat{X})= \\mathbf{E}(\\tilde{X}) = \\mu_x\\). Let’s see: library(corrplot) # Check if E(x_1)=E(x_2)=E(x_3) round(colMeans(samples),2) ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 ## 12.48 12.51 12.48 12.57 12.54 12.51 12.45 12.50 12.51 12.45 # Check if Var(x_1)=Var(x_2)=Var(x_3) apply(samples, 2, var) ## X1 X2 X3 X4 X5 X6 X7 X8 ## 5.215851 5.168121 5.275669 5.304244 5.181397 5.313774 5.211075 5.199022 ## X9 X10 ## 5.271664 5.308739 # Check correlation cor(samples) ## X1 X2 X3 X4 X5 ## X1 1.0000000000 0.019035237 0.0005187863 -0.003009403 0.012697691 ## X2 0.0190352374 1.000000000 0.0129212837 0.022095728 0.001996026 ## X3 0.0005187863 0.012921284 1.0000000000 0.008561261 0.007489914 ## X4 -0.0030094032 0.022095728 0.0085612611 1.000000000 -0.020250816 ## X5 0.0126976912 0.001996026 0.0074899136 -0.020250816 1.000000000 ## X6 -0.0012185505 0.003950908 0.0017053482 -0.004192274 -0.006258127 ## X7 -0.0050727838 0.012249661 -0.0158850355 -0.002273334 -0.004128538 ## X8 0.0004449995 -0.007632492 0.0158319052 0.015406963 0.006831127 ## X9 -0.0002149225 0.006525461 -0.0085471951 0.011092057 -0.003824813 ## X10 -0.0154635584 -0.015980621 0.0032570724 -0.006079606 0.006750404 ## X6 X7 X8 X9 X10 ## X1 -0.001218551 -0.005072784 0.0004449995 -0.0002149225 -0.015463558 ## X2 0.003950908 0.012249661 -0.0076324918 0.0065254614 -0.015980621 ## X3 0.001705348 -0.015885036 0.0158319052 -0.0085471951 0.003257072 ## X4 -0.004192274 -0.002273334 0.0154069633 0.0110920570 -0.006079606 ## X5 -0.006258127 -0.004128538 0.0068311271 -0.0038248132 0.006750404 ## X6 1.000000000 0.008001039 0.0171339484 -0.0160498563 -0.005940823 ## X7 0.008001039 1.000000000 -0.0056535157 0.0060161208 0.002218198 ## X8 0.017133948 -0.005653516 1.0000000000 0.0053987471 0.059894331 ## X9 -0.016049856 0.006016121 0.0053987471 1.0000000000 0.013852075 ## X10 -0.005940823 0.002218198 0.0598943307 0.0138520754 1.000000000 # Note that if you use only unique set of samples # you can get exact results uniqsam &lt;- unique(samples) colMeans(uniqsam) ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 ## 12.4802 12.5106 12.4758 12.5694 12.5352 12.5094 12.4474 12.4958 12.5138 12.4518 apply(uniqsam, 2, var) ## X1 X2 X3 X4 X5 X6 X7 X8 ## 5.215851 5.168121 5.275669 5.304244 5.181397 5.313774 5.211075 5.199022 ## X9 X10 ## 5.271664 5.308739 cor(uniqsam) ## X1 X2 X3 X4 X5 ## X1 1.0000000000 0.019035237 0.0005187863 -0.003009403 0.012697691 ## X2 0.0190352374 1.000000000 0.0129212837 0.022095728 0.001996026 ## X3 0.0005187863 0.012921284 1.0000000000 0.008561261 0.007489914 ## X4 -0.0030094032 0.022095728 0.0085612611 1.000000000 -0.020250816 ## X5 0.0126976912 0.001996026 0.0074899136 -0.020250816 1.000000000 ## X6 -0.0012185505 0.003950908 0.0017053482 -0.004192274 -0.006258127 ## X7 -0.0050727838 0.012249661 -0.0158850355 -0.002273334 -0.004128538 ## X8 0.0004449995 -0.007632492 0.0158319052 0.015406963 0.006831127 ## X9 -0.0002149225 0.006525461 -0.0085471951 0.011092057 -0.003824813 ## X10 -0.0154635584 -0.015980621 0.0032570724 -0.006079606 0.006750404 ## X6 X7 X8 X9 X10 ## X1 -0.001218551 -0.005072784 0.0004449995 -0.0002149225 -0.015463558 ## X2 0.003950908 0.012249661 -0.0076324918 0.0065254614 -0.015980621 ## X3 0.001705348 -0.015885036 0.0158319052 -0.0085471951 0.003257072 ## X4 -0.004192274 -0.002273334 0.0154069633 0.0110920570 -0.006079606 ## X5 -0.006258127 -0.004128538 0.0068311271 -0.0038248132 0.006750404 ## X6 1.000000000 0.008001039 0.0171339484 -0.0160498563 -0.005940823 ## X7 0.008001039 1.000000000 -0.0056535157 0.0060161208 0.002218198 ## X8 0.017133948 -0.005653516 1.0000000000 0.0053987471 0.059894331 ## X9 -0.016049856 0.006016121 0.0053987471 1.0000000000 0.013852075 ## X10 -0.005940823 0.002218198 0.0598943307 0.0138520754 1.000000000 It seems that the i.i.d condition is satisfied. Now we need to answer the second question, whether the estimators are unbiased. For this, we need to apply each estimator to each sample: # First Xbar X_bar &lt;- rep(0, nrow(samples)) #Container to have all Xbars for(i in 1:nrow(samples)){ X_bar[i] &lt;- sum(samples[i,])/ncol(samples) } EX_bar &lt;- sum(X_bar)/length(X_bar) EX_bar ## [1] 12.49894 # Xhat X_hat &lt;- rep(0, nrow(samples)) for(i in 1:nrow(samples)){ X_hat[i] &lt;- 0.5*samples[i,1] + 0.5*samples[i,10] } EX_hat &lt;- sum(X_hat)/length(X_hat) EX_hat ## [1] 12.466 # Xtilde X_tilde &lt;- rep(0, nrow(samples)) for(i in 1:nrow(samples)){ X_tilde[i] &lt;- 0.25*samples[i,1] + 0.75*samples[i,2] } EX_tilde &lt;- sum(X_tilde)/length(X_tilde) EX_tilde ## [1] 12.503 Yes, they are unbiased because \\(\\mathbf{E}(\\bar{X})\\approx \\mathbf{E}(\\hat{X}) \\approx \\mathbf{E}(\\tilde{X}) \\approx \\mu_x \\approx 12.5\\). (When we increase the sample size these expected values will be closer to 12.5) As you can see, none of the averages are exact same population average. There is a difference between the estimated value and the actual value. That small difference is called error. Ideally, we want that difference to be zero. When number of observations in our sample get closer to our population that difference vanishes. As we can never know the actual population characteristics, we assume this error approaches to zero. Upto this point, we showed all 3 estimators gave us unbiased estimate. However, unbiasness is not the only desirable property. We want our estimator should give a close estimate of the population parameter with higher probability. In another words, estimators probability density function to be concentrated around true value, i.e. it should be efficient. Thus, the unbiased estimator with the smallest variance is the best estimate. Just be careful. If one estimator is more efficient than other one, it does not mean it will always give more accurate estimate, it means it is more likely to be accurate than the other one. Let’s see which one has the smallest variance in our simulation: var(X_bar) ## [1] 0.5385286 var(X_hat) ## [1] 2.590462 var(X_tilde) ## [1] 3.27012 As seen above, the \\(\\bar{X}\\), the average of all sample, has the smallest variance. Let’s summarize the important steps in estimations: The main task is to estimate the population parameter using an estimator from a sample. The main requirement for a (linear) estimator is unbiasedness. An unbiased estimator is called as the Best Linear Unbiased Estimator (BLUE) of a population parameter if that estimator is efficient ,i.e. has the minimum variance among all other unbiased estimators. In the simulation above, we showed the sample average is the most efficient of the all unbiased estimators. We should use the term “Efficiency” when we compare different estimators, and when these alternative estimators use the same information (same data,same sample size) We cannot use it when you compare the same estimators variance obtained while using different sample sizes. Generally, the variance of an estimator decreases when sample size increases. We can not use one estimator is more efficient than another one, just because one variance is smaller than another one but these variances calculated using different sample sizes. So, there is always conflict between unbiasedness and the smallest possible variance. We can have 2 estimators to estimate population characteristics. First one can be unbiased but with higher variance, the other one can be biased but lower variance (figure page 31 from Dougherty book). Which estimator we choose depends on what we want. If we think errors in estimators is not a big problem, and errors will cancel is each other on average, then we may choose unbiased estimator even if it has higher variance. That is what we use nearly always in applied social sciences. You know this expected value of error term is 0, with a variance \\(sigma^2\\). However, in some research questions, we can not tolerate large errors. Thus we need to choose an estimator with smaller variance even if it has a small bias. (We will show this in the next chapter with simulation as well) In another words, the decision of choosing the estimator depends on the cost to you of an error as a function of its size. The function that gives that cost is called loss function . One of the most common loss function used in social sciences is mean square error (MSE) We can define MSE as the average of the squares of the difference between the estimated value and the actual value. The MSE of the estimators could be simply used for the efficiency comparison, which includes the information of estimator variance and bias. This is called MSE criterion. The MSE can be decomposed between its variance and bias as such: \\[ \\mathbf{MSE}(\\hat{\\theta})=\\mathbf{E}_{\\hat{\\theta}}\\left[(\\hat{\\theta}-\\theta)^{2}\\right]=\\mathbf{Var}\\left(\\hat{\\theta}\\right)+\\left[\\mathbf{bias}\\left(\\hat{\\theta}\\right)\\right]^{2} \\] You can check the formal decomposition of MSE in technical point section at the end of this chapter. In typical economic models some parameters are involved, the original role of econometrics was to quantify them. So in economics/econometrics models the parameters are the core of the theory. Them carried out the causal meaning that economists looking for (or it should be so). Exactly for this reason econometrics manuals are mostly focused on concept like endogeneity and, then, bias. As the main goal is to obtain unbiased parameters, most econometrics textbook even do not discuss this decomposition. Mainly, they discuss variance or its square root, i.e. standard error. Reminder: Assuming a true linear model \\(y=X \\beta+\\varepsilon\\), we estimate \\(\\hat{\\beta_{i}}\\). The Gauss-Markov theorem states that if your linear regression model satisfies the first six classical assumptions, then ordinary least squares (OLS) regression produces unbiased estimates that have the smallest variance of all possible linear estimators,i.e. OLS is BLUE. OLS Assumption 1: The regression model is linear in the coefficients and the error term. OLS Assumption 2: The error term has a population mean of zero. OLS Assumption 3: All independent variables are uncorrelated with the error term. OLS Assumption 4: Observations of the error term are uncorrelated with each other. OLS Assumption 5: The error term has a constant variance (no heteroscedasticity). OLS Assumption 6: No independent variable is a perfect linear function of other explanatory variables. OLS Assumption 7: The error term is normally distributed (optional) Reminder: Moreover, in practice, we have only one sample most of the time. We donot have 10 samples like in the simulation above. We know that if the sample size is big enough (more than 50, for example), the sampling distribution would be normal according to the Central Limit Theorem (CLT). In other words, if the number of observations in each sample large enough, \\(\\bar{X} \\sim N(\\mu_x, \\sigma^{2}/n)\\) or when population variance is not known \\(\\bar{X} \\sim \\mathcal{T}\\left(\\mu, S^{2}\\right)\\) where \\(S\\) is the standard deviation of the sample and \\(\\mathcal{T}\\) is the Student’s \\(t\\)-distribution. Why is this important? Because it works like a magic: with only one representative sample, we can generalize the results for the population. We will not cover the details of interval estimation here, but by knowing \\(\\bar{X}\\) and the sample variance \\(S\\), we can have the following interval for the \\(\\mu_{x}\\): \\[ \\left(\\bar{x}-t^{*} \\frac{s}{\\sqrt{n}}, \\bar{x}+t^{*} \\frac{s}{\\sqrt{n}}\\right) \\] where \\(t^*\\), the critical values in \\(t\\)-distribution, are usually around 1.96 for samples more than 100 observations and for the 95% confidence level. This interval would be completely wrong or misleading if \\(\\mathbf{E}(\\bar{X}) \\neq \\mu_x\\) and would be useless if it is very wide, which is caused by a large variance. That’s the reason why we don’t like large variances. 5.2 Prediction error- MSPE In the previous section, we defined mean square error (MSE), and then decomposed between its variance and bias. However, MSE differs according to whether one is describing an estimator or a predictor. We can define an estimator as a mathematical function mapping a sample of data to an estimate of a parameter of the population from which the data is sampled. We can define a predictor as a function mapping arbitrary inputs to a sample of values of some random variable. Most common function used in social sciences is OLS. Most people are familiar with MSE of OLS function as the following. Predictor of least-squares fit, then the within-sample MSE of the predictor is computed as \\[ \\mathrm{MSE}=\\frac{1}{n} \\sum_{i=1}^n\\left(Y_i-\\hat{Y}_i\\right)^2 \\] In matrix notation, \\[ \\mathrm{MSE}=\\frac{1}{n} \\sum_{i=1}^n\\left(e_i\\right)^2=\\frac{1}{n} \\mathbf{e}^{\\top} \\mathbf{e} \\] where \\(e_i\\) is \\(\\left(Y_i-\\hat{Y}_i\\right)\\) and \\(\\mathbf{e}\\) is the \\(n \\times 1\\) column vector. Eventhough OLS and its MSE is the most common used tools, we can use tons of other functions for prediction. Thus in this section we will define MSE for prediction for all functions. Also, Mean Square Prediction Error (MSPE) is more preferable term for prediction purposes. Our task is prediction of an outcome, Y (i.e. supervised learning as we know what outcome is, and regression set up when our outcome is non-binary): We assume that the response variable,Y, is some function of the features, X, plus some random noise. \\[Y=f(X)+ ϵ\\] To “predict” Y using features X, means to find some \\(f\\) such that \\(f(X)\\) is close to \\(Y\\). But how do we define close? There are many ways but the most common way is minimizing the average squared error loss. Loss function is \\((Y-f(X))^2\\), Average square loss function is the expected value of loss function. That is called Risk function, which is \\(\\mathbf{E}\\left[(Y-f(X))^{2}\\right]\\). So, we can say we want to minimize risk function to “predict” Y using X. However, we can never know real \\(f(X)\\). Thus our goal becomes to find a prediction function,\\(\\hat{f(X)}\\), which is an estimate of unknown f using the data we have. Then, there will be an expected prediction error of predicting Y using \\(\\hat{f(X)}\\). All in all, our goal becomes to minimize the average square of this error, called as Mean Square Prediction Error (MSPE) \\(\\mathbf{MSPE}=\\mathbf{E}\\left[(Y-\\hat{f(X)})^{2}\\right]\\). A good \\(\\hat{f(X)}\\) will have a low MSPE. This error can be decomposed into two errors. The reducible error(mean squared error), which is the expected squared error loss of estimation \\(f(X)\\) using \\(\\hat{f(X)}\\) at a fixed point \\(X\\). The irreducible error, which is simply the variance of \\(Y\\) given that \\(X=x\\) ,essentially noise that we do not want to learn. Reducible error: MSE of \\(\\hat{f(X)}\\) for a given \\(X=x\\) (mean square error obtained with-in test/training sample)) \\[ \\operatorname{MSE}(f(x), \\hat{f}(x))= \\underbrace{(f(x)-\\mathbb{E}[\\hat{f}(x)])^2}_{\\operatorname{bias}^2(\\hat{f}(x))}+\\underbrace{\\mathbb{E}\\left[(\\hat{f}(x)-\\mathbb{E}[\\hat{f}(x)])^2\\right]}_{\\operatorname{var}(\\hat{f}(x))} \\] Mean Square Prediction Error \\[ \\mathbf{MSPE}=\\mathbf{E}\\left[(Y-\\hat{f(X)})^{2}\\right]=\\mathbf{Bias}[\\hat{f(X)}]^{2}+\\mathbf{Var}[\\hat{f(X)}]+\\sigma^{2} \\] \\(\\sigma^{2}=E[\\varepsilon^{2}]\\) You can check the formal decomposition of MSPE in technical point section at the end of this chapter. (Note: if we assume our prediction function,\\(f(X)\\), is linear then this is OLS.) Our job is to pick a the best predictor, i.e. predictor that will have the minimum MSPE among alternatives. In perfect setting, we want prediction function with zero bias and low variance to have the minimum MSPE. However, this is never happens. Unlike an estimator, we can accept some bias as long as the MSPE is lower. More specifically, we can allow a predictor to have a bias if it reduces the variance more than the bias itself. Unlike estimations, this shows that, in predictions, we can have a reduction in MSPE by allowing a trade-off between variance and bias. We will discuss how we can achieve it in the next chapter. For instance, our predictor could be a constant, which, although it’s a biased estimator, has a zero variance. Or our predictor could be mean of \\(X\\) as this predictor has zero bias but it has high variance. Or we could choose predictor which has some bias and variance. We will show an example using these 3 predictors in the following simulation. We want to emphasize the difference between MSE and MSPE, and their decomposed forms between their variances and biases. Even though they look similar, they are really very different. For MSE, bias and variance comes from the parameter estimation. For MSPE, biad and variance derived from prediction functions. We try different prediction functions to find the best predictor function. Moreover, the bias-squared and the variance of \\(\\hat{f}\\) is called reducible error. Hence, the MSPE can be written as \\[ \\mathbf{MSPE}=\\mathbf{Reducible~Error}+\\mathbf{Irreducible~Error} \\] The predictor with the smallest MSPE will be our choice among other alternative predictor functions. Yet, we have another concern that leads over-fitting. We will discuss over fitting in detail later. //DISCUSS OVERFITTING HERE A BIT Let’s summarize some important facts about our MSPE here: \\(x_0\\) is the number we want to predict and \\(\\hat{f}\\) is the predictor, which could be \\(\\mathbf{E}(\\bar{X})\\), \\(\\mathbf{E}(\\hat{X})\\), or \\(\\mathbf{E}(\\tilde{X})\\) or any other predictor. \\(x_0 = \\mu_x + \\varepsilon_0\\), where \\(f = \\mu_x\\). Hence, \\(\\mathbf{E}[x_0]=f\\) so that \\(\\mathbf{E}[\\varepsilon_0]=0\\). \\(\\mathbf{E}[f]=f\\). In other words, the expected value of a constant is a constant: \\(\\mathbf{E}[\\mu_x]=\\mu_x\\). \\(\\mathbf{Var}[x_0]=\\mathbf{E}\\left[(x_0-\\mathbf{E}[x_0])^{2}\\right]=\\mathbf{E}\\left[(x_0-f)^{2}\\right]=\\mathbf{E}\\left[(f+\\varepsilon_0-f)^{2}\\right]=\\mathbf{E}\\left[\\varepsilon_0^{2}\\right]=\\mathbf{Var}[\\varepsilon_0]=\\sigma^{2}\\). (Remember that \\(\\mathbf{E}[\\varepsilon]=0\\)). Note that we can use MSPE here because our example is not a classification problem. When we have a binary outcome to predict, the loss function would have a different algebraic structure. We will see the performance evaluation in classification problems later. Let’s follow the same simulation example. Our task is now different. We want to predict the next persons years of schooling using the data we have. We want to predict the unobserved value of \\(X\\) rather than to estimate \\(\\mu_x\\). Therefore, we need a predictor, not an estimator. To answer these questions, we need to compare MSPEs or their square roots (RMSPE) as well.. As we know that, most developed countries require to go to school between age 6 to 16 years old, we may predict that the years of schooling for the individual is 10 years. or we can use the average years of schooling in our data as a good predictor for the next individuals schooling level. Thus we have 2 prediction function. First one is a constant, 10, which has bias but zero variance. The other one is mean of our sample for each observation (average of each row), which has smaller bias and higher variance. For simplicity, we can use 1 sample which consist from 5000 individuals in this simulation. The two predictors are \\(\\hat{f}_1 = 10\\) and \\(\\hat{f}_2 = \\bar{X}\\): We will use the same example we worked with before. We sample from this “population” multiple times. Now the task is to use each sample and come up with a predictor (a prediction rule) to predict a number or multiple numbers drawn from the same population. # Here is our population populationX &lt;- c(9,10,11,12,13,14,15,16) #Let&#39;s have a containers to have repeated samples (2000) Ms &lt;- 5000 samples &lt;- matrix(0, Ms, 10) colnames(samples) &lt;- c(&quot;X1&quot;, &quot;X2&quot;, &quot;X3&quot;, &quot;X4&quot;, &quot;X5&quot;, &quot;X6&quot;, &quot;X7&quot;, &quot;X8&quot;, &quot;X9&quot;, &quot;X10&quot;) # Let&#39;s have samples (with replacement always) set.seed(123) for (i in 1:nrow(samples)) { samples[i,] &lt;- sample(populationX, 10, replace = TRUE) } head(samples) ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 ## [1,] 15 15 11 14 11 10 10 14 11 13 ## [2,] 12 14 14 9 10 11 16 13 11 11 ## [3,] 9 12 9 9 13 11 16 10 15 10 ## [4,] 9 14 11 12 14 9 11 15 13 12 ## [5,] 15 16 10 13 15 9 9 10 15 11 ## [6,] 12 13 15 13 11 16 14 9 10 13 As you see, this is the same sample with the previous simulation. You can change the data either setting different values in the seed or changing the sammple size, Ms. Now, Let’s use our predictors and find MSPEs: # Container to record all predictions predictions &lt;- matrix(0, Ms, 2) # fhat_1 = 10 for (i in 1:Ms) { predictions[i,1] &lt;- 10 } # fhat_2 - mean for (i in 1:Ms) { predictions[i,2] &lt;- sum(samples[i,])/length(samples[i,]) } head(predictions) ## [,1] [,2] ## [1,] 10 12.4 ## [2,] 10 12.1 ## [3,] 10 11.4 ## [4,] 10 12.0 ## [5,] 10 12.3 ## [6,] 10 12.6 # MSPE MSPE &lt;- matrix(0, Ms, 2) for (i in 1:Ms) { MSPE[i,1] &lt;- mean((populationX-predictions[i,1])^2) MSPE[i,2] &lt;- mean((populationX-predictions[i,2])^2) } head(MSPE) ## [,1] [,2] ## [1,] 11.5 5.26 ## [2,] 11.5 5.41 ## [3,] 11.5 6.46 ## [4,] 11.5 5.50 ## [5,] 11.5 5.29 ## [6,] 11.5 5.26 colMeans(MSPE) ## [1] 11.500000 5.788422 The MSPE of the t \\(\\hat{f}_2\\) prediction function is the better as its MSPE is smaller than the other prediction function. What makes a good predictor? Is being unbiased predictor one of the required property? would being a biased estimator make it automatically a bad predictor? in predictions, we can have a reduction in MSPE by allowing a trade-off between variance and bias. We will discuss this trade-off in the next chapter. We will also show it by using the same simulation. 5.3 Technical points about MSE and MSPE The formal decomposition of MSE The MSE of an estimator \\(\\hat{\\theta}\\) with respect to an unknown parameter \\(\\theta\\) is defined as \\[ \\mathbf{MSE}(\\hat{\\theta})=\\mathbf{E}_{\\hat{\\theta}}\\left[(\\hat{\\theta}-\\theta)^{2}\\right]=\\mathbf{E}_{\\hat{\\theta}}\\left[(\\hat{\\theta}-\\mathbf{E}(\\hat{\\theta}))^{2}\\right] \\] Since we choose only unbiased estimators, \\(\\mathbf{E}(\\hat{\\theta})=\\theta\\), this expression becomes \\(\\mathbf{Var}(\\hat{\\theta})\\). Hence, evaluating the performance of all alternative unbiased estimators by MSE is actually comparing their variances and picking up the smallest one. More specifically, \\[\\begin{equation} \\mathbf{MSE}\\left(\\hat{\\theta}\\right)=\\mathbf{E}\\left[\\left(\\hat{\\theta}-\\theta\\right)^{2}\\right]=\\mathbf{E}\\left\\{\\left(\\hat{\\theta}-\\mathbf{E}\\left(\\hat{\\theta}\\right)+\\mathbf{E}\\left(\\hat{\\theta}\\right)-\\theta\\right)^{2}\\right\\} \\tag{5.1} \\end{equation}\\] \\[ =\\mathbf{E}\\left\\{\\left(\\left[\\hat{\\theta}-\\mathbf{E}\\left(\\hat{\\theta}\\right)\\right]+\\left[\\mathbf{E}\\left(\\hat{\\theta}\\right)-\\theta\\right]\\right)^{2}\\right\\} \\] \\[\\begin{equation} \\begin{aligned} =&amp; \\mathbf{E}\\left\\{\\left[\\hat{\\theta}-\\mathbf{E}\\left(\\hat{\\theta}\\right)\\right]^{2}\\right\\}+\\mathbf{E}\\left\\{\\left[\\mathbf{E}\\left(\\hat{\\theta}\\right)-\\theta\\right]^{2}\\right\\} \\\\ &amp;+2 \\mathbf{E}\\left\\{\\left[\\hat{\\theta}-\\mathbf{E}\\left(\\hat{\\theta}\\right)\\right]\\left[\\mathbf{E}\\left(\\hat{\\theta}\\right)-\\theta\\right]\\right\\} \\end{aligned} \\tag{5.2} \\end{equation}\\] The first term in 3.2 is the variance. The second term is outside of expectation, as \\([\\mathbf{E}(\\hat{\\theta})-\\theta]\\) is not random, which represents the bias. The last term is zero. This is because \\([\\mathbf{E}(\\hat{\\theta})-\\theta]\\) is not random, therefore it is again outside of expectations: \\[ 2\\left[\\mathbf{E}\\left(\\hat{\\theta}\\right)-\\theta\\right] \\mathbf{E}\\left\\{\\left[\\hat{\\theta}-\\mathbf{E}\\left(\\hat{\\theta}\\right)\\right]\\right\\}, \\] and the last term is zero since \\(\\mathbf{E}(\\hat{\\theta})-\\mathbf{E}(\\hat{\\theta}) = 0\\). Hence, \\[ \\mathbf{MSE}\\left(\\hat{\\theta}\\right)=\\mathbf{Var}\\left(\\hat{\\theta}\\right)+\\left[\\mathbf{bias}\\left(\\hat{\\theta}\\right)\\right]^{2} \\] Because we choose only unbiased estimators, \\(\\mathbf{E}(\\hat{\\theta})=\\theta\\), this expression becomes \\(\\mathbf{Var}(\\hat{\\theta})\\). In our case, the estimator can be \\(\\hat{\\theta}=\\bar{X}\\) and what we try to estimate \\(\\theta = \\mu_x\\). The formal decomposition of MSPE let’s look at MSPE closer. We will drop the subscript \\(0\\) to keep the notation simple. With a trick, adding and subtracting \\(\\mathbf{E}(\\hat{f})\\), MSPE becomes \\[ \\mathbf{MSPE}=\\mathbf{E}\\left[(x-\\hat{f})^{2}\\right]=\\mathbf{E}\\left[(f+\\varepsilon-\\hat{f})^{2}\\right]=\\mathbf{E}\\left[(f+\\varepsilon-\\hat{f}+\\mathbf{E}[\\hat{f}]-\\mathbf{E}[\\hat{f}])^{2}\\right] \\] \\[ =\\mathbf{E}\\left[(f-\\mathbf{E}[\\hat{f}])^{2}\\right]+\\mathbf{E}\\left[\\varepsilon^{2}\\right]+\\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right]+2 \\mathbf{E}[(f-\\mathbf{E}[\\hat{f}]) \\varepsilon]+2 \\mathbf{E}[\\varepsilon(\\mathbf{E}[\\hat{f}]-\\hat{f})]+\\\\2 \\mathbf{E}[(\\mathbf{E}[\\hat{f}]-\\hat{f})(f-\\mathbf{E}[\\hat{f}])], \\] which can be simplified with the following few steps: The first term, \\(\\mathbf{E}\\left[(f-\\mathbf{E}[\\hat{f}])^{2}\\right]\\), is \\((f-\\mathbf{E}[\\hat{f}])^{2}\\), because \\((f-\\mathbf{E}[\\hat{f}])^{2}\\) is a constant. Similarly, the same term, \\((f-\\mathbf{E}[\\hat{f}])^{2}\\) is in the \\(4^{th}\\) term. Hence, \\(2 \\mathbf{E}[(f-\\mathbf{E}[\\hat{f}]) \\varepsilon]\\) can be written as \\(2(f-\\mathbf{E}[\\hat{f}]) \\mathbf{E}[\\varepsilon]\\). Finally, the \\(5^{th}\\) term, \\(2 \\mathbf{E}[\\varepsilon(\\mathbf{E}[\\hat{f}]-\\hat{f})]\\) can be written as \\(2 \\mathbf{E}[\\varepsilon] \\mathbf{E}[\\mathbf{E}[\\hat{f}]-\\hat{f}]\\). (Note that \\(\\varepsilon\\) and \\(\\hat{f}\\) are independent) As a result we have: \\[ =(f-\\mathbf{E}[\\hat{f}])^{2}+\\mathbf{E}\\left[\\varepsilon^{2}\\right]+\\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right]+2(f-\\mathbf{E}[\\hat{f}]) \\mathbf{E}[\\varepsilon]+2 \\mathbf{E}[\\varepsilon] \\mathbf{E}[\\mathbf{E}[\\hat{f}]-\\hat{f}]+\\\\2 \\mathbf{E}[\\mathbf{E}[\\hat{f}]-\\hat{f}](f-\\mathbf{E}[\\hat{f}]) \\] The \\(4^{th}\\) and the \\(5^{th}\\) terms are zero because \\(\\mathbf{E}[\\varepsilon]=0\\). The last term is also zero because \\(\\mathbf{E}[\\mathbf{E}[\\hat{f}]-\\hat{f}]\\) is \\(\\mathbf{E}[\\hat{f}]-\\mathbf{E}[\\hat{f}]\\). Hence, we have: \\[ =(f-\\mathbf{E}[\\hat{f}])^{2}+\\mathbf{E}\\left[\\varepsilon^{2}\\right]+\\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right] \\] Let’s look at the second term first. It’s irreducible error because it comes with the data. Thus, we can write: \\[\\begin{equation} \\mathbf{MSPE}=(\\mu_x-\\mathbf{E}[\\hat{f}])^{2}+\\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right]+\\mathbf{Var}\\left[x\\right] \\tag{5.3} \\end{equation}\\] The first term of 3.4 is the bias squared. It would be zero for an unbiased estimator, that is, if \\(\\mathbf{E}[\\hat{f}]=\\mu_x.\\) The second term is the variance of the estimator. For example, if the predictor is \\(\\bar{X}\\) it would be \\(\\mathbf{E}\\left[(\\bar{X} -\\mathbf{E}[\\bar{X}])^{2}\\right]\\). Hence the variance comes from the sampling distribution. \\[ \\mathbf{MSPE}=\\mathbf{Bias}[\\hat{f}]^{2}+\\mathbf{Var}[\\hat{f}]+\\sigma^{2} \\] These two terms, the bias-squared and the variance of \\(\\hat{f}\\) is called reducible error. Hence, the MSPE can be written as \\[ \\mathbf{MSPE}=\\mathbf{Reducible~Error}+\\mathbf{Irreducible~Error} \\] The relation between MSE and MSPE Before going further, we need to see the connection between MSPE and MSE in a regression setting: \\[\\begin{equation} \\mathbf{MSPE}=\\mathbf{E}\\left[(y_0-\\hat{f})^{2}\\right]=(f-\\mathbf{E}[\\hat{f}])^{2}+\\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right]+\\mathbf{E}\\left[\\varepsilon^{2}\\right] \\tag{5.4} \\end{equation}\\] Equation 4.1 is simply an expected prediction error of predicting \\(y_0\\) using \\(\\hat{f}(x_0)\\). The estimate \\(\\hat{f}\\) is random depending on the sample we use to estimate it. Hence, it varies from sample to sample. We call the sum of the first two terms as “reducible error”, as we have seen before. The MSE of the estimator \\(\\hat{f}\\) is, on the other hand, shows the expected squared error loss of estimating \\(f(x)\\) by using \\(\\hat{f}\\) at a fixed point \\(x\\). \\[ \\mathbf{MSE}(\\hat{f})=\\mathbf{E}\\left[(\\hat{f}-f)^{2}\\right]=\\mathbf{E}\\left\\{\\left(\\hat{f}-\\mathbf{E}(\\hat{f})+\\mathbf{E}(\\hat{f})-f\\right)^{2}\\right\\} \\] \\[ =\\mathbf{E}\\left\\{\\left(\\left[\\hat{f}-\\mathbf{E}\\left(\\hat{f}\\right)\\right]+\\left[\\mathbf{E}\\left(\\hat{f}\\right)-f\\right]\\right)^{2}\\right\\} \\] \\[\\begin{equation} =\\mathbf{E}\\left\\{\\left[\\hat{f}-\\mathbf{E}(\\hat{f})\\right]^{2}\\right \\}+\\mathbf{E}\\left\\{\\left[\\mathbf{E}(\\hat{f})-f\\right]^{2}\\right\\}+2 \\mathbf{E}\\left\\{\\left[\\hat{f}-\\mathbf{E}(\\hat{f})\\right]\\left[\\mathbf{E}(\\hat{f})-f\\right]\\right\\} \\tag{5.5} \\end{equation}\\] The first term is the variance. The second term is outside of expectation, as \\([\\mathbf{E}(\\hat{f})-f]\\) is not random, which represents the bias. The last term is zero. Hence, \\[\\begin{equation} \\mathbf{MSE}(\\hat{f})=\\mathbf{E}\\left\\{\\left[\\hat{f}-\\mathbf{E}(\\hat{f})\\right]^{2}\\right\\}+\\mathbf{E}\\left\\{\\left[\\mathbf{E}(\\hat{f})-f\\right]^{2}\\right\\}=\\mathbf{Var}(\\hat{f})+\\left[\\mathbf{bias}(\\hat{f})\\right]^{2} \\tag{5.6} \\end{equation}\\] We can now see how MSPE is related to MSE. Since the estimator \\(\\hat{f}\\) is used in predicting \\(y_0\\), MSPE should include MSE: \\[ \\mathbf{MSPE}=(f-\\mathbf{E}[\\hat{f}])^{2}+\\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right]+\\mathbf{E}\\left[\\varepsilon^{2}\\right]=\\mathbf{MSE}(\\hat{f})+\\mathbf{E}\\left[\\varepsilon^{2}\\right] \\] The important difference between estimation and prediction processes is the data points that we use to calculate the mean squared error loss functions. In estimations, our objective is to find the estimator that minimizes the MSE, \\(\\mathbf{E}\\left[(\\hat{f}-f)^{2}\\right]\\). However, since \\(f\\) is not known to us, we use \\(y_i\\) as a proxy for \\(f\\) and calculate MSPE using in-sample data points. Therefore, using an estimator for predictions means that we use in-sample data points to calculate MSPE in predictions, which may result in overfitting and a poor out-of-sample prediction accuracy. "],["bias-variance-trade-off.html", "Chapter 6 Bias-Variance Trade-off 6.1 Biased estimator as a predictor", " Chapter 6 Bias-Variance Trade-off We already discuss bias and variance of estimator, and decomposition of MSE for estimation above in MSE section. Now, Lets discuss what is bias and variance and trade-off between them in predictive models. To remind,our task is prediction of an outcome, Y using the data (more accurately test/train data) . To “predict” Y using features X, means to find some \\(f\\) which is close to \\(Y\\). We assume that \\(Y\\) is some function of \\(X\\) plus some random noise. \\[Y=f(X)+ ϵ\\] However, we can never know real \\(f(X)\\). Thus our goal becomes to finding some \\(\\hat{f(X)}\\) that is a good estimate of the regression function \\(f(X)\\). There will be always difference between real \\(f(X)\\) and \\(\\hat{f(X)}\\). That difference is called reducible error. We find a good estimate of \\(f(X)\\) by reducing the expected mean square of error of test data as much as possible. Thus, we can write MSE for prediction function using training data (for test data, validation sample): \\[ \\operatorname{MSE}(f(x), \\hat{f}(x))= \\underbrace{(\\mathbb{E}[\\hat{f}(x)]-f(x))^2}_{\\operatorname{bias}^2(\\hat{f}(x))}+\\underbrace{\\mathbb{E}\\left[(\\hat{f}(x)-\\mathbb{E}[\\hat{f}(x)])^2\\right]}_{\\operatorname{var}(\\hat{f}(x))} \\] Then, Mean Square Prediction Error can be written as: \\[ \\mathbf{MSPE}=\\mathbf{E}\\left[(Y-\\hat{f(X)})^{2}\\right]=\\mathbf{Bias}[\\hat{f(X)}]^{2}+\\mathbf{Var}[\\hat{f(X)}]+\\sigma^{2} \\] \\(\\sigma^{2}=E[\\varepsilon^{2}]\\) Expected mean-squared prediction error (MSPE) on the validation/training sample is sum of squared bias of the fit and variance of the fit and variance of the error. Variance is the amount by which \\(\\hat{f(X)}\\) could change if we estimated it using different test/training data set. \\(\\hat{f(X)}\\) depends on the training data. (More complete notation would be \\(\\hat{f}(X; Y_{train},X_{train})\\) )If the \\(\\hat{f(X)}\\) is less complex/less flexible, then it is more likely to change if we use different samples to estimate it. However, if \\(\\hat{f(X)}\\) is more complex/more flexible function, then it is more likely to change between different test samples. For instance, Lets assume we have a data set (test or training data) and we want to “predict” Y using features X, thus estimate function \\(f(X)\\). and lets assume we have 1-degree and 10-degree polynomial functions as a potential prediction functions. We say \\(\\hat{f(X)}=\\hat{\\beta_{0}} + \\hat{\\beta_{1}} X\\) is less complex than 10-degree polynomial \\(\\hat{f(X)}=\\hat{\\beta_{0}} + \\hat{\\beta_{1}} X...+ \\hat{\\beta_{10}} X^{10}\\) function. As, 10-degree polynomial function has more parameters \\(\\beta_{0},..\\beta_{10}\\), it is more flexible. That also means it has high variance (As it has more parameters, all these parameters are more inclined to have different values in different training data sets). Thus, a prediction function has high variance if it can change substantially when we use different training samples to estimate \\(f(X)\\). We can also say less flexible functions (functions with less parameters) have low variance. Low variance functions are less likely to change when we use different training sample or adding new data to the test sample. We will show all these with simulation in overfitting chapter as well. Bias is the difference between the real! prediction function and expected estimated function. If the \\(\\hat{f(X)}\\) is less flexible, then it is more likely to have higher bias. We can think this as real function(reality) is always more complex than the function approximates the reality. So, it is more prone to have higher error, i.e. more bias. In the context of regression, Parametric models are biased when the form of the model does not incorporate all the necessary variables, or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic. In non-parametric models when the model provides too much smoothing. There is a bias-variance tradeoff. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. Flexible(i.e. complex) models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance. So for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate flexibility for the data. However, this selected model should not overfit the data as well which we will discuss in the next section. https://threadreaderapp.com/thread/1584515105374339073.html https://www.simplilearn.com/tutorials/machine-learning-tutorial/bias-and-variance Although conceptually the variance-bias trade-off seems intuitive, at least mathematically, we need to ask another practical question: Can we see the components of the decomposed MSPE? We may not see it in the actual data ( as we donot know real function and irreducible error) but we can show it with simulations. We will use the same example we worked with before. We have years of schooling , which changes between 9 and 16. We sample from this “population” multiple times. Now the task is to use each sample and come up with a predictor (a prediction rule) to predict a number or multiple numbers drawn from the same population. # Here is our population populationX &lt;- c(9,10,11,12,13,14,15,16) #Let&#39;s have a containers to have repeated samples (2000) Ms &lt;- 5000 samples &lt;- matrix(0, Ms, 10) colnames(samples) &lt;- c(&quot;X1&quot;, &quot;X2&quot;, &quot;X3&quot;, &quot;X4&quot;, &quot;X5&quot;, &quot;X6&quot;, &quot;X7&quot;, &quot;X8&quot;, &quot;X9&quot;, &quot;X10&quot;) # Let&#39;s have samples (with replacement always) set.seed(123) for (i in 1:nrow(samples)) { samples[i,] &lt;- sample(populationX, 10, replace = TRUE) } head(samples) ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 ## [1,] 15 15 11 14 11 10 10 14 11 13 ## [2,] 12 14 14 9 10 11 16 13 11 11 ## [3,] 9 12 9 9 13 11 16 10 15 10 ## [4,] 9 14 11 12 14 9 11 15 13 12 ## [5,] 15 16 10 13 15 9 9 10 15 11 ## [6,] 12 13 15 13 11 16 14 9 10 13 Now, Let’s use our predictors: # Container to record all predictions predictions &lt;- matrix(0, Ms, 2) # fhat_1 = 9 for (i in 1:Ms) { predictions[i,1] &lt;- 10 } # fhat_2 - mean for (i in 1:Ms) { predictions[i,2] &lt;- sum(samples[i,])/length(samples[i,]) } head(predictions) ## [,1] [,2] ## [1,] 10 12.4 ## [2,] 10 12.1 ## [3,] 10 11.4 ## [4,] 10 12.0 ## [5,] 10 12.3 ## [6,] 10 12.6 Now let’s have our MSPE decomposition: # MSPE MSPE &lt;- matrix(0, Ms, 2) for (i in 1:Ms) { MSPE[i,1] &lt;- mean((populationX-predictions[i,1])^2) MSPE[i,2] &lt;- mean((populationX-predictions[i,2])^2) } head(MSPE) ## [,1] [,2] ## [1,] 11.5 5.26 ## [2,] 11.5 5.41 ## [3,] 11.5 6.46 ## [4,] 11.5 5.50 ## [5,] 11.5 5.29 ## [6,] 11.5 5.26 # Bias bias1 &lt;- mean(populationX)-mean(predictions[,1]) bias2 &lt;- mean(populationX)-mean(predictions[,2]) # Variance (predictor) var1 &lt;- var(predictions[,1]) var1 ## [1] 0 var2 &lt;- var(predictions[,2]) var2 ## [1] 0.5385286 # Variance (epsilon) var_eps &lt;- mean((populationX-12.5)^2) var_eps ## [1] 5.25 Let’s put them in a table: VBtradeoff &lt;- matrix(0, 2, 4) rownames(VBtradeoff) &lt;- c(&quot;fhat_1&quot;, &quot;fhat_2&quot;) colnames(VBtradeoff) &lt;- c(&quot;Bias&quot;, &quot;Var(fhat)&quot;, &quot;Var(eps)&quot;, &quot;MSPE&quot;) VBtradeoff[1,1] &lt;- bias1^2 VBtradeoff[2,1] &lt;- bias2^2 VBtradeoff[1,2] &lt;- var1 VBtradeoff[2,2] &lt;- var2 VBtradeoff[1,3] &lt;- var_eps VBtradeoff[2,3] &lt;- var_eps VBtradeoff[1,4] &lt;- mean(MSPE[,1]) VBtradeoff[2,4] &lt;- mean(MSPE[,2]) round(VBtradeoff, 3) ## Bias Var(fhat) Var(eps) MSPE ## fhat_1 6.25 0.000 5.25 11.500 ## fhat_2 0.00 0.539 5.25 5.788 This table clearly shows the decomposition of MSPE. The first column is the contribution to the MSPE from the bias, and the second column is the contribution from the variance of the predictor. These together make up the reducible error. The third column is the variance that comes from the data, the irreducible error. The last column is, of course, the total MSPE, and we can see that \\(\\hat{f}_2\\) is the better predictor because of its lower MSPE. 6.1 Biased estimator as a predictor Upto this point, we showed in the simulation prediction function with zero bias but high variance produce better prediction than the prediction function with zero variance but high bias. However, we can obtain better prediction function which has some bias and some variance. Better prediction function means smaller MSPE. Thus if the decline in variance would be more than then the bias in the second prediction function, then we have better predictor. Lets show this with equation first and then with simulation. We saw earlier that \\(\\bar{X}\\) is a better estimator. Now, Let’s define a biased estimator of \\(\\mu_x\\): \\[ \\hat{X}_{biased} = \\hat{\\mu}_x=\\alpha \\bar{X} \\] The sample mean \\(\\bar{X}\\) is an unbiased estimator of \\(\\mu_x\\). The magnitude of the bias is \\(\\alpha\\) and as it goes to 1, the bias becomes zero. As before, we are given one sample with three observations from the same distribution (population). We want to guess the value of a new data point from the same distribution. We will make the prediction with the best predictor which has the minimum MSPE. By using the same decomposition we can show that: \\[ \\hat{\\mu}_x=\\alpha \\bar{X} \\] \\[ \\mathbf{E}[\\hat{\\mu}_x]=\\alpha \\mu_x \\] \\[ \\mathbf{MSPE}=[(1-\\alpha) \\mu_x]^{2}+\\frac{1}{n} \\alpha^{2} \\sigma_{\\varepsilon}^{2}+\\sigma_{\\varepsilon}^{2} \\] Our first observation is that when \\(\\alpha\\) is one, the bias will be zero. Since it seems that MSPE is a convex function of \\(\\alpha\\), we can search for \\(\\alpha\\) that minimizes MSPE. The first-order-condition would give us the solution: \\[ \\frac{\\partial \\mathbf{MSPE}}{\\partial \\alpha} =0 \\rightarrow ~~ \\alpha = \\frac{\\mu^2_x}{\\mu^2_x+\\sigma^2_\\varepsilon/n}&lt;1 \\] Using the same simulation sample above , lets calculate alpha and MSPE with this new biased prediction function, and compare all 3 MSPEs. pred &lt;-rep(0, Ms) # The magnitude of bias alpha &lt;- (mean(populationX))^2/((mean(populationX)^2+var_eps/10)) alpha ## [1] 0.9966513 # Biased predictor for (i in 1:Ms) { pred[i] &lt;- alpha*predictions[i,2] } # Check if E(alpha*Xbar) = alpha*mu_x mean(pred) ## [1] 12.45708 alpha*mean(populationX) ## [1] 12.45814 # MSPE MSPE_biased &lt;- rep(0, Ms) for (i in 1:Ms) { MSPE_biased[i] &lt;- mean((populationX-pred[i])^2) } mean(MSPE_biased) ## [1] 5.786663 Let’s add this predictor into our table: VBtradeoff &lt;- matrix(0, 3, 4) rownames(VBtradeoff) &lt;- c(&quot;fhat_1&quot;, &quot;fhat_2&quot;, &quot;fhat_3&quot;) colnames(VBtradeoff) &lt;- c(&quot;Bias&quot;, &quot;Var(fhat)&quot;, &quot;Var(eps)&quot;, &quot;MSPE&quot;) VBtradeoff[1,1] &lt;- bias1^2 VBtradeoff[2,1] &lt;- bias2^2 VBtradeoff[3,1] &lt;- (mean(populationX)-mean(pred))^2 VBtradeoff[1,2] &lt;- var1 VBtradeoff[2,2] &lt;- var2 VBtradeoff[3,2] &lt;- var(pred) VBtradeoff[1,3] &lt;- var_eps VBtradeoff[2,3] &lt;- var_eps VBtradeoff[3,3] &lt;- var_eps VBtradeoff[1,4] &lt;- mean(MSPE[,1]) VBtradeoff[2,4] &lt;- mean(MSPE[,2]) VBtradeoff[3,4] &lt;- mean(MSPE_biased) round(VBtradeoff, 3) ## Bias Var(fhat) Var(eps) MSPE ## fhat_1 6.250 0.000 5.25 11.500 ## fhat_2 0.000 0.539 5.25 5.788 ## fhat_3 0.002 0.535 5.25 5.787 As seen , increase in bias is lower than decrease in variance. The prediction function with some bias and variance is the best prediction function as it has the smallest MSPE. This example shows the difference between estimation and prediction for a simplest predictor, the mean of \\(X\\). We will see a more complex example when we have a regression later. Another simulation examples are https://blog.zenggyu.com/en/post/2018-03-11/understanding-the-bias-variance-decomposition-with-a-simulated-experiment/ https://www.r-bloggers.com/2019/06/simulating-the-bias-variance-tradeoff-in-r/ use this to create education-income simulation https://daviddalpiaz.github.io/r4sl/simulating-the-biasvariance-tradeoff.html "],["overfitting.html", "Chapter 7 Overfitting", " Chapter 7 Overfitting Overfitting is a phenomenon that occurs when a statistical model describes random error (noise, irreducible error) instead of the underlying relationship. A model that has been overfitted has poor predictive performance, as it overreacts to minor fluctuations in the training data. Overfitting is a common problem in statistics and machine learning. The more flexible (complex) a model is, the more likely it is to overfit the data. A good predictive model is able to learn the pattern from your data and then to generalize it on new data. Generalization(i.e. (learning)) is the ability of a model to perform well on unseen data. Overfitting occurs when a model does not generalize well. Underfitting occurs when a model is not flexible enough to capture the underlying relationship as well. A simple example is the curve-fitting of a polynomial of a certain degree to a set of data points. A low-degree polynomial, such as a straight line, may fit the data points reasonably well, but would fail to capture the behavior of the data outside the range of the data points. However, a high-degree polynomial may fit the data points well, but would have wild fluctuations outside the range of the data points. The wild fluctuations would be fitting to the random noise in the data, and would fail to generalize to new data. We will simulate this example below. Another example is a decision tree with a large number of branches, may overfit the training data. All in all, we say that The optimal model is the simplest model (less complexity (or flexibility)) which has the right amount of Bias–variance tradeoff and fits the data without overfitting. To detect overfitting, you need to monitor the training error and the test error during the training process. As the model becomes more complex, the training error will decrease. However, when the model overfits the training data (i.e when model complexity increases), the test error will begin to increase. To detect model complexity in the linear population model, we use Mallows’ \\(C_{p}\\) statistic, or Akaike information criterion (AIC) or Bayesian information criterion (BIC). Accuracy of \\(C_{p}\\), AIC, and BIC depends on some knowledge of population model. (https://online.stat.psu.edu/stat462/node/197/) Cross-validation is common way to for estimating prediction error of a model while handling overfitting, when data is (relatively) limited. The data is divided into a training set and a test set. The model is trained on the training set and evaluated on the test set. To reduce variability, the procedure is repeated for different partitions of the data into training and test sets, and the results are averaged. We discuss these model complexity scores and techniques in related chapters. Overfitting can be reduced or avoided by using a simpler model, collecting more data, using regularization, or applying early stopping. The simplest way to prevent overfitting is to use a simpler model. The model should be as simple as possible, consistent with the bias–variance tradeoff. For example, a polynomial of low degree may be used instead of a polynomial of high degree. Another way to prevent overfitting is to use more data. More data can help to reduce the variance of the model. For example, if a model overfits the training data because it has too many parameters relative to the number of data points, then the model may not overfit if more data points are used. A third way to prevent overfitting is to use regularization. Regularization techniques add a penalty term to the loss function that measures the complexity of the model. The penalty term is called a regularizer. In neural networks, overfitting can be reduced by early stopping. During training, the error on the training set and the error on a validation set are monitored. When the error on the validation set begins to increase, training is stopped. In time series, the problem of overfitting can be addressed by applying a smoothing technique, such as moving average, to the data. Overfitting can also be reduced by removing seasonal trends from the data. The seasonal trends are then reintroduced after forecasting has been performed. https://www.r-bloggers.com/2017/06/machine-learning-explained-overfitting/ regularization: https://towardsdatascience.com/regularization-the-path-to-bias-variance-trade-off-b7a7088b4577 https://statisticsbyjim.com/regression/overfitting-regression-models/ Although it seems that overfitting is a prediction problem, it is also a serious problem in estimations, where the unbiasedness is the main objective. \\[ MSPE=E\\left[(Y-\\hat{f(X)})^{2}\\right]=Bias[\\hat{f(X)}]^{2}+Var[\\hat{f(X)}]+\\sigma^{2}=MSE(\\hat{f(X)})+\\sigma^{2} \\] using an estimator for predictions means that we use in-sample data points to calculate MSPE in predictions, which may result in overfitting and a poor out-of-sample prediction accuracy. Let’s start with an example: # Getting one-sample. set.seed(123) x_1 &lt;- rnorm(100, mean= 0, sd= 1) f &lt;- 1 + 2*x_1 - 2*(x_1^2)+ 3*(x_1^3) # DGM #y &lt;- f + rnorm(100, 0, 8) yigits eqn y &lt;- f + rnorm(100, 0, 2) inn &lt;- data.frame(y, x_1) # OLS ols1 &lt;- lm(y~ poly(x_1, degree = 1), inn) ols2 &lt;- lm(y~ poly(x_1, degree = 2), inn) ols3 &lt;- lm(y~ poly(x_1, degree = 3), inn) ols4 &lt;- lm(y~ poly(x_1, degree = 20), inn) ror &lt;- order(x_1) plot(x_1, y, col=&quot;darkgrey&quot;) lines(x_1[ror], predict(ols1)[ror], col=&quot;pink&quot;, lwd = 1.5) lines(x_1[ror], predict(ols2)[ror], col=&quot;green&quot;, lwd = 1.5) lines(x_1[ror], predict(ols3)[ror], col=&quot;blue&quot;, lwd = 1.5) lines(x_1[ror], predict(ols4)[ror], col=&quot;red&quot; , lwd = 1.5) legend(&quot;bottomright&quot;, c(&quot;ols1&quot;, &quot;ols2&quot;, &quot;ols3&quot;, &quot;ols4&quot;), col = c(&quot;pink&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;red&quot;), lwd = 2) As this is simulation, we know that the “true” estimator, \\(f(x)\\), which is the “blue” line, is: \\[ f(x_i)=\\beta_0+\\beta_1 x_{1i}+\\beta_2 x_{1i}^2+\\beta_2 x_{1i}^3 = 1+2x_{1i}-2x_{1i}^2+3 x_{1i}^3. \\] Now we can calculate in-sample empirical MSPE: \\[~\\] # MSE MSPE1 &lt;- mean((predict(ols1)-y)^2) # which is also mean(ols1$residuals^2) MSPE2 &lt;- mean((predict(ols2)-y)^2) MSPE3 &lt;- mean((predict(ols3)-y)^2) MSPE4 &lt;- mean((predict(ols4)-y)^2) all &lt;- c(MSPE1, MSPE2, MSPE3, MSPE4) MSPE &lt;- matrix(all, 4, 1) row.names(MSPE) &lt;- c(&quot;ols1&quot;, &quot;ols2&quot;, &quot;ols3&quot;, &quot;ols4&quot;) colnames(MSPE) &lt;- &quot;In-sample MSPE&#39;s&quot; MSPE ## In-sample MSPE&#39;s ## ols1 27.400554 ## ols2 23.198011 ## ols3 3.650487 ## ols4 3.055368 As we see, the overfitted \\(\\hat{f}(x)\\), the \\(4^{th}\\) model, has a lower empirical in-sample MSPE. If we use nonparametric models, we can even find a better fitting model with a lower empirical in-sample MSPE. We call these MSPE’s empirical because they are not calculated based on repeated samples, which would give an expected value of squared errors over all samples. In practice, however, we have only one sample. Therefore, even if our objective is to find an unbiased estimator of \\({f}(x)\\), not a prediction of \\(y\\), since we choose our estimator, \\(\\hat{f}(x)\\), by the empirical in-sample MSPE, we may end up with an overfitted \\(\\hat{f}(x)\\), such as the \\(4^{th}\\) estimator. Would an overfitted model create a biased estimator? We will see the answer in a simulation later. However, in estimations, our objective is not only to find an unbiased estimator but also to find the one that has the minimum variance. We know that our \\(3^{rd}\\) model is unbiased estimator of \\(f(x)\\) as is the overfitted \\(4^{th}\\) estimator. Which one should we choose? We have answered this question at the beginning of this chapter: the one with the minimum variance. Since overfitting would create a greater variance, our choice must be the \\(3^{rd}\\) model. That is why we do not use the empirical in-sample MSPE as a “cost” or “risk” function in finding the best estimator. This process is called a “data mining” exercise based on one sample without any theoretical justification on what the “true” model would be. This is a general problem in empirical risk minimization specially in finding unbiased estimators of population parameters. To see all these issues in actions, let’s have a simulation for the decomposition of in-sample unconditional MSPE’s. # Function for X - fixed at repeated samples xfunc &lt;- function(n){ set.seed(123) x_1 &lt;- rnorm(n, 0, 1) return(x_1) } # Function for simulation (M - number of samples) simmse &lt;- function(M, n, sigma, poldeg){ x_1 &lt;- xfunc(n) # Repeated X&#39;s in each sample # Containers MSPE &lt;- rep(0, M) yhat &lt;- matrix(0, M, n) olscoef &lt;- matrix(0, M, poldeg+1) ymat &lt;- matrix(0, M, n) # Loop for samples for (i in 1:M) { f &lt;- 1 + 2*x_1 - 2*I(x_1^2) # DGM y &lt;- f + rnorm(n, 0, sigma) samp &lt;- data.frame(&quot;y&quot; = y, x_1) # Estimator ols &lt;- lm(y ~ poly(x_1, degree = poldeg, raw=TRUE), samp) olscoef[i, ] &lt;- ols$coefficients # Yhat&#39;s yhat[i,] &lt;- predict(ols, samp) # MSPE - That is, residual sum of squares MSPE[i] &lt;- mean((ols$residuals)^2) ymat[i,] &lt;- y } output &lt;- list(MSPE, yhat, sigma, olscoef, f, ymat) return(output) } # running different fhat with different polynomial degrees output1 &lt;- simmse(2000, 100, 7, 1) output2 &lt;- simmse(2000, 100, 7, 2) #True model (i.e fhat = f) output3 &lt;- simmse(2000, 100, 7, 5) output4 &lt;- simmse(2000, 100, 7, 20) # Table tab &lt;- matrix(0, 4, 5) row.names(tab) &lt;- c(&quot;ols1&quot;, &quot;ols2&quot;, &quot;ols3&quot;, &quot;ols4&quot;) colnames(tab) &lt;- c(&quot;bias^2&quot;, &quot;var(yhat)&quot;, &quot;MSE&quot;, &quot;var(eps)&quot;, &quot;In-sample MSPE&quot;) f &lt;- output1[[5]] # Var(yhat) - We use our own function instead of &quot;var()&quot; tab[1,2] &lt;- mean(apply(output1[[2]], 2, function(x) mean((x-mean(x))^2))) tab[2,2] &lt;- mean(apply(output2[[2]], 2, function(x) mean((x-mean(x))^2))) tab[3,2] &lt;- mean(apply(output3[[2]], 2, function(x) mean((x-mean(x))^2))) tab[4,2] &lt;- mean(apply(output4[[2]], 2, function(x) mean((x-mean(x))^2))) # Bias^2 = (mean(yhat))-f)^2 tab[1,1] &lt;- mean((apply(output1[[2]], 2, mean) - f)^2) tab[2,1] &lt;- mean((apply(output2[[2]], 2, mean) - f)^2) tab[3,1] &lt;- mean((apply(output3[[2]], 2, mean) - f)^2) tab[4,1] &lt;- mean((apply(output4[[2]], 2, mean) - f)^2) # MSE fmat &lt;- matrix(f, nrow(output1[[6]]), length(f), byrow = TRUE) tab[1,3] &lt;- mean(colMeans((fmat - output1[[2]])^2)) tab[2,3] &lt;- mean(colMeans((fmat - output2[[2]])^2)) tab[3,3] &lt;- mean(colMeans((fmat - output3[[2]])^2)) tab[4,3] &lt;- mean(colMeans((fmat - output4[[2]])^2)) # # MSPE - This can be used as well, which is RSS # tab[1,5] &lt;- mean(output1[[1]]) # tab[2,5] &lt;- mean(output2[[1]]) # tab[3,5] &lt;- mean(output3[[1]]) # tab[4,5] &lt;- mean(output4[[1]]) # MSPE tab[1,5] &lt;- mean(colMeans((output1[[6]] - output1[[2]])^2)) tab[2,5] &lt;- mean(colMeans((output2[[6]] - output2[[2]])^2)) tab[3,5] &lt;- mean(colMeans((output3[[6]] - output3[[2]])^2)) tab[4,5] &lt;- mean(colMeans((output4[[6]] - output4[[2]])^2)) # Irreducable error - var(eps) = var(y) tab[1,4] &lt;- mean(apply(output1[[6]], 2, function(x) mean((x-mean(x))^2))) tab[2,4] &lt;- mean(apply(output2[[6]] - output2[[2]], 2, function(x) mean(x^2))) tab[3,4] &lt;- mean(apply(output3[[6]], 2, function(x) mean((x-mean(x))^2))) tab[4,4] &lt;- mean(apply(output4[[6]], 2, function(x) mean((x-mean(x))^2))) round(tab, 4) ## bias^2 var(yhat) MSE var(eps) In-sample MSPE ## ols1 4.9959 0.9467 5.9427 49.1493 53.2219 ## ols2 0.0006 1.4224 1.4230 47.7574 47.7574 ## ols3 0.0010 2.9011 2.9021 49.1493 46.2783 ## ols4 0.0098 10.2528 10.2626 49.1493 38.9179 (SIMULATION TRAINING ERROR VS TEST ERROR CH 6, (https://web.stanford.edu/~rjohari/teaching/notes.html) The table verifies that \\(\\mathbf{MSE}(\\hat{f})=\\mathbf{Var}(\\hat{f})+\\left[\\mathbf{bias}(\\hat{f})\\right]^{2}.\\) However, it seems that the MSPE (in-sample) of each model is “wrong”, which is not the sum of MSE and \\(\\mathbf{Var}(\\varepsilon)\\). Now, back to our question: Why is the in-sample MSPE not the sum of MSE and \\(\\sigma^2\\)? Let’s look at MSPE again but this time with different angle. For linear regression with a linear population model We define MSPE over some data points, as we did in our simulation above, and re-write it as follows: \\[ \\mathbf{MSPE}_{out}=\\mathbf{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n}\\left(y&#39;_{i}-\\hat{f}(x_i)\\right)^{2}\\right],~~~~~~\\text{where}~~y&#39;_i=f(x_i)+\\varepsilon&#39;_i \\] This type of MSPE is also called as unconditional MSPE. Inside of the brackets is the “prediction error” for a range of out-of-sample data points. The only difference here is that we distinguish \\(y&#39;_i\\) as out-of-sample data points. Likewise, we define MSPE for in-sample data points \\(y_i\\) as \\[ \\mathbf{MSPE}_{in}=\\mathbf{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}(x_i)\\right)^{2}\\right],~~~~~~\\text{where}~~y_i=f(x_i)+\\varepsilon_i. \\] Note that \\(\\varepsilon&#39;_i\\) and \\(\\varepsilon_i\\) are independent but identically distributed. Moreover \\(y&#39;_i\\) and \\(y_i\\) has the same distribution. \\[ \\mathbf{MSPE}_{out} =\\mathbf{MSPE}_{in}+\\frac{2}{n} \\sigma^{2}(p+1). \\] The last term quantifies the overfitting, the the amount by which the in-sample MSPE systematically underestimates its true MSPE, i.e. out-of-sample MSPE. Note also that the overfitting grows with the “noise” (\\(\\sigma^2\\)) in the data, shrinks with the sample size (\\(n\\)), grows with the number of variables (\\(p\\)). Hence, as we had stated earlier, the overfitting problem gets worse as \\(p/n\\) gets bigger. Minimizing the in-sample MSPE completely ignores the overfitting by picking models which are too large and with a very poor out-of-sample prediction accuracy. Now we can calculate the size of overfitting in our simulation. # New Table tabb &lt;- matrix(0, 4, 3) row.names(tabb) &lt;- c(&quot;ols1&quot;, &quot;ols2&quot;, &quot;ols3&quot;, &quot;ols4&quot;) colnames(tabb) &lt;- c(&quot;Cov(yi, yhat)&quot;,&quot;True MSPE&quot;, &quot;TrueMSPE-Cov&quot;) #COV tabb[1,1] &lt;- 2*mean(diag(cov(output1[[2]], output1[[6]]))) tabb[2,1] &lt;- 2*mean(diag(cov(output2[[2]], output2[[6]]))) tabb[3,1] &lt;- 2*mean(diag(cov(output3[[2]], output3[[6]]))) tabb[4,1] &lt;- 2*mean(diag(cov(output4[[2]], output4[[6]]))) #True MSPE tabb[1,2] &lt;- tab[1,3] + tab[1,4] tabb[2,2] &lt;- tab[2,3] + tab[2,4] tabb[3,2] &lt;- tab[3,3] + tab[3,4] tabb[4,2] &lt;- tab[4,3] + tab[4,4] #True MSPE - Cov (to compare with the measures in the earlier table) tabb[1,3] &lt;- tabb[1,2] - tabb[1,1] tabb[2,3] &lt;- tabb[2,2] - tabb[2,1] tabb[3,3] &lt;- tabb[3,2] - tabb[3,1] tabb[4,3] &lt;- tabb[4,2] - tabb[4,1] t &lt;- cbind(tab, tabb) round(t, 4) ## bias^2 var(yhat) MSE var(eps) In-sample MSPE Cov(yi, yhat) True MSPE ## ols1 4.9959 0.9467 5.9427 49.1493 53.2219 1.8944 55.0920 ## ols2 0.0006 1.4224 1.4230 47.7574 47.7574 2.8463 49.1804 ## ols3 0.0010 2.9011 2.9021 49.1493 46.2783 5.8052 52.0514 ## ols4 0.0098 10.2528 10.2626 49.1493 38.9179 20.5158 59.4119 ## TrueMSPE-Cov ## ols1 53.1976 ## ols2 46.3341 ## ols3 46.2462 ## ols4 38.8961 Let’s have a pause and look at this table: We know that the “true” model is “ols2” in this simulation. However, we cannot know the true model and we have only one sample in practice. If we use the in-sample MSPE to choose a model, we pick “ols4” as it has the minimum MSPE. Not only “ols4” is the worst predictor among all models, it is also the worst estimator among the unbiased estimators “ols2”, “ols3”, and “ols4”, as it has the highest MSE. If our task is to find the best predictor, we cannot use in-sample MSPE, as it give us “ols4”, the worst predictor. https://web.stanford.edu/~rjohari/teaching/notes.html https://bookdown.org/marklhc/notes_bookdown/model-comparison-and-regularization.html use figure exaple.take data takesubset show that in figure Detailed explanation of Simulation for MSPE decomposition We will come back to this point, but before going further, to make the simulation calculations more understandable, I put here simple illustrations for each calculation. Think of a simulation as a big matrix: each row contains one sample and each column contains one observation of \\(x_i\\). For example, if we have 500 samples and each sample we have 100 observations, the “matrix” will be 500 by 100. The figures below show how the simulations are designed and each term is calculated Technical Explanation of MSPE out and in sample Let’s look at \\(\\mathbf{E}\\left[(y&#39;_i-\\hat{f}(x_i))^{2}\\right]\\) closer. By using the definition of variance, \\[ \\begin{aligned} \\mathbf{E}\\left[(y&#39;_i-\\hat{f}(x_i))^{2}\\right] &amp;=\\mathbf{Var}\\left[y&#39;_{i}-\\hat{f}(x_i)\\right]+\\left(\\mathbf{E}\\left[y&#39;_{i}-\\hat{f}(x_i)\\right]\\right)^{2}\\\\ &amp;=\\mathbf{Var}\\left[y&#39;_{i}\\right]+\\mathbf{Var}\\left[\\hat{f}(x_i)\\right]-2 \\mathbf{Cov}\\left[y&#39;_{i}, \\hat{f}(x_i)\\right]+\\left(\\mathbf{E}\\left[y&#39;_{i}\\right]-\\mathbf{E}\\left[\\hat{f}(x_i)\\right]\\right)^{2} \\end{aligned} \\] Similarly, \\[ \\begin{aligned} \\mathbf{E}\\left[(y_i-\\hat{f}(x_i))^{2}\\right] &amp;=\\mathbf{Var}\\left[y_{i}-\\hat{f}(x_i)\\right]+\\left(\\mathbf{E}\\left[y_{i}-\\hat{f}(x_i)\\right]\\right)^{2}\\\\ &amp;=\\mathbf{Var}\\left[y_{i}\\right]+\\mathbf{Var}\\left[\\hat{f}(x_i)\\right]-2 \\mathbf{Cov}\\left[y_{i}, \\hat{f}(x_i)\\right]+\\left(\\mathbf{E}\\left[y_{i}\\right]-\\mathbf{E}\\left[\\hat{f}(x_i)\\right]\\right)^{2} \\end{aligned} \\] Remember our earlier derivation of variance-bias decomposition: When we predict out-of-sample data points, we know that \\(y_0\\) and \\(\\hat{f}(x_0)\\) are independent. We had stated it differently: \\(\\varepsilon_0\\) is independent from \\(\\hat{f}(x_0)\\). In other words, how we estimate our estimator is an independent process from \\(y&#39;_i\\). Hence, \\(\\mathbf{Cov}\\left[y&#39;_{i}, \\hat{f}(x_i)\\right]=0\\). The critical point here is to understand is \\(\\mathbf{Cov}\\left[y_{i}, \\hat{f}(x_i)\\right]\\), is not zero. This is because the estimator \\(\\hat{f}(x_i)\\) is chosen in a way that its difference from \\(y_i\\) should be minimum. Hence, our estimator is not an independent than in-sample \\(y_i\\) data points, on the contrary, we use them to estimate \\(\\hat{f}(x_i)\\). In fact, we can even choose \\(\\hat{f}(x_i) = y_i\\) where the MSPE would be zero. In that case correlation between \\(\\hat{f}(x_i)\\) and \\(y_i\\) would be 1. Using the fact that \\(\\mathbf{E}(y&#39;_i) = \\mathbf{E}(y_i)\\) and \\(\\mathbf{Var}(y&#39;_i) = \\mathbf{Var}(y_i)\\), we can now re-write \\(\\mathbf{E}\\left[(y&#39;_i-\\hat{f}(x_i))^{2}\\right]\\) as follows: \\[ \\mathbf{E}\\left[(y&#39;_i-\\hat{f}(x_i))^{2}\\right]=\\mathbf{Var}\\left[y_{i}\\right]+\\mathbf{Var}\\left[\\hat{f}(x_i)\\right]+\\left(\\mathbf{E}\\left[y_{i}\\right]-\\mathbf{E}\\left[\\hat{f}(x_i)\\right]\\right)^{2}\\\\ =\\mathbf{E}\\left[(y_i-\\hat{f}(x_i))^{2}\\right]+2 \\mathbf{Cov}\\left[y_{i}, \\hat{f}(x_i)\\right]. \\] Averaging over data points, \\[ \\mathbf{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}^{\\prime}-\\hat{f}(x_i)\\right)^{2}\\right]=\\mathbf{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}(x_i)\\right)^{2}\\right]+\\frac{2}{n} \\sum_{i=1}^{n} \\mathbf{Cov}\\left[y_{i}, \\hat{f}(x_i)\\right]. \\] For a linear model, it can be shown that \\[ \\frac{2}{n} \\sum_{i=1}^{n} \\mathbf{Cov}\\left[y_{i}, \\hat{f}(x_i)\\right]=\\frac{2}{n} \\sigma^{2}(p+1). \\] Hence, \\[ \\mathbf{MSPE}_{out} =\\mathbf{MSPE}_{in}+\\frac{2}{n} \\sigma^{2}(p+1). \\] "],["regression-v.s.-classification.html", "Chapter 8 Regression v.s. Classification", " Chapter 8 Regression v.s. Classification Both regression and classification problems fall into the supervised learning category. Each task involves developing a model that learns from historical data which enables it to make predictions on new instances that we do not have answers for. More formally, all supervised learning tasks involve learning a function that maps an input to an output based on example input-output pairs. But the distinction between classification vs regression is how they are used on particular machine learning problems. For weather forecast/prediction, if we predict temperature, which is continuous outcome, then it is regression. if we predict whether it will rain or not,then it is classification as outcome is categorical value. The key distinction between Classification vs Regression algorithms is Regression algorithms are used to determine continuous values such as price, income, age, etc. Classification algorithms are used to predict or classify several categorical classes, i.e. discrete values. The regression Algorithm can be further separated into Non-linear and Linear Regression. The Classification algorithms can be classified into Multi-class Classifier and Binary Classifier. Regression attempt to find the best fit line, which predicts the output more accurately. Classification tries to find the decision boundary, which divides the dataset into different classes. Classification algorithms used in ML used to solve problems such as Voice Recognition, Identification of spam emails, Identification of cancer cells, etc. HEre are some common problems we can use Classification and regressions in social sciences: Regression: Y is a continuous variable (numeric). Examples: Predict income given demographic factors Predict consumption amount given profile Predict the price of a product Predict the test score of a student Predict Cholesterol levels Predict hospital expenditure Classification: Y is a categorical variable (factor). Examples: Predict whether the individual employed or not Predict whether the individual have kids or not Predict whether this customer going to buy an item or not Predict whether a company will default or not Predict whether a student will drop out of a school or not Predict whether an individual will have an heart attact or not Popular types of Regression Algorithms include: Simple Linear Regression; Multiple Linear Regression;Polynomial Regression; Ridge Regression; Lasso Regression; Neural Networks Regression; Support Vector Regression; Decision Tree Regression; Random Forest Regression Popular classification algorithms include: Logistic Regression; Naive Bayes; K-Nearest Neighbors; Decision Tree; Support Vector Machines; Kernel SVM; Naive Bayes; Random Forest Classification As a Final Thoughts, the way we determine whether a task is a classification or regression problem is by the output. Regression tasks are concerned with predicting a continuous value, whereas classification tasks are concerned with predicting discrete values. Also, the way we evaluate each type of problem is different for example the mean squared error is a useful metric for regression tasks but this wouldn’t work for a classification. Similarly, accuracy wouldn’t be an effective metric for regression tasks but it can be useful for classification tasks. "],["parametric-estimations---basics.html", "Chapter 9 Parametric Estimations - Basics 9.1 Parametric Estimations 9.2 LPM 9.3 Logistic Regression", " Chapter 9 Parametric Estimations - Basics Parametric vs. Nonparametric methods {-} https://towardsdatascience.com/parametric-vs-non-parametric-methods-2cea475da1a In parametric methods, we typically make an assumption with regards to the form of the function f. For example, you could make an assumption that the unknown function f is linear. parametric methods in Machine Learning usually take a model-based approach where we make an assumption with respect to form of the function to be estimated and then we select a suitable model based on this assumption in order to estimate the set of parameters.The biggest disadvantage of parametric methods is that the assumptions we make may not always be true. For instance, you may assume that the form of the function is linear, whilst it is not. Therefore, these methods involve less flexible algorithms and are usually used for less complex problems. However, parametric methods tend to be quite fast and they also require significantly less data compared to non-parametric methods (more on this in the following section). Additionally, since parametric methods tend to be less flexible and suitable for less complex problems, they are more interpretable. On the other hand, non-parametric methods refer to a set of algorithms that do not make any underlying assumptions with respect to the form of the function to be estimated. And since no assumption is being made, such methods are capable of estimating the unknown function f that could be of any form. Non-parametric methods tend to be more accurate as they seek to best fit the data points. However, this comes at the expense of requiring a very large number of observations that is needed in order to estimate the unknown function f accurately. Additionally, these methods tend to be less efficient when it comes to training the models. Furthermore, non-parametric methods may sometimes introduce overfitting. Since these algorithms tend to be more flexible, they may sometimes learn the errors and noise in a way that they cannot generalise well to new, unseen data points. On the flip side, non-parametric methods are quite flexible and can lead to better model performance since no assumptions are being made about the underlying function. http://www2.math.uu.se/~thulin/mm/breiman.pdf https://www.quora.com/What-was-Leo-Breiman-trying-to-convey-in-his-research-paper-Statistical-Modeling-The-Two-Cultures-Was-he-trying-to-say-that-the-field-is-odd-Did-he-mean-that-algorithmic-modelling-machine-learning-is-superior-to-traditional-data-modelling https://www.uio.no/studier/emner/matnat/math/STK9200/h21/two_cultures_pdf.pdf http://www.stat.columbia.edu/~gelman/research/published/gelman_breiman.pdf https://muse.jhu.edu/article/799731/pdf https://muse.jhu.edu/issue/45147 Leo Breiman (Breiman_2001?): Statistical Modeling: The Two Cultures: For instance, in the Journal of the American Statistical Association (JASA), virtually every article contains a statement of the form: Assume that the data are generated by the following model:… I am deeply troubled by the current and past use of data models in applications, where quantitative conclusions are drawn and perhaps policy decisions made. … assume the data is generated by independent draws from the model* \\[ y=b_{0}+\\sum_{1}^{M} b_{m} x_{m}+\\varepsilon \\] where the coefficients are to be estimated. The error term is N(0, \\(\\sigma^2\\)) and \\(\\sigma^2\\) is to be estimated. Given that the data is generated this way, elegant tests of hypotheses,confidence intervals,distributions of the residual sum-of-squares and asymptotics can be derived. This made the model attractive in terms of the mathematics involved. This theory was used both by academics statisticians and others to derive significance levels for coefficients on the basis of model (R), with little consideration as to whether the data on hand could have been generated by a linear model. Hundreds, perhaps thousands of articles were published claiming proof of something or other because the coefficient was significant at the 5% level… …With the insistence on data models, multivariate analysis tools in statistics are frozen at discriminant analysis and logistic regression in classification and multiple linear regression in regression. Nobody really believes that multivariate data is multivariate normal, but that data model occupies a large number of pages in every graduate text book on multivariate statistical analysis… According to Breiman, there are two “cultures”: The Data Modeling Culture : One assumes that the data are generated by a given stochastic data model (econometrics) … Algorithmic Modeling Culture: One uses algorithmic models and treats the data mechanism as unknown (machine learning) … He argues that the focus in the statistical community on data models has: Led to irrelevant theory and questionable scientific conclusions; Kept statisticians from using more suitable algorithmic models; Prevented statisticians from working on exciting new problems. In parametric econometrics we assume that the data come from a generating process that takes the following form: \\[ y=X \\beta+\\varepsilon \\] Model (\\(X\\)’s) are determined by the researcher and probability theory is a foundation of econometrics In Machine learning we do not make any assumption on how the data have been generated: \\[ y \\approx m(X) \\] Model (\\(X\\)’s) is not selected by the researcher and probability theory is not required Nonparametric econometrics makes the link between the two: Machine Learning: an extension of nonparametric econometrics To see the difference between two “cultures”, we start with parametric modeling in classification problems. 9.1 Parametric Estimations So far we have only considered models for numeric response variables. What happens if the response variable is categorical? Can we use linear models in these situations? Yes, we can. To understand how, let’s look at the model that we have been using, ordinary least-square (OLS) regression, which is actually a specific case of the more general, generalized linear model (GLM). So, in general, GLMs relate the mean of the response to a linear combination of the predictors, \\(\\eta(x)\\), through the use of a link function, \\(g()\\). That is, \\[\\begin{equation} \\eta(\\mathbf{x})=g(\\mathrm{E}[Y | \\mathbf{X}=\\mathbf{x}]), \\tag{9.1} \\end{equation}\\] Or, \\[\\begin{equation} \\eta(\\mathbf{x})=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\ldots+\\beta_{p-1} x_{p-1} = g(\\mathrm{E}[Y | \\mathbf{X}=\\mathbf{x}]) \\tag{9.2} \\end{equation}\\] In the case of a OLS, \\[ g(\\mathrm{E}[Y | \\mathbf{X}=\\mathbf{x}]) = E[Y | \\mathbf{X}=\\mathbf{x}], \\] To illustrate the use of a GLM we’ll focus on the case of binary responses variable coded using 0 and 1. In practice, these 0 and 1s will code for two classes such as yes/no, committed-crime/not, sick/healthy, etc. \\[ Y=\\left\\{\\begin{array}{ll}{1} &amp; {\\text { yes }} \\\\ {0} &amp; {\\text { no }}\\end{array}\\right. \\] 9.2 LPM Estimate the regression function, which under squared error loss is the conditional mean of \\(Y\\) , the response, given \\(X\\), the features. These goal are essentially the same. We want to fit a model that “generalizes” well, that is, works well on new data that was not used to fit the model. To do this, we want to use a model of appropriate flexibility so as not to overfit to the training data. Linear models are a family of parametric models which assume that the regression function (outcome is known and continuous) is a linear combination of the features. The \\(\\beta\\) coefficients are model parameters that are learned from the data via least squares or maximum likelihood. A linear classifier (like LPM and Logistic) is one where a “hyperplane” is formed by taking a linear combination of the features, such that one side of the hyperplane predicts one class and the other side predicts the other. nonparametric classifier such as knn Let’s use the same dataset, Vehicles, that we used in the lab sections and create a new variable, mpg: #Inspect the data before doing anything library(fueleconomy) #install.packages(&quot;fueleconomy&quot;) data(vehicles) df &lt;- as.data.frame(vehicles) #Keep only observations without NA dim(df) ## [1] 33442 12 data &lt;- df[complete.cases(df), ] dim(data) ## [1] 33382 12 #Let&#39;s create a binary variable, mpg = 1 if hwy &gt; mean(hwy), 0 otherwise mpg &lt;- c(rep(0, nrow(data))) #Create vector mpg data2 &lt;- cbind(data, mpg) # add it to data data2$mpg[data2$hwy &gt; mean(data2$hwy)] &lt;- 1 We are going to have a model that will predict whether the vehicle is a high mpg (i.e. mpg = 1) or low mpg (mpg = 0) car. As you notice, we have lots of character variables. Our model cannot accept character variables, but we can convert them into factor variables that give each unique character variable a number. This allows our model to accept our data. Let’s convert them to factor variables now: for (i in 1:ncol(data2)) { if(is.character(data2[,i])) data2[,i] &lt;- as.factor(data2[,i]) } str(data2) ## &#39;data.frame&#39;: 33382 obs. of 13 variables: ## $ id : num 13309 13310 13311 14038 14039 ... ## $ make : Factor w/ 124 levels &quot;Acura&quot;,&quot;Alfa Romeo&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ model: Factor w/ 3174 levels &quot;1-Ton Truck 2WD&quot;,..: 28 28 28 29 29 29 29 29 29 30 ... ## $ year : num 1997 1997 1997 1998 1998 ... ## $ class: Factor w/ 34 levels &quot;Compact Cars&quot;,..: 29 29 29 29 29 29 29 29 29 1 ... ## $ trans: Factor w/ 46 levels &quot;Auto (AV-S6)&quot;,..: 32 43 32 32 43 32 32 43 32 32 ... ## $ drive: Factor w/ 7 levels &quot;2-Wheel Drive&quot;,..: 5 5 5 5 5 5 5 5 5 5 ... ## $ cyl : num 4 4 6 4 4 6 4 4 6 5 ... ## $ displ: num 2.2 2.2 3 2.3 2.3 3 2.3 2.3 3 2.5 ... ## $ fuel : Factor w/ 12 levels &quot;CNG&quot;,&quot;Diesel&quot;,..: 11 11 11 11 11 11 11 11 11 7 ... ## $ hwy : num 26 28 26 27 29 26 27 29 26 23 ... ## $ cty : num 20 22 18 19 21 17 20 21 17 18 ... ## $ mpg : num 1 1 1 1 1 1 1 1 1 0 ... Done! We are ready to have a model to predict mpg. For now, we’ll use only fuel. model1 &lt;- lm(mpg ~ fuel + 0, data = data2) #No intercept summary(model1) ## ## Call: ## lm(formula = mpg ~ fuel + 0, data = data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8571 -0.4832 -0.2694 0.5168 0.7306 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## fuelCNG 0.362069 0.065383 5.538 3.09e-08 *** ## fuelDiesel 0.479405 0.016843 28.463 &lt; 2e-16 *** ## fuelGasoline or E85 0.269415 0.015418 17.474 &lt; 2e-16 *** ## fuelGasoline or natural gas 0.277778 0.117366 2.367 0.0180 * ## fuelGasoline or propane 0.000000 0.176049 0.000 1.0000 ## fuelMidgrade 0.302326 0.075935 3.981 6.87e-05 *** ## fuelPremium 0.507717 0.005364 94.650 &lt; 2e-16 *** ## fuelPremium and Electricity 1.000000 0.497942 2.008 0.0446 * ## fuelPremium Gas or Electricity 0.857143 0.188205 4.554 5.27e-06 *** ## fuelPremium or E85 0.500000 0.053081 9.420 &lt; 2e-16 *** ## fuelRegular 0.483221 0.003311 145.943 &lt; 2e-16 *** ## fuelRegular Gas and Electricity 1.000000 0.176049 5.680 1.36e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4979 on 33370 degrees of freedom ## Multiple R-squared: 0.4862, Adjusted R-squared: 0.486 ## F-statistic: 2631 on 12 and 33370 DF, p-value: &lt; 2.2e-16 What we estimated is LPM. Since \\(Y\\) is 1 or 0, \\[ E[Y | \\mathbf{X}=\\mathbf{Regular}]) = Probability(Y|X = \\mathbf{Regular}), \\] In this context, the link function is called “identity” because it directly “links” the probability to the linear function of the predictor variables. Let’s see if we can verify this: tab &lt;- table(data2$fuel, data2$mpg) ftable(addmargins(tab)) ## 0 1 Sum ## ## CNG 37 21 58 ## Diesel 455 419 874 ## Gasoline or E85 762 281 1043 ## Gasoline or natural gas 13 5 18 ## Gasoline or propane 8 0 8 ## Midgrade 30 13 43 ## Premium 4242 4375 8617 ## Premium and Electricity 0 1 1 ## Premium Gas or Electricity 1 6 7 ## Premium or E85 44 44 88 ## Regular 11688 10929 22617 ## Regular Gas and Electricity 0 8 8 ## Sum 17280 16102 33382 prop.table(tab, 1) ## ## 0 1 ## CNG 0.6379310 0.3620690 ## Diesel 0.5205950 0.4794050 ## Gasoline or E85 0.7305849 0.2694151 ## Gasoline or natural gas 0.7222222 0.2777778 ## Gasoline or propane 1.0000000 0.0000000 ## Midgrade 0.6976744 0.3023256 ## Premium 0.4922827 0.5077173 ## Premium and Electricity 0.0000000 1.0000000 ## Premium Gas or Electricity 0.1428571 0.8571429 ## Premium or E85 0.5000000 0.5000000 ## Regular 0.5167794 0.4832206 ## Regular Gas and Electricity 0.0000000 1.0000000 Yes! That’s why OLS with a binary \\(Y\\) is actually LPM. That is, \\[ Pr[Y = 1 | x=\\mathbf{Regular}]) = \\beta_{0}+\\beta_{1} x_{i}. \\] A more formal explanation is related to how \\(Y\\) is distributed. Since \\(Y\\) has only 2 possible outcomes (1 and 0), it has a specific probability distribution. First, let’s refresh our memories about Binomial and Bernoulli distributions. In general, if a random variable, \\(X\\), follows the binomial distribution with parameters \\(n \\in \\mathbb{N}\\) and \\(p \\in [0,1]\\), we write \\(X \\sim B(n, p)\\). The probability of getting exactly \\(k\\) successes in \\(n\\) trials is given by the probability mass function: \\[\\begin{equation} \\operatorname{Pr}(X=k)=\\left(\\begin{array}{l}{n} \\\\ {k}\\end{array}\\right) p^{k}(1-p)^{n-k} \\tag{9.3} \\end{equation}\\] for \\(k = 0, 1, 2, ..., n\\), where \\[ \\left(\\begin{array}{l}{n} \\\\ {k}\\end{array}\\right)=\\frac{n !}{k !(n-k) !} \\] Formula 5.3 can be understood as follows: \\(k\\) successes occur with probability \\(p^k\\) and \\(n-k\\) failures occur with probability \\((1-p)^{n−k}\\). However, the \\(k\\) successes can occur anywhere among the \\(n\\) trials, and there are \\(n!/k!(n!-k!)\\) different ways of distributing \\(k\\) successes in a sequence of \\(n\\) trials. Suppose a biased coin comes up heads with probability 0.3 when tossed. What is the probability of achieving 4 heads after 6 tosses? \\[ \\operatorname{Pr}(4 \\text { heads})=f(4)=\\operatorname{Pr}(X=4)=\\left(\\begin{array}{l}{6} \\\\ {4}\\end{array}\\right) 0.3^{4}(1-0.3)^{6-4}=0.059535 \\] The Bernoulli distribution on the other hand, is a discrete probability distribution of a random variable which takes the value 1 with probability \\(p\\) and the value 0 with probability \\(q = (1 - p)\\), that is, the probability distribution of any single experiment that asks a yes–no question. The Bernoulli distribution is a special case of the binomial distribution, where \\(n = 1\\). Symbolically, \\(X \\sim B(1, p)\\) has the same meaning as \\(X \\sim Bernoulli(p)\\). Conversely, any binomial distribution, \\(B(n, p)\\), is the distribution of the sum of \\(n\\) Bernoulli trials, \\(Bernoulli(p)\\), each with the same probability \\(p\\). \\[ \\operatorname{Pr}(X=k) =p^{k}(1-p)^{1-k} \\quad \\text { for } k \\in\\{0,1\\} \\] Formally, the outcomes \\(Y_i\\) are described as being Bernoulli-distributed data, where each outcome is determined by an unobserved probability \\(p_i\\) that is specific to the outcome at hand, but related to the explanatory variables. This can be expressed in any of the following equivalent forms: \\[\\begin{equation} \\operatorname{Pr}\\left(Y_{i}=y | x_{1, i}, \\ldots, x_{m, i}\\right)=\\left\\{\\begin{array}{ll}{p_{i}} &amp; {\\text { if } y=1} \\\\ {1-p_{i}} &amp; {\\text { if } y=0}\\end{array}\\right. \\tag{9.4} \\end{equation}\\] The expression 5.4 is the probability mass function of the Bernoulli distribution, specifying the probability of seeing each of the two possible outcomes. Similarly, this can be written as follows, which avoids having to write separate cases and is more convenient for certain types of calculations. This relies on the fact that \\(Y_{i}\\) can take only the value 0 or 1. In each case, one of the exponents will be 1, which will make the outcome either \\(p_{i}\\) or 1−\\(p_{i}\\), as in 5.4.1 \\[ \\operatorname{Pr}\\left(Y_{i}=y | x_{1, i}, \\ldots, x_{m, i}\\right)=p_{i}^{y}\\left(1-p_{i}\\right)^{(1-y)} \\] Hence this shows that \\[ \\operatorname{Pr}\\left(Y_{i}=1 | x_{1, i}, \\ldots, x_{m, i}\\right)=p_{i}=E[Y_{i} | \\mathbf{X}=\\mathbf{x}]) \\] Let’s have a more complex model: model2 &lt;- lm(mpg ~ fuel + drive + cyl, data = data2) summary(model2) ## ## Call: ## lm(formula = mpg ~ fuel + drive + cyl, data = data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.09668 -0.21869 0.01541 0.12750 0.97032 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.858047 0.049540 17.320 &lt; 2e-16 *** ## fuelDiesel 0.194540 0.047511 4.095 4.24e-05 *** ## fuelGasoline or E85 0.030228 0.047277 0.639 0.52258 ## fuelGasoline or natural gas 0.031187 0.094466 0.330 0.74129 ## fuelGasoline or propane 0.031018 0.132069 0.235 0.81432 ## fuelMidgrade 0.214471 0.070592 3.038 0.00238 ** ## fuelPremium 0.189008 0.046143 4.096 4.21e-05 *** ## fuelPremium and Electricity 0.746139 0.353119 2.113 0.03461 * ## fuelPremium Gas or Electricity 0.098336 0.140113 0.702 0.48279 ## fuelPremium or E85 0.307425 0.059412 5.174 2.30e-07 *** ## fuelRegular 0.006088 0.046062 0.132 0.89485 ## fuelRegular Gas and Electricity 0.092330 0.132082 0.699 0.48454 ## drive4-Wheel Drive 0.125323 0.020832 6.016 1.81e-09 *** ## drive4-Wheel or All-Wheel Drive -0.053057 0.016456 -3.224 0.00126 ** ## driveAll-Wheel Drive 0.333921 0.018879 17.687 &lt; 2e-16 *** ## driveFront-Wheel Drive 0.497978 0.016327 30.499 &lt; 2e-16 *** ## drivePart-time 4-Wheel Drive -0.078447 0.039258 -1.998 0.04570 * ## driveRear-Wheel Drive 0.068346 0.016265 4.202 2.65e-05 *** ## cyl -0.112089 0.001311 -85.488 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3501 on 33363 degrees of freedom ## Multiple R-squared: 0.5094, Adjusted R-squared: 0.5091 ## F-statistic: 1924 on 18 and 33363 DF, p-value: &lt; 2.2e-16 Since OLS is a “Gaussian” member of GLS family, we can also estimate it as GLS. We use glm() and define the family as “gaussian”. model3 &lt;- glm(mpg ~ fuel + drive + cyl, family = gaussian, data = data2) #You can check it by comparing model2 above to summary(model3) #Let&#39;s check only the coefficients identical(round(coef(model2),2), round(coef(model3),2)) ## [1] TRUE With this LPM model, we can now predict the classification of future cars in term of high (mpg = 1) or low (mpg = 0), which was our objective. Let’s see how successful we are in identifying cars with mpg = 1 in our own sample. #How many cars we have with mpg = 1 and mpg = 0 in our data table(data2$mpg) ## ## 0 1 ## 17280 16102 #In-sample fitted values or predicted probabilities for mpg = 1 #Remember our E(Y|X) is Pr(Y=1|X) mpg_hat &lt;- fitted(model2) #If you think that any predicted mpg above 0.5 should be consider 1 then length(mpg_hat[mpg_hat &gt; 0.5]) ## [1] 14079 length(mpg_hat[mpg_hat &lt;= 0.5]) ## [1] 19303 This is Problem 1: we are using 0.5 as our threshold (discriminating) probability to convert predicted probabilities to predicted “labels”. When we use 0.5 as our threshold probability though, our prediction is significantly off: we predict many cars with mpg = 0 as having mpg = 1. And here is Problem 2: summary(mpg_hat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.7994 0.2187 0.4429 0.4824 0.9138 1.2088 The predicted probabilities (of mpg = 1) are not bounded between 1 and 0. We will talk about these issues later. First let’s see our next classification model or GLM. 9.3 Logistic Regression Logistic Regression Linear vs. Logistic Probability Models, which is better and when? We will briefly talk about it here. You can find a nice summary by Paul Von Hippel here as well (https://statisticalhorizons.com/linear-vs-logistic) (Hippel_2015?). First, let’s define some notation that we will use throughout. (Note that many machine learning texts use \\(p\\) as the number of parameters. Here we use as a notation for probability. You should be aware of it.) \\[ p(\\mathbf{x})=P[Y=1 | \\mathbf{X}=\\mathbf{x}] \\] With a binary (Bernoulli) response, we will mostly focus on the case when \\(Y = 1\\), since, with only two possibilities, it is trivial to obtain probabilities when \\(Y = 0\\). \\[ \\begin{array}{c}{P[Y=0 | \\mathbf{X}=\\mathbf{x}]+P[Y=1 | \\mathbf{X}=\\mathbf{x}]=1} \\\\\\\\ {P[Y=0 | \\mathbf{X}=\\mathbf{x}]=1-p(\\mathbf{x})}\\end{array} \\] An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a sigmoid function, which takes any real input \\(z\\), and outputs a value between zero and one. The standard logistic function is defined as follows: \\[\\begin{equation} \\sigma(z)=\\frac{e^{z}}{e^{z}+1}=\\frac{1}{1+e^{-z}} \\tag{9.5} \\end{equation}\\] Let’s see possible \\(\\sigma(z)\\) values and plot them against \\(z\\). set.seed(1) n &lt;- 500 x = rnorm(n, 0,2) sigma &lt;- 1/(1+exp(-x)) plot(sigma ~ x, col =&quot;blue&quot;, cex.axis = 0.7) This logistic function is nice because: (1) whatever the \\(x\\)’s are \\(\\sigma(z)\\) is always between 0 and 1, (2) The effect of \\(x\\) on \\(\\sigma(z)\\) is not linear. That is, there is lower and upper thresholds in \\(x\\) that before and after those values (around -2 and 2 here) the marginal effect of \\(x\\) on \\(\\sigma(z)\\) is very low. Therefore, it seems that if we use a logistic function and replace \\(\\sigma(z)\\) with \\(p(x)\\), we can solve issues related to these 2 major drawbacks of LPM. Let us assume that \\(z = y = \\beta_{0}+\\beta_{1} x_{1}\\), the general logistic function can now be written as: \\[\\begin{equation} p(x)=P[Y=1|\\mathbf{X}=\\mathbf{x}]=\\frac{1}{1+e^{-\\left(\\beta_{0}+\\beta_{1} x\\right)}} \\tag{9.6} \\end{equation}\\] To understand why nonlinearity would be a desirable future in some probability predictions, let’s imagine we try to predict the effect of saving (\\(x\\)) on home ownership (\\(p(x)\\)). If you have no saving now (\\(x=0\\)), additional $10K saving would not make a significant difference in your decision to buy a house (\\(P(Y=1|x)\\)). Similarly, when you have $500K (\\(x\\)) saving but without having house, additional $10K (\\(dx\\)) saving should not make you buy a house (with $500K, you could’ve bought a house already, had you wanted one). That is why flat lower and upper tails of \\(\\sigma(z)\\) are nice futures reflecting very low marginal effects of \\(x\\) on the probability of having a house in this case. After a simple algebra, we can also write the same function as follows, \\[\\begin{equation} \\ln \\left(\\frac{p(x)}{1-p(x)}\\right)=\\beta_{0}+\\beta_{1} x, \\tag{9.7} \\end{equation}\\] where \\(p(x)/[1-p(x)]\\) is called odds, a ratio of success over failure. The natural log (ln) of this ratio is called, log odds, or Logit, usually denoted as \\(\\mathbf(L)\\). Let’s see if this expression is really linear. p_x &lt;- sigma Logit &lt;- log(p_x/(1-p_x)) #By defult log() calculates natural logarithms plot(Logit ~ x, col =&quot;red&quot;, cex.axis = 0.7) In many cases, researchers use a logistic function, when the outcome variable in a regression is dichotomous. Although there are situations where the linear model is clearly problematic (as described above), there are many common situations where the linear model is just fine, and even has advantages. Let’s start by comparing the two models explicitly. If the outcome \\(Y\\) is dichotomous with values 1 and 0, we define \\(P[Y=1|X] = E(Y|X)\\) as proved earlier, which is just the probability that \\(Y\\) is 1, given some value of the regressors \\(X\\). Then the linear and logistic probability models are: \\[ P[Y = 1|\\mathbf{X}=\\mathbf{x}]=E[Y | \\mathbf{X}=\\mathbf{x}] = \\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\ldots+\\beta_{k} x_{k}, \\] \\(~\\) \\[ \\ln \\left(\\frac{P[Y=1|\\mathbf{X}]}{1-P[Y=1|\\mathbf{X}]}\\right)=\\beta_{0}+\\beta_{1} x_{1}+\\ldots+\\beta_{k} x_{k} \\] \\(~\\) The linear model assumes that the probability \\(P\\) is a linear function of the regressors, while the logistic model assumes that the natural log of the odds \\(P/(1-P)\\) is a linear function of the regressors. Note that applying the inverse logit transformation allows us to obtain an expression for \\(P(x)\\). With LPM you don’t need that transformation to have \\(P(x)\\). While LPM can be estimated easily with OLS, the Logistic model needs MLE. \\[ p(\\mathbf{x})=E[Y | \\mathbf{X}=\\mathbf{x}]=P[Y=1 | \\mathbf{X}=\\mathbf{x}]=\\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{p-1} x_{(p-1)})}} \\] \\(~\\) The major advantage of LPM is its interpretability. In the linear model, if \\(\\beta_{2}\\) is (say) 0.05, that means that a one-unit increase in \\(x_{2}\\) is associated with a 5-percentage point increase in the probability that \\(Y\\) is 1. Just about everyone has some understanding of what it would mean to increase by 5 percentage points their probability of, say, voting, or dying, or becoming obese. The logistic model is less interpretable. In the logistic model, if \\(\\beta_{1}\\) is 0.05, that means that a one-unit increase in \\(x_{1}\\) is associated with a 0.05 “unit” increase in the log odds, \\(\\text{log}(P/{(1-P)})\\), that \\(Y\\) is 1. And what does that mean? I’ve never had any intuition for log odds. So you have to convert it to the odd ratio (OR) or use the above equation to calculate fitted (predicted) probabilities. Not a problem with R, Stata, etc. But the main question is when we should use the logistic model? The logistic model is unavoidable if it fits the data much better than the linear model. And sometimes it does. But in many situations the linear model fits just as well, or almost as well, as the logistic model. In fact, in many situations, the linear and logistic model give results that are practically indistinguishable except that the logistic estimates are harder to interpret. Here is the difference: For the logistic model to fit better than the linear model, it must be the case that the log odds are a linear function of X, but the probability is not. Lets review these concepts in a simulation exercise: #Creating random data set.seed(1) # In order to get the same data everytime n &lt;- 500 # number of observation x = rnorm(n) # this is our x z = -2 + 3 * x #Probablity is defined by a logistic function #Therefore it is not a linear function of x! p = 1 / (1 + exp(-z)) #Remember Bernoulli distribution defines Y as 1 or 0 #Bernoulli is the special case of the binomial distribution with size = 1 y = rbinom(n, size = 1, prob = p) #And we create our data data &lt;- data.frame(y, x) head(data) ## y x ## 1 0 -0.6264538 ## 2 0 0.1836433 ## 3 0 -0.8356286 ## 4 0 1.5952808 ## 5 0 0.3295078 ## 6 0 -0.8204684 table(y) ## y ## 0 1 ## 353 147 We know that probablity is defined by a logistic function (see above). What happens if we fit it as LPM, which is \\(Pr[Y = 1 | x=\\mathbf{x}]) = \\beta_{0}+\\beta_{1} x_{i}\\)? lpm &lt;- lm(y ~ x, data = data) summary(lpm) ## ## Call: ## lm(formula = y ~ x, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.76537 -0.25866 -0.08228 0.28686 0.82338 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.28746 0.01567 18.34 &lt;2e-16 *** ## x 0.28892 0.01550 18.64 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3504 on 498 degrees of freedom ## Multiple R-squared: 0.411, Adjusted R-squared: 0.4098 ## F-statistic: 347.5 on 1 and 498 DF, p-value: &lt; 2.2e-16 #Here is the plot Probabilities (fitted and DGM) vs x. plot(x, p, col = &quot;green&quot;, cex.lab = 0.7, cex.axis = 0.8) abline(lpm, col = &quot;red&quot;) legend(&quot;topleft&quot;, c(&quot;Estimated Probability by LPM&quot;, &quot;Probability&quot;), lty = c(1, 1), pch = c(NA, NA), lwd = 2, col = c(&quot;red&quot;, &quot;green&quot;), cex = 0.7) How about a logistic regression? logis &lt;- glm(y ~ x, data = data, family = binomial) summary(logis) ## ## Call: ## glm(formula = y ~ x, family = binomial, data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.8253 0.1867 -9.776 &lt;2e-16 *** ## x 2.7809 0.2615 10.635 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 605.69 on 499 degrees of freedom ## Residual deviance: 328.13 on 498 degrees of freedom ## AIC: 332.13 ## ## Number of Fisher Scoring iterations: 6 #Here is the plot Probabilities (fitted and DGM) vs x. plot(x, p, col = &quot;green&quot;, cex.lab = 0.8, cex.axis = 0.8) curve(predict(logis, data.frame(x), type = &quot;response&quot;), add = TRUE, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;Estimated Probability by GLM&quot;, &quot;Probability&quot;), lty = c(1, 1), pch = c(NA, NA), lwd = 2, col = c(&quot;red&quot;, &quot;green&quot;), cex = 0.7) As you can see, the estimated logistic regression coefficients are in line with our DGM coefficients (-2, 3). \\[ \\log \\left(\\frac{\\hat{p}(\\mathbf{x})}{1-\\hat{p}(\\mathbf{x})}\\right)=-1.8253+2.7809 x \\] Intuitively, when \\(n=1\\), achieving head once (\\(k=1\\)) is \\(P(head)= p^{k}(1-p)^{1-k}=p\\) or \\(P(tail)= p^{k}(1-p)^{1-k}=1-p.\\)↩︎ "],["nonparametric-estimations---basics.html", "Chapter 10 Nonparametric Estimations - Basics 10.1 Density estimations 10.2 Kernel regression 10.3 Knn", " Chapter 10 Nonparametric Estimations - Basics The models we see in the previous chapters are parametric, which means that they have to assume a certain structure on the regression function \\(m\\) controlled by parameters before the estimations. Therefore, the results from parametric models are the best for estimating \\(m\\), if and only if their model specifications are correct. Avoiding this assumption is the strongest point of nonparametric methods, which do not require any hard-to-satisfy pre-determined regression functions. Before talking about a nonparametric estimator for the regression function \\(m\\), we should first look at a simple nonparametric density estimation of the predictor \\(X\\). This estimator is aimed to estimate \\(f(x)\\), the density of \\(X\\), from a sample and without assuming any specific form for \\(f\\). That is, without assuming, for example, that the data is normally distributed. Therefore, we first start with nonparametric density estimations. 10.1 Density estimations We are only going to look at one-variable Kernel density estimation. Let’s assume that a sample of \\(n\\) observations, \\(y_1,...,y_n\\), is drawn from a parametric distribution \\(f(y,\\theta)\\). If the data are i.i.d., the joint density function is: \\[\\begin{equation} f(y;\\theta)=\\prod_{i=1}^{n} f\\left(y_{i} ; \\theta\\right) \\tag{10.1} \\end{equation}\\] To estimate the parameters that maximize this density function (“likelihood”), or more easily, its logarithmic transformation: \\[\\begin{equation} \\ell(y ; \\theta)=\\log f(y ; \\theta)=\\sum_{i=1}^{n} \\log f\\left(y_{i} ; \\theta\\right) \\tag{10.2} \\end{equation}\\] We use MLE to estimate \\(\\theta\\). This is called parametric estimation and if our pre-determined density model is not right, that is, if \\(f\\) is misspecified, we will have biased estimators on \\(\\theta\\). To avoid this problem, we can use nonparametric estimation. The starting point for a density estimation is a histogram. We define the intervals by choosing a number of bins and a starting value for the first interval. Here, we use 0 as a starting value and 10 bins: #Random integers from 1 to 100 set.seed(123) data &lt;- sample(1:100, 100, replace = TRUE) stem(data) ## ## The decimal point is 1 digit(s) to the right of the | ## ## 0 | 46777999 ## 1 | 23344456 ## 2 | 12335555677 ## 3 | 00111224456889 ## 4 | 01122337 ## 5 | 0012337 ## 6 | 003477999 ## 7 | 1222466899 ## 8 | 112366799 ## 9 | 0011123334566799 foo &lt;- hist(data, nclass = 10, col = &quot;lightblue&quot;, cex.main = 0.80, cex.axis = 0.75) foo$counts ## [1] 8 8 13 13 9 7 7 10 11 14 foo$density ## [1] 0.008 0.008 0.013 0.013 0.009 0.007 0.007 0.010 0.011 0.014 sum(foo$density) ## [1] 0.1 Not that the sum of these densities is not one. The vertical scale of a ‘frequency histogram’ shows the number of observations in each bin. From above, we know that the tallest bar has 14 observations, so this bar accounts for relative frequency 14/100=0.14 of the observations. As the relative frequency indicate probability their total would be 1. We are looking for a density function which gives the “height” of each observation. Since the width of this bar is 10, the density of each observation in the bin is 0.014. Can we have a formula that we calculate the density for each bin? \\[\\begin{equation} \\hat{f}(y)=\\frac{1}{n} \\times \\frac{\\text{ number of observations in the interval of } y}{\\text { width of the interval }} \\tag{10.3} \\end{equation}\\] Here is the pdf on the same data with binwidth = 4 for our example: # to put pdf and X&#39;s on the same graph, we scale the data foo &lt;- hist(data/(10*mean(data)), nclass = 25, cex.main = 0.80, cex.axis = 0.75, xlim = c(0, 0.2)) lines(foo$mids, foo$density, col=&quot;blue&quot;, lwd = 2) #Naive The number of bins defines the degree of smoothness of the histogram. We can have the following general expression for a nonparametric density estimation: \\[\\begin{equation} f(x) \\cong \\frac{k}{n h} \\text { where }\\left\\{\\begin{array}{ll}{h} &amp; {\\text { binwidth }} \\\\ {n} &amp; {\\text { total } \\text { number of observation points }} \\\\ {k} &amp; {\\text { number of observations inside } h}\\end{array}\\right. \\tag{10.4} \\end{equation}\\] Note that, in practical density estimation problems, two basic approaches can be adopted: (1) we can fix \\(h\\) (width of the interval) and determine \\(k\\) in each bin from the data, which is the subject of this chapter and called kernel density estimation (KDE); or (2) we can fix \\(k\\) in each bin and determine \\(h\\) from the data. This gives rise to the k-nearest-neighbors (kNN) approach, which we cover in the next chapters. The global density is obtained with a moving window (intervals with intersections). This also called as Naive estimator (a.k.a. Parzen windows), which is not sensitive to the position of bins, but it is not smooth either. We can rewrite it as \\[\\begin{equation} \\hat{f}(y)=\\frac{1}{n h} \\sum_{i=1}^{n} I\\left(y-\\frac{h}{2}&lt;y_{i}&lt;y+\\frac{h}{2}\\right), \\tag{10.5} \\end{equation}\\] where \\(I(.)\\) is an indicator function, which results in value of 1 if the expression inside of the function is satisfied (0 otherwise). Hence it counts the number of observations in a given window. The binwidth (\\(h\\)) defines the bin range by adding and subtracting \\(h/2\\) from \\(y\\). Let’s rearrange 6.5 differently: \\[ \\hat{f}(y)=\\frac{1}{2n h} \\sum_{i=1}^{n} I\\left(y-h&lt;y_{i}&lt;y+h\\right) \\] If we rewrite the inequality by subtracting \\(y\\) and divide it by \\(h\\): \\[ \\hat{f}(y)=\\frac{1}{2n h} \\sum_{i=1}^{n} I\\left(-1&lt;\\frac{y-y_{i}}{h}&lt;1\\right) \\] which can be written more compactly, \\[\\begin{equation} \\hat{f}(y)=\\frac{1}{2nh} \\sum_{i=1}^{n} w\\left(\\frac{y-y_{i}}{h}\\right) \\quad \\text { where } \\quad w(x)=\\left\\{\\begin{array}{ll}{1} &amp; {\\text { if }|x|&lt;1} \\\\ {0} &amp; {\\text { otherwise }}\\end{array}\\right. \\tag{10.6} \\end{equation}\\] Consider a sample \\(\\left\\{X_{i}\\right\\}_{i=1}^{10}\\), which is 4, 5, 5, 6, 12, 14, 15, 15, 16, 17. And the bin width is \\(h=4\\). What’s the density of 3, i.e, \\(\\hat{f}(3)\\)? Note that there is not 3 in the data. \\[ \\begin{aligned} \\hat{f}(3) &amp;=\\frac{1}{2*10*4}\\left\\{w\\left(\\frac{3-4}{4}\\right)+w\\left(\\frac{3-5}{4}\\right)+\\ldots+w\\left(\\frac{3-17}{4}\\right)\\right\\} \\\\ &amp;=\\frac{1}{80}\\left\\{1+1+1+1+0+\\ldots+0\\right\\} \\\\ &amp;=\\frac{1}{20} \\end{aligned} \\] This “naive” estimator yields density estimates that have discontinuities and weights equal at all points \\(x_i\\) regardless of their distance to the estimation point \\(x\\). In other words, in any given bin, \\(x\\)’s have a uniform distribution. That’s why, \\(w(x)\\) is commonly replaced with a smooth kernel function \\(K(x)\\). Kernel replaces it with usually, but not always, with a radially symmetric and unimodal pdf, such as the Gaussian. You can choose “gaussian”, “epanechnikov”, “rectangular”, “triangular”, “biweight”, “cosine”, “optcosine” distributions in the R’s density() function. With the Kernel density estimator replacing \\(w\\) by a kernel function \\(K\\): \\[\\begin{equation} \\hat{f}(y)=\\frac{1}{2n h} \\sum_{i=1}^{n} K\\left(\\frac{y-y_{i}}{h}\\right), \\tag{10.7} \\end{equation}\\] Here are the samples of kernels, \\(K(x)\\): \\[ \\text { Rectangular (Uniform): } ~~ K(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{2}} &amp; {|x|&lt;1} \\\\ {0} &amp; {\\text { otherwise }}\\end{array}\\right. \\] \\[ \\text { Epanechnikov: } ~~ K(x)=\\left\\{\\begin{array}{cc}{\\frac{3}{4}\\left(1-\\frac{1}{5} x^{2}\\right) / \\sqrt{5}} &amp; {|x|&lt;\\sqrt{5}} \\\\ {0} &amp; {\\text { otherwise }}\\end{array}\\right. \\] \\[ \\text { Gaussian: } ~~ K(x)=\\frac{1}{\\sqrt{2 \\pi}} e^{(-1 / 2) x^{2}} \\] Although the kernel density estimator depends on the choices of the kernel function \\(K\\), it is very sensitive to \\(h\\), not to \\(K\\). In R, the standard kernel density estimation is obtained by density(), which uses Silverman rule-of-thumb to select the optimal bandwidth, \\(h\\), and the Gaussian kernel. Here is an example with our artificial data: #```{r, warning=FALSE, message=FALSE} #X &lt;- readRDS(“fes73.rds”) #X &lt;- X/mean(X) #hist(X[X&lt;3.5], nclass = 130 , probability = TRUE, col = “white”, # cex.axis = 0.75, cex.main = 0.8) #lines(density(X, adjust = 1/4), col = “red”) # bw/4 #lines(density(X, adjust = 1), col = “blue”) # bw #lines(density(X, adjust = 4), col = “green”) # bw * 4 #lines(density(X, kernel = “rectangular”, adjust = 1/4), col = “black”) # bw * 4 #Here is the details of the last one #density(X, adjust = 4) #``` Bigger the bandwidth \\(h\\) smoother the pdf. Which one is better? Here, we again have a bias-variance tradeoff. As we will see later, \\(h\\) can be found by cross-validation methods. An example of cross-validation methods on KDE can be found here (Garcia-Portugues2022?). Why do we estimate pdf with KDE? Note that, when you explore our density object by str(), you’ll see that \\(Y\\) will get you the pdf values of density for each value of \\(X\\) you have in our data. Of course pdf is a function: the values of pdf are \\(Y\\) and the input values are \\(X\\). Hence, given a new data point on \\(X\\), we may want to find the outcome of \\(Y\\) (the value of pdf for that data point) based on the function, the kernel density estimator that we have from the density() function result. How can we do that? Lets look at an example. #```{r, warning=FALSE, message=FALSE} #poo &lt;- density(X, adjust = 4) #dens &lt;- with(poo, approxfun(x, y, rule=1)) # Here y is the pdf of a specified x. #dens(1.832) #``` This is a predicted value of pdf when \\(x=1.832\\) estimated by KDE without specifying the model apriori, which is like a magic! Based on the sample we have, we just predicted \\(Y\\) without explicitly modeling it. Keep in mind that our objective here is not to estimate probabilities. We can do it if we want. But then, of course we have to remember that values of a density curve are not the same thing as probabilities. As with any continuous distribution, the probability that \\(X\\) is exactly 1.832, for example, is 0. Taking the integral of the desired section in the estimated pdf would give us the corresponding probability. 10.2 Kernel regression Theoretically, nonparametric density estimation can be easily extended to several dimensions (multivariate distributions). For instance, suppose you are interested in the relationship between one endogenous variable \\(Y\\) and a few exogenous variables of \\(X\\). The ultimate goal is forecasting new realizations of \\(Y\\) given new realizations of \\(X\\)’s. You have little clue what functional form the relationship could take. Suppose you have a sufficiently large sample, so that you may obtain a reasonably accurate estimate of the joint probability density (by kernel density estimation or similar) of \\(Y\\) and the \\(X\\)’s. In practice, however, you rarely actually have a good enough sample to perform highly accurate density estimation. As the dimension increases, KDE rapidly needs many more samples. Hence, KDE is rarely useful beyond the order of 10 dimensions. Even in low dimensions, a density estimation-based model has essentially no ability to generalize; if your test set has any examples outside the support of your training distribution, you’re likely in trouble. In regression functions, the outcome is the conditional mean of \\(Y\\) given \\(X\\)’s. Unlike linear regression, nonparametric regression is agnostic about the functional form between the outcome and the covariates and is therefore not subject to misspecification error. In nonparametric regression, you do not specify the functional form. You specify the outcome variable and the covariates. In traditional parametric regression models, the functional form of the model is specified before the model is fit to data, and the objective is to estimate the parameters of the model. In nonparametric regression, in contrast, the objective is to estimate the regression function directly without specifying its form explicitly 2. The traditional regression model fits the model: \\[\\begin{equation} y=m(\\mathbf{x}, \\boldsymbol{\\theta})+\\varepsilon \\tag{10.8} \\end{equation}\\] where \\(\\theta\\) is a vector of parameters to be estimated, and x is a vector of predictors. The errors, \\(\\varepsilon\\) are assumed to be normally and independently distributed with mean 0 and constant variance \\(\\sigma^2\\). The function \\(m(\\mathbf{x},\\theta)\\), relating the average value of the response \\(y\\) to the predictors, is specified in advance, as it is in a linear regression model. The general nonparametric regression model is written in a similar manner, but the function \\(m\\) is left unspecified for the \\(p\\) predictors: \\[ \\begin{aligned} y &amp;=m(\\mathbf{x})+\\varepsilon \\\\ &amp;=m\\left(x_{1}, x_{2}, \\ldots, x_{p}\\right)+\\varepsilon \\end{aligned} \\] Moreover, the objective of nonparametric regression is to estimate the regression function \\(m(\\mathbf{x})\\) directly, rather than to estimate parameters. Most methods of nonparametric regression implicitly assume that \\(m\\) is a smooth, continuous function. As in nonlinear regression, it is standard to assume that error is normally and identically distributed \\(\\varepsilon \\sim NID(0, \\sigma^2)\\). An important special case of the general model is nonparametric simple regression, where there is only one predictor: \\[ y=m(x)+\\varepsilon \\] Because it is difficult to fit the general nonparametric regression model when there are many predictors, and because it is difficult to display the fitted model when there are more than two or three predictors, more restrictive models have been developed. One such model is the additive regression model, \\[\\begin{equation} y=\\beta_{0}+m_{1}\\left(x_{1}\\right)+m_{2}\\left(x_{2}\\right)+\\cdots+m_{p}\\left(x_{p}\\right)+\\varepsilon \\tag{10.9} \\end{equation}\\] Variations on the additive regression model include semiparametric models, in which some of the predictors enter linearly or interactively.3 Let’s consider the simplest situation with one continuous predictor, \\(X\\). Due to its definition, we can rewrite \\(m\\) as \\[\\begin{equation} \\begin{split} \\begin{aligned} m(x) &amp;=\\mathbb{E}[Y | X=x] \\\\ &amp;=\\int y f_{Y | X=x}(y) \\mathrm{d} y \\\\ &amp;=\\frac{\\int y f(x, y) \\mathrm{d} y}{f_{X}(x)} \\end{aligned} \\end{split} \\tag{10.10} \\end{equation}\\] This shows that the regression function can be computed from the joint density \\(f(x,y)\\) and the marginal \\(f(x)\\). Therefore, given a sample \\(\\left\\{\\left(X_{i}, Y_{i}\\right)\\right\\}_{i=1}^{n}\\), a nonparametric estimate of \\(m\\) may follow by replacing the previous densities by their kernel density estimators, which we’ve just seen in the previous section. A limitation of the bin smoothing approach in kernel density estimations is that we need small windows for the approximately constant assumptions to hold. As a result, we end up with a small number of data points to average and obtain imprecise estimates of \\(f(x)\\). Local weighted regression (loess) permits us to consider larger window sizes. loess() is a nonparametric approach that fits multiple regressions in local neighborhood. It is called local regression because, instead of assuming the function is approximately constant in a window, it fits a local regression at the “neighborhood” of \\(x_0\\). The distance from \\(x_0\\) is controlled by the span setting, which determines the width of the moving (sliding) window when smoothing the data. span (also defined as alpha) represents the proportion of the data (size of the sliding window) that is considered to be neighboring \\(x_0\\) 4. Moreover, the weighting in the regression is proportional to \\(1-(\\text{distance}/\\text{maximum distance})^3)^3\\), which is called the Tukey tri-weight. Different than the Gaussian kernel, the Tukey tri-weight covers more points closer to the center point. We will not see the theoretical derivations of kernel regressions but an illustration of local polynomial of order 0, 1 and 2, below. (Examples from Ahamada and Flachaire) (Ibrahim_2011?). The Nadaraya–Watson estimator is a local polynomial of order 0, which estimates a local mean of \\(Y_1...Y_n\\) around \\(X=x_0\\). #Simulating our data n = 300 set.seed(1) x &lt;- sort(runif(n)*2*pi) y &lt;- sin(x) + rnorm(n)/4 plot(x, y) #Estimation loe0 &lt;- loess(y~x, degree=0, span = 0.5) #Nadaraya-Watson loe1 &lt;- loess(y~x, degree=1, span = 0.5) #Local linear loe2 &lt;- loess(y~x, degree=2, span = 0.5) #Locally quadratic #To have a plot, we first calculate the fitted values on a grid, t &lt;- seq(min(x), max(x), length.out = 100) fit0 &lt;- predict(loe0, t) fit1 &lt;- predict(loe1, t) fit2 &lt;- predict(loe2, t) plot(x, y, col = &quot;gray&quot;, cex.main = 0.80, cex.axis = 0.75) lines(t, fit0, col = &quot;green&quot;, lwd = 3) lines(t, fit1, col = &quot;red&quot;) lines(t, fit2, col = &quot;blue&quot;) Let’s see sensitivity of local quadratic to the bandwidth: fit0 &lt;- predict(loess(y~x, degree=2, span = 0.05)) #minimum, 5%*300 = 14 obs. fit1 &lt;- predict(loess(y~x, degree=2, span = 0.75)) #default fit2 &lt;- predict(loess(y~x, degree=2, span = 2)) plot(x, y, col = &quot;gray&quot;, cex.main = 0.80, cex.axis = 0.75) lines(x, fit0, lwd = 2, col = &quot;green&quot;) lines(x, fit1, lwd = 2, col = &quot;red&quot;) lines(x, fit2, lwd = 2, col = &quot;blue&quot;) As we have seen in the concept of bias-variance trade off before, which bandwidth we choose will be determined by the prediction accuracy. This subject is related to cross-validation, which we will see later as a whole chapter. 10.3 Knn With k-nearest neighbors (kNN) we estimate \\(p(x_1, x_2)\\) by using a method similar to bin smoothing. kNN is a nonparametric method used for classification or regression. That’s why sometimes it is called as a black-box method, as it does not return an explicit model but the direct classified outcome. In kNN classification, the output is a class membership. An object is assigned to the class most common among its k nearest neighbors. In kNN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors, which we’ve seen in bin smoothing applications. Here are the steps to understand it: Suppose we have to classify (identify) the red dot as 7 or 2. Since it’s a nonparametric approach, we have to define bins. If the number of observations in bins set to 1 (\\(k = 1\\)), then we need to find one observation that is nearest to the red dot. How? Since we know to coordinates (\\(x_1, x_2\\)) of that red dot, we can calculate find its nearest neighbors by some distance functions for all points (observations) in the data. A popular choice is the Euclidean distance given by \\[ d\\left(x, x^{\\prime}\\right)=\\sqrt{\\left(x_{1}-x_{1}^{\\prime}\\right)^{2}+\\ldots+\\left(x_{n}-x_{n}^{\\prime}\\right)^{2}}. \\] Other measures are also available and can be more suitable in different settings including the Manhattan, Chebyshev and Hamming distance. The last one is used if the features are binary. In our case the features are continuous so we can use the Euclidean distance. We now have to calculate this measure for every point (observation) in our data. In our graph we have 10 points, and we have to have 10 distance measures from the red dot. Usually, in practice, we calculate all distance measures between each point, which becomes a symmetric matrix with \\(n\\)x\\(n\\) dimensions. When \\(k=1\\), the observation that has the shortest distance is going to be the one to predict what the red dot could be. This is shown in the figure below: If we define the bin as \\(k=3\\), we look for the 3 nearest points to the red dot and then take an average of the 1s (7s) and 0s (2s) associated with these points. Here is an example: Using \\(k\\) neighbors to estimate the probability of \\(Y=1\\) (the dot is 7), that is \\[\\begin{equation} \\hat{P}_{k}(Y=1 | X=x)=\\frac{1}{k} \\sum_{i \\in \\mathcal{N}_{k}(x, D)} I\\left(y_{i}=1\\right) \\tag{10.11} \\end{equation}\\] With this predicted probability, we classify the red dot to the class with the most observations in the \\(k\\) nearest neighbors (we assign a class at random to one of the classes tied for highest). Here is the rule in our case: \\[ \\hat{C}_{k}(x)=\\left\\{\\begin{array}{ll}{1} &amp; {\\hat{p}_{k 0}(x)&gt;0.5} \\\\ {0} &amp; {\\hat{p}_{k 1}(x)&lt;0.5}\\end{array}\\right. \\] Suppose our red dot has \\(x=(x_1,x_2)=(4,3)\\) \\[ \\begin{aligned} \\hat{P}\\left(Y=\\text { Seven } | X_{1}=4, X_{2}=3\\right)=\\frac{2}{3} \\\\ \\hat{P}\\left(Y=\\text { Two} | X_{1}=4, X_{2}=3\\right)=\\frac{1}{3} \\end{aligned} \\] Hence, \\[ \\hat{C}_{k=4}\\left(x_{1}=4, x_{2}=3\\right)=\\text { Seven } \\] As it’s clear from this application, \\(k\\) is our hyperparameter and we need to tune it as to have the best predictive kNN algorithm. The following section will show its application. But before that, let’s see the hyperplane to understand its nonparametric structure. We will use knn3() from the Caret package. We will not train a model but only see how the separation between classes will be nonlinear and different for different \\(k\\). #With k = 5 One with \\(k=2\\) shows signs for overfitting, the other one with \\(k=400\\) indicates oversmoothing or underfitting. We need to tune \\(k\\) such a way that it will be best in terms of prediction accuracy. 10.3.1 Adult dataset This dataset provides information on income and attributes that may effect it. Information on the dataset is give at its website (Kohavi_1996?): Extraction from 1994 US. Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE&gt;16) &amp;&amp; (AGI&gt;100) &amp;&amp; (AFNLWGT&gt;1)&amp;&amp; (HRSWK&gt;0)). The prediction task is to determine whether a person makes over 50K a year. # Download adult income data # SET YOUR WORKING DIRECTORY FIRST # url.train &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&quot; # url.test &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test&quot; # url.names &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&quot; # download.file(url.train, destfile = &quot;adult_train.csv&quot;) # download.file(url.test, destfile = &quot;adult_test.csv&quot;) # download.file(url.names, destfile = &quot;adult_names.txt&quot;) # Read the training set into memory #train &lt;- read.csv(&quot;adult_train.csv&quot;, header = FALSE) #str(train) # Read the test set into memory #test &lt;- read.csv(&quot;adult_test.csv&quot;, header = FALSE) The data doesn’t have the variable names. That’s bad because we don’t know which one is which. Check the adult_names.txt file. The list of variables is given in that file. Thanks to Matthew Baumer (Baumer_2015?), we can write them manually: #varNames &lt;- c(&quot;Age&quot;, # &quot;WorkClass&quot;, # &quot;fnlwgt&quot;, # &quot;Education&quot;, # &quot;EducationNum&quot;, # &quot;MaritalStatus&quot;, # &quot;Occupation&quot;, # &quot;Relationship&quot;, # &quot;Race&quot;, # &quot;Sex&quot;, # &quot;CapitalGain&quot;, # &quot;CapitalLoss&quot;, # &quot;HoursPerWeek&quot;, # &quot;NativeCountry&quot;, # &quot;IncomeLevel&quot;) #names(train) &lt;- varNames #names(test) &lt;- varNames #str(train) Since the dataset is large we are not going to use the test set but split the train set into our own test and train sets. Note that, however, if we had used the original test set, we would have had to make some adjustments/cleaning before using it. For example, if you look at Age variable, it seems as a factor variable. It’s an integer in the training set. We have to change it first. Moreover, our \\(Y\\) has two levels in the train set, it has 3 levels in the test set. We have to go over each variable and make sure that the test and train sets have the same features and class types. This task is left to you if you want to use the original train and test sets. A final tip: remove the first row in the original test set! #Caret needs some preparations! #table(train$IncomeLevel) # this is b/c we will use it the same data for LPM later in class examples #train$Y &lt;- ifelse(train$IncomeLevel==&quot; &lt;=50K&quot;, 0, 1) #train &lt;- train[, -15] # kNN needs Y to be a factor variable #train$Y &lt;- as.factor(train$Y) #levels(train$Y)[levels(train$Y)==&quot;0&quot;] &lt;- &quot;Less&quot; #levels(train$Y)[levels(train$Y)==&quot;1&quot;] &lt;- &quot;More&quot; #levels(train$Y) #kNN #set.seed(3033) #train_df &lt;- caret::createDataPartition(y = train$Y, p= 0.7, list = FALSE) #training &lt;- train[train_df,] #testing &lt;- train[-train_df,] #Training/Model building with 10-k cross validation #It will take a long time. If you want to run it #make sure that you have something to read:-) #cv &lt;- caret::trainControl(method = &quot;cv&quot;, number = 10, p = 0.9) #model_knn3 &lt;- caret::train(Y ~ ., method = &quot;knn&quot;, data = training, # tuneGrid = data.frame(k=seq(9, 41 ,2)), # trControl = cv) #ggplot(model_knn3, highlight = TRUE) Now we are going to use the test set to see the model’s performance. #caret::confusionMatrix(predict(model_knn3, testing, type = &quot;raw&quot;), # testing$Y) Next, as you can guess, we will delve into these performance measures. But before that, let’s ask some questions. Why would you go with kNN? LPM may be as good as kNN. How can you see the individual effect of each feature on this classification? It seems that there is no way that we can interpret the results by looking at each feature. A learning algorithm may not be evaluated only by its predictive capacity. We may want to interpret the results by identifying the important predictors and their importance in predicting the outcome. There is always a trade-off between interpretability and predictive accuracy. Here is a an illustration. We will talk about this later in the book. For more theoretical explanations, see the link here to one of the good sources) (Fox_2018?)↩︎ Without theoretical background, which can be found at (Fox_2018?) and (Garcia-Portugues2022?). We will see several important applications later.↩︎ if N is the number of data points and span= 0.5, then for a given x, loess will use the 0.5*N closest points to x for the fit. Usually span should be between 0 and 1. When its larger than 1, then the regression will be over-smoothed↩︎ "],["hyperparameter-tuning.html", "Chapter 11 Hyperparameter Tuning 11.1 Training and Validation 11.2 Splitting the data randomly 11.3 k-fold cross validation 11.4 Grid Search 11.5 Cross-validated grid search", " Chapter 11 Hyperparameter Tuning How do we know that an estimated regression model is generalizable beyond the sample data used to fit it? Ideally, we can obtain new independent data with which to validate our model. For example, we could refit the model to the new dataset to see if the various characteristics of the model (e.g., estimates regression coefficients) are consistent with the model fit to the original dataset. Alternatively, we could use the regression equation of the model fit to the original dataset to make predictions of the response variable for the new dataset. Then we can calculate the prediction errors (differences between the actual response values and the predictions) and summarize the predictive ability of the model by the mean squared prediction error (MSPE). This gives an indication of how well the model will predict in the future. Sometimes the MSPE is rescaled to provide a cross-validation R2. However, most of the time we cannot obtain new independent data to validate our model. An alternative is to partition the sample data into a training (or model-building) set, which we can use to develop the model, and a validation (or prediction) set, which is used to evaluate the predictive ability of the model. This is called cross-validation. Again, we can compare the model fit to the training set to the model refit to the validation set to assess consistency. Or we can calculate the MSPE for the validation set to assess the predictive ability of the model. Another way to employ cross-validation is to use the validation set to help determine the final selected model. Suppose we have found a handful of “good” models that each provide a satisfactory fit to the training data and satisfy the model (LINE) conditions. We can calculate the MSPE for each model on the validation set. Our final selected model is the one with the smallest MSPE. The simplest approach to cross-validation is to partition the sample observations randomly with 50% of the sample in each set. This assumes there is sufficient data to have 6-10 observations per potential predictor variable in the training set; if not, then the partition can be set to, say, 60%/40% or 70%/30%, to satisfy this constraint. If the dataset is too small to satisfy this constraint even by adjusting the partition allocation then K-fold cross-validation can be used. This partitions the sample dataset into K parts which are (roughly) equal in size. For each part, we use the remaining K – 1 parts to estimate the model of interest (i.e., the training sample) and test the predictability of the model with the remaining part (i.e., the validation sample). We then calculate the sum of squared prediction errors, and combine the K estimates of prediction error to produce a K-fold cross-validation estimate. When K = 2, this is a simple extension of the 50%/50% partition method described above. The advantage of this method is that it is usually preferable to residual diagnostic methods and takes not much longer to compute. However, its evaluation can have high variance since evaluation may depend on which data points end up in the training sample and which end up in the test sample. When K = n, this is called leave-one-out cross-validation. That means that n separate data sets are trained on all of the data (except one point) and then prediction is made for that one point. The evaluation of this method is very good, but often computationally expensive. Note that the K-fold cross-validation estimate of prediction error is identical to the PRESS statistic. In general, there are multiple tuning parameters or so-called hyperparameters associated with each prediction method. The value of the hyperparameter has to be set before the learning process begins because those tuning parameters are external to the model and their value cannot be estimated from data. Therefore, we usually need to perform a grid search to identify the optimal combination of these parameters that minimizes the prediction error. For example, \\(k\\) in kNN, the number of hidden layers in Neural Networks, even the degree of of polynomials in a linear regression have to be tuned before the learning process starts. In contrast, a parameter (in a paramteric model) is an internal characteristic of the model and its value can be estimated from data for any given hyperparameter. For example, \\(\\lambda\\), the penalty parameter that shrinks the number of variables in Lasso, which we will see in Section 5, is a hyperparameter and has to be set before the estimation. When it’s set, the coefficients of Lasso are estimated from the process. We start with k-fold cross validation process and perform a cross-validated grid search to identify the optimal mix of those parameters. Here, we will learn the rules and simple application how to set up a grid serach that evaluates many different combinations of hyperparameters. This chapter covers the key concept in modern machine learning applications and many learning algorithms. 11.1 Training and Validation Before learning how to split the data into subsections randomly, we need to know what these sets are for and how we define them properly. This section is inspired by the article, What is the Difference Between Test and Validation Datasets?, by Jason Brownlee (Brownlee_2017?). The article clarifies how validation and test datasets are different, which can be confusing in practice. Let’s define them formally first: Training Dataset: The sample of data used to fit (train) the predictive model. Validation Dataset: The sample of data used for tuning model hyperparameters, and selecting variables (feature selection). Test Dataset: The sample of data reserved to provide an unbiased evaluation of a final model fit on the training dataset. However, in practice, validation and test datasets are not named separately. Let’s summarize a usual process in modern machine learning applications: You have a dataset for building a predictive model. Given the data and the prediction problem on your hand, you usually have multiple alternatives or competing models to start with. Each model needs a training, which is a process of tuning their hyperparameters and selecting their features (variables) for the best predictive performance. Therefore, this process requires two different datasets: training and validation datasets. The intuition behind this split is very simple: the prediction is an out-of-sample problem. If we use the same sample that we use to fit the model for assessing the prediction accuracy of our model, we face the infamous overfitting problem. Since we usually don’t have another unseen dataset available to us, we split the data and leave one part out of our original dataset. We literally pretend that one that is left out is “unseen” by us. Now the question is how we do this split. Would it be 50-50?. The general approach is k-fold cross validation with a grid search. Here are the main steps: Suppose Model 1 requires to pick a value for \\(\\lambda\\), perhaps it is a degree of polynomials in the model. We establish a grid, a set of sequential numbers, that is a set of possible values of \\(\\lambda\\). We split the data into \\(k\\) random sections, let’s say 10 proportionally equal sections. We leave one section out and use 9 sections. The combination of these 9 sections is our training set. The one that is left out is our validation set. We fit the model using each value in the set of possible values of \\(\\lambda\\). For example, if we have 100 values of \\(\\lambda\\), we fit the model to the training set 100 times, once for each possible value of \\(\\lambda\\). We evaluate each of 100 models by using their predictive accuracy on the validation set, the one that is left out. We pick a \\(\\lambda\\) that gives the highest prediction accuracy. We do this process 10 times (if it is 10-fold cross validation) with each time using a different section of the data as the validation set. So, note that each time our training and validation sets are going to be different. At the end, in total we will have 10 best \\(\\lambda\\)s. We pick the average or modal value of \\(\\lambda\\) as our optimal hyperparameter that tunes our predictive model for its best performance. Note that the term validation is sometimes is mixed-up with test for the dataset we left out from our sample. This point often confuses practitioners. So what is the test set? We have now Model 1 tuned with the optimal \\(\\lambda\\). This is Model 1 among several alternative models (there are more than 300 predictive models and growing in practice). Besides, the steps above we followed provides a limited answer whether if Model 1 has a good or “acceptable” prediction accuracy or not. In other words, tuning Model 1 doesn’t mean that it does a good or a bad job in prediction. How do we know and measure its performance in prediction? Usually, if the outcome that we try to predict is quantitative variable, we use root mean squared prediction error (RMSPE). There are several other metrics we will see later. If it’s an indicator outcome, we have to apply some other methods, one of which is called as Receiver Operating Curve (ROC). We will see and learn all of them all in detail shortly. But, for now, let’s pretend that we know a metric that measures the prediction accuracy of Model 1 as well as other alternative models. The only sensible way to do it would be to test the “tuned” model on a new dataset. In other words, you need to use the trained model on a real and new dataset and calculate the prediction accuracy of Model 1 by RMSPE or ROC. But, we do not have one. That’s why we have to go the beginning and create a split before starting the training process: training and test datasets. We use the training data for the feature selection and tuning the parameter. After you “trained” the model by validation, we can use the test set to see its performance. Finally, you follow the same steps for other alternative learning algorithms and then pick the winner. Having trained each model using the training set, and chosen the best model using the validation set, the test set tells you how good your final choice of model is. Here is a visualization of the split: Before seeing every step with an application in this chapter, let’s have a more intuitive and simpler explanation about “training” a model. First, what’s learning? We can summarize it this way: observe the facts, do some generalizations, use these generalizations to predict previously unseen facts, evaluate your predictions, and adjust your generalizations (knowledge) for better predictions. It’s an infinite loop. Here is the basic paradigm: Observe the facts (training data), Make generalizations (build prediction models), Adjust your prediction to make them better (train your model with validation data), Test your predictions on unseen data to see how they hold up (test data) As the distinction between validation and test datasets is now clear, we can conclude that, even if we have the best possible predictive model given the training dataset, our generalization of the seen data for prediction of unseen facts would be fruitless in practice. In fact, we may learn nothing at the end of this process and remain unknowledgeable about the unseen facts. Why would this happen? The main reason would be the lack of enough information in training set. If we do not have enough data to make and test models that are applicable to real life, our predictions may not be valid. The second reason would be modeling inefficiencies in a sense that it requires a very large computing power and storage capacity. This subject is also getting more interesting everyday. The Google’s quantum computers are one of them. 11.2 Splitting the data randomly We already know how to sample a set of observation by using sample(). We can use this function again to sort the data into k-fold sections. Here is an example with just 2 sections: #We can create a simple dataset with X and Y using a DGM set.seed(2) n &lt;- 10000 X &lt;- rnorm(n, 3, 6) Y &lt;- 2 + 13*X + rnorm(n, 0, 1) data &lt;- data.frame(Y, X) #We need to shuffle it random &lt;- sample(n, n, replace = FALSE) data &lt;- data[random, ] #Now we have a dataset shuffled randomly. #Since the order of data is now completely random, #we can divide it as many slices as we wish k &lt;- 2 #2-fold (slices-sections) nslice &lt;- floor(n/k) #number of observations in each fold/slice #Since we have only 2 slices of data #we can call one slice as a &quot;validation set&quot; the other one as a &quot;training set&quot; train &lt;- data[1:nslice, ] str(train) ## &#39;data.frame&#39;: 5000 obs. of 2 variables: ## $ Y: num -92.7 52.35 -114 133.6 7.39 ... ## $ X: num -7.344 3.868 -9.006 9.978 0.468 ... val &lt;- data[(nslice+1):n,] str(val) ## &#39;data.frame&#39;: 5000 obs. of 2 variables: ## $ Y: num -49.1 -53.7 -25 -46.2 135.2 ... ## $ X: num -3.9 -4.22 -1.99 -3.67 10.37 ... This is good, we now have it split into a 2-fold with 50-50% splitting. But what if we want a slicing that gives a 10% validation set and a 90% training set? How can we do that? One of the most common ways to do 10%-90% splitting is 10-fold slicing. Here is how: #Again, first, we need to shuffle it set.seed(2) random &lt;- sample(n, n, replace = FALSE) data &lt;- data[random, ] #number of folds k &lt;- 10 nslice &lt;- floor(n/k) #number of observations in each fold/slice #Now we have 10 slices of data and each slice has 10% of data. #We can call one slice as a &quot;validation set&quot; #And we group the other 9 slices as a &quot;training set&quot; val &lt;- data[1:nslice, ] str(val) ## &#39;data.frame&#39;: 1000 obs. of 2 variables: ## $ Y: num -9.14 -87.79 -74.7 130.82 31.2 ... ## $ X: num -0.908 -6.95 -5.881 9.809 2.21 ... train &lt;- data[(nslice+1):n, ] str(train) ## &#39;data.frame&#39;: 9000 obs. of 2 variables: ## $ Y: num -71.6 12.1 27.9 136.1 26.3 ... ## $ X: num -5.59 0.83 1.98 10.37 1.92 ... How can we use this method to tune a model? Let’s use Kernel regressions applied by loess() that we have seen before. #Let&#39;s simulate our data n = 1000 set.seed(1) x &lt;- sort(runif(n)*2*pi) y &lt;- sin(x) + rnorm(n)/4 data2 &lt;- data.frame(y, x) plot(x, y) #Estimation with degree = 2 (locally quadratic) loe0 &lt;- loess(y~x, degree=2, span = 0.02) loe1 &lt;- loess(y~x, degree=2, span = 0.1) loe2 &lt;- loess(y~x, degree=2, span = 1) #Plots t &lt;- seq(min(x), max(x), length.out = 700) fit0 &lt;- predict(loe0, t) fit1 &lt;- predict(loe1, t) fit2 &lt;- predict(loe2, t) plot(x, y, col = &quot;gray&quot;, cex.main = 0.80, cex.axis = 0.75) lines(t, fit0, col = &quot;green&quot;) lines(t, fit1, col = &quot;red&quot;) lines(t, fit2, col = &quot;blue&quot;) The sensitivity of kernel regression estimations (with the locally quadratic loess()) to the bandwidth (span) is obvious from the plot. Which bandwidth should we choose for the best prediction accuracy? There are actually 2 hyperparameters in loess(): degree and span. We can tune both of them at the same time, but for the sake of simplicity, let’s set the degree = 2 (locally polynomial) and tune only the bandwidth. #Again, first, we need to shuffle it set.seed(2) random &lt;- sample(n, n, replace = FALSE) data2 &lt;- data2[random, ] #number of folds k &lt;- 10 nslice &lt;- floor(n/k) #number of observations in each fold/slice val &lt;- data2[1:nslice, ] train &lt;- data2[(nslice+1):n, ] Our validation and train sets are ready. We are going to use the train set to train our models with different values of span in each one. Then, we will validate each model by looking at the RMSPE of the model results against our validation set. The winner will be the one with the lowest RMSPE. That’s the plan. Let’s use the set of span = 0.02, 0.1, and 1. #Estimation with degree = 2 (locally quadratic) by training set loe0 &lt;- loess(y~x, degree=2, span = 0.02, data = train) loe1 &lt;- loess(y~x, degree=2, span = 0.1, data = train) loe2 &lt;- loess(y~x, degree=2, span = 1, data = train) #Predicting by using validation set fit0 &lt;- predict(loe0, val$x) fit1 &lt;- predict(loe1, val$x) fit2 &lt;- predict(loe2, val$x) We must also create our performance metric, RMSPE; #Estimation with degree = 2 (locally quadratic) by training set rmspe0 &lt;- sqrt(mean((val$y-fit0)^2)) rmspe1 &lt;- sqrt(mean((val$y-fit1)^2)) rmspe2 &lt;- sqrt(mean((val$y-fit2)^2)) c(paste(&quot;With span = 0.02&quot;, &quot;rmspe is &quot;, rmspe0), paste(&quot;With span = 0.1&quot;, &quot;rmspe is &quot;, rmspe1), paste(&quot;With span = 1&quot;, &quot;rmspe is &quot;, rmspe2)) ## [1] &quot;With span = 0.02 rmspe is 0.2667567209162&quot; ## [2] &quot;With span = 0.1 rmspe is 0.247223047309939&quot; ## [3] &quot;With span = 1 rmspe is 0.309649140565879&quot; We are now able to see which bandwidth is better. When we set span = 0.1, RMSPE is the lowest. But we have several problems with this algorithm. First, we only used three arbitrary values for span. If we use 0.11, for example, we don’t know if its RMSPE could be better or not. Second, we only see the differences across RMSPE’s by manually comparing them. If we had tested for a large set of span values, this would have been difficult. Third, we have used only one set of validation and training sets. If we do it multiple times, we may have different results and different rankings of the models. How are we going to address these issues? Let’s address this last issue, about using more than one set of training and validation sets, first: 11.3 k-fold cross validation We can start here with the following figure about k-fold cross-validation. It shows 5-fold cross validation. It splits the data into k-folds, then trains the data on k-1 folds and validation on the one fold that was left out. Although, this type cross validation is the most common one, there are also several different cross validation methods, such as leave-one-out (LOOCV), leave-one-group-out, and time-series cross validation methods, which we will see later. This figure illustrates 5-k CV. We have done this with the 10-fold version for only one split and did not repeat it 10 times. The only job now is to create a loop that does the first slicing 10 times. If we repeat the same loess() example with 10-k cross validation, we will have 10 RMSPE’s for each span value. To evaluate which one is the lowest, we take the average of those 10 RSMPE’s for each model. Before that, however, let’s start with a simple example of building a loop. Suppose we have a random variable \\(X\\) and we need to calculate means of \\(X\\) in training and validation sets to see if they are similar or not. A 10-k cross-validation example is here: #data n &lt;- 10000 X &lt;- rnorm(n, 2, 5) #Shuffle the order of observations by their index mysample &lt;- sample(n, n, replace = FALSE) #Since we will have 10 means from each set #we need empty &quot;containers&quot; for those values fold &lt;- 10 means_validate &lt;- c() means_train &lt;- c() #Here is the loop nvalidate &lt;- round(n/fold) #number of observation in each set for(i in 1:fold){ cat(&quot;K-fold loop: &quot;, i, &quot;\\r&quot;) #This tells us which fold we are at the moment #No need to have it as it slows the loop speed if(i &lt; fold) validate &lt;- mysample[((i-1)*nvalidate+1):(i*nvalidate)] else validate &lt;- mysample[((i-1)*nvalidate+1):n] train &lt;- seq(1, n)[-validate] X_validate &lt;- X[validate] X_train &lt;- X[-validate] means_validate[i] &lt;- mean(X_validate) means_train[i] &lt;- mean(X_train) } ## K-fold loop: 1 K-fold loop: 2 K-fold loop: 3 K-fold loop: 4 K-fold loop: 5 K-fold loop: 6 K-fold loop: 7 K-fold loop: 8 K-fold loop: 9 K-fold loop: 10 means_validate ## [1] 1.960626 2.078920 1.844554 2.028020 2.223252 2.090933 2.082902 1.842147 ## [9] 2.106923 2.184972 mean(means_validate) ## [1] 2.044325 means_train ## [1] 2.053625 2.040481 2.066522 2.046137 2.024444 2.039146 2.040039 2.066789 ## [9] 2.037370 2.028697 mean(means_train) ## [1] 2.044325 This is impressive. The only difference between this simple example and a more complex one is some adjustments that we need to make in the type of calculations. Let’s do it with our loess() example: #Let&#39;s simulate our data n = 10000 set.seed(1) x &lt;- sort(runif(n)*2*pi) y &lt;- sin(x) + rnorm(n)/4 data &lt;- data.frame(y, x) #Shuffle the order of observations by their index mysample &lt;- sample(n, n, replace = FALSE) fold &lt;- 10 #10-k CV RMSPE &lt;- c() # we need an empty container to store RMSPE from validate set #loop nvalidate &lt;- round(n/fold) for(i in 1:fold){ if(i &lt; fold) validate &lt;- mysample[((i-1)*nvalidate+1):(i*nvalidate)] else validate &lt;- mysample[((i-1)*nvalidate+1):n] train &lt;- seq(1, n)[-validate] data_validate &lt;- data[validate,] data_train &lt;- data[-validate,] model &lt;- loess(y ~ x, control=loess.control(surface=&quot;direct&quot;), degree=2, span = 0.1, data = data_train) #loess.control() is used for the adjustment #for x values that are outside of x values used in training fit &lt;- predict(model, data_validate$x) RMSPE[i] &lt;- sqrt(mean((data_validate$y-fit)^2)) } RMSPE ## [1] 0.2427814 0.2469909 0.2387873 0.2472059 0.2489808 0.2510570 0.2553914 ## [8] 0.2517241 0.2521053 0.2429688 mean(RMSPE) ## [1] 0.2477993 How can we use this k-fold cross validation in tuning the parameters by 3 possible values of span = 0.02, 0.1, and 1: #Using the same data mysample &lt;- sample(n, n, replace = FALSE) fold &lt;- 10 #10-k CV #Possible values for span, GRID! span &lt;- c(0.02, 0.1, 1) #We need a container to store RMSPE from validate set #but we have to have a matrix now with 3 columns for each paramater in span RMSPE &lt;- matrix(0, nrow = fold, ncol = length(span)) #Loop nvalidate &lt;- round(n/fold) for(i in 1:fold){ if(i &lt; fold) validate &lt;- mysample[((i-1)*nvalidate+1):(i*nvalidate)] else validate &lt;- mysample[((i-1)*nvalidate+1):n] train &lt;- seq(1, n)[-validate] data_validate &lt;- data[validate,] data_train &lt;- data[-validate,] #We need another loop running each value in span: for (j in 1:length(span)) { #cat(&quot;K-fold loop: &quot;, i, j, &quot;\\r&quot;) model &lt;- loess(y ~ x, control=loess.control(surface=&quot;direct&quot;), degree=2, span = span[j], data = data_train) fit &lt;- predict(model, data_validate$x) RMSPE[i,j] &lt;- sqrt(mean((data_validate$y-fit)^2)) } } RMSPE ## [,1] [,2] [,3] ## [1,] 0.2553526 0.2537322 0.3221018 ## [2,] 0.2498332 0.2490425 0.3136585 ## [3,] 0.2457066 0.2438678 0.3156779 ## [4,] 0.2537020 0.2515660 0.3124925 ## [5,] 0.2541144 0.2506757 0.3135304 ## [6,] 0.2462051 0.2438164 0.3139374 ## [7,] 0.2589021 0.2571637 0.3147272 ## [8,] 0.2453394 0.2435748 0.3145081 ## [9,] 0.2479946 0.2461617 0.3002658 ## [10,] 0.2399564 0.2393222 0.3006061 #Here are the values you compare to choose an optimal span colMeans(RMSPE) ## [1] 0.2497106 0.2478923 0.3121506 11.4 Grid Search The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. This is how grid search is defined by Wikipedia. Although we did not call it as grid search, we have already done it in the last example. It was a simple search for optimal span with three arbitrary numbers, 0.02, 0.1 and 1. Besides, we didn’t search for degree, which is another hyperparamater in loess(). Hence the first job is to set the hyperparameter grid. In each hyperparameter, we need to know the maximum and the minimum values of this subset. For example, span in loess() sets the size of the neighborhood, which ranges between 0 to 1. This controls the degree of smoothing. So, the greater the value of span, smoother the fitted curve is. For example, if \\(n\\) is the number of data points and span = 0.5, then for a given \\(X\\), loess will use the 0.5 * \\(n\\) closest points to \\(X\\) for the fit. Additionally the degree argument in loess() is defined as the degree of the polynomials to be used, normally 1 or 2. (Degree 0 is also allowed, but see the ‘Note’. in ?loess). For each learning algorithm, the number of tuning parameters and their ranges will be different. Before running any grid search, therefore, we need to understand their function and range. Let’s use our example again without cross validation first: #Using the same data with reduced size n = 1000 set.seed(1) x &lt;- sort(runif(n)*2*pi) y &lt;- sin(x) + rnorm(n)/4 data &lt;- data.frame(y, x) #Creating a set of possible values for span: GRID span &lt;- seq(from = 0.01, to = 1, by = 0.02) #this creates 50 options degree &lt;- c(1,2) #We need a (span x degree) matrix to store RMSPE RMSPE &lt;- matrix(0, nrow = length(span), ncol = length(degree)) #loop for grid search over span for(i in 1:length(span)){ #loop for grid search for degree for (j in 1:length(degree)) { #cat(&quot;Loops: &quot;, i, j, &quot;\\r&quot;) model &lt;- loess(y ~ x, control=loess.control(surface=&quot;direct&quot;), degree=degree[j], span = span[i], data = data) fit &lt;- predict(model, data$x) RMSPE[i,j] &lt;- sqrt(mean((data$y-fit)^2)) } } The RMSPE matrix is a \\(50 \\times 2\\) matrix. Each row gives us RMSPE calculated for each span parameter for 2 different degree parameters. Note that this is not a correct RSMPE as it uses only one sample and calculates in-sample RMSPE. Moreover, it is not efficient way to search a grid as it uses double loops. We will improve it step by step. Now our job is to find the smallest RMSPE in the matrix. head(RMSPE) ## [,1] [,2] ## [1,] 0.2241721 0.2019041 ## [2,] 0.2466408 0.2413781 ## [3,] 0.2512451 0.2465098 ## [4,] 0.2538281 0.2496574 ## [5,] 0.2551257 0.2523178 ## [6,] 0.2557593 0.2534789 #Here are the values you compare for an optimal degree which(RMSPE == min(RMSPE), arr.ind = TRUE) ## row col ## [1,] 1 2 We will the same method in tuning our model, loess(), with cross validation 11.5 Cross-validated grid search All we need to do is to put our grid search algorithm inside the cross-validation loop. We will improve two things: we will have a “better” grid so that the number of loops will be fewer. Here is an example: #Creating a set of possible values for span: GRID grid &lt;- expand.grid(seq(from = 0.01, to = 1, by = 0.02), c(1,2)) head(grid) ## Var1 Var2 ## 1 0.01 1 ## 2 0.03 1 ## 3 0.05 1 ## 4 0.07 1 ## 5 0.09 1 ## 6 0.11 1 #or span &lt;- seq(from = 0.01, to = 1, by = 0.02) degree &lt;- c(rep(1,length(span)), rep(2,length(span))) grid_mine &lt;- cbind(span, degree) head(grid) ## Var1 Var2 ## 1 0.01 1 ## 2 0.03 1 ## 3 0.05 1 ## 4 0.07 1 ## 5 0.09 1 ## 6 0.11 1 Now we need to go through each row of grid that contains two tuning parameters in each CV: the first column is span and the second column is degree. #Using the same data with reduced size n = 1000 set.seed(1) x &lt;- sort(runif(n)*2*pi) y &lt;- sin(x) + rnorm(n)/4 data &lt;- data.frame(y, x) #Setting CV mysample &lt;- sample(n, n, replace = FALSE) fold &lt;- 10 #10-fold CV #Since we will do the same tuning 10 times #we need to have a container that stores #10 optimal sets of span and degree values OPT &lt;- c() #loop nvalidate &lt;- round(n/fold) for(i in 1:fold){ #cat(&quot;K-fold loop: &quot;, i, &quot;\\r&quot;) if(i &lt; fold) validate &lt;- mysample[((i-1)*nvalidate+1):(i*nvalidate)] else validate &lt;- mysample[((i-1)*nvalidate+1):n] train &lt;- seq(1, n)[-validate] data_validate &lt;- data[validate,] data_train &lt;- data[-validate,] #we need a vector to store RMSPE of each row in the grid RMSPE &lt;- c() #we need to have another loop running each row in grid: for(s in 1:nrow(grid)){ model &lt;- loess(y ~ x, control=loess.control(surface=&quot;direct&quot;), degree=grid[s,2], span = grid[s,1], data = data_train) fit &lt;- predict(model, data_validate$x) RMSPE[s] &lt;- sqrt(mean((data_validate$y-fit)^2)) } OPT[i] &lt;- which(RMSPE == min(RMSPE), arr.ind = TRUE) } opgrid &lt;- grid[OPT,] colnames(opgrid) &lt;- c(&quot;span&quot;, &quot;degree&quot;) rownames(opgrid) &lt;- c(1:10) opgrid ## span degree ## 1 0.09 2 ## 2 0.63 2 ## 3 0.23 2 ## 4 0.03 2 ## 5 0.33 2 ## 6 0.59 2 ## 7 0.75 2 ## 8 0.57 2 ## 9 0.25 1 ## 10 0.21 1 These results are good but how are we going to pick one set, the coordinates of the optimal span and degree? It seems that most folds agree that we should use degree = 2, but which span value is the optimal? If the hyperparameter is a discrete value, we can use majority rule with the modal value, which is just the highest number of occurrences in the set. This would be appropriate for degree but not for span. Instead, we should use the mean of all 10 optimal span values, each of which is calculated from each fold. library(raster) opt_degree &lt;- modal(opgrid[,2]) opt_span &lt;- mean(opgrid[,1]) opt_degree ## [1] 2 opt_span ## [1] 0.368 Now, the last job is to use them in predictions to see how good they are. Remember, we used the whole sample to tune our hyperparameters. At the outset, we said that this type of application should be avoided. Therefore, we need to create a test set at the beginning and put that subset a side, and use only the remaining data to tune our hyperparameters. Here is an illustration about this process: Let’s do it: # Using the same data n = 1000 set.seed(1) x &lt;- sort(runif(n)*2*pi) y &lt;- sin(x) + rnorm(n)/4 data &lt;- data.frame(y, x) # Creating a set of possible values for span: GRID grid &lt;- expand.grid(seq(from = 0.01, to = 1, by = 0.02), c(1,2)) # Train - Test Split set.seed(321) shuf &lt;- sample(nrow(data), nrow(data), replace = FALSE) k &lt;- 10 indx &lt;- shuf[1:(nrow(data)/k)] testset &lt;- data[indx, ] #10% of data set a side trainset &lt;- data[-indx,] # k-CV, which is the same as before set.seed(123) mysample &lt;- sample(nrow(trainset), nrow(trainset), replace = FALSE) fold &lt;- 10 OPT &lt;- c() # CV loop nvalid &lt;- round(nrow(trainset)/fold) for(i in 1:fold){ if(i &lt; fold) valid &lt;- mysample[((i-1)*nvalid+1):(i*nvalid)] # Simpler version data_valid &lt;- trainset[valid, ] data_train &lt;- trainset[-valid, ] RMSPE &lt;- c() for(s in 1:nrow(grid)){ model &lt;- loess(y ~ x, control=loess.control(surface=&quot;direct&quot;), degree=grid[s,2], span = grid[s,1], data = data_train) fit &lt;- predict(model, data_valid$x) RMSPE[s] &lt;- sqrt(mean((data_valid$y-fit)^2)) } OPT[i] &lt;- which(RMSPE == min(RMSPE)) } # Hyperparameters opgrid &lt;- grid[OPT,] colnames(opgrid) &lt;- c(&quot;span&quot;, &quot;degree&quot;) rownames(opgrid) &lt;- c(1:10) opt_degree &lt;- modal(opgrid[,2]) opt_degree ## [1] 2 opt_span &lt;- mean(opgrid[,1]) opt_span ## [1] 0.23 # **** Using the test set for final evaluation ****** model &lt;- loess(y ~ x, control=loess.control(surface=&quot;direct&quot;), degree=opt_degree, span = opt_span, data = trainset) fit &lt;- predict(model, testset$x) RMSPE_test &lt;- sqrt(mean((testset$y-fit)^2)) RMSPE_test ## [1] 0.2527365 What we have built is an algorithm that learns by trial-and-error. However, we need one more step to finalize this process: instead of doing only one 90%-10% train split, we need to do it multiple times and use the average RMSPE_test and the uncertainty (its variation) associated with it as our final performance metrics. Here again: #Using the same data n = 1000 set.seed(1) x &lt;- sort(runif(n)*2*pi) y &lt;- sin(x) + rnorm(n)/4 data &lt;- data.frame(y, x) #Creating a set of possible values for span: GRID grid &lt;- expand.grid(seq(from = 0.01, to = 1, by = 0.02), c(1,2)) # loop for Train - Test split 100 times t = 100 # number of times we loop RMSPE_test &lt;- c() # container for 100 RMSPE&#39;s for (l in 1:t) { set.seed(10+l) shuf &lt;- sample(nrow(data), nrow(data), replace = FALSE) k &lt;- 10 indx &lt;- shuf[1:(nrow(data)/k)] testset &lt;- data[indx, ] #10% of data set a side trainset &lt;- data[-indx,] # k-CV, which is the same as before set.seed(5*l) mysample &lt;- sample(nrow(trainset), nrow(trainset), replace = FALSE) fold &lt;- 10 OPT &lt;- c() #CV loop nvalid &lt;- round(nrow(trainset)/fold) for(i in 1:fold){ if(i &lt; fold) valid &lt;- mysample[((i-1)*nvalid+1):(i*nvalid)] data_valid &lt;- trainset[valid, ] data_train &lt;- trainset[-valid, ] RMSPE &lt;- c() for(s in 1:nrow(grid)){ model &lt;- loess(y ~ x, control=loess.control(surface=&quot;direct&quot;), degree=grid[s,2], span = grid[s,1], data = data_train) fit &lt;- predict(model, data_valid$x) RMSPE[s] &lt;- sqrt(mean((data_valid$y-fit)^2)) } OPT[i] &lt;- which(RMSPE == min(RMSPE)) } # Hyperparameters opgrid &lt;- grid[OPT,] colnames(opgrid) &lt;- c(&quot;span&quot;, &quot;degree&quot;) rownames(opgrid) &lt;- c(1:10) opt_degree &lt;- modal(opgrid[,2]) opt_span &lt;- mean(opgrid[,1]) # **** Using the test set for final evaluation ****** model &lt;- loess(y ~ x, control=loess.control(surface=&quot;direct&quot;), degree=opt_degree, span = opt_span, data = trainset) fit &lt;- predict(model, testset$x) RMSPE_test[l] &lt;- sqrt(mean((testset$y-fit)^2)) } We can now see the average RMSPE and its variance: plot(RMSPE_test, col = &quot;red&quot;) abline(a=mean(RMSPE_test), b= 0, col = &quot;green&quot;) mean(RMSPE_test) ## [1] 0.2564112 var(RMSPE_test) ## [1] 0.0003384585 "],["optimization-algorithms---basics.html", "Chapter 12 Optimization Algorithms - Basics 12.1 Brute-force optimization 12.2 Derivative-based methods 12.3 ML Estimation with logistic regression 12.4 Gradient Descent Algorithm 12.5 Optimization with R", " Chapter 12 Optimization Algorithms - Basics Here is a definition of algorithmic optimization from Wikipedia: An optimization algorithm is a procedure which is executed iteratively by comparing various solutions until an optimum or a satisfactory solution is found. Optimization algorithms help us to minimize or maximize an objective function \\(F(x)\\) with respect to the internal parameters of a model mapping a set of predictors (\\(X\\)) to target values(\\(Y\\)). There are three types of optimization algorithms which are widely used; Zero-Order Algorithms, First-Order Optimization Algorithms, and Second-Order Optimization Algorithms. Zero-order (or derivative-free) algorithms use only the criterion value at some positions. It is popular when the gradient and Hessian information are difficult to obtain, e.g., no explicit function forms are given. First Order Optimization Algorithms minimize or maximize a Loss function \\(F(x)\\) using its Gradient values with respect to the parameters. Most widely used First order optimization algorithm is Gradient Descent. The First order derivative displays whether the function is decreasing or increasing at a particular point. In this appendix, we will review some important concepts in algorithmic optimization. 12.1 Brute-force optimization Let’s look at a simplified example about optimal retirement-plan and solve it with a zero-order algorithm. Suppose that there are 2 groups of workers who are planning for their retirement at the age of 65. Both consider spending 40,000 dollars each year for the rest of their lives after retirement. On average, people in both groups expect to live 20 more years after retirement with some uncertainty. The people in the first group (A) have the following risk profile: 85% chance to live 20 years and 15% chance to live 30 years. The same risk profile for the people in the second group (B) is: 99% for 20 years and 1% for 30 years. Suppose that in each group, their utility (objective) function is \\(U=C^{0.5}\\). What’s the maximum premium (lump-sum payment) that a person in each group would be willing to pay for a life-time annuity of 40K? Without a pension plan, people in each group have the following utilities: #For people in group A U_A = 0.85*sqrt(40000*20) + 0.15*sqrt(10*0) U_A ## [1] 760.2631 #For people in group B U_B = 0.99*sqrt(40000*20) + 0.01*sqrt(10*0) U_B ## [1] 885.4829 For example, they would not pay 200,000 dollars to cover their retirement because that would make them worse than their current situation (without a pension plan). #For people in group A U_A = 0.85*sqrt(40000*20 - 200000) + 0.15*sqrt(40000*10 - 200000) U_A ## [1] 725.4892 #For people in group B U_B = 0.99*sqrt(40000*20 - 200000) + 0.01*sqrt(40000*10 - 200000) U_B ## [1] 771.3228 Hence, the payment they would be willing to make for reduction in uncertainty during their retirement should not make them worse off. Or more technically, their utility should not be lower than their current utility levels. Therefore Pmax, the maximum premium that a person would be willing to pay, can be found by minimizing the following cost function for people, for example, in Group A: \\[ f(Pmax) = p \\times \\sqrt{40000 \\times 20~\\text{years}-Pmax}+ \\\\ (1-p) \\times \\sqrt{40000 \\times 10~ \\text{years}-Pmax} - p \\times \\sqrt{ 40000 \\times 20~\\text{years}} \\] Here is the iteration to solve for Pmax for people in Group A. We created a cost function, costf, that we try to minimize. Change the parameters to play with it. The same algorithm can be used to find Pmax for people in Group B. library(stats) p &lt;- 0.85 w1 &lt;- 800000 w2 &lt;- 400000 converged = F iterations = 0 maxiter &lt;- 600000 learnrate &lt;- 0.5 Pmax &lt;- 10000 while(converged == FALSE){ costf &lt;- p*sqrt(w1 - Pmax) + (1 - p)*sqrt(w2 - Pmax) - p*sqrt(w1) if(costf &gt; 0){ Pmax &lt;- Pmax + learnrate iterations = iterations + 1 if(iterations &gt; maxiter) { print(&quot;It cannot converge before finding the optimal Pmax&quot;) break } converged = FALSE }else{ converged = TRUE print(paste(&quot;Maximum Premium:&quot;, Pmax, &quot;achieved with&quot;, iterations, &quot;iterations&quot;)) } } ## [1] &quot;Maximum Premium: 150043 achieved with 280086 iterations&quot; #let&#39;s verify it by `uniroot()` which finds the roots for f(x) = 0 costf &lt;- function(x){ p * sqrt(800000 - x) + (1-p) * sqrt(400000 - x) - p*sqrt(800000) } paste(&quot;Unitroot for f(x) = 0 is &quot;, uniroot(costf, c(10000, 200000))$root) ## [1] &quot;Unitroot for f(x) = 0 is 150042.524874307&quot; There are better functions that we could use for this purpose, but this example works well for our experiment. There several of important parameters in our algorithm. The first one is the starting Pmax, which can be set up manually. If the starting value is too low, iteration could not converge. If it’s too high, it can give us an error. Another issue is that our iteration does not know if the learning rate should increase or decrease when the starting value is too high or too low. This can be done with additional lines of code, but we will not address it here. This situation leads us to the learning rate: the incremental change in the value of the parameter. This parameter should be conditioned on the value of cost function. If the cost function for a given Pmax is negative, for example, the learning rate should be negative. Secondly, the number of maximum iterations must be set properly, otherwise the algorithm may not converge or take too long to converge. In the next section, we will address these issues with a smarter algorithm. There are other types of approaches. For example, the algorithm may create a grid of Pmax and then try all the possible values to see which one approximately makes the cost function minimum. 12.2 Derivative-based methods One of the derivative-based methods is the Newton-Raphson method. If we assume that the function is differentiable and has only one minimum (maximum), we can develop an optimization algorithm that looks for the point in parameter space where the derivative of the function is zero. There are other methods, like Fisher Scoring and Iteratively Reweighted Least Squares, that we will not see here. First, let’s see the Newton-Raphson method. This is a well-known extension of your calculus class about derivatives in High School. The method is very simple and used to find the roots of \\(f(x)=0\\) by iterations. In first-year computer science courses, this method is used to teach loop algorithms that calculate the value of, for example, \\(e^{0.71}\\) or \\(\\sqrt{12}\\). It is a simple iteration that converges in a few steps. \\[ x_{n+1}=x_{n}-\\frac{f\\left(x_{n}\\right)}{f^{\\prime}\\left(x_{n}\\right)} \\] To understand it, let’s look at the function \\(y=f(x)\\) shown in the following graph: It has a zero at \\(x=x_r\\), which is not known. To find it, we start with \\(x_0\\) as an initial estimate of \\(X_r\\). The tangent line to the graph at the point \\(\\left(x_0, f\\left(x_0\\right)\\right)\\) has the point \\(x_1\\) at which the tangent crosses the \\(x\\)-axis. The slope of this line can be defined as \\[ \\frac{y-f\\left(x_0\\right)}{x-x_0}=f^{\\prime}\\left(x_0\\right) \\] Hence, \\[ y-f\\left(x_0\\right)=f^{\\prime}\\left(x_0\\right)\\left(x-x_0\\right) \\] At the point where the tangent line cross the \\(x\\)-axis, \\(y=0\\) and \\(x=x_1\\). Hence solving the equation for \\(x_1\\), we get \\[ x_{1}=x_{0}-\\frac{f\\left(x_{0}\\right)}{f^{\\prime}\\left(x_{0}\\right)} \\] And the second approximations: \\[ x_{2}=x_{1}-\\frac{f\\left(x_{1}\\right)}{f^{\\prime}\\left(x_{1}\\right)} \\] And with multiple iterations one can find the solution. Here is the example: newton &lt;- function(f, x0, tol = 1e-5, n = 1000) { require(numDeriv) # Package for computing f&#39;(x) for (i in 1:n) { dx &lt;- genD(func = f, x = x0)$D[1] # First-order derivative f&#39;(x0) x1 &lt;- x0 - (f(x0) / dx) # Calculate next value x1 if (abs(x1 - x0) &lt; tol) { res &lt;- paste(&quot;Root approximation is &quot;, x1, &quot; in &quot;, i, &quot; iterations&quot;) return(res) } # If Newton-Raphson has not yet reached convergence set x1 as x0 and continue x0 &lt;- x1 } print(&#39;Too many iterations in method&#39;) } func2 &lt;- function(x) { x^15 - 2 } newton(func2, 1) ## [1] &quot;Root approximation is 1.04729412282063 in 5 iterations&quot; #Check it paste(&quot;Calculator result: &quot;, 2^(1/15)) ## [1] &quot;Calculator result: 1.04729412282063&quot; Newton’s method is often used to solve two different, but related, problems: Finding \\(x\\) such that \\(f(x)=0\\) (try to solve our insurance problem with this method) Finding \\(x\\) that \\(g&#39;(x)=0\\), or find \\(x\\) that minimizes/maximizes \\(g(x)\\). The relation between these two problems is obvious when we define \\(f(x) = g&#39;(x)\\). Hence, for the second problem, the Newton-Raphson method becomes: \\[ x_{n+1}=x_{n}-\\frac{g^{\\prime}\\left(x_{n}\\right)}{g^{\\prime \\prime}\\left(x_{n}\\right)} \\] Connection between these two problems are defined in this post (Gulzar_2018?) very nicely. Let’s pretend that we are interested in determining the parameters of a random variable \\(X \\sim N(\\mu, \\sigma^{2})\\). Here is the log-likelihood function for \\(X\\): \\[ \\log (\\mathcal{L}(\\mu, \\sigma))=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2} \\] We have seen it in Chapter 2 before. But this time we will use dnorm() which calculates the pdf of a normal variable. First let’s have the data and the log-likelihood: # Let&#39;s create a sample of normal variables set.seed(2019) X &lt;- rnorm(100, 0, 1) # And the log-likelihood of this function. # Remember likelihood function would be prod(dnorm()) with log=F normalLL &lt;- function(prmt){ sum(dnorm(X, mean = prmt[1], sd = prmt[2], log = TRUE)) } # Let&#39;s try several parameters normalLL(prmt = c(1,1.5)) ## [1] -176.078 normalLL(prmt = c(2,1)) ## [1] -347.4119 normalLL(prmt = c(mean(X),sd(X))) ## [1] -131.4619 As you can see, the last one is the best. And we can verify it because we had created \\(X\\) with 0 mean and 1 sd, approximately. Now we will use the Newton-Raphson method to calculate those parameters that minimize the negative log-likelihood. First, let’s build a function that estimates the slope of the function (first-derivative) numerically at any arbitrary point in parameter space for mean and sd, separately. Don’t forget, the log-likelihood is a function of parameters (mean and sd) not X. # First partial (numerical) derivative w.r.t. mean firstM &lt;- function(p1, p2, change = 0.0001){ prmt &lt;- c(p1, p2) high &lt;- normalLL(prmt + c(change,0)) low &lt;- normalLL(prmt - c(change,0)) slope &lt;- (high-low)/(change*2) return(slope) } firstM(mean(X), sd(X)) ## [1] 1.421085e-10 # First partial (numerical) derivative w.r.t. sd firstSD &lt;- function(p1, p2, change = 0.0001){ prmt &lt;- c(p1, p2) high &lt;- normalLL(prmt + c(0, change)) low &lt;- normalLL(prmt - c(0, change)) slope &lt;- (high-low)/(change*2) return(slope) } firstSD(mean(X), sd(X)) ## [1] -1.104417 #Verify them with the grad() library(numDeriv) f &lt;- function(x) { a &lt;- x[1]; b &lt;- x[2] sum(dnorm(X, mean = a, sd = b, log = TRUE)) } grad(f,c(mean(X),sd(X)))[1] ## [1] -1.367073e-12 grad(f,c(mean(X),sd(X)))[2] ## [1] -1.104419 # Or better round(jacobian(f,c(mean(X),sd(X))), 4) #First derivatives ## [,1] [,2] ## [1,] 0 -1.1044 round(hessian(f,c(mean(X),sd(X))), 4) #Second derivatives ## [,1] [,2] ## [1,] -121.9741 0.000 ## [2,] 0.0000 -240.289 Let’s try them now in the Newton-Raphson method. \\[ x_{n+1}=x_{n}-\\frac{g^{\\prime}\\left(x_{n}\\right)}{g^{\\prime \\prime}\\left(x_{n}\\right)} \\] Similar to the first one, we can also develop a function that calculates the second derivatives. However, instead of using our own functions, let’s use grad() and hessian() from the numDeriv package. set.seed(2019) X &lt;- rnorm(100, 2, 2) NR &lt;- function(f, x0, y0, tol = 1e-5, n = 1000) { for (i in 1:n) { dx &lt;- grad(f,c(x0, y0))[1] # First-order derivative f&#39;(x0) ddx &lt;- hessian(f,c(x0, y0))[1,1] # Second-order derivative f&#39;&#39;(x0) x1 &lt;- x0 - (dx / ddx) # Calculate next value x1 if (abs(x1 - x0) &lt; tol) { res &lt;- paste(&quot;The mean approximation is &quot;, x1, &quot; in &quot;, i, &quot; iterations&quot;) return(res) } # If Newton-Raphson has not yet reached convergence set x1 as x0 and continue x0 &lt;- x1 } print(&#39;Too many iterations in method&#39;) } func &lt;- function(x) { a &lt;- x[1]; b &lt;- x[2] sum(dnorm(X, mean = a, sd = b, log = TRUE)) } NR(func, -3, 1.5) ## [1] &quot;The mean approximation is 1.85333200301108 in 2 iterations&quot; #Let;s verify it mean(X) ## [1] 1.853332 Finding sd is left to the practice questions. But the way to do it should be obvious. Use our approximation of the mean (1.853332) as a fixed parameter in the function and run the same algorithm for finding sd. When the power of computers and the genius of mathematics intercepts, beautiful magics happen. 12.3 ML Estimation with logistic regression The pdf of Bernoulli distribution is \\[ P(Y=y)=p^y(1-p)^{1-y} \\] It’s likelihood \\[ \\begin{aligned} L(\\boldsymbol{\\beta} \\mid \\mathbf{y} ; \\mathbf{x}) &amp;=L\\left(\\beta_0, \\beta_1 \\mid\\left(y_1, \\ldots, y_n\\right) ;\\left(x_1, \\ldots, x_n\\right)\\right) \\\\ &amp;=\\prod_{i=1}^n p_i^{y_i}\\left(1-p_i\\right)^{1-y_i} \\end{aligned} \\] And log-likelihood \\[ \\begin{aligned} \\ell(\\boldsymbol{\\beta} \\mid \\mathbf{y} ; \\mathbf{x}) &amp;=\\log \\left(\\prod_{i=1}^n p_i^{y_i}\\left(1-p_i\\right)^{1-y_i}\\right) \\\\ &amp;=\\sum_{i=1}^n\\left( \\log \\left(p_i^{y_i}\\right)+\\log \\left(1-p_i\\right)^{1-y_i}\\right) \\\\ &amp;=\\sum_{i=1}^n y_i \\left(\\log \\left(p_i\\right)+\\left(1-y_i\\right) \\log \\left(1-p_i\\right)\\right) \\end{aligned} \\] where \\[ \\begin{aligned} \\operatorname{L}\\left(p_i\\right) &amp;=\\log \\left(\\frac{p_i}{1-p_i}\\right) \\\\ &amp;=\\beta_0+\\beta_1 x_1 \\end{aligned} \\] So, \\[ p_i=\\frac{\\exp \\left(\\beta_0+x_1 \\beta_1\\right)}{1+\\exp \\left(\\beta_0+x_1 \\beta_1\\right)} \\] First partial derivative with respect to \\(\\beta_0\\) \\[ \\begin{aligned} \\frac{\\partial p_i}{\\partial \\beta_0} &amp;=\\frac{\\exp \\left(\\beta_0+x_1 \\beta_1\\right)}{\\left(1+\\exp \\left(\\beta_0+x_1 \\beta_1\\right)\\right)^2} \\\\ &amp;=p_i\\left(1-p_i\\right) \\end{aligned} \\] And \\[ \\begin{aligned} \\frac{\\partial p_i}{\\partial \\beta_1} &amp;=\\frac{x_1 \\exp \\left(\\beta_0+x_1 \\beta_1\\right)}{\\left(1+\\exp \\left(\\beta_0+x_1 \\beta_1\\right)\\right)^2} \\\\ &amp;=x_1 p_i\\left(1-p_i\\right) \\end{aligned} \\] Newton-Raphson’s equation is \\[ \\boldsymbol{\\beta}^{(t+1)}=\\boldsymbol{\\beta}^{(t)}-\\left(\\boldsymbol{H}^{(t)}\\right)^{-1} \\boldsymbol{u}^{(t)}, \\] where \\[ \\boldsymbol{\\beta}^{(t)}=\\left[\\begin{array}{c} \\beta_0^{(t)} \\\\ \\beta_1^{(t)} \\end{array}\\right] \\] \\[ \\boldsymbol{u}^{(t)}=\\left[\\begin{array}{c} u_0^{(t)} \\\\ u_1^{(t)} \\end{array}\\right]=\\left[\\begin{array}{c} \\frac{\\partial \\ell\\left(\\beta^{(t)} \\mid y ; x\\right)}{\\partial \\beta_0} \\\\ \\frac{\\partial \\ell\\left(\\beta^{(t)} \\mid y ; x\\right)}{\\partial \\beta_1} \\end{array}\\right]=\\left[\\begin{array}{c} \\sum_{i=1}^n \\left(y_i-p_i^{(t)}\\right) \\\\ \\sum_{i=1}^n x_i\\left(y_i-p_i^{(t)}\\right) \\end{array}\\right] \\] where, \\[ p_i^{(t)}=\\frac{\\exp \\left(\\beta_0^{(t)}+x_1 \\beta_1^{(t)}\\right)}{1+\\exp \\left(\\beta_0^{(t)}+x_1 \\beta_1^{(t)}\\right)} \\] \\(\\boldsymbol{H}^{(t)}\\) can be considered as Jacobian matrix of \\(\\boldsymbol{u}(\\cdot)\\), \\[ \\boldsymbol{H}^{(t)}=\\left[\\begin{array}{ll} \\frac{\\partial u_0^{(t)}}{\\partial \\beta_0} &amp; \\frac{\\partial u_0^{(t)}}{\\partial \\beta_1} \\\\ \\frac{\\partial u_1^{(t)}}{\\partial \\beta_0} &amp; \\frac{\\partial u_1^{(t)}}{\\partial \\beta_1} \\end{array}\\right] \\] Let’s simulate data and solve it the Newton-Raphson’s method described above. rm(list=ls()) #Simulating data set.seed(1) n &lt;- 500 X = rnorm(n) # this is our x z = -2 + 3 * X #Prob. is defined by logistic function p = 1 / (1 + exp(-z)) #Bernoulli is the special case of the binomial distribution with size = 1 y = rbinom(n, size = 1, prob = p) #And we create our data df &lt;- data.frame(y, X) head(df) ## y X ## 1 0 -0.6264538 ## 2 0 0.1836433 ## 3 0 -0.8356286 ## 4 0 1.5952808 ## 5 0 0.3295078 ## 6 0 -0.8204684 logis &lt;- glm(y ~ X, data = df, family = binomial) summary(logis) ## ## Call: ## glm(formula = y ~ X, family = binomial, data = df) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.8253 0.1867 -9.776 &lt;2e-16 *** ## X 2.7809 0.2615 10.635 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 605.69 on 499 degrees of freedom ## Residual deviance: 328.13 on 498 degrees of freedom ## AIC: 332.13 ## ## Number of Fisher Scoring iterations: 6 library(numDeriv) func_u &lt;- function(b) { c(sum(df$y - exp(b[1] + b[2] * df$X)/ (1 + exp(b[1] + b[2] * df$X))), sum(df$X * (df$y - exp(b[1] + b[2] * df$X)/ (1 + exp(b[1] + b[2] * df$X))))) } # Starting points delta &lt;- matrix(1:2, nrow = 2) # starting delta container (with any number &gt; 0) b &lt;- array(c(-2,3)) while(abs(sum(delta)) &gt; 0.0001){ B &lt;- b #current b b &lt;- as.matrix(b) - solve(jacobian(func_u, x = b)) %*% func_u(b) #new b delta &lt;- b - as.matrix(B) } b ## [,1] ## [1,] -1.825347 ## [2,] 2.780929 12.4 Gradient Descent Algorithm Let’s start with a regression problem. The cost function in OLS is the residual sum of squares, \\(\\mathrm{RSS}=\\sum_{i=1}^n\\left(\\widehat{e}_i\\right)^2=\\sum_{i=1}^n\\left(y_i-\\hat{y}\\right)^2=\\sum_{i=1}^n\\left(y_i-\\left(b_1+b_2 x_i\\right)\\right)^2\\), which is a convex function. Our objective to find \\(b_1\\) and \\(b_2\\) that minimize RSS. How can we find those parameters to minimize a cost function if we don’t know much about it? The trick is to start with some point and move a bit (locally) in the direction that reduces the value of the cost function. In general, this search process for finding the minimizing point has two components: the direction and the step size. The direction tells us which direction we move next, and the step size determines how far we move in that direction. For example, the iterative search for \\(b_2\\) of gradient descent can be described by the following recursive rule: \\[ b_2^{(k+1)}=b_2^{(k)}-lr \\nabla RSS^{k} \\] Here, \\(lr\\) is learning rate and \\(\\nabla RSS^{k}\\) is the slope of RSS at step \\(k\\). Hence, \\(lr \\nabla RSS^{k}\\) is the total step size at step \\(k\\). Note that, as we move from either directions towards \\(b^*_2\\), \\(\\nabla RSS^{k}\\) gets smaller. In fact, it becomes zero at \\(b^*_2\\). Therefore, \\(\\nabla RSS^{k}\\) helps iterations find the proper adjustment in each step in terms of direction and magnitude. Since RSS is a convex function, it’s easy to see how sign of \\(\\nabla RSS^{k}\\) will direct the arbitrary \\(b_2^{&#39;&#39;}\\) towards the optimal \\(b_2\\). Since first-order approximation at \\(b_2^{&#39;&#39;}\\) is good only for small \\(\\Delta b_2\\), a small \\(lr&gt;0\\) is needed to o make \\(\\Delta b_2\\) small in magnitude. Moreover, when a high learning rate used it leads to “overshooting” past the local minima and may result in diverging algorithm. Below, we first use a simple linear regression function on simulated data and estimate its parameters with lm(). Let’s simulate a sample with our DGM. set.seed(1001) N &lt;- 100 int &lt;- rep(1, N) x1 &lt;- rnorm(N, mean = 10, sd = 2) Y &lt;- rnorm(N, 2*x1 + int, 1) model &lt;- lm(Y ~ x1) b &lt;- coef(model) b ## (Intercept) x1 ## 1.209597 1.979643 plot(x1, Y, col = &quot;blue&quot;, pch = 20) abline(b) The cost function that we want to minimize is \\[ y_i = 1 + 2x_i + \\epsilon_i \\\\ RSS = \\sum{\\epsilon_i^2}=\\sum{(y_i-1-2x_i)^2} \\] And, its plot for a range of coefficients is already shown earlier. 12.4.1 One-variable Below, we create a function, grdescent, to show how sensitive gradient descent algorithms would be to different calibrations: grdescent &lt;- function(x, y, lr, maxiter) { #starting points set.seed(234) b &lt;- runif(1, 0, 1) c &lt;- runif(1, 0, 1) n &lt;- length(x) #function yhat &lt;- c + b * x #gradient MSE &lt;- sum((y - yhat) ^ 2) / n converged = F iterations = 0 #while loop while (converged == F) { b_new &lt;- b - ((lr * (1 / n)) * (sum((y - yhat) * x * (-1)))) c_new &lt;- c - ((lr * (1 / n)) * (sum(y - yhat) * (-1))) b &lt;- b_new c &lt;- c_new yhat &lt;- b * x + c MSE_new &lt;- sum((y - yhat) ^ 2) / n MSE &lt;- c(MSE, MSE_new) d = tail(abs(diff(MSE)), 1) if (round(d, 12) == 0) { converged = T return(paste(&quot;Iterations: &quot;, iterations, &quot;Intercept: &quot;, c, &quot;Slope: &quot;, b)) } iterations = iterations + 1 if (iterations &gt; maxiter) { converged = T return(paste(&quot;Max. iter. reached, &quot;, &quot;Intercept:&quot;, c, &quot;Slope:&quot;, b)) } } } Note that the key part in this algorithm is b_new &lt;- b + (learnrate * (1 / n)) * sum((y - yhat) * x*(-1). The first \\(b\\) that is picked randomly by b &lt;- runif(1, 0, 1) is adjusted by learnrate * (1 / n) * (sum((y - yhat) * -x)). Note that sum((y - yhat) * x) is the first order condition of the cost function (MSE - Residual Sum of Squares) for the slope coefficient. The cost function is a convex function where the minimum can be achieved by the optimal \\(b\\). It is a linear Taylor approximation of MSE at \\(b\\) that provides the steepest descent, that is just a simple adjustment for identifying the direction of the adjustment of \\(b\\) until the minimum MSE is reached. Now we will see if this function will give us the same intercept and slope coefficients already calculated with lm() above. grdescent(x1, Y, 0.01, 100000) ## [1] &quot;Iterations: 16388 Intercept: 1.20949474169039 Slope: 1.97965284879392&quot; This is good. But, if start a very low number with a small learning rate, then we need more iteration grdescent(x1, Y, 0.005, 1000000) ## [1] &quot;Iterations: 31363 Intercept: 1.20945256472045 Slope: 1.97965686098386&quot; Yes, the main question is how do we find out what the learning rate should be? A general suggestion is to keep it small and tune it within the training process. Obviously, we can have an adaptive learning rate that changes at each iteration depending on the change in the MSE. If the change is positive, for example, the learning rate can be reduced to keep the descent. 12.4.2 Adjustable lr and SGD An adjustable learning rate has several advantages over a fixed learning rate in gradient-based optimization algorithms like stochastic gradient descent: Faster convergence: An adjustable learning rate can help the algorithm converge faster by starting with a larger learning rate, allowing the model to make bigger steps in the initial phase. This can help escape local minima or saddle points more quickly and reach the vicinity of the global minimum. Improved precision: As the learning rate decreases over time, the algorithm takes smaller steps, allowing for more precise updates to the model parameters. This can help the model fine-tune its parameters and potentially achieve a lower loss value compared to a fixed learning rate. Prevent oscillations: A fixed learning rate might cause oscillations around the optimal solution, whereas an adjustable learning rate can help dampen these oscillations by gradually reducing the step size. This can result in a more stable convergence. Adaptive to problem complexity: Some optimization problems might require different learning rates for different stages of the optimization process. An adjustable learning rate can adapt to the problem’s complexity, allowing the model to learn at a more suitable pace for each stage. Robustness: An adjustable learning rate can make the optimization algorithm more robust to the choice of the initial learning rate. Even if the initial learning rate is not perfect, the algorithm can adapt over time and still reach a reasonable solution. Although an adjustable learning rate can lead to faster convergence, improved precision, and better overall performance in gradient-based optimization algorithms, it also introduces additional hyperparameters (e.g., decay rate, annealing schedule) that need to be tuned for optimal performance. Below, we made some changes to earlier gradient descent to make it stochastic with an adaptive learning rate. In this modified code, we have implemented the following changes: Shuffled the data points using the sample() function. Iterated over the data points in mini-batches of size 1 (batch_size = 1). This makes it stochastic gradient descent. Re-calculated the gradients and updated the weights for each mini-batch. This should give us a simple stochastic gradient descent implementation for our linear regression problem. To implement an adjustable learning rate, we can use a learning rate scheduler or a learning rate annealing method. The following example shows how to use a simple exponential learning rate annealing method, which will decrease the learning rate over time: # Set the seed set.seed(1001) # Generate data N &lt;- 100 int &lt;- rep(1, N) x1 &lt;- rnorm(N, mean = 10, sd = 2) Y &lt;- rnorm(N, 2*x1 + int, 1) model &lt;- lm(Y ~ x1) b &lt;- coef(model) b ## (Intercept) x1 ## 1.209597 1.979643 # Starting points set.seed(234) b &lt;- runif(1, 0, 1) c &lt;- runif(1, 0, 1) n &lt;- length(x1) # Parameters initial_learning_rate &lt;- 0.01 decay_rate &lt;- 0.99999 batch_size &lt;- 1 max_iterations &lt;- 300000 tolerance &lt;- 1e-12 # Function yhat &lt;- c + b * x1 # Gradient MSE &lt;- sum((Y - yhat) ^ 2) / n converged = F iterations = 0 num_batches &lt;- ceiling(n / batch_size) # While loop while (converged == F) { # Shuffle data points indices &lt;- sample(n, n) for (i in seq(1, n, by = batch_size)) { idx &lt;- indices[i:min(i + batch_size - 1, n)] x_batch &lt;- x1[idx] y_batch &lt;- Y[idx] yhat_batch &lt;- c + b * x_batch learning_rate &lt;- initial_learning_rate * decay_rate^iterations b_new &lt;- b - learning_rate * ((1 / length(idx)) * sum((y_batch - yhat_batch) * x_batch * (-1))) c_new &lt;- c - learning_rate * ((1 / length(idx)) * sum(y_batch - yhat_batch) * (-1)) b &lt;- b_new c &lt;- c_new } yhat &lt;- b * x1 + c MSE_new &lt;- sum((Y - yhat) ^ 2) / n d = abs(MSE_new - tail(MSE, 1)) if (d &lt; tolerance) converged = T MSE &lt;- c(MSE, MSE_new) iterations = iterations + 1 if (iterations &gt; max_iterations) converged = T } c(iterations, c, b) ## [1] 3.000010e+05 1.205426e+00 1.966007e+00 Stochastic Gradient Descent (SGD) tends to be faster than plain Gradient Descent (GD) when working with large datasets. The main reason for this is that SGD updates the model parameters more frequently, using only a random subset of data points (or even a single data point) in each update, while GD uses the entire dataset for each update. The main advantage of using SGD over plain GD is related to the convergence speed and the ability to escape local minima. In SGD, the model parameters are updated after each mini-batch (in this case, a single data point), whereas in GD, the updates happen after going through the entire dataset. As a result, SGD can converge faster than GD because it performs more frequent updates, which can be especially beneficial when working with large datasets. Moreover, SGD introduces randomness in the optimization process due to the random sampling of data points. This stochastic nature can help the algorithm to escape local minima and find a better (global) minimum. In the case of plain GD, the algorithm always follows the true gradient, which can cause it to get stuck in sharp, non-optimal minima. However, there are some trade-offs when using SGD. The updates in SGD can be noisy because they are based on a random subset of data points, which can lead to fluctuations in the learning process. This can make the algorithm’s convergence path look less smooth than in the case of plain GD. Further, SGD often requires more careful tuning of hyperparameters, such as the learning rate and batch size. In some cases, a learning rate schedule (decreasing the learning rate over time) can be used to improve convergence. In summary, while SGD can offer faster convergence and better ability to escape local minima, it comes with the trade-off of noisier updates and may require more careful hyperparameter tuning. When working with large datasets, we can also consider using mini-batch gradient descent, which is a compromise between GD and SGD. Mini-batch gradient descent uses a small batch of data points to compute the gradient, rather than the entire dataset (GD) or a single data point (SGD). This can offer a good balance between computational efficiency and convergence properties. In the SGD code above, the decay rate is a hyperparameter that controls the rate at which the learning rate decreases over time in an adjustable learning rate schedule. Choosing an appropriate decay rate depends on the specific problem, the model, and the optimization algorithm being used. In practice, the decay rate is often chosen empirically through experimentation or by using techniques such as cross-validation or grid search. Here are some guidelines to help us choose an appropriate decay rate: A common starting point for the decay rate is 0.99, as it provides a relatively slow decay of the learning rate. However, this value might not be optimal for all problems, so you should treat it as a starting point and experiment with different values to see what works best for your specific problem. If the optimization problem is complex or has a highly non-convex loss surface, you might want to choose a smaller decay rate (e.g., 0.9 or 0.95) to allow for a faster reduction in the learning rate. This can help the model escape local minima or saddle points more quickly. On the other hand, if the problem is relatively simple, you might want to choose a larger decay rate (e.g., 0.995 or 0.999) to keep the learning rate higher for a longer period. Since there is no one-size-fits-all answer for the decay rate, it is essential to experiment with different values and observe how they affect the optimization process. You can use techniques such as cross-validation or grid search to systematically explore different decay rate values and choose the one that yields the best performance. It can be helpful to monitor the learning rate during training to ensure that it is decaying at an appropriate pace. If the learning rate is decreasing too quickly, it might result in slow convergence or getting stuck in local minima. If the learning rate is decreasing too slowly, it might cause oscillations around the optimal solution and prevent the model from converging. Learning rate scheduler is a more general concept than the specific exponential decay method we demonstrated here. A learning rate scheduler is a technique used to adjust the learning rate during the training process according to a pre-defined schedule or rule. The exponential decay method is just one example of a learning rate scheduler. There are various learning rate scheduler strategies: Exponential decay: The learning rate is multiplied by a fixed decay rate at each iteration or epoch, as demonstrated in the previous example. Step decay: The learning rate is reduced by a fixed factor at specific intervals, such as every N epochs. For example, the learning rate could be reduced by a factor of 0.5 every 10 epochs. Time-based decay: The learning rate is reduced according to a function of the elapsed training time or the number of iterations. For example, the learning rate could be reduced by a factor proportional to the inverse of the square root of the number of iterations. Cosine annealing: The learning rate is reduced following a cosine function, which allows for periodic “restarts” of the learning rate, helping the optimization process escape local minima or saddle points. Cyclic learning rates: The learning rate is varied cyclically within a predefined range, allowing the model to explore different areas of the loss surface more effectively. Adaptive learning rates: These learning rate schedulers adjust the learning rate based on the progress of the optimization process, such as the improvement in the loss function or the validation accuracy. Some well-known adaptive learning rate methods include AdaGrad, RMSprop, and Adam. The choice of the learning rate scheduler depends on the specific problem, the model, and the optimization algorithm being used. It’s essential to experiment with different learning rate schedulers and monitor the training progress to find the best strategy for a particular problem. 12.4.3 Multivariable We will expand the gradient descent algorithms with an multivariable example using matrix algebra. First, the data and model simulation: set.seed(1001) N &lt;- 100 int &lt;- rep(1, N) x1 &lt;- rnorm(N, mean = 10, sd = 2) x2 &lt;- rnorm(N, mean = 5, sd = 1) x3 &lt;- rbinom(N, 1, 0.5) x4 &lt;- rbinom(N, 1, 0.5) x5 &lt;- rbinom(N, 1, 0.5) x6 &lt;- rnorm(N, 1, 0.25) x7 &lt;- rnorm(N, 1, 0.2) x2x3 &lt;- x2*x3 x4x5 &lt;- x4*x5 x4x6 &lt;- x5*x6 x3x7 &lt;- x3*x7 Y &lt;- rnorm(N, 2*x1 + -0.5*x2 - 1.75*x2x3 + 2*x4x5 - 3*x4x6 + 1.2*x3x7 + int, 1) X &lt;- cbind(int, x1, x2, x2x3, x4x5, x4x6, x3x7) We can solve it with linear algebra manually: betaOLS &lt;- solve(t(X)%*%X)%*%t(X)%*%Y print(betaOLS) ## [,1] ## int 0.4953323 ## x1 1.9559022 ## x2 -0.3511182 ## x2x3 -1.9112623 ## x4x5 1.7424723 ## x4x6 -2.8323934 ## x3x7 2.1015442 We can also solve it with lm() model1.lm &lt;- lm(Y ~ X -1) summary(model1.lm) ## ## Call: ## lm(formula = Y ~ X - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.84941 -0.45289 -0.09686 0.57679 2.07154 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## Xint 0.49533 0.75410 0.657 0.51290 ## Xx1 1.95590 0.03868 50.571 &lt; 2e-16 *** ## Xx2 -0.35112 0.12600 -2.787 0.00645 ** ## Xx2x3 -1.91126 0.13358 -14.308 &lt; 2e-16 *** ## Xx4x5 1.74247 0.24396 7.143 2.01e-10 *** ## Xx4x6 -2.83239 0.18831 -15.041 &lt; 2e-16 *** ## Xx3x7 2.10154 0.64210 3.273 0.00149 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8448 on 93 degrees of freedom ## Multiple R-squared: 0.9972, Adjusted R-squared: 0.997 ## F-statistic: 4677 on 7 and 93 DF, p-value: &lt; 2.2e-16 Now the function for gradient descent: grdescentM &lt;- function(x, y, lr, maxiter) { set.seed(123) b &lt;- runif(ncol(x), 0, 1) yhat &lt;- x%*%b e &lt;- y - yhat RSS &lt;- t(e)%*%e converged = F iterations = 0 n &lt;- length(y) while(converged == F) { b_new &lt;- b - (lr*(1/n))*t(x)%*%(x%*%b - y) b &lt;- b_new yhat &lt;- x%*%b e &lt;- y - yhat RSS_new &lt;- t(e)%*%e RSS &lt;- c(RSS, RSS_new) d = tail(abs(diff(RSS)), 1) if (round(d, 12) == 0) { converged = T return(b) } iterations = iterations + 1 if(iterations &gt; maxiter) { converged = T return(b) } } } grdescentM(X, Y, 0.01, 100000) ## [,1] ## int 0.4953843 ## x1 1.9559009 ## x2 -0.3511257 ## x2x3 -1.9112548 ## x4x5 1.7424746 ## x4x6 -2.8323944 ## x3x7 2.1015069 12.5 Optimization with R A good summary of tools for optimization in R given in this guide: Optimization and Mathematical Programming. There are many optimization methods, each of which would only be appropriate for specific cases. In choosing a numerical optimization method, we need to consider following points: We need to know if it is a constrained or unconstrained problem. For example, the MLE method is an unconstrained problem. Most regularization problems, like Lasso or Ridge, are constraint optimization problems. Do we know how the objective function is shaped a priori? MLE and OLS methods have well-known objective functions (Residual Sum of Squares and Log-Likelihood). Maximization and minimization problems can be used in both cases by flipping the sign of the objective function. Multivariate optimization problems are much harder than single-variable optimization problems. There is, however, a large set of available optimization methods for multivariate problems. In multivariate cases, the critical point is whether the objective function has available gradients. If only the objective function is available without gradient or Hessian, the Nelder-Mead algorithm is the most common method for numerical optimization. If gradients are available, the best and most used method is the gradient descent algorithm. We have seen its application for OLS. This method can be applied to MLE as well. It is also called a Steepest Descent Algorithm. In general, the gradient descent method has three types: Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent. If the gradient and Hessian are available, we can also use the Newton-Raphson Method. This is only possible if the dataset is not high-dimensional, as calculating the Hessian would otherwise be very expensive. Usually the Nelder-Mead method is slower than the Gradient Descent method. Optim() uses the Nelder-Mead method, but the optimization method can be chosen in its arguments. More details can be found in this educational slides. The most detailed and advance source is Numerical Recipes, which uses C++ and R. "],["prediction-intervals.html", "Chapter 13 Prediction Intervals 13.1 Prediction interval for unbiased OLS predictor", " Chapter 13 Prediction Intervals When we look back how we built our estimation and prediction simulations, we can see one thing: we had withdrawn 2000 samples and applied our estimators and predictors to each sample. More specifically, we had 2000 estimates from the estimator \\(\\bar{X}\\) and 2000 predictions by \\(\\hat{f}\\). Hence, we have a sampling uncertainty that is captured by the variance of the distribution of estimates, which is also known as the sampling distribution. Sampling distributions are probability distributions that provide the set of possible values for the estimates and will inform us of how appropriately our current estimator is able to explain the population data. And if the estimator is BLUE of \\(\\mu_x\\), the sampling distribution of \\(\\bar{X}\\) can be defined as \\(\\bar{X}\\sim \\mathcal{T}\\left(\\mu, S^{2}\\right)\\) where \\(S\\) is the standard deviation of the sample and \\(\\mathcal{T}\\) is the Student’s \\(t\\)-distribution. This concept is the key point in inferential statistics as it helps us build the interval estimation of the true parameter, \\(\\mu_x\\). The variation of \\(\\bar{X}\\) from sample to sample is important as it makes the interval wider or narrower. Similar to estimation, we make predictions in each sample by the best \\(\\hat{f}\\). Since each sample is a random pick from the population, the prediction would be different from sample to sample. Unlike estimations, however, we allow bias in predictors in exchange with a reduction in variance, which captures the variation of predictions across samples. Although it was easy to calculate the variance of our predictions across samples with simulations, in practice, we have only one sample to calculate our prediction. While we can consider developing a theoretical concept similar to sampling distribution to have an interval prediction, since we allow a variance-bias trade-off in predictions, it would not be as simple as before to develop a confidence interval around our predictions. This is one of the drawbacks in machine learning and the subject of the recent research. One of the posts from Ivan Svetunkov (the builder of R package smooth()) describes the situation: (…)capturing the uncertainty is a difficult task, and there is still a lot of things that can be done in terms of model formulation and estimation. But at least, when applying models on real data, we can have an idea about their performance in terms of the uncertainty captured(…) (Svetunkov_2019?). It is tempting to come to an idea that, when we are able to use an unbiased estimator as a predictor, perhaps due to an insignificant difference between their MSPEs, we may have a more reliable interval prediction, which quantifies the uncertainty in predictions. However, although machine learning predictions are subject to a lack of reliable interval predictions, finding an unbiased estimator specifically in regression-based models is not a simple task either. There are many reasons that the condition of unbiasedness, \\(\\mathbf{E}(\\hat{\\theta})=\\theta\\), may be easily violated. Reverse causality, simultaneity, endogeneity, unobserved heterogeneity, selection bias, model misspecification, measurement errors in covariates are some of the well-known and very common reasons for biased estimations in the empirical world and the major challenges in the field of econometrics today. This section will summarize the forecast error, F, and the prediction interval when we use an unbiased estimator as a predictor. Here is the definition of forecast error, which is the difference between \\(x_0\\) and the predicted \\(\\hat{x}_0\\) in our case: \\[ F=x_0-\\hat{x}_0=\\mu_x+\\varepsilon_0-\\bar{X} \\] If we construct a standard normal variable from \\(F\\): \\[ z= \\frac{F-\\mathbf{E}[F]}{\\sqrt{\\mathbf{Var}(F)}}=\\frac{F}{\\sqrt{\\mathbf{Var}(F)}}=\\frac{x_0-\\hat{x}_0}{\\sqrt{\\mathbf{Var}(F)}}\\sim N(0,1) \\] where \\(\\mathbf{E}[F]=0\\) because \\(\\mathbf{E}[\\bar{X}]=\\mu_x\\) and \\(\\mathbf{E}[\\varepsilon]=0\\). We know that approximately 95% observations of any standard normal variable can be between \\(\\pm{1.96}\\mathbf{sd}\\) Since the standard deviation is 1: \\[ \\mathbf{Pr} = (-1.96 \\leqslant z \\leqslant 1.96) = 0.95. \\] Or, \\[ \\mathbf{Pr} = \\left(-1.96 \\leqslant \\frac{x_0-\\hat{x}_0}{\\mathbf{sd}(F)} \\leqslant 1.96\\right) = 0.95. \\] With a simple algebra this becomes, \\[ \\mathbf{Pr} \\left(\\hat{x}_0-1.96\\mathbf{sd}(F) \\leqslant x_0 \\leqslant \\hat{x}_0+1.96\\mathbf{sd}(F)\\right) = 0.95. \\] This is called a 95% confidence interval or prediction interval for \\(x_0\\). We need to calculate \\(\\mathbf{sd}(F)\\). We have derived it before, but let’s repeat it here again: \\[ \\mathbf{Var}(F) = \\mathbf{Var}\\left(\\mu_x+\\varepsilon_0-\\bar{X}\\right)=\\mathbf{Var}\\left(\\mu_x\\right)+\\mathbf{Var}\\left(\\varepsilon_0\\right)+\\mathbf{Var}\\left(\\bar{X}\\right)\\\\ = \\mathbf{Var}\\left(\\varepsilon_0\\right)+\\mathbf{Var}\\left(\\bar{X}\\right)\\\\=\\sigma^2+\\mathbf{Var}\\left(\\bar{X}\\right) \\] What’s \\(\\mathbf{Var}(\\bar{X})\\)? With the assumption of i.i.d. \\[ \\mathbf{Var}(\\bar{X}) = \\mathbf{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} x_{i}\\right) =\\frac{1}{n^2} \\sum_{i=1}^{n}\\mathbf{Var}(x_{i})=\\frac{1}{n^2} \\sum_{i=1}^{n}\\sigma^2=\\frac{1}{n^2} n\\sigma^2=\\frac{\\sigma^2}{n}. \\] We do not know \\(\\sigma^2\\) but we can approximate it by \\(\\hat{\\sigma}^2\\), which is the variance of the sample. \\[ \\mathbf{Var}(\\bar{X}) = \\frac{\\hat{\\sigma}^2}{n}~~\\Rightarrow~~ \\mathbf{se}(\\bar{X}) = \\frac{\\hat{\\sigma}}{\\sqrt{n}} \\] Note that the terms, standard deviation and standard error, often lead to confusion about their interchangeability. We use the term standard error for the sampling distribution (standard error of the mean - SEM): the standard error measures how far the sample mean is likely to be from the population mean. Whereas the standard deviation of the sample (population) is the degree to which individuals within the sample (population) differ from the sample (population) mean. Now we can get \\(\\mathbf{sd}(F)\\): \\[ \\mathbf{sd}(F) =\\hat{\\sigma}+\\frac{\\hat{\\sigma}}{\\sqrt{n}}=\\hat{\\sigma}\\left(1+\\frac{1}{\\sqrt{n}}\\right) \\] Therefore, \\(\\mathbf{se}(\\bar{X})\\) changes from sample to sample, as \\(\\hat{\\sigma}\\) will be different in each sample. As we discussed earlier, when we use \\(\\hat{\\sigma}\\) we should use \\(t\\)-distribution, instead of standard normal distribution. Although they have the same critical vaules for 95% intervals, which is closed to 1.96 when the sample size larger than 100, we usually use critical \\(t\\)-values for the interval estimations. Note that when \\(\\mathbf{E}[\\bar{X}]\\neq\\mu_x\\) the whole process of building a prediction interval collapses at the beginning. Moreover, confidence or prediction intervals require data that must follow a normal distribution. If the sample size is large enough (more than 35, roughly) the central limit theorem makes sure that the sampling distribution would be normal regardless of how the population is distributed. In our example, since our sample sizes 3, the CLT does not hold. Let’s have a more realistic case in which we have a large population and multiple samples with \\(n=100\\). # Better example set.seed(123) popx &lt;- floor(rnorm(10000, 10, 2)) summary(popx) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 8.00 9.00 9.49 11.00 17.00 samples &lt;- matrix(0, 1000, 200) set.seed(1) for (i in 1:nrow(samples)) { samples[i,] &lt;- sample(popx, ncol(samples), replace = TRUE) } head(samples[, 1:10]) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 10 10 9 10 8 9 11 8 13 10 ## [2,] 11 13 5 11 6 9 9 9 10 9 ## [3,] 12 12 11 9 7 7 5 10 9 6 ## [4,] 9 8 12 8 10 11 8 8 10 10 ## [5,] 15 9 10 10 10 10 10 11 9 7 ## [6,] 8 9 11 9 10 10 10 13 11 15 hist(rowMeans(samples), breaks = 20, cex.main=0.8, cex.lab = 0.8, main = &quot;Histogram of X_bar&#39;s&quot;, xlab = &quot;X_bar&quot;) summary(rowMeans(samples)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 9.005 9.390 9.485 9.486 9.581 9.940 mean(popx) ## [1] 9.4895 As you can see, the sampling distribution of \\(\\bar{X}\\) is almost normal ranging from 9 to 9.94 with mean 9.486. We can see also that it’s an unbiased estimator of \\(\\mu_x\\). When we use \\(\\bar{X}\\) from a sample to predict \\(x\\), we can quantify the uncertainty in this prediction by building a 95% confidence interval. Let’s use sample 201 to show the interval. # Our sample sample_0 &lt;- samples[201,] mean(sample_0) ## [1] 9.35 # sd(F) sdF &lt;- sqrt(var(sample_0))*(1+1/sqrt(length(sample_0))) upper &lt;- mean(sample_0) + 1.96*sdF lower &lt;- mean(sample_0) - 1.96*sdF c(lower, upper) ## [1] 5.387422 13.312578 The range of this 95% prediction interval quantifies the prediction accuracy when we use 9.35 as a predictor, which implies that the value of a randomly picked \\(x\\) from the same population could be predicted to be between those numbers. When we change the sample, the interval changes due to differences in the mean and the variance of the sample. 13.1 Prediction interval for unbiased OLS predictor We will end this chapter by setting up a confidence interval for predictions made by an unbiased \\(\\hat{f}(x)\\). We follow the same steps as in section 4. Note that the definition of the forecast error, \\[ F=y_0-\\hat{f}(x_0)=f(x_0)+\\varepsilon_0-\\hat{f}(x_0), \\] is the base in MSPE. We will have here a simple textbook example to identify some important elements in prediction interval. Our model is, \\[ y_{i}=\\beta_0+\\beta_1 x_{1i}+\\varepsilon_{i}, ~~~~ i=1, \\ldots, n \\] where \\(\\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right)\\), \\(\\mathbf{Cov}\\left(\\varepsilon_{i}, \\varepsilon_{j}\\right)=0\\) for \\(i\\neq j\\). We can write this function as \\[ y_{i}=f(x_i)+\\varepsilon_{i}, ~~~~ i=1, \\ldots, n \\] Based on a sample and the assumption about DGM, we choose an estimator of \\(f(x)\\), \\[ \\hat{f}(x) = \\hat{\\beta}_0+\\hat{\\beta}_1 x_{1i}, \\] which is BLUE of \\(f(x)\\), when it is estimated with OLS given the assumptions about \\(\\varepsilon_i\\) stated above. Then the forecast error is \\[ F=y_0-\\hat{f}(x_0)=\\beta_0+\\beta_1 x_{0}+\\varepsilon_{0}-\\hat{\\beta}_0+\\hat{\\beta}_1 x_{0}, \\] Since our \\(\\hat{f}(x)\\) is an unbiased estimator of \\(f(x)\\), \\(\\mathbf{E}(F)=0\\). And, given that \\(\\varepsilon_{0}\\) is independent of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) and \\(\\beta_0\\) as well as \\(\\beta_1 x_{0}\\) are non-stochastic (i.e. they have variance zero), then \\[ \\mathbf{Var}(F)=\\mathbf{Var}\\left(\\varepsilon_{0}\\right)+\\mathbf{Var}\\left(\\hat{\\beta_0}+\\hat{\\beta_1} x_{0}\\right), \\] which is \\[ \\mathbf{Var}(F)=\\sigma^{2}+\\mathbf{Var}(\\hat{\\beta}_0)+x_{0}^{2} \\mathbf{Var}(\\hat{\\beta}_1)+2 x_{0} \\mathbf{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1). \\] More specifically, \\[ \\mathbf{Var}(F)=\\sigma^{2}+\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\bar{x}^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)+x_{0}^{2}\\left( \\frac{\\sigma^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)-2 x_{0}\\left( \\sigma^{2} \\frac{\\bar{x}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right). \\] After simplifying it, we get the textbook expression of the forecast variance: \\[ \\mathbf{Var}(F)=\\sigma^{2}\\left(1+\\frac{1}{n}+\\frac{\\left(x_{0}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right) \\] We have seen it before: as the noise in the data (\\(\\sigma^2\\)) goes up, the variance increases. More importantly, as \\(x_0\\) moves away from \\(\\bar{x}\\), \\(\\mathbf{Var}(F)\\) rises further. Intuitively, rare incidence in data should have less uncertainty in predicting the outcome. The rarity of \\(x_0\\) will be quantified by \\(x_0-\\bar{x}\\) and the uncertainty in prediction is captured by \\(\\mathbf{Var}(F)\\). Finally, using the fact that \\(\\varepsilon\\) is normally distributed, with \\(\\mathbf{E}(F)=0\\), we just found that \\(F \\sim N(0, \\mathbf{Var}(F))\\). Hence, the 95% prediction interval for \\(n&gt;100\\) will approximately be: \\[ \\mathbf{Pr} \\left(\\hat{f}_0-1.96\\mathbf{sd}(F) \\leqslant y_0 \\leqslant \\hat{f}_0+1.96\\mathbf{sd}(F)\\right) = 0.95. \\] When we replace \\(\\sigma^2\\) with \\(\\hat{\\sigma}^2\\), \\(F\\) will have a Student’s \\(t\\) distribution and the critical values (1.96) will be different specially if \\(n&lt;100\\). Since this interval is for \\(x_0\\), we can have a range of \\(x\\) and have a nice plot showing the conficence interval around the point predictions for each \\(x\\). Let’s have a simulation with a simple one-variable regression to see the uncertainty in prediction. We need one sample and one out-sample dataset for prediction. # Getting one-sample. set.seed(123) x_1 &lt;- rnorm(100, 0, 1) f &lt;- 1 - 2*x_1 # DGM y &lt;- f + rnorm(100, 0, 1) inn &lt;- data.frame(y, x_1) # Getting out-of-sample data points. set.seed(321) x_1 &lt;- rnorm(100, 0, 10) # sd =10 to see the prediction of outlier X&#39;s f &lt;- 1 - 2*x_1 # DGM y &lt;- f + rnorm(100, 0, 1) out &lt;- data.frame(y, x_1) # OLS ols &lt;- lm(y~., inn) yhat &lt;- predict(ols, out) # Let&#39;s have a Variance(f) function # since variance is not fixed and changes by x_0 v &lt;- function(xzero){ n &lt;- nrow(inn) sigma2_hat &lt;- sum((inn$y -yhat)^2)/(n-2) #we replace it with sample variance num= (xzero-mean(inn$x_1))^2 denom = sum((inn$x_1-mean(inn$x_1))^2) var &lt;- sigma2_hat*(1 + 1/n + num/denom) x0 &lt;- xzero outcome &lt;- c(var, x0) return(outcome) } varF &lt;- matrix(0, nrow(out), 2) for (i in 1:nrow(out)) { varF[i, ] &lt;- v(out$x_1[i]) } data &lt;- data.frame(&quot;sd&quot; = c(sqrt(varF[,1])), &quot;x0&quot; = varF[,2], &quot;yhat&quot; = yhat, &quot;upper&quot; = c(yhat + 1.96*sqrt(varF[,1])), &quot;lower&quot; = c(yhat - 1.96*sqrt(varF[,1]))) require(plotrix) plotCI(data$x0, data$yhat , ui=data$upper, li=data$lower, pch=21, pt.bg=par(&quot;bg&quot;), scol = &quot;blue&quot;, col=&quot;red&quot;, main = &quot;Prediction interval for each y_0&quot;, ylab=&quot;yhat(-)(+)1.96sd&quot;, xlab=&quot;x_0&quot;, cex.main = 0.8, cex.lab = 0.8, cex.axis = 0.7) As the \\(x_0\\) moves away from the mean, which is zero in our simulation, the prediction uncertainty captured by the range of confidence intervals becomes larger. "],["interpretability.html", "Chapter 14 Interpretability 14.1 Interpretable vs NonInterpretable Models", " Chapter 14 Interpretability 14.1 Interpretable vs NonInterpretable Models "],["shrinkage-models.html", "Chapter 15 Shrinkage Models 15.1 Ridge 15.2 Lasso 15.3 Adaptive Lasso 15.4 Sparsity", " Chapter 15 Shrinkage Models In simple regression or classification problems, we cannot train a parametric model in a way that the fitted model minimizes the out-of-sample prediction error. We could (and did) fit the parametric models manually by adding or removing predictors and their interactions and polynomials. As we have seen in earlier chapters, by dropping a variable in a regression, for example, it is possible to reduce the variance at the cost of a negligible increase in bias. In fitting the predictive model, some of the variables used in a regression may not be well associated with the response. Keeping those “irrelevant” variables often leads to unnecessary complexity in the resulting model. Regularization or penalization is an alternative and automated fitting procedure that refers to a process that removes irrelevant variables or shrinks the magnitude of their parameters, which can yield better prediction accuracy and model interpretability by preventing overfitting. There are several types of regularization techniques that can be used in parametric models. Each of these techniques adds a different type of penalty term to the objective function and can be used in different situations depending on the characteristics of the data and the desired properties of the model. Two methods, Ridge and Lasso, are two of well-known benchmark techniques that reduce the model complexity and prevent overfitting resulting from simple linear regression. The general principle in penalization can be shown as \\[ \\widehat{m}_\\lambda(\\boldsymbol{x})=\\operatorname{argmin}\\left\\{\\sum_{i=1}^n \\underbrace{\\mathcal{L}\\left(y_i, m(\\boldsymbol{x})\\right)}_{\\text {loss function }}+\\underbrace{\\lambda\\|m\\|_{\\ell_q}}_{\\text {penalization }}\\right\\} \\] where \\(\\mathcal{L}\\) could be conditional mean, quantiles, expectiles, \\(m\\) could be linear, logit, splines, tree-based models, neural networks. The penalization, \\(\\ell_q\\), could be lasso (\\(\\ell_1\\)) or ridge (\\(\\ell_2\\)). And, \\(\\lambda\\) regulates overfitting that can be determined by cross-validation or other methods. It puts a price to pay for a having more flexible model: \\(\\lambda\\rightarrow0\\): it interpolates data, low bias, high variance \\(\\lambda\\rightarrow\\infty\\): linear model high bias, low variance There are two fundamental goals in statistical learning: achieving a high prediction accuracy and identifying relevant predictors. The second objective, variable selection, is particularly important when there is a true sparsity in the underlying model. By their nature, penalized parametric models are not well-performing tools for prediction. But, they provide important tools for model selection specially when \\(p&gt;N\\) and the true model is sparse. This section starts with two major models in regularized regressions, Ridge and Lasso, and develops an idea on sparse statistical modelling with Adaptive Lasso. Although there are many sources on the subject, perhaps the most fundamental one is Statistical Learning with Sparsity by Hastie et al. (2015). 15.1 Ridge The least squares fitting procedure is that one estimates \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p}\\) that minimize the residual sum of squares: \\[ \\mathrm{RSS}=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2} \\] Ridge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. \\[ \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2} =\\mathrm{RSS}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}, \\] where \\(\\lambda\\) is the hyperparameter that can be tuned by cross-validation and grid search. The last term, \\(\\lambda \\sum_{j} \\beta_{j}^{2}\\), is a constraint, which is also called shrinkage penalty. This type of penalty is called as \\(\\ell_{2}\\) (L-2 penalty). As with Ordinary Least Squares (OLS), this cost function tries to minimize RSS but also penalizes the size of the coefficients. More specifically, \\[ \\hat{\\beta}_\\lambda^{\\text {ridge }}=\\operatorname{argmin}\\left\\{\\left\\|\\mathbf{y}-\\left(\\beta_0+\\mathbf{X} \\beta\\right)\\right\\|_{\\ell_2}^2+\\lambda\\|\\beta\\|_{\\ell_2}^2\\right\\}, \\] which has the solution: \\[ \\hat{\\beta}_\\lambda=\\left(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y} \\] where, If \\(\\lambda \\rightarrow 0, \\quad \\hat{\\beta}_0^{\\text {ridge }}=\\hat{\\beta}^{\\text {ols }}\\), If \\(\\lambda \\rightarrow \\infty, \\quad \\hat{\\beta}_{\\infty}^{\\text {ridge }}=\\mathbf{0}\\). The hyperparameter \\(\\lambda\\) controls the relative impact of the penalization on the regression coefficient estimates. When \\(\\lambda = 0\\), the cost function becomes RSS (residual sum of squares), that is the cost function of OLS and the estimations, produce the least squares estimates. However, as \\(\\lambda\\) gets higher, the impact of the shrinkage penalty grows, and the coefficients of the ridge regression will approach zero. Note that, the shrinkage penalty is applied to slope coefficients not to the intercept, which is simply the mean of the response, when all features are zero. Let’s apply this to the same data we used earlier, Hitters from the ISLR (ISLR_2021?) package: library(ISLR) remove(list = ls()) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] We will use the glmnet package to fit a ridge regression. The generic function in glmnet is defined by \\[ \\min _{\\beta_0, \\beta} \\frac{1}{N} \\sum_{i=1}^N w_i l\\left(y_i, \\beta_0+\\beta^T x_i\\right)+\\lambda\\left[(1-\\alpha)\\|\\beta\\|_2^2 / 2+\\alpha\\|\\beta\\|_1\\right] \\text {, } \\] where \\(l\\left(y_i, \\eta_i\\right)\\) is the negative log-likelihood contribution for observation \\(i\\) and \\(\\alpha\\) is the elastic net penalty. When \\(\\alpha=1\\) ( the default), the penalty term becomes \\(\\ell_{1}\\) and the resulting model is called lasso regression (least absolute shrinkage and selection operator). When \\(\\alpha=1\\), the penalty term becomes \\(\\ell_{2}\\) and the resulting model is called ridge regression (some authors use the term Tikhonov–Phillips regularization). As before, the tuning parameter \\(\\lambda\\) controls the overall strength of the penalty. Since the penalty shrinks the coefficients of correlated variables (in Ridge) or pick one of them and discard the others (in Lasso), the variables are supposed to be standardized, which is done by glmnet. The glmnet function has a slightly different syntax from other model-fitting functions that we have used so far in this book (y ~ X). Therefore, before we execute the syntax, we have the prepare the model so that X will be a matrix and y will be a vector. The matrix X has to be prepared before we proceed, which must be free of NAs. X &lt;- model.matrix(Salary ~ ., df)[, -1] y &lt;- df$Salary The glmnet package is maintained by Trevor Hastie who provides a friendly vignette (Hastie_glmnet?). They describe the importance of model.matrix() in glmnet as follows: (…)particularly useful for creating \\(x\\); not only does it produce a matrix corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because glmnet() can only take numerical, quantitative inputs. Here is the example for a ridge regression: library(glmnet) grid = 10 ^ seq(10,-2, length = 100) model &lt;- glmnet(X, y, alpha = 0, lambda = grid) Although we defined the grid, we did not do a grid search explicitly by cross validation. Moreover, we do not need to select a grid. By default, the glmnet() function performs ridge regression for an automatically selected range of \\(\\lambda\\) values. It ranges from the null model - only intercept when \\(\\lambda\\) is at the upper bound and the least squares fit when the \\(\\lambda\\) is at lower bound. The application above is to show that we can also choose to implement the function over a grid of values. Further, the glmnet() function standardizes the variables so that they are on the same scale. To turn off this default setting, we use the argument standardize=FALSE. The methods here, ridge and lasso, are parametric models. Unlike non-parametric methods, each model is defined by a set of parameters or, as in our case, coefficients. Therefore, when we do a grid search, each value of the hyperparameter (\\(\\lambda\\)) is associated with one model defined by a set of coefficients. In order to see the coefficients we need to apply another function, coef(). Remember, we have 100 \\(\\lambda&#39;s\\). Hence, coef() produces a 20 x 100 matrix, with 20 rows (one for each predictor, plus an intercept) and 100 columns (one for each value of \\(\\lambda\\)). dim(coef(model)) ## [1] 20 100 model$lambda[c(20, 80)] ## [1] 4.977024e+07 2.656088e+00 coef(model)[, c(20, 80)] ## 20 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## s19 s79 ## (Intercept) 5.358880e+02 156.6073700 ## AtBat 1.093664e-05 -1.7526436 ## Hits 3.967221e-05 6.1739859 ## HmRun 1.598556e-04 1.3285278 ## Runs 6.708833e-05 -0.7689372 ## RBI 7.086606e-05 -0.1297830 ## Walks 8.340541e-05 5.5357165 ## Years 3.410894e-04 -9.2923000 ## CAtBat 9.390097e-07 -0.0792321 ## CHits 3.455823e-06 0.2132942 ## CHmRun 2.606160e-05 0.6557328 ## CRuns 6.933126e-06 0.8349167 ## CRBI 7.155123e-06 0.4090719 ## CWalks 7.570013e-06 -0.6623253 ## LeagueN -1.164983e-04 62.0427219 ## DivisionW -1.568625e-03 -121.5286522 ## PutOuts 4.380543e-06 0.2809457 ## Assists 7.154972e-07 0.3124435 ## Errors -3.336588e-06 -3.6852362 ## NewLeagueN -2.312257e-05 -27.9849755 As we see, the coefficient estimates are much smaller when a large value of \\(\\lambda\\) is used. We generally use the predict() function as before. But, here we can also use it to estimate the ridge regression coefficients for a new value of \\(\\lambda\\). Hence, if we don’t want to rely on the internal grid search provided by glmnet(), we can do our own grid search by predict(). This is an example when \\(\\lambda = 50\\), which wasn’t in the grid. predict(model, s = 50, type = &quot;coefficients&quot;) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 4.876610e+01 ## AtBat -3.580999e-01 ## Hits 1.969359e+00 ## HmRun -1.278248e+00 ## Runs 1.145892e+00 ## RBI 8.038292e-01 ## Walks 2.716186e+00 ## Years -6.218319e+00 ## CAtBat 5.447837e-03 ## CHits 1.064895e-01 ## CHmRun 6.244860e-01 ## CRuns 2.214985e-01 ## CRBI 2.186914e-01 ## CWalks -1.500245e-01 ## LeagueN 4.592589e+01 ## DivisionW -1.182011e+02 ## PutOuts 2.502322e-01 ## Assists 1.215665e-01 ## Errors -3.278600e+00 ## NewLeagueN -9.496680e+00 There are two ways that we can train ridge (and Lasso): We use our own training algorithm; Or, we rely on 'glmnet internal cross-validation process. Here is an example for our own algorithm for training ridge regression: grid = 10^seq(10, -2, length = 100) MSPE &lt;- c() MMSPE &lt;- c() for(i in 1:length(grid)){ for(j in 1:100){ set.seed(j) ind &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[ind, ] xtrain &lt;- model.matrix(Salary~., train)[,-1] ytrain &lt;- df[ind, &quot;Salary&quot;] test &lt;- df[-ind, ] xtest &lt;- model.matrix(Salary~., test)[,-1] ytest &lt;- df[-ind, &quot;Salary&quot;] model &lt;- glmnet(xtrain, ytrain, alpha = 0, lambda = grid[i], thresh = 1e-12) yhat &lt;- predict(model, s = grid[i], newx = xtest) MSPE[j] &lt;- mean((yhat - ytest)^2) } MMSPE[i] &lt;- mean(MSPE) } min(MMSPE) ## [1] 119058.3 grid[which.min(MMSPE)] ## [1] 14.17474 plot(log(grid), MMSPE, type = &quot;o&quot;, col = &quot;red&quot;, lwd = 3) What is the tuned model using the last training set with this \\(\\lambda\\)? lambda &lt;- grid[which.min(MMSPE)] coeff &lt;- predict(model, s = lambda , type = &quot;coefficients&quot;, newx = xtrain) coeff ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 285.78834247 ## AtBat -1.27240085 ## Hits 2.06931134 ## HmRun 0.04319066 ## Runs 2.75588969 ## RBI 0.45631590 ## Walks 3.46189297 ## Years -8.82528502 ## CAtBat -0.26127780 ## CHits 1.28540111 ## CHmRun 1.31904979 ## CRuns 0.05880843 ## CRBI -0.05103190 ## CWalks -0.34003983 ## LeagueN 131.98795986 ## DivisionW -119.25402540 ## PutOuts 0.19785230 ## Assists 0.64820842 ## Errors -6.97397640 ## NewLeagueN -54.55149894 We may want to compare the ridge with a simple OLS: MSPE &lt;- c() for (j in 1:100) { set.seed(j) ind &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[ind,] test &lt;- df[-ind, ] model &lt;- lm(Salary ~ ., data = train) yhat &lt;- predict(model, newdata = test) MSPE[j] &lt;- mean((yhat - test$Salary) ^ 2) } mean(MSPE) ## [1] 124217.3 summary(model) ## ## Call: ## lm(formula = Salary ~ ., data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -715.51 -187.40 -32.85 148.29 1686.38 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 285.95478 126.06479 2.268 0.0248 * ## AtBat -1.26497 0.94674 -1.336 0.1837 ## Hits 2.02174 3.61275 0.560 0.5766 ## HmRun -0.01383 8.03787 -0.002 0.9986 ## Runs 2.79786 4.23051 0.661 0.5095 ## RBI 0.47768 3.56888 0.134 0.8937 ## Walks 3.44099 2.57671 1.335 0.1839 ## Years -8.76533 17.25334 -0.508 0.6122 ## CAtBat -0.26610 0.20435 -1.302 0.1950 ## CHits 1.31361 1.09982 1.194 0.2343 ## CHmRun 1.35851 2.30018 0.591 0.5557 ## CRuns 0.04142 1.02393 0.040 0.9678 ## CRBI -0.06982 1.08722 -0.064 0.9489 ## CWalks -0.33312 0.45479 -0.732 0.4651 ## LeagueN 132.36961 113.39037 1.167 0.2450 ## DivisionW -119.16837 56.96453 -2.092 0.0382 * ## PutOuts 0.19795 0.10911 1.814 0.0718 . ## Assists 0.64902 0.29986 2.164 0.0321 * ## Errors -6.97871 5.97011 -1.169 0.2444 ## NewLeagueN -54.96821 111.81338 -0.492 0.6238 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 335.6 on 140 degrees of freedom ## Multiple R-squared: 0.4428, Adjusted R-squared: 0.3672 ## F-statistic: 5.856 on 19 and 140 DF, p-value: 1.346e-10 The second way is to rely on the glmnet internal training process, cv.glmnet, which is the main function to do cross-validation along with various supporting methods such as plotting and prediction. A part of the following scripts follows the same algorithm as the one in the book (Introduction to Statistical Learning - ISLR p.254). This approach uses a specific grid on \\(\\lambda\\). We also run the same grid search 100 times to see the associated uncertainty. # With a defined grid on lambda bestlam &lt;- c() mse &lt;- c() grid = 10 ^ seq(10, -2, length = 100) for(i in 1:100){ set.seed(i) train &lt;- sample(1:nrow(X), nrow(X) * 0.5) # 50% split test &lt;- c(-train) ytest &lt;- y[test] #finding lambda cv.out &lt;- cv.glmnet(X[train,], y[train], alpha = 0) bestlam[i] &lt;- cv.out$lambda.min #Predicting with that lambda ridge.mod &lt;- glmnet(X[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12) yhat &lt;- predict(ridge.mod, s = bestlam[i], newx = X[test,]) mse[i] &lt;- mean((yhat - ytest)^2) } mean(bestlam) ## [1] 290.227 mean(mse) ## [1] 127472.6 plot(bestlam, col = &quot;blue&quot;) plot(mse, col = &quot;pink&quot;) Now the same application without a specific grid: bestlam &lt;- c() mse &lt;- c() # Without a pre-defined grid on lambda for(i in 1:100){ set.seed(i) train &lt;- sample(1:nrow(X), nrow(X) * 0.5) # arbitrary split test &lt;- c(-train) ytest &lt;- y[test] cv.out &lt;- cv.glmnet(X[train,], y[train], alpha = 0) yhat &lt;- predict(cv.out, s = &quot;lambda.min&quot;, newx = X[test,]) mse[i] &lt;- mean((yhat - ytest) ^ 2) } mean(mse) ## [1] 127481.6 plot(mse, col = &quot;pink&quot;) Ridge regression adds a penalty term that is the sum of the squares of the coefficients of the features in the model. This results in a penalty that is continuous and differentiable, which makes Ridge regression easy to optimize using gradient descent. Ridge regression can be useful when we have a large number of features but we still want to keep all of the features in the model. Ridge regression works best in situations where the least squares estimates have high variance. On the other hand, Lasso (Least Absolute Shrinkage and Selection Operator) adds a penalty term that is the sum of the absolute values of the coefficients in the model. This results in a penalty that is non-differentiable, which makes it more difficult to optimize using gradient descent. However, Lasso has the advantage of being able to set the coefficients of some features to exactly zero, effectively eliminating those features from the model. This can be useful when we have a large number of features, and we want to select a subset of the most important features to include in the model. 15.2 Lasso The penalty in ridge regression, \\(\\lambda \\sum_{j} \\beta_{j}^{2}\\), will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero. This may present a problem in model interpretation when the number of variables is quite large. One of the key advantages of Lasso is that it can set the coefficients of some features to exactly zero, effectively eliminating those features from the model. By eliminating unnecessary or redundant features from the model, Lasso can help to improve the interpretability and simplicity of the model. This can be particularly useful when you have a large number of features and you want to identify the most important ones for predicting the target variable. The lasso, a relatively recent alternative to ridge regression, minimizes the following quantity: \\[\\begin{equation} \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|=\\operatorname{RSS}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right| \\tag{15.1} \\end{equation}\\] The lasso also shrinks the coefficient estimates towards zero. However, the \\(\\ell_{1}\\) penalty, the second term of equation 18.1, has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large. Hence, the lasso performs variable selection. As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression. In general, one might expect lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients and the remaining predictors have no significant effect on the outcome. This property is known as “sparsity”, because it results in a model with a relatively small number of non-zero coefficients. In some cases, Lasso can find a true sparsity pattern in the data by identifying a small subset of the most important features that are sufficient to accurately predict the target variable. Now, we apply lasso to the same data, Hitters. Again, we will follow a similar way to compare ridge and lasso as in Introduction to Statistical Learning (ISLR). library(glmnet) library(ISLR) remove(list = ls()) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] X &lt;- model.matrix(Salary ~ ., df)[,-1] y &lt;- df$Salary # Without a specific grid on lambda set.seed(1) train &lt;- sample(1:nrow(X), nrow(X) * 0.5) test &lt;- c(-train) ytest &lt;- y[test] # Ridge set.seed(1) ridge.out &lt;- cv.glmnet(X[train,], y[train], alpha = 0) yhatR &lt;- predict(ridge.out, s = &quot;lambda.min&quot;, newx = X[test,]) mse_r &lt;- mean((yhatR - ytest)^2) # Lasso set.seed(1) lasso.out &lt;- cv.glmnet(X[train,], y[train], alpha = 1) yhatL &lt;- predict(lasso.out, s = &quot;lambda.min&quot;, newx = X[test,]) mse_l &lt;- mean((yhatL - ytest) ^ 2) mse_r ## [1] 139863.2 mse_l ## [1] 143668.8 Now, we will define our own grid search: # With a specific grid on lambda + lm() grid = 10 ^ seq(10, -2, length = 100) set.seed(1) train &lt;- sample(1:nrow(X), nrow(X)*0.5) test &lt;- c(-train) ytest &lt;- y[test] #Ridge ridge.mod &lt;- glmnet(X[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12) set.seed(1) cv.outR &lt;- cv.glmnet(X[train,], y[train], alpha = 0) bestlamR &lt;- cv.outR$lambda.min yhatR &lt;- predict(ridge.mod, s = bestlamR, newx = X[test,]) mse_R &lt;- mean((yhatR - ytest) ^ 2) # Lasso lasso.mod &lt;- glmnet(X[train,], y[train], alpha = 1, lambda = grid, thresh = 1e-12) set.seed(1) cv.outL &lt;- cv.glmnet(X[train,], y[train], alpha = 1) bestlamL &lt;- cv.outL$lambda.min yhatL &lt;- predict(lasso.mod, s = bestlamL, newx = X[test,]) mse_L &lt;- mean((yhatL - ytest) ^ 2) mse_R ## [1] 139856.6 mse_L ## [1] 143572.1 Now, we apply our own algorithm: grid = 10 ^ seq(10, -2, length = 100) MSPE &lt;- c() MMSPE &lt;- c() for(i in 1:length(grid)){ for(j in 1:100){ set.seed(j) ind &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[ind, ] xtrain &lt;- model.matrix(Salary ~ ., train)[,-1] ytrain &lt;- df[ind, 19] test &lt;- df[-ind, ] xtest &lt;- model.matrix(Salary~., test)[,-1] ytest &lt;- df[-ind, 19] model &lt;- glmnet(xtrain, ytrain, alpha = 1, lambda = grid[i], thresh = 1e-12) yhat &lt;- predict(model, s = grid[i], newx = xtest) MSPE[j] &lt;- mean((yhat - ytest) ^ 2) } MMSPE[i] &lt;- mean(MSPE) } min(MMSPE) ## [1] 119855.1 grid[which.min(MMSPE)] ## [1] 2.656088 plot(log(grid), MMSPE, type=&quot;o&quot;, col = &quot;red&quot;, lwd = 3) What are the coefficients? coef_lasso &lt;- coef(model, s=grid[which.min(MMSPE)], nonzero = T) coef_lasso ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 285.73172897 ## AtBat -1.26603002 ## Hits 2.04074005 ## HmRun 0.02355750 ## Runs 2.77938363 ## RBI 0.45867292 ## Walks 3.44852914 ## Years -8.78839869 ## CAtBat -0.26343169 ## CHits 1.29690477 ## CHmRun 1.32913790 ## CRuns 0.05007662 ## CRBI -0.05515544 ## CWalks -0.33624685 ## LeagueN 132.06438132 ## DivisionW -119.26618910 ## PutOuts 0.19772257 ## Assists 0.64809649 ## Errors -6.97381705 ## NewLeagueN -54.62728800 We can also try a classification problem with LPM or Logistic regression when the response is categorical. If there are two possible outcomes, we use the binomial distribution, else we use the multinomial. 15.3 Adaptive Lasso Adaptive lasso is a method for regularization and variable selection in regression analysis that was introduced by Zou (2006) in The Adaptive Lasso and Its Oracle Properties. In this paper, the author proposed the use of a weighted \\(\\ell_{1}\\) penalty in the objective function, with the weights chosen to adapt to the correlation structure of the data. He showed that this method can result in a more stable model with fewer coefficients being exactly zero, compared to the standard lasso method which uses a simple \\(\\ell_{1}\\) penalty. Since its introduction, adaptive lasso has been widely used in a variety of applications in statistical modeling and machine learning. It has been applied to problems such as feature selections in genomic data, high-dimensional regressions, and model selections with generalized linear models. Adaptive lasso is useful in situations where the predictors are correlated and there is a need to select a small subset of important variables to include in the model. It has been shown that adaptive lasso is an oracle efficient estimator (consistency in variable selection and asymptotic normality in coefficient estimation), while the plain lasso is not. Consider the linear regression model: \\[ y_i=x_i^{\\prime} \\beta+\\epsilon_i, ~~~~i=1, \\ldots, n ~~~~\\text{and} ~~~~\\beta \\text { is } (p \\times 1) \\] The adaptive Lasso estimates \\(\\beta\\) by minimizing \\[ L(\\beta)=\\sum_{i=1}^n\\left(y_i-x_i^{\\prime} \\beta\\right)^2+\\lambda_n \\sum_{j=1}^p \\frac{1}{w_j}\\left|\\beta_j\\right| \\] where, typically \\(w_j=(\\left|\\hat{\\beta}_{O L S_j}\\right|)^{\\gamma}\\) or \\(w_j=(\\left|\\hat{\\beta}_{Ridge_j}\\right|)^{\\gamma}\\), where \\(\\gamma\\) is a positive constant for adjustment of the Adaptive Weights vector, and suggested to be the possible values of 0.5, 1, and 2. The weights in adaptive lasso (AL) provides a prior “intelligence” about variables such that,while the plain Lasso penalizes all parameters equally, the adaptive Lasso is likely to penalize non-zero coefficients less than the zero ones. This is because the weights can be obtained from the consistent least squares estimator. If \\(\\beta_{AL, j}=0\\), then \\(\\hat{\\beta}_{O L S, j}\\) is likely to be close to zero leading to a very small \\(w_j\\). Hence, truly zero coefficients are penalized a lot. Calculating the weights in adaptive lasso requires a two-step procedure Here is an example where we use the ridge weight in adaptive lasso: library(ISLR) library(glmnet) remove(list = ls()) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] X &lt;- model.matrix(Salary~., df)[,-1] y &lt;- df$Salary # Ridge weights with gamma = 1 g = 1 set.seed(1) modelr &lt;- cv.glmnet(X, y, alpha = 0) coefr &lt;- as.matrix(coef(modelr, s = modelr$lambda.min)) w.r &lt;- 1/(abs(coefr[-1,]))^g ## Adaptive Lasso set.seed(1) alasso &lt;- cv.glmnet(X, y, alpha=1, penalty.factor = w.r) ## Lasso set.seed(1) lasso &lt;- cv.glmnet(X, y, alpha=1) # Sparsity cbind(LASSO = coef(lasso, s=&quot;lambda.1se&quot;), ALASSO = coef(alasso, s=&quot;lambda.1se&quot;)) ## 20 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 s1 ## (Intercept) 127.95694754 -7.109481 ## AtBat . . ## Hits 1.42342566 2.054867 ## HmRun . . ## Runs . . ## RBI . . ## Walks 1.58214111 3.573120 ## Years . 31.573334 ## CAtBat . . ## CHits . . ## CHmRun . . ## CRuns 0.16027975 . ## CRBI 0.33667715 . ## CWalks . . ## LeagueN . 29.811080 ## DivisionW -8.06171262 -138.088953 ## PutOuts 0.08393604 . ## Assists . . ## Errors . . ## NewLeagueN . . We can see the difference between lasso and adaptive lasso in this example: PutOuts, CRuns, and CRBI picked by lasso are not selected by adaptive lasso. There are only three common features in both methods: Hits, Walks, and DivisionW. To understand which model is better in terms of catching the true sparsity, we will have a simulation to illustrate some of the properties of the Lasso and the adaptive Lasso. 15.4 Sparsity This is a simulation to illustrate some of the properties of Lasso-type estimations. There are two objectives in using these penalized regressions: model selection (identifying “correct” sparsity) and prediction accuracy. These two objectives require different optimization approaches and usually are not compatible. In model selection, the objective is to shrink the dimension of the model to the “true” sparsity. This is usually evaluated by checking whether the Oracle properties are satisfied. These asymptotic properties look at (1) if the model identified by the penalized regression converges to the “true” sparsity, (2) if the coefficients are consistent. The literature suggests that Lasso is not an oracle estimator. Adaptive Lasso was developed (Zou 2006) to fill this gap. Let’s specify a data generating process with a linear regression model: \\[ y_i=x_i^{\\prime} \\beta+u_i, ~~~~~i=1, \\ldots, n \\] where \\(\\beta\\) is \\(p \\times 1\\). First, we consider the case where \\(p&lt;n\\) then move to the case where \\(p \\geq n\\). We define \\(\\beta=(1,1,0,0)^{\\prime}\\) and \\(n=100\\). #This function generates the data dgp &lt;- function(N, Beta) { p = length(Beta) X &lt;- matrix(rnorm(N * p), ncol = p) u &lt;- matrix(rnorm(N), ncol = 1) dgm &lt;- X %*% Beta y &lt;- X %*% Beta + u return &lt;- list(y, X) } N = 100 Beta = c(1, 1, 0, 0) set.seed(148) Output &lt;- dgp(N, Beta) y &lt;- Output[[1]] X &lt;- Output[[2]] First, we apply lasso library(glmnet) set.seed(432) lasso &lt;- glmnet(x = X, y = y, family = &quot;gaussian&quot;) beta_hat &lt;- lasso$beta S_matrix &lt;- cbind(t(beta_hat), &quot;lambda&quot; = lasso$lambda) S_matrix[c(1:8, 25:30, 55:60), ] # selected rows ## 20 x 5 sparse Matrix of class &quot;dgCMatrix&quot; ## V1 V2 V3 V4 lambda ## s0 . . . . 1.083220708 ## s1 0.09439841 0.0283513 . . 0.986990366 ## s2 0.17344129 0.1097255 . . 0.899308862 ## s3 0.24546220 0.1838706 . . 0.819416741 ## s4 0.31108496 0.2514289 . . 0.746622016 ## s5 0.37087798 0.3129855 . . 0.680294174 ## s6 0.42535915 0.3690736 . . 0.619858715 ## s7 0.47500037 0.4201789 . . 0.564792175 ## s24 0.87944075 0.8365481 . . 0.116150206 ## s25 0.88874261 0.8461243 . . 0.105831742 ## s26 0.89685610 0.8542117 -0.00686322 . 0.096429941 ## s27 0.90418482 0.8614679 -0.01432988 . 0.087863371 ## s28 0.91086250 0.8680794 -0.02113323 . 0.080057832 ## s29 0.91694695 0.8741036 -0.02733218 . 0.072945714 ## s54 0.98352129 0.9289175 -0.09282009 0.05192379 0.007126869 ## s55 0.98423271 0.9294382 -0.09350608 0.05278151 0.006493738 ## s56 0.98488092 0.9299126 -0.09413113 0.05356303 0.005916852 ## s57 0.98547155 0.9303449 -0.09470066 0.05427512 0.005391215 ## s58 0.98600972 0.9307388 -0.09521958 0.05492395 0.004912274 ## s59 0.98650007 0.9310977 -0.09569241 0.05551515 0.004475881 Which set of beta_hat should we select? To answer this question we need to find the lambda. We need \\(\\lambda_n \\rightarrow \\infty\\) in order to shrink the truly zero coefficients to zero. This requires \\(\\lambda_n\\) to be sufficiently large. This would introduce asymptotic bias to the non-zero coefficients. In practice, choosing \\(\\lambda_n\\) by \\(\\mathrm{BIC}\\) (Bayesian Information Criterion) results in a consistent model selection in the fixed \\(p\\) setting. That is, let \\(\\mathcal{A}=\\left\\{j: \\beta_{0, j} \\neq 0\\right\\}\\), active set or relevant variables, \\[ P\\left(\\hat{\\mathcal{A}}_{\\lambda_{BIC}}=\\mathcal{A}\\right) \\rightarrow 1 \\] Thus, let \\(S S E_\\lambda\\) be the sum of squared error terms for a given value of \\(\\lambda\\) and \\(n z_\\lambda\\) be the number of non-zero coefficients. Then, it can be shown that \\[ B I C_\\lambda=\\log \\left(S S E_\\lambda\\right)+\\frac{\\log (n)}{n} n z_\\lambda \\] # Predict yhat for each of 61 lambda (s) y_hat = predict(lasso, newx = X) dim(y_hat) ## [1] 100 60 # SSE for each lambda (s) SSE &lt;- c() for (i in 1:ncol(y_hat)) { SSE_each &lt;- sum((y_hat[, i] - y[, 1]) ^ (2)) SSE &lt;- c(SSE, SSE_each) } # BIC nz &lt;- colSums(beta_hat != 0) # Number of non-zero coefficients for each lambda BIC &lt;- log(SSE) + (log(N) / N) * nz # BIC BIC ## s0 s1 s2 s3 s4 s5 s6 s7 ## 5.598919 5.595359 5.468287 5.348947 5.237755 5.135013 5.040883 4.955387 ## s8 s9 s10 s11 s12 s13 s14 s15 ## 4.878394 4.809638 4.748729 4.695181 4.648437 4.607898 4.572946 4.542971 ## s16 s17 s18 s19 s20 s21 s22 s23 ## 4.517383 4.495631 4.477205 4.461646 4.448541 4.437530 4.428295 4.420563 ## s24 s25 s26 s27 s28 s29 s30 s31 ## 4.414098 4.408698 4.448661 4.443309 4.438844 4.435121 4.432021 4.429439 ## s32 s33 s34 s35 s36 s37 s38 s39 ## 4.427290 4.425503 4.424017 4.468004 4.466218 4.464732 4.463498 4.462471 ## s40 s41 s42 s43 s44 s45 s46 s47 ## 4.461618 4.460910 4.460321 4.459832 4.459426 4.459088 4.458808 4.458575 ## s48 s49 s50 s51 s52 s53 s54 s55 ## 4.458382 4.458222 4.458088 4.457978 4.457886 4.457810 4.457746 4.457694 ## s56 s57 s58 s59 ## 4.457650 4.457614 4.457584 4.457559 And, the selected model that has the minimum BIC beta_lasso &lt;- beta_hat[, which(BIC == min(BIC))] beta_lasso ## V1 V2 V3 V4 ## 0.8887426 0.8461243 0.0000000 0.0000000 This is the beta_hat that identifies the true sparsity. And, the second Oracle property, the \\(\\ell_2\\) error: l_2 &lt;- sqrt(sum((beta_lasso - Beta) ^ 2)) l_2 ## [1] 0.189884 Here we will create a simulation that will report two Oracle Properties for Lasso and Adaptive Lasso: True sparsity, \\(\\ell_2\\) error. Lasso We first have a function, msc(), that executes a simulation with all the steps shown before: mcs &lt;- function(mc, N, Beta) { mcmat &lt;- matrix(0, nrow = mc, ncol = 3) beta_lasso_mat &lt;- matrix(0, nr = mc, nc = length(Beta)) for (i in 1:mc) { set.seed(i) data &lt;- dgp(N, Beta) y &lt;- data[[1]] X &lt;- data[[2]] set.seed(i) lasso &lt;- glmnet(x = X, y = y, family = &quot;gaussian&quot;) beta_hat &lt;- lasso$beta # beta_hat is a matrix y_hat = predict(lasso, newx = X) SSE &lt;- c() for (j in 1:ncol(y_hat)) { SSE_each &lt;- sum((y_hat[, j] - y[, 1]) ^ (2)) SSE &lt;- c(SSE, SSE_each) } nz &lt;- colSums(beta_hat != 0) BIC &lt;- log(SSE) + (log(N) / N) * nz beta_lasso &lt;- beta_hat[, which(BIC == min(BIC))] nonz_beta = length(Beta[Beta == 0]) nonz_beta_hat = length(beta_lasso[beta_lasso == 0]) mcmat[i, 1] &lt;- sqrt(sum((beta_lasso - Beta) ^ 2)) mcmat[i, 2] &lt;- ifelse(nonz_beta != nonz_beta_hat, 0, 1) mcmat[i, 3] &lt;- sum(beta_lasso != 0) beta_lasso_mat[i, ] &lt;- beta_lasso } return(list(mcmat, beta_lasso_mat)) } We are ready for simulation: mc &lt;- 500 N &lt;- 1000 Beta &lt;- matrix(c(1, 1, 0, 0), nc = 1) output &lt;- mcs(mc, N, Beta) #see the function MC_betas = output[[2]] MC_performance = output[[1]] sum(MC_performance[, 2]) #how many times lasso finds true sparsity ## [1] 400 This is the first property: lasso identifies the true sparsity \\(400/500 = 80\\%\\) of cases. And the second property, \\(\\ell_2\\) error, in the simulation is (in total): sum(MC_performance[, 1]) ## [1] 29.41841 Adaptive Lasso This time we let our adaptive lasso use lasso coefficients as penalty weights in glmnet(). Let’s have the same function with Adaptive Lasso for the simulation: # Adaptive LASSO mcsA &lt;- function(mc, N, Beta) { mcmat &lt;- matrix(0, nr = mc, nc = 3) beta_lasso_mat &lt;- matrix(0, nr = mc, nc = length(Beta)) for (i in 1:mc) { data &lt;- dgp(N, Beta) y &lt;- data[[1]] X &lt;- data[[2]] lasso &lt;- glmnet(x = X, y = y, family = &quot;gaussian&quot;) beta_hat &lt;- lasso$beta y_hat = predict(lasso, newx = X) SSE &lt;- c() for (j in 1:ncol(y_hat)) { SSE_each &lt;- sum((y_hat[, j] - y[, 1]) ^ (2)) SSE &lt;- c(SSE, SSE_each) } nz &lt;- colSums(beta_hat != 0) BIC &lt;- log(SSE) + (log(N) / N) * nz beta_lasso &lt;- beta_hat[, which(BIC == min(BIC))] weights = abs(beta_lasso) ^ (-1) weights[beta_lasso == 0] = 10 ^ 10 # to handle inf&#39;s #Now Adaptive Lasso lasso &lt;- glmnet( x = X, y = y, family = &quot;gaussian&quot;, penalty.factor = weights ) beta_hat &lt;- lasso$beta y_hat = predict(lasso, newx = X) SSE &lt;- c() for (j in 1:ncol(y_hat)) { SSE_each &lt;- sum((y_hat[, j] - y[, 1]) ^ (2)) SSE &lt;- c(SSE, SSE_each) } nz &lt;- colSums(beta_hat != 0) BIC &lt;- log(SSE) + (log(N) / N) * nz beta_lasso &lt;- beta_hat[, which(BIC == min(BIC))] nonz_beta = length(Beta[Beta == 0]) nonz_beta_hat = length(beta_lasso[beta_lasso == 0]) mcmat[i, 1] &lt;- sqrt(sum((beta_lasso - Beta) ^ 2)) mcmat[i, 2] &lt;- ifelse(nonz_beta != nonz_beta_hat, 0, 1) mcmat[i, 3] &lt;- sum(beta_lasso != 0) beta_lasso_mat[i, ] &lt;- beta_lasso } return(list(mcmat, beta_lasso_mat)) } Here are the results for adaptive lasso: mc &lt;- 500 N &lt;- 1000 beta &lt;- matrix(c(1, 1, 0, 0), nc = 1) output &lt;- mcsA(mc, N, beta) #see the function MC_betas = output[[2]] MC_performance = output[[1]] sum(MC_performance[, 2]) ## [1] 492 And, sum(MC_performance[,1]) ## [1] 20.21311 The simulation results clearly show that Adaptive Lasso is an Oracle estimator and a better choice for sparsity applications. We saw here a basic application of adaptive lasso, which has several different variations in practice, such as Thresholded Lasso and Rigorous Lasso. Model selections with lasso has been an active research area. One of the well-known applications is the double-selection lasso linear regression method that can be used for variable selections. Moreover, lasso type applications are also used in time-series forecasting and graphical network analysis for dimension reductions. "],["regression-trees.html", "Chapter 16 Regression Trees 16.1 CART - Classification Tree 16.2 rpart - Recursive Partitioning 16.3 Pruning 16.4 Classification with Titanic 16.5 Regression Tree", " Chapter 16 Regression Trees Tree-based predictive models are one of the best and most used supervised learning methods. Unlike linear models, they handle non-linear relationships quite well. They can be applied for both classification or regression problems, which aspires its name: Classification And Regression Trees. The foundation of their models is based on a decision tree, which is a flowchart where each internal node represents a decision point (goes left or right), each branch represents those decisions, and each leaf at the end of a branch represents the outcome of the decision. Here is a simple decision tree about a gamble: How can we use a decision tree in a learning algorithm? Let’s start with a classification problem: 16.1 CART - Classification Tree Let’s start with a very simple example: suppose we have the following data: y &lt;- c(1, 1, 1, 0, 0, 0, 1, 1, 0, 1) x1 &lt;- c(0.09, 0.11, 0.17, 0.23, 0.33, 0.5, 0.54, 0.62, 0.83, 0.88) x2 &lt;- c(0.5, 0.82, 0.2, 0.09, 0.58, 0.5, 0.93, 0.8, 0.3, 0.83) data &lt;- data.frame(y = y, x1 = x1, x2 = x2) plot( data$x1, data$x2, col = (data$y + 1), lwd = 4, ylab = &quot;x2&quot;, xlab = &quot;x1&quot; ) What’s the best rule on \\(x_2\\) to classify black (\\(0\\)) and red balls (\\(1\\))? Find a cutoff point on \\(x_2\\) such that the maximum number of observations is correctly classified To minimize the misclassification, we find that the cutoff point should be between \\(\\{0.6: 0.79\\}\\). Hence the rule is \\(x_2 &lt; k\\), where \\(k \\in\\{0.6: 0.79\\}.\\) plot(data$x1, data$x2, col = (data$y + 1), lwd = 4) abline(h = 0.62, col = &quot;blue&quot;, lty = 5, lwd = 2) From this simple rule, we have two misclassified balls. We can add a new rule in the area below the horizontal blue line: plot(data$x1, data$x2, col = (data$y + 1), lwd = 4) abline( h = 0.62, col = c(&quot;blue&quot;, &quot;darkgreen&quot;), lty = 5, lwd = 2 ) segments( x0 = 0.2, y0 = 0, x1 = 0.2, y1 = 0.62, col = &quot;darkgreen&quot;, lty = 5, lwd = 2 ) Using these two rules, we correctly classified all balls (\\(Y\\)). We did the classification manually by looking at the graph. How can we do it by an algorithm? First, we need to create an index that is going to measure the impurity in each node. Instead of counting misclassified \\(y\\)’s, the impurity index will give us a continuous metric. The first index is the Gini Index, which can be defined at some node \\(\\mathcal{N}\\): \\[ G(\\mathcal{N}) = \\sum_{k=1}^{K} p_{k}\\left(1-p_{k}\\right) = 1-\\sum_{k=1}^{K} p_{k}^{2} \\] where, with \\(p_k\\) is the fraction of items labeled with class \\(k\\) in the node. If we have a binary outcome \\((k=2)\\), when \\(p_k \\in \\{0, 1\\}\\), \\(G(\\mathcal{N})=0\\) and when \\(p_k = 0.5,\\) \\(G(\\mathcal{N})=0.25\\). The former implies the minimal impurity (diversity), the latter shows the maximal impurity. A small \\(G\\) means that a node contains observations predominantly from a single class. As in the previous example, when we have a binary outcome with two classes, \\(y_i \\in \\{0, 1\\}\\), this index can be written as: \\[ G(\\mathcal{N})=\\sum_{k=1}^{2} p_{k}\\left(1-p_{k}\\right)=2p\\left(1-p\\right) \\] If we split the node into two leaves, \\(\\mathcal{N}_L\\) (left) and \\(\\mathcal{N}_R\\) (right), the \\(G\\) will be: \\[ G\\left(\\mathcal{N}_{L}, \\mathcal{N}_{R}\\right)=p_{L} G\\left(\\mathcal{N}_{L}\\right)+p_{R} G\\left(\\mathcal{N}_{R}\\right) \\] Where \\(p_L\\), \\(p_R\\) are the proportion of observations in \\(\\mathcal{N}_L\\) and \\(\\mathcal{N}_R\\). Remember, we are trying to find the rule that gives us the best cutoff point (split). Now we can write the rule: \\[ \\Delta=G(\\mathcal{N})-G\\left(\\mathcal{N}_{L}, \\mathcal{N}_{R}\\right)&gt;\\epsilon \\] When the impurity is reduced substantially, the difference will be some positive number (\\(\\epsilon\\)). Hence, we find the cutoff point on a single variable that minimizes the impurity (maximizes \\(\\Delta\\)). Let’s use a dataset5, which reports about heart attacks and fatality (our binary variable). library(readr) #Data #myocarde = read.table(&quot;http://freakonometrics.free.fr/myocarde.csv&quot;,head=TRUE, sep=&quot;;&quot;) myocarde &lt;- read_delim( &quot;myocarde.csv&quot;, delim = &quot;;&quot; , escape_double = FALSE, trim_ws = TRUE, show_col_types = FALSE ) myocarde &lt;- data.frame(myocarde) str(myocarde) ## &#39;data.frame&#39;: 71 obs. of 8 variables: ## $ FRCAR: num 90 90 120 82 80 80 94 80 78 100 ... ## $ INCAR: num 1.71 1.68 1.4 1.79 1.58 1.13 2.04 1.19 2.16 2.28 ... ## $ INSYS: num 19 18.7 11.7 21.8 19.7 14.1 21.7 14.9 27.7 22.8 ... ## $ PRDIA: num 16 24 23 14 21 18 23 16 15 16 ... ## $ PAPUL: num 19.5 31 29 17.5 28 23.5 27 21 20.5 23 ... ## $ PVENT: num 16 14 8 10 18.5 9 10 16.5 11.5 4 ... ## $ REPUL: num 912 1476 1657 782 1418 ... ## $ PRONO: chr &quot;SURVIE&quot; &quot;DECES&quot; &quot;DECES&quot; &quot;SURVIE&quot; ... The variable definitions are as follows: FRCAR (heart rate), INCAR (heart index), INSYS (stroke index), PRDIA (diastolic pressure), PAPUL (pulmonary arterial pressure), PVENT (ventricular pressure), REPUL (lung resistance), PRONO, which is our outcome variable (death “DECES”, survival “SURVIE”). We are ready to calculate \\(G\\)-index: # Recode PRONO y &lt;- ifelse(myocarde$PRONO == &quot;SURVIE&quot;, 1, 0) # Find G(N) without L and R G &lt;- 2 * mean(y) * (1 - mean(y)) G ## [1] 0.4832375 This is the level of “impurity” in our data. Now, we need to pick one variable and find a cutoff point in the variable. Then, we will calculate the same \\(G\\) for both left and right of that point. The goal is the find the best cutoff point that reduces the “impurity”. Let’s pick FRCAR arbitrarily for now. Later we will see how to find the variable that the first split (left and right) should start from so that the reduction in “impurity” will be maximized. # Let&#39;s pick FRCAR to start x_1 &lt;- myocarde$FRCAR # Put x and y in table tab = table(y, x_1) tab ## x_1 ## y 60 61 65 67 70 75 78 79 80 81 82 84 85 86 87 90 92 94 95 96 99 100 102 103 ## 0 1 0 1 0 1 1 0 1 4 0 0 0 1 0 2 2 2 1 3 0 0 1 1 1 ## 1 0 2 1 1 0 3 1 0 7 1 3 1 0 4 0 4 2 1 1 1 1 3 0 0 ## x_1 ## y 105 108 110 116 118 120 122 125 ## 0 1 0 2 1 1 1 0 0 ## 1 0 1 1 0 1 0 1 1 We are ready to calculate \\[ G\\left(\\mathcal{N}_{L}, \\mathcal{N}_{R}\\right)=p_{L} G\\left(\\mathcal{N}_{L}\\right)+p_{R} G\\left(\\mathcal{N}_{R}\\right), \\] when \\(x = 60\\), for example. # x = 60, for example to see if (GL + GR &gt; GN) GL &lt;- 2 * mean(y[x_1 &lt;= 60]) * (1 - mean(y[x_1 &lt;= 60])) GR &lt;- 2 * mean(y[x_1 &gt; 60]) * (1 - mean(y[x_1 &gt; 60])) pL &lt;- length(x_1[x_1 &lt;= 60]) / length(x_1) #Proportion of obs. on Left pR &lt;- length(x_1[x_1 &gt; 60]) / length(x_1) #Proportion of obs. on Right How much did we improve \\(G\\)? delta = G - pL * GL - pR * GR delta ## [1] 0.009998016 We need to go trough each number on \\(x_1\\) and identify the point that maximizes delta. A function can do that: GI &lt;- function(x) { GL &lt;- 2 * mean(y[x_1 &lt;= x]) * (1 - mean(y[x_1 &lt;= x])) GR &lt;- 2 * mean(y[x_1 &gt; x]) * (1 - mean(y[x_1 &gt; x])) pL &lt;- length(x_1[x_1 &lt;= x]) / length(x_1) pR &lt;- length(x_1[x_1 &gt; x]) / length(x_1) del = G - pL * GL - pR * GR return(del) } # Let&#39;s test it GI(60) ## [1] 0.009998016 It works! Now, we can use this function in a loop that goes over each unique \\(x\\) and calculate their delta. xm &lt;- sort(unique(x_1)) delta &lt;- c() # Since we don&#39;t split at the last number for (i in 1:length(xm) - 1) { delta[i] &lt;- GI(xm[i]) } delta ## [1] 9.998016e-03 4.978782e-04 1.082036e-05 1.041714e-03 8.855953e-05 ## [6] 7.363859e-04 2.295303e-03 2.546756e-04 1.142757e-03 2.551599e-03 ## [11] 9.862318e-03 1.329134e-02 8.257492e-03 2.402430e-02 1.160767e-02 ## [16] 1.634414e-02 1.352527e-02 1.229951e-02 3.109723e-03 5.692941e-03 ## [21] 9.212475e-03 1.919591e-02 1.244092e-02 6.882353e-03 2.747959e-03 ## [26] 6.282533e-03 1.547312e-03 1.082036e-05 4.978782e-04 9.671419e-03 ## [31] 4.766628e-03 Let’s see the cutoff point that gives us the highest delta. max(delta) ## [1] 0.0240243 xm[which.max(delta)] ## [1] 86 Although this is a simple and an imperfect algorithm, it shows us how we can build a learning system based on a decision tree. On one variable, FRCAR, and with only one split we improved the Gini index by 2.5%. Obviously this is not good enough. Can we do more splitting? Since we now have two nodes (Left and Right at \\(x_1 = 86\\)), we can consider each of them as one node and apply the same formula to both left and right nodes. As you can guess, this may give us a zero-\\(G\\), as we end up with splitting at every \\(x_{1i}\\). We can prevent this overfitting by pruning, which we will see later. Wouldn’t it be a good idea if we check all seven variables and start with the one that has a significant improvements in delta when we split? We can do it easily with a loop: # Adjust our function a little: add &quot;tr&quot;, the cutoff GI &lt;- function(x, tr) { G &lt;- 2 * mean(y) * (1 - mean(y)) GL &lt;- 2 * mean(y[x &lt;= tr]) * (1 - mean(y[x &lt;= tr])) GR &lt;- 2 * mean(y[x &gt; tr]) * (1 - mean(y[x &gt; tr])) pL &lt;- length(x[x &lt;= tr]) / length(x) pR &lt;- length(x[x &gt; tr]) / length(x) del = G - pL * GL - pR * GR return(del) } # The loop that applies GI on every x d &lt;- myocarde[, 1:7] split &lt;- c() maxdelta &lt;- c() for (j in 1:ncol(d)) { xm &lt;- sort(unique(d[, j])) delta &lt;- c() for (i in 1:length(xm) - 1) { delta[i] &lt;- GI(d[, j], xm[i]) } maxdelta[j] &lt;- max(delta) split[j] &lt;- xm[which.max(delta)] } data.frame(variables = colnames(d), delta = maxdelta) ## variables delta ## 1 FRCAR 0.02402430 ## 2 INCAR 0.26219024 ## 3 INSYS 0.28328013 ## 4 PRDIA 0.13184706 ## 5 PAPUL 0.09890283 ## 6 PVENT 0.04612125 ## 7 REPUL 0.26790701 This is good. We can identify that INSYS should be our first variable to split, as it has the highest delta. round(split[which.max(maxdelta)], 0) ## [1] 19 We now know where to split on INSYS, which is 19. After splitting INSYS left and right, we move on to the next variable to split, which would be the second best: REBUL. For a better interpretability, we can rank the importance of each variable by their gain in Gini. We can approximately order them by looking at our delta: dm &lt;- matrix(maxdelta, 7, 1) rownames(dm) &lt;- c(names(myocarde[1:7])) dm &lt;- dm[order(dm[, 1]), ] barplot( dm, horiz = TRUE, col = &quot;darkgreen&quot;, xlim = c(0, 0.3), cex.names = 0.5, cex.axis = 0.8, main = &quot;Variable Importance at the 1st Split&quot; ) The package rpart (Recursive PARTitioning) implements all these steps that we experimented above. 16.2 rpart - Recursive Partitioning As in our case, when the response variable is categorical, the resulting tree is called classification tree. The default criterion, which is maximized in each split is the Gini coefficient. The method-argument can be switched according to the type of the response variable. It is class for categorical, anova for numerical, poisson for count data and exp for survival data. If the outcome variable is a factor variable, as in our case, we do not have to specify the method. The tree is built by the following process in rpart: first the single variable is found that best splits the data into two groups. After the data is separated, this process is applied separately to each sub-group. This goes on recursively until the subgroups either reach a minimum size or until no improvement can be made. Details can be found in this vignette (Atkinson_2022?). Here, we apply rpart to our data without any modification to its default arguments: library(rpart) tree = rpart(PRONO ~ ., data = myocarde, method = &quot;class&quot;) # Plot it library(rpart.plot) # You can use plot() but prp() is much better prp( tree, type = 2, extra = 1, split.col = &quot;red&quot;, split.border.col = &quot;blue&quot;, box.col = &quot;pink&quot; ) This shows that the left node (DECES) cannot be significantly improved by a further split on REPUL. But the right node (SURVIE) can be improved. Note that we haven’t trained our model explicitly. There are two ways to control the growth of a tree: We can limit the growth of our tree by using its control parameters and by checking if the split is worth it, which is, as a default, what rpart is doing with 10-fold cross-validation. We can grow the tree without any limitation and then prune it. Since we use the default control parameters with 10-fold CV, our first tree was grown by the first strategy. Before going further, let’s spend some time on the main arguments of rpart(): rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...) The control argument controls how the tree grows. We briefly describe its arguments based on An Introduction to Recursive Partitioning Using the RPART Routines by Atkinson et.al. (Atkinson_2000?): rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, maxdepth = 30, ...) minsplit: The minimum number of observations in a node for which the routine will even try to compute a split. The default is 20. minbucket: The minimum number of observations in a terminal node: This defaults to minsplit/3. cp: The threshold complexity parameter. Default is 0.01. maxcompete: The number of alternative splits in addition to the best that will be printed. maxsurrogate: The maximum number of surrogate variables to retain at each node. usesurrogate: If the value is 0, then a subject (observation) who is missing the primary split variable does not progress further down the tree. xval: The number of cross-validations to be done. Default is 10. maxdepth: The maximum depth of any node of the final tree Remember, rpart does not drop the subject if it has a missing observation on a predictor. When the observation missing on the primary split on that variable, rpart find a surrogate for the variable so that it can carry out the split. We can see the the growth of the tree by looking at its CV table: printcp(tree) ## ## Classification tree: ## rpart(formula = PRONO ~ ., data = myocarde, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] INSYS REPUL ## ## Root node error: 29/71 = 0.40845 ## ## n= 71 ## ## CP nsplit rel error xerror xstd ## 1 0.724138 0 1.00000 1.00000 0.14282 ## 2 0.034483 1 0.27586 0.51724 0.11861 ## 3 0.010000 2 0.24138 0.51724 0.11861 The rel error of each iteration of the tree is the fraction of mislabeled elements in the iteration relative to the fraction of mislabeled elements in the root. Hence it’s 100% (1.00000 in the table) in the root node. In other words, rel error gives the percentage of misclassified labels, when it’s multiplied with the Root node error (0.40845 x 0.24138 = 0.0986). This is the error rate when the fitted model applied to the training sets used by rpart’s CV. sum(predict(tree, type=&quot;class&quot;)!=myocarde$PRONO)/nrow(myocarde) ## [1] 0.09859155 The xerror also provides the same information. But since it is applied to test sets, it shows the cross-validation error. The relative improvement, or gain, due to a split is given by CP (cost complexity pruning), which is 0.724138 in the first split on INSYS. Therefore, the first split on INSYS reduces (improves) this error to 27.5862% (rel error). This relative gain (CP) can be calculated as follows: \\[ \\frac{\\Delta}{G(\\mathcal{N})}=\\frac{G(\\mathcal{N})-G\\left(\\mathcal{N}_{L}, \\mathcal{N}_{R}\\right)}{G(\\mathcal{N})}. \\] If this gain exceeds 1%, which is the default value, rpart() splits a variable. As you can see from the table above, since there is no significant relative gain at the \\(3^{rd}\\) split more than the default parameter 0.01, rpart() decides to stop growing the tree after the \\(2^{nd}\\) split. Note that, we also calculated both the nominator and the denominator in our own algorithm: \\(\\Delta = 0.2832801\\) and \\(G(\\mathcal{N}) = 0.4832375\\). Hence the relative gain was \\(\\frac{\\Delta}{G(\\mathcal{N})}=0.586213\\) in our case. We can replicate the same results if we change our outcome from factor to numeric: myocarde_v2 &lt;- myocarde myocarde_v2$PRONO = ifelse(myocarde$PRONO == &quot;SURVIE&quot;, 1, 0) cart = rpart(PRONO ~ ., data = myocarde_v2) printcp(cart) ## ## Regression tree: ## rpart(formula = PRONO ~ ., data = myocarde_v2) ## ## Variables actually used in tree construction: ## [1] INSYS REPUL ## ## Root node error: 17.155/71 = 0.24162 ## ## n= 71 ## ## CP nsplit rel error xerror xstd ## 1 0.586213 0 1.00000 1.04497 0.047317 ## 2 0.101694 1 0.41379 0.77352 0.158659 ## 3 0.028263 2 0.31209 0.73381 0.159985 ## 4 0.010000 3 0.28383 0.69759 0.152268 As you see, when the outcome is not a factor variable, rpart applies a regression tree method, which minimizes the sum of squares, \\(\\sum_{i=1}^{n}\\left(y_i-f(x_i)\\right)^2\\). However, when \\(y_i\\) is a binary number with two values 0 and 1, the sum of squares becomes \\(np(1-p)\\), which gives the same relative gain as Gini. This is clear as both relative gains (our calculation and the calculation by rpart above) are the same. What’s the variable importance of rpart()? # Variable Importance vi &lt;- tree$variable.importance vi &lt;- vi[order(vi)] barplot( vi / 100, horiz = TRUE, col = &quot;lightgreen&quot;, cex.names = 0.5, cex.axis = 0.8, main = &quot;Variable Importance - rpart()&quot; ) It seems that the order of variables are similar, but magnitudes are slightly different due to the differences in calculating methods. In rpart, the value is calculated as the sum of the decrease in impurity both when the variable appear as a primary split and when it appears as a surrogate. 16.3 Pruning We can now apply the second method to our case by removing the default limits in growing our tree. We can do it by changing the parameters of the rpart fit. Let’s see what happens if we override these parameters. # let&#39;s change the minsplit and minbucket tree2 = rpart( PRONO ~ ., data = myocarde, control = rpart.control( minsplit = 2, minbucket = 1, cp = 0 ), method = &quot;class&quot; ) library(rattle) # You can use plot() but prp() is an alternative fancyRpartPlot(tree2, caption = NULL) This is our fully grown tree with a “perfect” fit, because it identifies every outcome (DECES and SURVIE) correctly at the terminal nodes (%’s give proportion of observations). Obviously, this is not a good idea as it overfits. Let’s summarize what we have seen so far: we can either go with the first strategy and limit the growth of the tree or we can have a fully developed tree then we can prune it. The general idea in pruning is to reduce the tree’s complexity by keeping only the most important splits. When we grow a tree, rpart performs 10-fold cross-validation on the data. We can see the cross-validation result by printcp(). printcp(tree2) ## ## Classification tree: ## rpart(formula = PRONO ~ ., data = myocarde, method = &quot;class&quot;, ## control = rpart.control(minsplit = 2, minbucket = 1, cp = 0)) ## ## Variables actually used in tree construction: ## [1] FRCAR INCAR INSYS PVENT REPUL ## ## Root node error: 29/71 = 0.40845 ## ## n= 71 ## ## CP nsplit rel error xerror xstd ## 1 0.724138 0 1.000000 1.00000 0.14282 ## 2 0.103448 1 0.275862 0.58621 0.12399 ## 3 0.034483 2 0.172414 0.55172 0.12140 ## 4 0.017241 6 0.034483 0.51724 0.11861 ## 5 0.000000 8 0.000000 0.51724 0.11861 plotcp(tree2) min_cp = tree2$cptable[which.min(tree2$cptable[,&quot;xerror&quot;]),&quot;CP&quot;] min_cp ## [1] 0.01724138 Remember rpart has a built-in process for cross-validation. The xerror is the cross-validation error, the classification error that is calculated on the test data with a cross-validation process. In general, the cross-validation error grows as the tree gets more levels (each row represents a different height of the tree). There are two common ways to prune a tree by rpart: Use the first level (i.e. least nsplit) with minimum xerror. The first level only kicks in when there are multiple levels having the same, minimum xerror. This is the most common used method. Use the first level where xerror &lt; min(xerror) + xstd, the level whose xerror is at or below horizontal line. This method takes into account the variability of xerror resulting from cross-validation. If we decide to prune our tree at the minimum cp: ptree2 &lt;- prune(tree2, cp = min_cp) printcp(ptree2) ## ## Classification tree: ## rpart(formula = PRONO ~ ., data = myocarde, method = &quot;class&quot;, ## control = rpart.control(minsplit = 2, minbucket = 1, cp = 0)) ## ## Variables actually used in tree construction: ## [1] INCAR INSYS PVENT ## ## Root node error: 29/71 = 0.40845 ## ## n= 71 ## ## CP nsplit rel error xerror xstd ## 1 0.724138 0 1.000000 1.00000 0.14282 ## 2 0.103448 1 0.275862 0.58621 0.12399 ## 3 0.034483 2 0.172414 0.55172 0.12140 ## 4 0.017241 6 0.034483 0.51724 0.11861 fancyRpartPlot(ptree2) Now we have applied two approaches: limiting the tree’s growth and pruning a fully grown tree. Hence, we have two different trees: tree and ptree2. In the first case, we can use cp or other control parameters in rpart.control as hyperparameters and tune them on the test set. In the second case, we can grow the tree to its maximum capacity and tune its pruning as to maximize the prediction accuracy on the test set. We will not show the tuning of a tree here. Instead, we will see many improved tree-based models and tuned them in this section. 16.4 Classification with Titanic We can use rpart to predict survival on the Titanic. library(PASWR) data(titanic3) str(titanic3) ## &#39;data.frame&#39;: 1309 obs. of 14 variables: ## $ pclass : Factor w/ 3 levels &quot;1st&quot;,&quot;2nd&quot;,&quot;3rd&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ survived : int 1 1 0 0 0 1 1 0 1 0 ... ## $ name : Factor w/ 1307 levels &quot;Abbing, Mr. Anthony&quot;,..: 22 24 25 26 27 31 46 47 51 55 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 1 2 1 2 1 2 1 2 ... ## $ age : num 29 0.917 2 30 25 ... ## $ sibsp : int 0 1 1 1 1 0 1 0 2 0 ... ## $ parch : int 0 2 2 2 2 0 0 0 0 0 ... ## $ ticket : Factor w/ 929 levels &quot;110152&quot;,&quot;110413&quot;,..: 188 50 50 50 50 125 93 16 77 826 ... ## $ fare : num 211 152 152 152 152 ... ## $ cabin : Factor w/ 187 levels &quot;&quot;,&quot;A10&quot;,&quot;A11&quot;,..: 45 81 81 81 81 151 147 17 63 1 ... ## $ embarked : Factor w/ 4 levels &quot;&quot;,&quot;Cherbourg&quot;,..: 4 4 4 4 4 4 4 4 4 2 ... ## $ boat : Factor w/ 28 levels &quot;&quot;,&quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 13 4 1 1 1 14 3 1 28 1 ... ## $ body : int NA NA NA 135 NA NA NA NA NA 22 ... ## $ home.dest: Factor w/ 369 levels &quot;&quot;,&quot;?Havana, Cuba&quot;,..: 309 231 231 231 231 237 163 25 23 229 ... We will use the following variables: survived - 1 if true, 0 otherwise; sex - the gender of the passenger; age - age of the passenger in years; pclass - the passengers class of passage; sibsp - the number of siblings/spouses aboard; parch - the number of parents/children aboard. What predictors are associated with those who perished compared to those who survived? titan &lt;- rpart(survived ~ sex + age + pclass + sibsp + parch, data = titanic3, method = &quot;class&quot;) prp( titan, extra = 1, faclen = 5, box.col = c(&quot;indianred1&quot;, &quot;aquamarine&quot;)[tree$frame$yval] ) barplot( titan$variable.importance, horiz = TRUE, col = &quot;yellow3&quot;, cex.axis = 0.7, cex.names = 0.7 ) If we want to see the cross-validation error and the cp table: printcp(titan) ## ## Classification tree: ## rpart(formula = survived ~ sex + age + pclass + sibsp + parch, ## data = titanic3, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] age parch pclass sex sibsp ## ## Root node error: 500/1309 = 0.38197 ## ## n= 1309 ## ## CP nsplit rel error xerror xstd ## 1 0.424000 0 1.000 1.000 0.035158 ## 2 0.021000 1 0.576 0.576 0.029976 ## 3 0.015000 3 0.534 0.536 0.029198 ## 4 0.011333 5 0.504 0.516 0.028785 ## 5 0.010000 9 0.458 0.512 0.028701 plotcp(titan) Of course, we would like to see the tree’s prediction accuracy by using a test dataset and the confusion table metrics. library(ROCR) #test/train split set.seed(1) ind &lt;- sample(nrow(titanic3), nrow(titanic3) * 0.7) train &lt;- titanic3[ind,] test &lt;- titanic3[-ind,] #Tree on train titan2 &lt;- rpart(survived ~ sex + age + pclass + sibsp + parch, data = train, method = &quot;class&quot;) phat &lt;- predict(titan2, test, type = &quot;prob&quot;) #AUC pred_rocr &lt;- prediction(phat[, 2], test$survived) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.814118 Here, we report only AUC in this simple example. We can use Moreover, we can reweigh variables so that the loss or the cost of a wrong split would be more or less important (see cost argument in rpart). Finally, as in every classification, we can put a different weight on the correct classifications than the wrong classifications (or vise verse). This can easily be done in rpart by the loss matrix. Before commenting on the strengths and weaknesses of CART, let’s see a regression tree. 16.5 Regression Tree The same partitioning procedure can be applied when the outcome variable is not qualitative. For a classification problem, a splitting criterion was either the Gini or log-likelihood function. When we have numerical outcome variable, we can can use the anova method to decide which variable gives the best split: \\[ S S_{T}-\\left(S S_{L}+S S_{R}\\right), \\] where \\[ SS=\\sum\\left(y_{i}-\\bar{y}\\right)^{2}, \\] which is the sum of squares for the node (T), the right (R), and the left (L) splits. Similar to our delta method, if \\(SS_{T}-\\left(SS_{L}+SS_{R}\\right)\\) is positive and significant, we make the split on the node (the variable). After the split, the fitted value of the node is the mean of \\(y\\) of that node. The anova method is the default method if \\(y\\) a simple numeric vector. However, when \\(y_i \\in (0,1)\\), \\[ SS_{T}=\\sum\\left(y_{i}-\\bar{y}\\right)^{2}=\\sum y_{i}^2 -n\\bar{y}^2=\\sum y_{i} -n\\bar{y}^2=n\\bar y -n\\bar{y}^2=np(1-p) \\] Hence, we can show that the relative gain would be the same in regression trees using \\(SS_T\\) or Gini when \\(y_i \\in (0,1)\\). It is not hard to write a simple loop similar to our earlier algorithm, but it would be redundant. We will use rpart in an example: # simulated data set.seed(1) x &lt;- runif(100,-2, 2) y &lt;- 1 + 1 * x + 4 * I(x ^ 2) - 4 * I(x ^ 3) + rnorm(100, 0, 6) dt &lt;- data.frame(&quot;y&quot; = y, &quot;x&quot; = x) plot(x, y, col = &quot;gray&quot;) # Tree fit1 &lt;- rpart(y ~ x, minsplit = 83, dt) # we want to have 1 split fancyRpartPlot(fit1) When we have split at \\(x=-0.65\\), rpart calculates two constant \\(\\hat{f}(x_i)\\)’s both for the left and right splits: mean(y[x &lt;= -0.65]) ## [1] 15.33681 mean(y[x &gt; -0.65]) ## [1] 0.9205211 Here, we see them on the plot: z &lt;- seq(min(x), max(x), length.out = 1000) plot(x, y, col = &quot;gray&quot;) lines(z, predict(fit1, data.frame(x = z)), col = &quot;blue&quot;, lwd = 3) abline(v = -0.65, col = &quot;red&quot;) If we reduce the minsplit, # Tree fit2 &lt;- rpart(y ~ x, minsplit = 6, dt) fancyRpartPlot(fit2) # On the plot plot(x, y, col = &quot;gray&quot;) lines(z, predict(fit2, data.frame(x = z)), col = &quot;green&quot;, lwd = 3) We will use an example of predicting Baseball players’ salaries from the ISLR package (ISLR_2021?). This data set is deduced from the Baseball fielding data set reflecting the fielding performance that includes the numbers of Errors, Putouts and Assists made by each player. # Hitters data library(ISLR) data(&quot;Hitters&quot;) str(Hitters) ## &#39;data.frame&#39;: 322 obs. of 20 variables: ## $ AtBat : int 293 315 479 496 321 594 185 298 323 401 ... ## $ Hits : int 66 81 130 141 87 169 37 73 81 92 ... ## $ HmRun : int 1 7 18 20 10 4 1 0 6 17 ... ## $ Runs : int 30 24 66 65 39 74 23 24 26 49 ... ## $ RBI : int 29 38 72 78 42 51 8 24 32 66 ... ## $ Walks : int 14 39 76 37 30 35 21 7 8 65 ... ## $ Years : int 1 14 3 11 2 11 2 3 2 13 ... ## $ CAtBat : int 293 3449 1624 5628 396 4408 214 509 341 5206 ... ## $ CHits : int 66 835 457 1575 101 1133 42 108 86 1332 ... ## $ CHmRun : int 1 69 63 225 12 19 1 0 6 253 ... ## $ CRuns : int 30 321 224 828 48 501 30 41 32 784 ... ## $ CRBI : int 29 414 266 838 46 336 9 37 34 890 ... ## $ CWalks : int 14 375 263 354 33 194 24 12 8 866 ... ## $ League : Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 1 2 1 2 2 1 2 1 2 1 ... ## $ Division : Factor w/ 2 levels &quot;E&quot;,&quot;W&quot;: 1 2 2 1 1 2 1 2 2 1 ... ## $ PutOuts : int 446 632 880 200 805 282 76 121 143 0 ... ## $ Assists : int 33 43 82 11 40 421 127 283 290 0 ... ## $ Errors : int 20 10 14 3 4 25 7 9 19 0 ... ## $ Salary : num NA 475 480 500 91.5 750 70 100 75 1100 ... ## $ NewLeague: Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 1 2 1 2 2 1 1 1 2 1 ... What predictors are associated with baseball player’s Salary (1987 annual salary on opening day in thousands of dollars)? Let’s consider 3 covariates for the sake of simplicity: Years (Number of years in the major leagues); Hits (Number of hits in 1986); Atbat (Number of times at bat in 1986). # Remove NA&#39;s df &lt;- Hitters[complete.cases(Hitters$Salary),] dfshort &lt;- df[, c(19, 7, 2, 1)] # cp=0, so it&#39;s fully grown tree &lt;- rpart(log(Salary) ~ Years + Hits + AtBat, data = dfshort, cp = 0) prp(tree, extra = 1, faclen = 5) It works on the same principle as we described before: find terminal nodes that minimize the sum of squares. This process may give us a good prediction on the training set but not on the test set, as it overfits the data. Hence, we use a pruned tree found by rpart by cross-validation: ptree &lt;- rpart(log(Salary) ~ Years + Hits + AtBat, data = dfshort) prp(ptree, extra=1, faclen=5) We can see its prediction power similar to what we did in the Titanic data example. Since this is a regression, we can ask which one is better, a tree or a linear model? If the relationship between \\(y\\) and \\(X\\) is linear, a linear model should perform better. We can test this: # Test/train split set.seed(123) ind &lt;- sample(nrow(dfshort), nrow(dfshort) * 0.7) train &lt;- dfshort[ind,] test &lt;- dfshort[-ind,] # Tree and lm() on train ptree &lt;- rpart(log(Salary) ~ Years + Hits + AtBat, data = dfshort) predtree &lt;- predict(ptree, test) lin &lt;- lm(log(Salary) ~ ., data = dfshort) predlin &lt;- predict(lin, test) # RMSPE rmspe_tree &lt;- sqrt(mean((log(test$Salary) - predtree) ^ 2)) rmspe_tree ## [1] 0.4601892 rmspe_lin &lt;- sqrt(mean((log(test$Salary) - predlin) ^ 2)) rmspe_lin ## [1] 0.6026888 In this simple example, our the tree would do a better job. Trees tend to work well for problems where there are important nonlinearities and interactions. Yet, they are known to be quite sensitive to the original sample. Therefore, the models trained in one sample may have poor predictive accuracy on another sample. These problems motivate Random Forest and Boosting methods, as we will describe in following chapters. freakonometrics (Charpentier_scratch?)↩︎ "],["ensemble-methods.html", "Chapter 17 Ensemble Methods 17.1 Bagging 17.2 Random Forest 17.3 Boosting 17.4 Ensemble Applications 17.5 Classification 17.6 Regression 17.7 Exploration 17.8 Boosting Applications", " Chapter 17 Ensemble Methods Bagging, random forests and, boosting methods are the main methods of ensemble learning - a machine learning method where multiple models are trained to solve the same problem. The main idea is that, instead of using all features (predictors) in one complex base model running on the whole data, we combine multiple models each using selected number of features and subsections of the data. With this, we can have a more robust learning system. What inspires ensemble learning is the idea of the “wisdom of crowds”. It suggests that “many are smarter than the few” so that collective decision-making of a diverse and larger group of individuals is better than that of a single expert. When we use a single robust model, poor predictors would be eliminated in the training procedure. Although each poor predictor has a very small contribution in training, their combination would be significant. Ensemble learning systems help these poor predictors have their “voice” in the training process by keeping them in the system rather than eliminating them. That is the main reason why ensemble methods represent robust learning algorithms in machine learning. 17.1 Bagging Bagging gets its name from Bootstrap aggregating of trees. The idea is simple: we train many trees each of which use a separate bootstrapped sample then aggregate them to one tree for the final decision. It works with few steps: Select number of trees (B), and the tree depth (D), Create a loop (B) times, In each loop, (a) generate a bootstrap sample from the original data; (b) estimate a tree of depth D on that sample. Let’s see an example with the titanic dataset: library(PASWR) library(rpart) library(rpart.plot) data(titanic3) # This is for a set of colors in each tree clr = c(&quot;pink&quot;,&quot;red&quot;,&quot;blue&quot;,&quot;yellow&quot;,&quot;darkgreen&quot;, &quot;orange&quot;,&quot;brown&quot;,&quot;purple&quot;,&quot;darkblue&quot;) n = nrow(titanic3) par(mfrow=c(3,3)) for(i in 1:9){ # Here B = 9 set.seed(i*2) idx = sample(n, n, replace = TRUE) #Bootstrap sampling with replacement tr &lt;- titanic3[idx,] cart = rpart(survived~sex+age+pclass+sibsp+parch, cp = 0, data = tr, method = &quot;class&quot;) #unpruned prp(cart, type=1, extra=1, box.col=clr[i]) } What are we going to do with these 9 trees? In regression trees, the prediction will be the average of the resulting predictions. In classification trees, we take a majority vote. Since averaging a set of observations by bootstrapping reduces the variance, the prediction accuracy increases. More importantly, compared to CART, the results would be much less sensitive to the original sample. As a result, they show impressive improvement in accuracy. Below, we have an algorithm that follows the steps for bagging in classification. Let’s start with a single tree and see how we can improve it with bagging: library(ROCR) #test/train split set.seed(1) ind &lt;- sample(nrow(titanic3), nrow(titanic3) * 0.8) train &lt;- titanic3[ind, ] test &lt;- titanic3[-ind, ] #Single tree cart &lt;- rpart(survived ~ sex + age + pclass + sibsp + parch, data = train, method = &quot;class&quot;) #Pruned phat1 &lt;- predict(cart, test, type = &quot;prob&quot;) #AUC pred_rocr &lt;- prediction(phat1[,2], test$survived) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.8352739 Now, we apply bagging: B = 100 # number of trees phat2 &lt;- matrix(0, B, nrow(test)) # Loops for(i in 1:B){ set.seed(i) # to make it reproducible idx &lt;- sample(nrow(train), nrow(train), replace = TRUE) dt &lt;- train[idx, ] cart_B &lt;- rpart(survived ~ sex + age + pclass + sibsp + parch, cp = 0, data = dt, method = &quot;class&quot;) # unpruned phat2[i,] &lt;- predict(cart_B, test, type = &quot;prob&quot;)[, 2] } dim(phat2) ## [1] 100 262 You can see in that phat2 is a \\(100 \\times 262\\) matrix. Each column is representing the predicted probability that survived = 1. We have 100 trees (rows in phat2) and 100 predicted probabilities for each the observation in the test data. The only job we will have now to take the average of 100 predicted probabilities for each column. # Take the average phat_f &lt;- colMeans(phat2) #AUC pred_rocr &lt;- prediction(phat_f, test$survived) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.8765668 Hence, we have a slight improvement over a single tree. We can see how the number of trees (B) would cumulatively increases AUC (reduces MSPE in regressions). B = 300 phat3 &lt;- matrix(0, B, nrow(test)) AUC &lt;- c() for (i in 1:B) { set.seed(i) idx &lt;- sample(nrow(train), nrow(train), replace = TRUE) dt &lt;- train[idx,] fit &lt;- rpart( survived ~ sex + age + pclass + sibsp + parch, cp = 0, data = dt, method = &quot;class&quot; ) phat3[i, ] &lt;- predict(fit, test, type = &quot;prob&quot;)[, 2] phat_f &lt;- colMeans(phat3) #AUC pred_rocr &lt;- prediction(phat_f, test$survived) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUC[i] &lt;- auc_ROCR@y.values[[1]] } plot(AUC, type = &quot;l&quot;, col = &quot;red&quot;, xlab = &quot;B - Number of trees&quot;, lwd = 2) As it is clear from the plot that, when we use a large value of B, the AUC (or error in regression) becomes stable. Therefore, we do not need to tune the number of trees with bagging. Using a value of B sufficiently large would suffice. The main idea behind bagging is to reduce the variance in prediction. The reason for this reduction is simple: we take the mean prediction of all bootstrapped samples. Remember, when we use a simple tree and make 500 bootstrapping validations, each one of them gives a different MSPE (in regressions) or AUC (in classification). The difference now is that we average yhat in regressions or phat in classifications (or yhat with majority voting). This reduces the prediction uncertainty drastically. Bagging works very well for high-variance base learners, such as decision trees or kNN. If the learner is stable, bagging adds little to the model’s performance. This brings us to another and improved ensemble method, random forests. 17.2 Random Forest Random Forest = Bagging + subsample of covariates at each node. We have done the first part before. Random forests algorithm produces many single trees based on randomly selected a subset of observations and features. Since the algorithm leads to many single trees (like a forest) with a sufficient variation, the averaging them provides relatively a smooth and, more importantly, better predictive power than a single tree. Random forests and regression trees are particularly effective in settings with a large number of features that are correlated each other. The splits will generally ignore those covariates, and as a result, the performance will remain strong even in settings with many features. We will use the Breiman and Cutler’s Random Forests for Classification and Regression: randomForest()(Brei_2004?). Here are the steps and the loop structure: Select number of trees (ntree), subsampling parameter (mtry), and the tree depth maxnodes, Create a loop ntree times, In each loop, (a) generate a bootstrap sample from the original data; (b) estimate a tree of depth maxnodes on that sample, But, for each split in the tree (this is our second loop), randomly select mtry original covariates and do the split among those. Hence, bagging is a special case of random forest, with mtry = number of features (\\(P\\)). As we think on the idea of “subsampling covariates at each node” little bit more, we can see the rationale: suppose there is one very strong covariate in the sample. Almost all trees will use this covariate in the top split. All of the trees will look quite similar to each other. Hence the predictions will be highly correlated. Averaging many highly correlated quantities does not lead to a large reduction in variance. Random forests decorrelate the trees and, thus, further reduces the sensitivity of trees to the data points that are not in the original dataset. How are we going to pick mtry? In practice, default values are mtry = \\(P/3\\) in regression and mtry = \\(\\sqrt{P}\\) classification. (See mtry in ?randomForest). Note that, with this parameter (mtry), we can run a pure bagging model with randomForest(), instead of rpart(), if we set mtry = \\(P\\). With the bootstrap resampling process for each tree, random forests have an efficient and reasonable approximation of the test error calculated from out-of-bag (OOB) sets. When bootstrap aggregating is performed, two independent sets are created. One set, the bootstrap sample, is the data chosen to be “in-the-bag” by sampling with replacement. The out-of-bag set is all data not chosen in the sampling process. Hence, there is no need for cross-validation or a separate test set to obtain an unbiased estimate of the prediction error. It is estimated internally as each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases (observations) are left out of the bootstrap sample and not used in the construction of the \\(k^{th}\\) tree. In this way, a test set classification is obtained for each case in about one-third of the trees. In each run, the class is selected when it gets most of the votes among the OOB cases. The proportion of times that the selected class for the observation is not equal to the true class over all observations in OOB set is called the OOB error estimate. This has proven to be unbiased in many tests. Note that the forest’s variance decreases as the number of trees grows. Thus, more accurate predictions are likely to be obtained by choosing a large number of trees. You can think of a random forest model as a robust version of CART models. There are some default parameters that can be tuned in randomForest(). It is argued that, however, the problem of overfitting is minor in random forests. Segal (2004) demonstrates small gains in performance by controlling the depths of the individual trees grown in random forests. Our experience is that using full-grown trees seldom costs much, and results in one less tuning parameter. Figure 15.8 shows the modest effect of depth control in a simple regression example. (Hastie et al., 2009, p.596) Let’s start with a simulation to show random forest and CART models: library(randomForest) # Note that this is actually Bagging since we have only 1 variable # Our simulated data set.seed(1) n = 500 x &lt;- runif(n) y &lt;- sin(12 * (x + .2)) / (x + .2) + rnorm(n) / 2 # Fitting the models fit.tree &lt;- rpart(y ~ x) #CART fit.rf1 &lt;- randomForest(y ~ x) #No depth control fit.rf2 &lt;- randomForest(y ~ x, maxnodes = 20) # Control it # Plot observations and predicted values z &lt;- seq(min(x), max(x), length.out = 1000) par(mfrow = c(1, 1)) plot(x, y, col = &quot;gray&quot;, ylim = c(-4, 4)) lines(z, predict(fit.rf1, data.frame(x = z)), col = &quot;green&quot;, lwd = 2) lines(z, predict(fit.rf2, data.frame(x = z)), col = &quot;red&quot;, lwd = 2) lines(z, predict(fit.tree, data.frame(x = z)), col = &quot;blue&quot;, lwd = 1.5) legend(&quot;bottomright&quot;, c(&quot;Random Forest: maxnodes=max&quot;, &quot;Random Forest: maxnodes=20&quot;, &quot;CART: single regression tree&quot;), col = c(&quot;green&quot;, &quot;red&quot;, &quot;blue&quot;), lty = c(1, 1, 1), bty = &quot;n&quot; ) The random forest models are definitely improvements over CART, but which one is better? Although random forest models should not overfit when increasing the number of trees (ntree) in the forest, in practice maxnodes and mtry are used as hyperparameters in the field. Let’s use out-of-bag (OOB) MSE to tune Random Forest parameters in our case to see if there is any improvement. maxnode &lt;- c(10, 50, 100, 500) for (i in 1:length(maxnode)) { a &lt;- randomForest(y ~ x, maxnodes = maxnode[i]) print(c(maxnode[i], a$mse[500])) } ## [1] 10.0000000 0.3878058 ## [1] 50.000000 0.335875 ## [1] 100.0000000 0.3592119 ## [1] 500.0000000 0.3905135 # Increase ntree = 1500 maxnode &lt;- c(10, 50, 100, 500) for (i in 1:length(maxnode)) { a &lt;- randomForest(y ~ x, maxnodes = maxnode[i], ntree = 2500) print(c(maxnode[i], a$mse[500])) } ## [1] 10.000000 0.391755 ## [1] 50.0000000 0.3353198 ## [1] 100.0000000 0.3616621 ## [1] 500.0000000 0.3900982 We can see that OOB-MSE is smaller with maxnodes = 50 even when we increase ntree = 1500 . Of course we can have a finer sequence of maxnodes series to test. Similarly, we can select parameter mtry with a grid search. In a bagged model we set mtry = \\(P\\). If we don’t set it, the default values for mtry are square-root of \\(p\\) for classification and \\(p/3\\) in regression, where \\(p\\) is number of features. If we want, we can tune both parameters with cross-validation. The effectiveness of tuning random forest models in improving their prediction accuracy is an open question in practice. Bagging and random forest models tend to work well for problems where there are important nonlinearities and interactions. More importantly, they are robust to the original sample and more efficient than single trees. However, the results would be less intuitive and difficult to interpret. Nevertheless, we can obtain an overall summary of the importance of each covariates using SSR (for regression) or Gini index (for classification). The index records the total amount that the SSR or Gini is decreased due to splits over a given covariate, averaged over all ntree trees. rf &lt;- randomForest( as.factor(survived) ~ sex + age + pclass + sibsp + parch, data = titanic3, na.action = na.omit, localImp = TRUE, ) plot(rf, main = &quot;Learning curve of the forest&quot;) legend( &quot;topright&quot;, c( &quot;Error for &#39;Survived&#39;&quot;, &quot;Misclassification error&quot;, &quot;Error for &#39;Dead&#39;&quot; ), lty = c(1, 1, 1), col = c(&quot;green&quot;, &quot;black&quot;, &quot;red&quot;) ) The plot shows the evolution of out-of-bag errors when the number of trees increases. The learning curves reach to a stable section right after a couple of trees. With 500 trees, which is the default number, the OOB estimate of our error rate is around 0.2, which can be seen in the basic information concerning our model below rf ## ## Call: ## randomForest(formula = as.factor(survived) ~ sex + age + pclass + sibsp + parch, data = titanic3, localImp = TRUE, , na.action = na.omit) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 19.69% ## Confusion matrix: ## 0 1 class.error ## 0 562 57 0.09208401 ## 1 149 278 0.34894614 We will see a several applications on CART, bagging and Random Forest later in Chapter 14. 17.3 Boosting Boosting is an ensemble method that combines a set of “weak learners” into a strong learner to improve the prediction accuracy in self-learning algorithms. In boosting, the constructed iteration selects a random sample of data, fits a model, and then train sequentially. In each sequential model, the algorithm learns from the weaknesses of its predecessor (predictions errors) and tries to compensate for the weaknesses by “boosting” the weak rules from each individual classifier. The first original boosted application was offered in 1990 by Robert Schapire (1990) (Schapire?).6 Today, there are many boosting algorithms that are mainly grouped in the following three types: Gradient descent algorithm, AdaBoost algorithm, Xtreme gradient descent algorithm, We will start with a general application to show the idea behind the algorithm using the package gbm, Generalized Boosted Regression Models 17.3.1 Sequential ensemble with gbm Similar to bagging, boosting also combines a large number of decision trees. However, the trees are grown sequentially without bootstrap sampling. Instead each tree is fit on a modified version of the original dataset, the error. In regression trees, for example, each tree is fit to the residuals from the previous tree model so that each iteration is focused on improving previous errors. This process would be very weird for an econometrician. The accepted model building practice in econometrics is that you should have a model that the errors (residuals) should be orthogonal (independent from) to covariates. Here, what we suggest is the opposite of this practice: start with a very low-depth (shallow) model that omits many relevant variables, run a linear regression, get the residuals (prediction errors), and run another regression that explains the residuals with covariates. This is called learning from mistakes. Since there is no bootstrapping, this process is open to overfitting problem as it aims to minimize the in-sample prediction error. Hence, we introduce a hyperparameter that we can tune the learning process with cross-validation to stop the overfitting and get the best predictive model. This hyperparameter (shrinkage parameter, also known as the learning rate or step-size reduction) limits the size of the errors. Let’s see the whole process in a simple example inspired by Freakonometrics (Hyp_2018?): # First we will simulate our data n &lt;- 300 set.seed(1) x &lt;- sort(runif(n) * 2 * pi) y &lt;- sin(x) + rnorm(n) / 4 df &lt;- data.frame(&quot;x&quot; = x, &quot;y&quot; = y) plot(df$x, df$y, ylab = &quot;y&quot;, xlab = &quot;x&quot;, col = &quot;grey&quot;) We will “boost” a single regression tree: Step 1: Fit the model by using in-sample data # Regression tree with rpart() fit &lt;- rpart(y ~ x, data = df) # First fit: y~x yp &lt;- predict(fit) # using in-sample data # Plot for single regression tree plot(df$x, df$y, ylab = &quot;y&quot;, xlab = &quot;x&quot;, col = &quot;grey&quot;) lines(df$x, yp, type = &quot;s&quot;, col = &quot;blue&quot;, lwd = 3) Now, we will have a loop that will “boost” the model. What we mean by boosting is that we seek to improve yhat, i.e. \\(\\hat{f}(x_i)\\), in areas where it does not perform well by fitting trees to the residuals. Step 2: Find the “error” and introduce a hyperparameter h. h &lt;- 0.1 # shrinkage parameter # Calculate the prediction error adjusted by h yr &lt;- df$y - h * yp # Add this adjusted prediction error, `yr` to our main data frame, # which will be our target variable to predict later df$yr &lt;- yr # Store the &quot;first&quot; predictions in YP YP &lt;- h * yp Note that if h= 1, it would give us usual “residuals”. Hence, h controls for “how much error” we would like to reduce. Step 3: Now, we will predict the “error” in a loop that repeats itself many times. # Boosting loop for t times (trees) for (t in 1:100) { fit &lt;- rpart(yr ~ x, data = df) # here it&#39;s yr~x. # We try to understand the prediction error by x&#39;s yp &lt;- predict(fit, newdata = df) # This is your main prediction added to YP YP &lt;- cbind(YP, h * yp) df$yr &lt;- df$yr - h * yp # errors for the next iteration # i.e. the next target to predict! } str(YP) ## num [1:300, 1:101] 0.00966 0.00966 0.00966 0.00966 0.00966 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:300] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## ..$ : chr [1:101] &quot;YP&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... Look at YP now. We have a matrix 300 by 101. This is a matrix of predicted errors, except for the first column. So what? # Function to plot a single tree and boosted trees for different t viz &lt;- function(M) { # Boosting yhat &lt;- apply(YP[, 1:M], 1, sum) # This is predicted y for depth M plot(df$x, df$y, ylab = &quot;&quot;, xlab = &quot;&quot;) # Data points lines(df$x, yhat, type = &quot;s&quot;, col = &quot;red&quot;, lwd = 3) # line for boosting # Single Tree fit &lt;- rpart(y ~ x, data = df) # Single regression tree yp &lt;- predict(fit, newdata = df) # prediction for the single tree lines(df$x, yp, type = &quot;s&quot;, col = &quot;blue&quot;, lwd = 3) # line for single tree lines(df$x, sin(df$x), lty = 1, col = &quot;black&quot;) # Line for DGM } # Run each viz(5) viz(101) Each of 100 trees is given in the YP matrix. Boosting combines the outputs of many “weak” learners (each tree) to produce a powerful “committee”. What if we change the shrinkage parameter? Let’s increase it to 1.8. h &lt;- 1.8 # shrinkage parameter df$yr &lt;- df$y - h*yp # Prediction errors with &quot;h&quot; after rpart YP &lt;- h*yp #Store the &quot;first&quot; prediction errors in YP # Boosting Loop for t (number of trees) times for(t in 1:100){ fit &lt;- rpart(yr~x, data=df) # here it&#39;s yr~x. yhat &lt;- predict(fit, newdata=df) df$yr &lt;- df$yr - h*yhat # errors for the next iteration YP &lt;- cbind(YP, h*yhat) # This is your main prediction added to YP } viz(101) It overfits. Unlike random forests, boosting can overfit if the number of trees (B) and depth of each tree (D) are too large. By averaging over a large number of trees, bagging and random forests reduces variability. Boosting does not average over the trees. This shows that \\(h\\) should be tuned by a proper process. The generalized boosted regression modeling (GBM) (Ridgeway_2020?) can also be used for boosting regressions. Note that there are many arguments with their specific default values in the function. For example, n.tree (B) is 100 and shrinkage (\\(h\\)) is 0.1. The gbm() function also has interaction.depth (D) specifying the maximum depth of each tree. When it is 1, the model is just an additive model, while 2 implies a model with up to 2-way interactions. A smaller \\(h\\) typically requires more trees \\(B\\). It allows more and different shaped trees to attack the residuals. Here is the application of gbm to our simulated data: library(gbm) # Note bag.fraction = 1 (no CV). The default is 0.5 bo1 &lt;- gbm(y ~ x, distribution = &quot;gaussian&quot;, n.tree = 100, data = df, shrinkage = 0.1, bag.fraction = 1) bo2 &lt;- gbm(y ~ x, distribution = &quot;gaussian&quot;, data = df) # All default plot(df$x, df$y, ylab = &quot;&quot;, xlab = &quot;&quot;) #Data points lines(df$x, predict(bo1, data = df, n.trees = t), type = &quot;s&quot;, col = &quot;red&quot;, lwd = 3) #line for without CV lines(df$x, predict(bo2, n.trees = t, data = df), type = &quot;s&quot;, col = &quot;green&quot;, lwd = 3) #line with default parameters with CV We will have more applications with gbm in the next chapter. We can also have a boosting application for classification problems. While the gbm function can be used for classification that requires a different distribution (“bernoulli” - logistic regression for 0-1 outcomes), there is a special boosting method for classification problems, AdaBoost.M1, which is what we will look at next. 17.3.2 AdaBoost One of the most popular boosting algorithm is AdaBost.M1 due to Freund and Schpire (1997). We consider two-class problem where \\(y \\in\\{-1,1\\}\\), which is a categorical outcome. With a set of predictor variables \\(X\\), a classifier \\(\\hat{m}_{b}(x)\\) at tree \\(b\\) among B trees, produces a prediction taking the two values \\(\\{-1,1\\}\\). To understand how AdaBoost works, let’s look at the algorithm step by step: Select the number of trees B, and the tree depth D; Set initial weights, \\(w_i=1/n\\), for each observation. Fit a classification tree \\(\\hat{m}_{b}(x)\\) at \\(b=1\\), the first tree. Calculate the following misclassification error for \\(b=1\\): \\[ \\mathbf{err}_{b=1}=\\frac{\\sum_{i=1}^{n} \\mathbf{I}\\left(y_{i} \\neq \\hat{m}_{b}\\left(x_{i}\\right)\\right)}{n} \\] By using this error, calculate \\[ \\alpha_{b}=0.5\\log \\left(\\frac{1-e r r_{b}}{e r r_{b}}\\right) \\] For example, suppose \\(err_b = 0.3\\), then \\(\\alpha_{b}=0.5\\text{log}(0.7/0.3)\\), which is a log odds or \\(\\log (\\text{success}/\\text{failure})\\). If the observation \\(i\\) is misclassified, update its weights, if not, use \\(w_i\\) which is \\(1/n\\): \\[ w_{i} \\leftarrow w_{i} e^{\\alpha_b} \\] Let’s try some numbers: #Suppose err = 0.2, n=100 n = 100 err = 0.2 alpha &lt;- 0.5 * log((1 - err) / err) alpha ## [1] 0.6931472 exp(alpha) ## [1] 2 So, the new weight for the misclassified \\(i\\) in the second tree (i.e., \\(b=2\\) stump) will be # For misclassified observations weight_miss &lt;- (1 / n) * (exp(alpha)) weight_miss ## [1] 0.02 # For correctly classified observations weight_corr &lt;- (1 / n) * (exp(-alpha)) weight_corr ## [1] 0.005 This shows that as the misclassification error goes up, it increases the weights for each misclassified observation and reduces the weights for correctly classified observations. With this procedure, in each loop from \\(b\\) to B, it applies \\(\\hat{m}_{b}(x)\\) to the data using updated weights \\(w_i\\) in each \\(b\\): \\[ \\mathbf{err}_{b}=\\frac{\\sum_{i=1}^{n} w_{i} \\mathbf{I}\\left(y_{i} \\neq \\hat{m}_{b}\\left(x_{i}\\right)\\right)}{\\sum_{i=1}^{n} w_{i}} \\] We normalize all weights between 0 and 1 so that sum of the weights would be one in each iteration. Hence, the algorithm works in a way that it randomly replicates the observations as new data points by using the weights as their probabilities. This process also resembles to under- and oversampling at the same time so that the number of observations stays the same. The new dataset now is used again for the next tree (\\(b=2\\)) and this iteration continues until B. We can use rpart in each tree with weights option as we will shown momentarily. Here is an example with the myocarde data that we only use the first 6 observations: library(readr) myocarde &lt;- read_delim(&quot;myocarde.csv&quot;, delim = &quot;;&quot; , escape_double = FALSE, trim_ws = TRUE, show_col_types = FALSE) myocarde &lt;- data.frame(myocarde) df &lt;- head(myocarde) df$Weights = 1 / nrow(df) df ## FRCAR INCAR INSYS PRDIA PAPUL PVENT REPUL PRONO Weights ## 1 90 1.71 19.0 16 19.5 16.0 912 SURVIE 0.1666667 ## 2 90 1.68 18.7 24 31.0 14.0 1476 DECES 0.1666667 ## 3 120 1.40 11.7 23 29.0 8.0 1657 DECES 0.1666667 ## 4 82 1.79 21.8 14 17.5 10.0 782 SURVIE 0.1666667 ## 5 80 1.58 19.7 21 28.0 18.5 1418 DECES 0.1666667 ## 6 80 1.13 14.1 18 23.5 9.0 1664 DECES 0.1666667 Suppose that our first stump misclassifies the first observation. So the error rate # Alpha n = nrow(df) err = 1 / n alpha &lt;- 0.5 * log((1 - err) / err) alpha ## [1] 0.804719 exp(alpha) ## [1] 2.236068 # Weights for misclassified observations weight_miss &lt;- (1 / n) * (exp(alpha)) weight_miss ## [1] 0.372678 # Weights for correctly classified observations weight_corr &lt;- (1 / n) * (exp(-alpha)) weight_corr ## [1] 0.0745356 Hence, our new sample weights df$New_weights &lt;- c(weight_miss, rep(weight_corr, 5)) df$Norm_weights &lt;- df$New_weight / sum(df$New_weight) # normalizing # Not reporting X&#39;s for now df[, 8:11] ## PRONO Weights New_weights Norm_weights ## 1 SURVIE 0.1666667 0.3726780 0.5 ## 2 DECES 0.1666667 0.0745356 0.1 ## 3 DECES 0.1666667 0.0745356 0.1 ## 4 SURVIE 0.1666667 0.0745356 0.1 ## 5 DECES 0.1666667 0.0745356 0.1 ## 6 DECES 0.1666667 0.0745356 0.1 We can see that the misclassified observation (the first one) has 5 times more likelihood than the other correctly classified observations. We now need to incorporate these weights and resample these six observations. Since incorrectly classified records have higher sample weights, the probability to select those records is very high. If we use a simple tree as our base classifier, we can directly incorporate these weights into rpart. We can use other base classifier. In that case, we can do resampling with these probability weights: set.seed(123) ind &lt;- sample(6, 6, replace = TRUE, prob = df$Norm_weights) df[ind, -c(9:12)] # After ## FRCAR INCAR INSYS PRDIA PAPUL PVENT REPUL PRONO ## 1 90 1.71 19.0 16 19.5 16.0 912 SURVIE ## 5 80 1.58 19.7 21 28.0 18.5 1418 DECES ## 1.1 90 1.71 19.0 16 19.5 16.0 912 SURVIE ## 6 80 1.13 14.1 18 23.5 9.0 1664 DECES ## 2 90 1.68 18.7 24 31.0 14.0 1476 DECES ## 1.2 90 1.71 19.0 16 19.5 16.0 912 SURVIE As we can see, the misclassified observation is repeated three times in the new sample. Hence, observations that are misclassified will have more influence in the next classifier. This is an incredible boost that forces the classification tree to adjust its prediction to do better job for misclassified observations. Finally, in the output, the contributions from classifiers that fit the data better are given more weight (a larger \\(\\alpha_b\\) means a better fit). Unlike a random forest algorithm where each tree gets an equal weight in final decision, here some stumps get more say in final classification. Moreover, “forest of stumps” the order of trees is important. Hence, the final prediction on \\(y_i\\) will be combined from all trees, \\(b\\) to B, through a weighted majority vote: \\[ \\hat{y}_{i}=\\operatorname{sign}\\left(\\sum_{b=1}^{B} \\alpha_{b} \\hat{m}_{b}(x)\\right), \\] which is a signum function defined as follows: \\[ \\operatorname{sign}(x):=\\left\\{\\begin{array}{ll} {-1} &amp; {\\text { if } x&lt;0} \\\\ {0} &amp; {\\text { if } x=0} \\\\ {1} &amp; {\\text { if } x&gt;0} \\end{array}\\right. \\] Here is a simple simulation to show how \\(\\alpha_b\\) will make the importance of each tree (\\(\\hat{m}_{b}(x)\\)) different: n = 1000 set.seed(1) err &lt;- sample(seq(0, 1, 0.01), n, replace = TRUE) alpha = 0.5 * log((1 - err) / err) ind = order(err) plot( err[ind], alpha[ind], xlab = &quot;error (err)&quot;, ylab = &quot;alpha&quot;, type = &quot;o&quot;, col = &quot;red&quot;, lwd = 2 ) We can see that when there is no misclassification error (err = 0), “alpha” will be a large positive number. When the classifier very weak and predicts as good as a random guess (err = 0.5), the importance of the classifier will be 0. If all the observations are incorrectly classified (err = 1), our alpha value will be a negative integer. The AdaBoost.M1 is known as a “discrete classifier” because it directly calculates discrete class labels \\(\\hat{y}_i\\), rather than predicted probabilities, \\(\\hat{p}_i\\). What type of classifier, \\(\\hat{m}_{b}(x)\\), would we choose? Usually a “weak classifier” like a “stump” (a two terminal-node classification tree, i.e one split) would be enough. The \\(\\hat{m}_{b}(x)\\) choose one variable to form a stump that gives the lowest Gini index. Here is our simple example with the myocarde data to show how we can boost a simple weak learner (stump) by using AdaBoost algorithm: library(rpart) # Data myocarde &lt;- read_delim(&quot;myocarde.csv&quot;, delim = &quot;;&quot; , escape_double = FALSE, trim_ws = TRUE, show_col_types = FALSE) myocarde &lt;- data.frame(myocarde) y &lt;- (myocarde[ , &quot;PRONO&quot;] == &quot;SURVIE&quot;) * 2 - 1 x &lt;- myocarde[ , 1:7] df &lt;- data.frame(x, y) # Setting rnd = 100 # number of rounds m = nrow(x) whts &lt;- rep(1 / m, m) # initial weights st &lt;- list() # container to save all stumps alpha &lt;- vector(mode = &quot;numeric&quot;, rnd) # container for alpha y_hat &lt;- vector(mode = &quot;numeric&quot;, m) # container for final predictions set.seed(123) for(i in 1:rnd) { st[[i]] &lt;- rpart(y ~., data = df, weights = whts, maxdepth = 1, method = &quot;class&quot;) yhat &lt;- predict(st[[i]], x, type = &quot;class&quot;) yhat &lt;- as.numeric(as.character(yhat)) e &lt;- sum((yhat != y) * whts) # alpha alpha[i] &lt;- 0.5 * log((1 - e) / e) # Updating weights # Note that, for true predictions, (y * yhat) will be +, otherwise - whts &lt;- whts * exp(-alpha[i] * y * yhat) # Normalizing weights whts &lt;- whts / sum(whts) } # Using each stump for final predictions for (i in 1:rnd) { pred = predict(st[[i]], df, type = &quot;class&quot;) pred = as.numeric(as.character(pred)) y_hat = y_hat + (alpha[i] * pred) } # Let&#39;s see what y_hat is y_hat ## [1] 3.132649 -4.135656 -4.290437 7.547707 -3.118702 -6.946686 ## [7] 2.551433 1.960603 9.363346 6.221990 3.012195 6.982287 ## [13] 9.765139 8.053999 8.494254 7.454104 4.112493 5.838279 ## [19] 4.918513 9.514860 9.765139 -3.519537 -3.172093 -7.134057 ## [25] -3.066699 -4.539863 -2.532759 -2.490742 5.412605 2.903552 ## [31] 2.263095 -6.718090 -2.790474 6.813963 -5.131830 3.680202 ## [37] 3.495350 3.014052 -7.435835 6.594157 -7.435835 -6.838387 ## [43] 3.951168 5.091548 -3.594420 8.237515 -6.718090 -9.582674 ## [49] 2.658501 -10.282682 4.490239 9.765139 -5.891116 -5.593352 ## [55] 6.802687 -2.059754 2.832103 7.655197 10.635851 9.312842 ## [61] -5.804151 2.464149 -5.634676 1.938855 9.765139 7.023157 ## [67] -6.078756 -7.031840 5.651634 -1.867942 9.472835 # sign() function pred &lt;- sign(y_hat) # Confusion matrix table(pred, y) ## y ## pred -1 1 ## -1 29 0 ## 1 0 42 This is our in-sample confusion table. We can also see several stumps: library(rpart.plot) plt &lt;- c(1,5,10,30, 60, 90) p = par(mfrow=c(2,3)) for(i in 1:length(plt)){ prp(st[[i]], type = 2, extra = 1, split.col = &quot;red&quot;, split.border.col = &quot;blue&quot;, box.col = &quot;pink&quot;) } par(p) Let’s see it with the JOUSBoost package: library(JOUSBoost) ada &lt;- adaboost(as.matrix(x), y, tree_depth = 1, n_rounds = rnd) summary(ada) ## Length Class Mode ## alphas 100 -none- numeric ## trees 100 -none- list ## tree_depth 1 -none- numeric ## terms 3 terms call ## confusion_matrix 4 table numeric pred &lt;- predict(ada, x) table(pred, y) ## y ## pred -1 1 ## -1 29 0 ## 1 0 42 These results provide in-sample predictions. When we use it in a real example, we can train AdaBoost.M1 by the tree depths (1 in our example) and the number of iterations (100 trees in our example). An application is provided in the next chapter. 17.3.3 XGBoost Extreme Gradient Boosting (XGBoost) the most efficient version of the gradient boosting framework by its capacity to implement parallel computation on a single machine. It can be used for regression and classification problems with two modes: linear models and tree learning algorithm. That means XGBoost can also be used for regularization in linear models (Section VIII). As decision trees are much better to catch a nonlinear link between predictors and outcome, comparison between two modes can provide quick information to the practitioner, specially in causal analyses, about the structure of alternative models. The XGBoost has several unique advantages: its speed is measured as “10 times faster than the gbm” (see its vignette) and it accepts very efficient input data structures, such as a sparse matrix7. This special input structure in xgboost requires some additional data preparation: a matrix input for the features and a vector for the response. Therefore, a matrix input of the features requires to encode our categorical variables. The matrix can also be selected several possible choices: a regular R matrix, a sparse matrix from the Matrix package, and its own class, xgb.Matrix. We start with a regression example here and leave the classification example to the next chapter in boosting applications. We will use the Ames housing data to see the best “predictors” of the sale price. library(xgboost) library(mltools) library(data.table) library(modeldata) # This can also be loaded by the tidymodels package data(ames) dim(ames) ## [1] 2930 74 Since, the xgboost algorithm accepts its input data as a matrix, all categorical variables have be one-hot coded, which creates a large matrix even with a small size data. That’s why using more memory efficient matrix types (sparse matrix etc.) speeds up the process. We ignore it here and use a regular R matrix, for now. ames_new &lt;- one_hot(as.data.table(ames)) df &lt;- as.data.frame(ames_new) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind,] test &lt;- df[-ind,] X &lt;- as.matrix(train[,-which(names(train) == &quot;Sale_Price&quot;)]) Y &lt;- train$Sale_Price Now we are ready for finding the optimal tuning parameters. One strategy in tuning is to see if there is a substantial difference between train and CV errors. We first start with the number of trees and the learning rate. If the difference still persists, we introduce regularization parameters. There are three regularization parameters: gamma, lambda, and alpha. The last two are similar to what we will see in regularization in Section VIII. Here is our first run without a grid search. We will have a regression tree. The default booster is gbtree for tree-based models. For linear models it should be set to gblinear. The number of parameters and their combinations are very extensive in XGBoost. Please see them here: https://xgboost.readthedocs.io/en/latest/parameter.html#global-configuration. The combination of parameters we picked below is just an example. # params = list( eta = 0.1, # Step size in boosting (default is 0.3) max_depth = 3, # maximum depth of the tree (default is 6) min_child_weight = 3, # minimum number of instances in each node subsample = 0.8, # Subsample ratio of the training instances colsample_bytree = 1.0 # the fraction of columns to be subsampled ) set.seed(123) boost &lt;- xgb.cv( data = X, label = Y, nrounds = 3000, # the max number of iterations nthread = 4, # the number of CPU cores objective = &quot;reg:squarederror&quot;, # regression tree early_stopping_rounds = 50, # Stop if doesn&#39;t improve after 50 rounds nfold = 10, # 10-fold-CV params = params, verbose = 0 #silent ) Let’s see the RMSE and the best iteration: best_it &lt;- boost$best_iteration best_it ## [1] 1781 min(boost$evaluation_log$test_rmse_mean) ## [1] 16807.16 # One possible grid would be: # param_grid &lt;- expand.grid( # eta = 0.01, # max_depth = 3, # min_child_weight = 3, # subsample = 0.5, # colsample_bytree = 0.5, # gamma = c(0, 1, 10, 100, 1000), # lambda = seq(0, 0.01, 0.1, 1, 100, 1000), # alpha = c(0, 0.01, 0.1, 1, 100, 1000) # ) # After going through the grid in a loop with `xgb.cv` # we save multiple `test_rmse_mean` and `best_iteration` # and find the parameters that gives the minimum rmse Now after identifying the tuning parameters, we build the best model: tr_model &lt;- xgboost( params = params, data = X, label = Y, nrounds = best_it, objective = &quot;reg:squarederror&quot;, verbose = 0 ) We can obtain the top 10 influential features in our final model using the impurity (gain) metric: library(vip) vip(tr_model, aesthetics = list(color = &quot;green&quot;, fill = &quot;orange&quot;)) Now, we can use our trained model for predictions using our test set. Note that, again, xgboost would only accept matrix inputs. yhat &lt;- predict(tr_model, as.matrix(test[, -which(names(test) == &quot;Sale_Price&quot;)])) rmse_test &lt;- sqrt(mean((test[, which(names(train) == &quot;Sale_Price&quot;)] - yhat) ^ 2)) rmse_test ## [1] 23364.86 Note the big difference between training and test RMSPE’s. This is an indication that our “example grid” is not doing a good job. We should include regularization tuning parameters and run a full scale grid search. We will look at a classification example in the next chapter (Chapter 13). 17.4 Ensemble Applications To conclude this section we will cover classification and regression applications using bagging, random forest and, boosting. First we will start with a classification problem. In comparing different ensemble methods, we must look not only at their accuracy, but evaluate their stability as well. 17.5 Classification We will again predict survival on the Titanic, using CART, bagging and random forest. We will use the following variables: survived - 1 if true, 0 otherwise; sex - the gender of the passenger; age - age of the passenger in years; pclass - the passengers class of passage; sibsp - the number of siblings/spouses aboard; parch - the number of parents/children aboard. library(PASWR) library(ROCR) library(rpart) library(randomForest) # Data data(titanic3) nam &lt;- c(&quot;survived&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;pclass&quot;, &quot;sibsp&quot;, &quot;parch&quot;) df &lt;- titanic3[, nam] dfc &lt;- df[complete.cases(df), ] dfc$survived &lt;- as.factor(dfc$survived) AUC1 &lt;- c() AUC2 &lt;- c() AUC3 &lt;- c() n = 100 B = 100 for (i in 1:n) { set.seed(i+i*100) ind &lt;- sample(nrow(dfc), nrow(dfc), replace = TRUE) train &lt;- dfc[ind, ] test &lt;- dfc[-ind, ] p = ncol(train)-1 #3 Methods model1 &lt;- rpart(survived~sex+age+pclass+sibsp+parch, data=train, method=&quot;class&quot;) #Single tree, pruned model2 &lt;- randomForest(survived~sex+age+pclass+sibsp+parch, ntree = B, mtry = p, data = train) #Bagged model3 &lt;- randomForest(survived~sex+age+pclass+sibsp+parch, ntree = B, data = train, localImp = TRUE) # RF phat1 &lt;- predict(model1, test, type = &quot;prob&quot;) phat2 &lt;- predict(model2, test, type = &quot;prob&quot;) phat3 &lt;- predict(model3, test, type = &quot;prob&quot;) #AUC1 pred_rocr1 &lt;- prediction(phat1[,2], test$survived) auc_ROCR1 &lt;- performance(pred_rocr1, measure = &quot;auc&quot;) AUC1[i] &lt;- auc_ROCR1@y.values[[1]] #AUC2 pred_rocr2 &lt;- prediction(phat2[,2], test$survived) auc_ROCR2 &lt;- performance(pred_rocr2, measure = &quot;auc&quot;) AUC2[i] &lt;- auc_ROCR2@y.values[[1]] #AUC3 pred_rocr3 &lt;- prediction(phat3[,2], test$survived) auc_ROCR3 &lt;- performance(pred_rocr3, measure = &quot;auc&quot;) AUC3[i] &lt;- auc_ROCR3@y.values[[1]] } model &lt;- c(&quot;Single-Tree&quot;, &quot;Bagging&quot;, &quot;RF&quot;) AUCs &lt;- c(mean(AUC1), mean(AUC2), mean(AUC3)) sd &lt;- c(sqrt(var(AUC1)), sqrt(var(AUC2)), sqrt(var(AUC3))) data.frame(model, AUCs, sd) ## model AUCs sd ## 1 Single-Tree 0.8129740 0.02585391 ## 2 Bagging 0.8129736 0.01713075 ## 3 RF 0.8413922 0.01684504 There is a consensus that we can determine a bagged model’s test error without using cross-validation. We used randomForest for bagging in the previous application. By default, bagging grows classification trees to their maximal size. If we want to prune each tree, however, it is not clear whether or not this may decrease prediction error. Let’s see if we can obtain a similar result with our manual bagging using rpart pruned and unpruned: n &lt;- 100 B &lt;- 500 AUCp &lt;- c() AUCup &lt;- c() for (i in 1:n) { set.seed(i+i*100) ind &lt;- sample(nrow(dfc), nrow(dfc), replace = TRUE) train &lt;- dfc[ind, ] test &lt;- dfc[-ind, ] phatp &lt;- matrix(0, B, nrow(test)) phatup &lt;- matrix(0, B, nrow(test)) for (j in 1:B) { set.seed(j+j*2) ind &lt;- sample(nrow(train), nrow(train), replace = TRUE) tr &lt;- train[ind, ] modelp &lt;- rpart(survived ~ sex + age + pclass + sibsp + parch, data = tr, method = &quot;class&quot;) # Pruned modelup &lt;- rpart(survived ~ sex + age + pclass + sibsp + parch, data = tr, control = rpart.control(minsplit = 2, minbucket = 1 , cp = 0), method = &quot;class&quot;) # Unpruned phatp[j, ] &lt;- predict(modelp, test, type = &quot;prob&quot;)[, 2] phatup[j, ] &lt;- predict(modelup, test, type = &quot;prob&quot;)[, 2] } # Averaging for B Trees phatpr &lt;- apply(phatp, 2, mean) phatupr &lt;- apply(phatup, 2, mean) # AUC pruned pred_rocr &lt;- prediction(phatpr, test$survived) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUCp[i] &lt;- auc_ROCR@y.values[[1]] # AUC unpruned pred_rocr &lt;- prediction(phatupr, test$survived) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUCup[i] &lt;- auc_ROCR@y.values[[1]] } model &lt;- c(&quot;Pruned&quot;, &quot;Unpruned&quot;) AUCs &lt;- c(mean(AUCp), mean(AUCup)) sd &lt;- c(sqrt(var(AUCp)), sqrt(var(AUCup))) data.frame(model, AUCs, sd) ## model AUCs sd ## 1 Pruned 0.8523158 0.01626892 ## 2 Unpruned 0.8180811 0.01692990 We can see a significant reduction in uncertainty and improvement in accuracy relative to a single tree. When we use “unpruned” single-tree using rpart() for bagging, the result becomes very similar to one that we obtain with random forest. Using pruned trees for bagging improves the accuracy in our case. This would also be the case in regression trees, where we would be averaging yhat’s and calculating RMSPE and its standard deviations instead of AUC. 17.6 Regression Consider the data we used earlier chapters to predict baseball player’s salary: library(ISLR) remove(list = ls()) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] Let’s use only a single tree with bagging: library(rpart) # Data df$logsal &lt;- log(df$Salary) df &lt;- df[, -19] n = 100 B = 500 RMSPEp &lt;- c() RMSPEup &lt;- c() for (i in 1:n) { set.seed(i+i*8) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] yhatp &lt;- matrix(0, B, nrow(test)) yhatup &lt;- matrix(0, B, nrow(test)) for (j in 1:B) { set.seed(j+j*2) ind &lt;- sample(nrow(train), nrow(train), replace = TRUE) tr &lt;- train[ind, ] modelp &lt;- rpart(logsal ~ ., data = tr, method = &quot;anova&quot;) # Pruned modelup &lt;- rpart(logsal ~ ., data = tr, control = rpart.control(minsplit = 2, minbucket = 1 ,cp = 0), method = &quot;anova&quot;) # unpruned yhatp[j,] &lt;- predict(modelp, test) yhatup[j,] &lt;- predict(modelup, test) } # Averaging for B Trees yhatpr &lt;- apply(yhatp, 2, mean) yhatupr &lt;- apply(yhatup, 2, mean) RMSPEp[i] &lt;- sqrt(mean((test$logsal - yhatpr)^2)) RMSPEup[i] &lt;- sqrt(mean((test$logsal - yhatupr)^2)) } model &lt;- c(&quot;Pruned&quot;, &quot;Unpruned&quot;) RMSPEs &lt;- c(mean(RMSPEp), mean(RMSPEup)) sd &lt;- c(sqrt(var(RMSPEp)), sqrt(var(RMSPEup))) data.frame(model, RMSPEs, sd) ## model RMSPEs sd ## 1 Pruned 0.5019840 0.05817388 ## 2 Unpruned 0.4808079 0.06223845 With and without pruning, the results are very similar. Let’s put all these together and do it with Random Forest: library(randomForest) library(rpart) # Data remove(list = ls()) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] df$logsal &lt;- log(df$Salary) df &lt;- df[, -19] n &lt;- 100 B &lt;- 500 RMSPE1 &lt;- c() RMSPE2 &lt;- c() RMSPE3 &lt;- c() for (i in 1:n) { set.seed(i+i*8) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] p = ncol(train)-1 model1 &lt;- rpart(logsal~., data =train) # Single Tree model2 &lt;- randomForest(logsal~., ntree = B, mtry = p, data = train) #Bagged model3 &lt;- randomForest(logsal~., ntree = B, localImp = TRUE, data = train) # RF yhat1 &lt;- predict(model1, test) yhat2 &lt;- predict(model2, test) yhat3 &lt;- predict(model3, test) RMSPE1[i] &lt;- sqrt(mean((test$logsal - yhat1)^2)) RMSPE2[i] &lt;- sqrt(mean((test$logsal - yhat2)^2)) RMSPE3[i] &lt;- sqrt(mean((test$logsal - yhat3)^2)) } model &lt;- c(&quot;Single-Tree&quot;, &quot;Bagging&quot;, &quot;RF&quot;) RMSPEs &lt;- c(mean(RMSPE1), mean(RMSPE2), mean(RMSPE3)) sd &lt;- c(sqrt(var(RMSPE1)), sqrt(var(RMSPE2)), sqrt(var(RMSPE3))) data.frame(model, RMSPEs, sd) ## model RMSPEs sd ## 1 Single-Tree 0.5739631 0.05360920 ## 2 Bagging 0.4807307 0.06122742 ## 3 RF 0.4631913 0.06038268 Random forest has the lowest RMSPE. 17.7 Exploration While the task in machine learning is to achieve the best predictive capacity, for many applications identifying the major predictors could be the major objective. Of course, finding the most important predictors is contingent on the model’s predictive performance. As we discussed earlier, however, there is a trade-off between prediction accuracy and interpretability. Although there are many different aspects of interpretability, it refer to understanding the relationship between the predicted outcome and the predictors. The interpretability in predictive modeling is an active research area. Two excellent books on the subject provide much needed comprehensive information about the interpretability and explanatory analysis in machine learning: Interpretable Machine Learning by Christoph Molnar and Explanatory Model Analysis by Biecek and Burzykowski (2020). Explorations of predictive models are classified in two major groups. The first one is the instance-level exploration, or example-based explanation methods, which present methods for exploration of a model’s predictions for a single observation. For example, for a particular subject (person, firm, patient), we may want to know contribution of the different features to the predicted outcome for the subject. The main idea is to understand marginal effect of a predictor on the prediction for a specific subject. There are two important methods in this level: Shapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME). We will not explain and apply them here in this book. These two methods are easily accessible with multiple examples in both books we cited ealrier. The second group of explanation methods focuses on dataset-level explainers, which help understand the average behavior of a machine learning model for an entire set of observations. Here, we will focus on several variable-importance measures. They are permutation-based variable importance metrics offering a model-agnostic approach to the assessment of the influence of an explanatory variable on a model’s performance. There are several options to evaluate how important is the variable \\(x\\) in predictions. One major method is the permutation-based variable-importance in which the effect of a variable is removed through a random reshuffling of the data in \\(x\\). This method takes the original data under \\(x\\), permutates (mixes) its values, and gets “new” data, on which computes the weighted decrease of accuracy corresponding to splits along the variable \\(x\\) and averages this quantity over all trees. If a variable is an important predictor in the model, after its permutation, the mean decrease accuracy (MDA) rises. It stems from the idea that if the variable is not important, rearranging its values should not degrade prediction accuracy. The MDA relies on a different principle and uses the out-of-bag error estimate. Every tree in the forest has its own out-of-bag sample, on which the prediction accuracy is measured. To calculate MDA, the values of the variable in the out-of-bag-sample are randomly shuffled and the decrease in prediction accuracy on the shuffled data is measured. This process is repeated for all variables and trees. The MDA averaged over all trees is ranked. If a variable has insignificant predictive power, shuffling may not lead to substantial decrease in accuracy. It is shown that building a tree with additional irrelevant variables does not alter the importance of relevant variables in an infinite sample setting. Another measure of significance is Mean Decrease Impurity (MDI). It is not permutation-based; instead, it is based on the impurity decrease attributable to a particular feature during the construction of the decision trees that make up the random forest. In a Random Forest model, multiple decision trees are built using a random subset of features and a random subset of the training dataset. Each decision tree is constructed through a process called recursive binary splitting, where the best split for a particular node is determined by maximizing the impurity decrease. Impurity is a measure of how well the samples at a node in the decision tree are classified. Common impurity measures include Gini impurity and entropy. The impurity decrease is calculated by comparing the impurity of the parent node with the weighted average impurity of the child nodes. For each feature, the impurity decrease is calculated at every split where the feature is used. The impurity decreases are then summed up across all the trees in the random forest for that feature. The sum of the impurity decreases is then normalized by the total sum of the impurity decreases across all the features to calculate the MDI value for each feature. The MDI values represent the average contribution of a feature to the decrease in impurity across all the trees in the random forest. A higher MDI value for a feature indicates that it is more important for making accurate predictions, while a lower value indicates a less important feature. In contrast, permutation-based feature importance, such as Mean Decrease in Accuracy (MDA), measures the impact of a feature on model performance by randomly permuting the feature’s values and evaluating the change in model accuracy. This approach provides an estimate of the importance of a feature by assessing the performance drop when the feature’s information is removed or disrupted. For a numeric outcome (regression problem) there are two similar measures. The percentage increase in mean square error (%IncMSE), which is calculated by shuffling the values of the out-of-bag samples, is analogous to MDA. Increase in node purity (IncNodePurity), which is calculated based on the reduction in sum of squared errors whenever a variable is chosen to split is, analogous to MDI. Here are the variable importance measures for our random forest application (model3): library(randomForest) varImpPlot(model3) And, the partial dependence plot gives a graphical representation of the marginal effect of a variable on the class probability (classification) or response (regression). The intuition behind it is simple: change the value of a predictor and see how much the prediction will change (log wage in our example). partialPlot(model3, test, CRuns, xlab=&quot;CRuns&quot;, main=&quot;Effects of CRuns&quot;, col = &quot;red&quot;, lwd = 3) Partial dependence plots (PDPs) are a graphical tool used to visualize the relationship between a single feature and the predicted outcome in a machine learning model, while averaging out the effects of all other features. For each unique value of the chosen feature, the algorithm fixes the value and keeps all other feature values unchanged. Then, the modified dataset with the fixed feature value is used by the Random Forest model to obtain predictions for each instance. We compute the average prediction across all instances for the fixed feature value. This represents the partial dependence of the outcome on the chosen feature value. We perform these steps for all unique values of the chosen feature, and obtain the partial dependence values for each feature value. A plot with the chosen feature values on the x-axis and the corresponding partial dependence values on the y-axis is the Partial dependence plot. The resulting partial dependence plot illustrates the relationship between the chosen feature and the model’s predictions, while accounting for the average effect of all other features. The plot helps to identify the direction (positive or negative) and strength of the relationship, as well as any non-linear patterns or interactions with other features. Keep in mind that partial dependence plots are most useful for understanding the effects of individual features in isolation, and they may not capture the full complexity of the model if there are strong interactions between features. There are several libraries that we can use to improve presentation of permutation-based variable importance metrics: the randomForestExplainer package (see its vignette) (Palu_2012?) and.the DALEX packages. library(randomForestExplainer) importance_frame &lt;- measure_importance(model3) importance_frame ## variable mean_min_depth no_of_nodes mse_increase node_purity_increase ## 1 Assists 4.385264 2351 0.0111643040 2.0354183 ## 2 AtBat 2.880632 2691 0.0823060539 9.8976694 ## 3 CAtBat 2.378316 2598 0.2180919045 38.3175006 ## 4 CHits 2.254316 2711 0.2219603757 34.6913645 ## 5 CHmRun 3.444948 2556 0.0465389503 6.5334618 ## 6 CRBI 2.826000 2752 0.1037441042 19.5413640 ## 7 CRuns 2.076316 2731 0.2415297175 35.0893626 ## 8 CWalks 3.090316 2579 0.0842675407 18.0455320 ## 9 Division 7.025920 691 0.0009003443 0.2610306 ## 10 Errors 4.626844 2080 0.0091803849 1.2750433 ## 11 Hits 3.086316 2582 0.0891232078 9.3889994 ## 12 HmRun 4.019580 2229 0.0229235515 3.5544146 ## 13 League 7.723940 442 0.0007442309 0.1574101 ## 14 NewLeague 7.097292 627 0.0012483369 0.2430058 ## 15 PutOuts 3.654632 2593 0.0174281111 3.9026093 ## 16 RBI 3.486948 2620 0.0406771125 6.9162313 ## 17 Runs 3.518948 2543 0.0515670394 5.8962241 ## 18 Walks 3.532316 2576 0.0397964535 5.9405180 ## 19 Years 4.597688 1716 0.0246697278 5.5647402 ## no_of_trees times_a_root p_value ## 1 496 0 3.136068e-04 ## 2 498 5 2.277643e-26 ## 3 499 133 2.885642e-18 ## 4 499 110 2.632589e-28 ## 5 497 7 4.203385e-15 ## 6 500 55 1.727502e-32 ## 7 499 101 2.602255e-30 ## 8 499 52 8.510193e-17 ## 9 380 0 1.000000e+00 ## 10 491 0 9.939409e-01 ## 11 499 7 5.036363e-17 ## 12 495 0 2.179972e-01 ## 13 285 0 1.000000e+00 ## 14 363 0 1.000000e+00 ## 15 498 0 7.131388e-18 ## 16 497 7 4.777556e-20 ## 17 497 1 3.461522e-14 ## 18 499 0 1.432750e-16 ## 19 482 22 1.000000e+00 This table shows few more metrics in addition to mse_increase and node_purity_increase. The first column, mean_min_depth, the average of the first time this variable is used to split the tree. Therefore, more important variables have lower minimum depth values. The metric no_of_nodes shows the total number of nodes that use for splitting. Finally, times_a_root shows how many times the split occurs at the root. The last column, p_value for the one-sided binomial test, which tells us whether the observed number of of nodes in which the variable was used for splitting exceeds the theoretical number of successes if they were random. We can take advantage of several multidimensional plots from the randomForestExplainer package: plot_multi_way_importance(importance_frame, x_measure = &quot;mean_min_depth&quot;, y_measure = &quot;mse_increase&quot;, size_measure = &quot;p_value&quot;, no_of_labels = 6) min_depth_frame &lt;- min_depth_distribution(model3) plot_min_depth_distribution(min_depth_frame, mean_sample = &quot;all_trees&quot;, k =20, main = &quot;Distribution of minimal depth and its mean&quot;) 17.8 Boosting Applications We need to tune the boosting applications with gbm(). There are two groups of tuning parameters: boosting parameters and tree parameters. Boosting parameters: The number iterations (n.trees = 100) and learning rate (shrinkage = 0.1). Tree parameters: The maximum depth of each tree (interaction.depth = 1) and the minimum number of observations in the terminal nodes of the trees (n.minobsinnode = 10) The gbm algorithm offers three tuning options internally to select the best iteration: OOB, test, and cv.fold. The test uses a single holdout test set to select the optimal number of iterations. It’s regulated by train.fraction, which creates a test set by train.fraction × nrow(data). This is not a cross validation but could be used with multiple loops running externally. The k-fold cross validation is regulated by cv.fold that canbe used to find the optimal number of iterations. For example, if cv.folds=5 then gbm fits five gbm models to compute the cross validation error. The using the best (average iterations) it fits a sixth and final gbm model using all of the data. The cv.error reported this final model will determined the the best iteration. Finally, there is one parameter, bag.fraction, the fraction of the training set observations randomly selected to propose the next tree in the expansion. This introduces randomnesses into the model fit, hence, reduces overfitting possibilities. The “improvements” the error (prediected errors) in each iterations is reported by oobag.improve. Below, we show these three methods to identify the best iteration library(ISLR) library(gbm) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] df$Salary &lt;- log(df$Salary) model_cv &lt;- gbm(Salary~., distribution =&quot;gaussian&quot;, n.trees=1000, interaction.depth = 3, shrinkage = 0.01, data = df, bag.fraction = 0.5, cv.folds = 5) best &lt;- which.min(model_cv$cv.error) sqrt(model_cv$cv.error[best]) ## [1] 0.4666599 # or this can be obtained gbm.perf(model_cv, method=&quot;cv&quot;) ## [1] 988 The following method can be combined with an external loops that runs several times, for example. model_test &lt;- gbm(Salary~., distribution =&quot;gaussian&quot;, n.trees=1000, interaction.depth = 3, shrinkage = 0.01, data = df, bag.fraction = 0.5, train.fraction = 0.8) gbm.perf(model_test, method=&quot;test&quot;) ## [1] 999 which.min(model_test$valid.error) ## [1] 999 min(model_test$valid.error) ## [1] 0.303144 The OOB is option is not suggested for model selection (see A guide to the gbm package. The bag.fraction, however, can be used to reduce overfitting as the fraction of the training set observations randomly selected to propose the next tree in the expansion. This introduces randomnesses into the model fit, hence, reduces overfitting. We can also override all the internal process and apply our own grid search. Below, we show several examples. We should also note that the gbm function uses parallel processing in iterations. 17.8.1 Regression This will give you an idea how tuning the boosting by using h would be done: # Test/Train Split set.seed(1) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] h &lt;- seq(0.01, 1.8, 0.01) test_mse &lt;- c() # D = 1 and B = 1000 for(i in 1:length(h)){ boos &lt;- gbm(Salary~., distribution = &quot;gaussian&quot;, n.trees = 1000, interaction.depth = 1, shrinkage = h[i], data = train) prboos &lt;- predict(boos, test, n.trees = 100) test_mse[i] &lt;- mean((test$Salary - prboos) ^ 2) } plot(h, test_mse, type = &quot;l&quot;, col = &quot;blue&quot;, main = &quot;MSE - Prediction&quot;) h[which.min(test_mse)] ## [1] 0.08 min(test_mse) ## [1] 0.181286 A complete but limited grid search is here: library(gbm) h &lt;- seq(0.01, 0.3, 0.01) B &lt;- c(100, 300, 500, 750, 900) D &lt;- 1:2 grid &lt;- as.matrix(expand.grid(D, B, h)) mse &lt;-c() sdmse &lt;-c() for(i in 1:nrow(grid)){ test_mse &lt;- c() for (j in 1:20) { try({ set.seed(j) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] boos &lt;- gbm(Salary~., distribution =&quot;gaussian&quot;, n.trees = 1000, interaction.depth = grid[i,1], shrinkage = grid[i,3], data = train) prboos &lt;- predict(boos, test, n.trees = grid[i,2]) test_mse[j] &lt;- mean((test$Salary - prboos) ^ 2) }, silent = TRUE) } mse[i] &lt;- mean(test_mse) sdmse[i] &lt;- sd(test_mse) } min(mse) ## [1] 0.2108654 grid[as.numeric(which.min(mse)), ] ## Var1 Var2 Var3 ## 2e+00 9e+02 1e-02 17.8.2 Random search with parallel processing Now, we will apply a random grid search introduced by Bergstra and Bengio in Random Search for Hyper-Parameter Optimization) (Bergs_2012?). This paper shows that randomly chosen trials are more efficient for hyperparameter optimization than trials on a grid. Random search is a slight variation on grid search. Instead of searching over the entire grid, random search evaluates randomly selected parts on the grid. To characterize the performance of random search, the authors use the analytic form of the expectation. The expected probability of finding the target is \\(1.0\\) minus the probability of missing the target with every single one of \\(T\\) trials in the experiment. If the volume of the target relative to the unit hypercube is \\((v / V=0.01)\\) and there are \\(T\\) trials, then this probability of finding the target is \\[ 1-\\left(1-\\frac{v}{V}\\right)^T=1-0.99^T . \\] In more practical terms, for any distribution over a sample space with a maximum, we can find the number of randomly selected points from the grid. First, we define the confidence level, say 95%. Then we decide how many points we wish to have around the maximum. We can decide as a number or directly as a percentage. Let’s say we decide 0.01% interval around the maximum. Then the formula will be \\[ 1-(1-0.01)^T&gt;0.95, \\] which can be solved as \\[ \\text{T} = \\log (1-0.95)/\\log (1-0.01) \\] We also apply a parallel multicore processing using doParallel and foreach() to accelerate the grid search. More details can be found at Getting Started with doParallel and foreach. library(gbm) library(doParallel) library(foreach) h &lt;- seq(0.001, 0.25, 0.001) B &lt;- seq(100, 800, 20) D &lt;- 1:4 grid &lt;- as.matrix(expand.grid(D, B, h)) #Random grid-search conf_lev &lt;- 0.95 num_max &lt;- 5 # we define it by numbers n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) set.seed(123) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) comb &lt;- grid[ind, ] # Set-up for multicore loops trials &lt;- 1:nrow(comb) numCores &lt;- detectCores() registerDoParallel(numCores) # Bootstrapping with parallel process lst &lt;- foreach(k=trials, .combine=c, .errorhandling = &#39;remove&#39;) %dopar% { test_mse &lt;- c() for (j in 1:10) { try({ set.seed(j) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] boos &lt;- gbm(Salary~., distribution =&quot;gaussian&quot;, n.trees=1000, interaction.depth =comb[k,1], shrinkage = comb[k,3], data = train) prboos &lt;- predict(boos, test, n.trees = comb[k,2]) test_mse[j] &lt;- mean((test$Salary - prboos)^2) }, silent = TRUE) } list(c(k, mean(test_mse), sd(test_mse))) } stopImplicitCluster() unlst &lt;- do.call(rbind, lst) result &lt;- cbind(comb[unlst[,1],], unlst) sorted &lt;- result[order(result[,5]), -4] colnames(sorted) &lt;- c(&quot;D&quot;, &quot;B&quot;, &quot;h&quot;, &quot;MSPE&quot;, &quot;sd&quot;) head(sorted) ## D B h MSPE sd ## [1,] 2 360 0.024 0.2057671 0.05657079 ## [2,] 2 300 0.024 0.2060013 0.05807494 ## [3,] 2 340 0.022 0.2061847 0.05827857 ## [4,] 2 340 0.023 0.2061895 0.05823719 ## [5,] 2 320 0.023 0.2062056 0.05874694 ## [6,] 2 360 0.021 0.2062124 0.05785775 You can increase for (j in 1:10) to for (j in 1:50) depending on your computer’s capacity. 17.8.3 Boosting vs. Others Let’s add OLS to this competition just for curiosity. Here is a one possible script: library(ISLR) library(randomForest) library(rpart) df &lt;- Hitters[complete.cases(Hitters$Salary), ] df$Salary &lt;- log(df$Salary) # Containers mse_cart &lt;- c(0) mse_bag &lt;- c(0) mse_rf &lt;- c(0) mse_boost &lt;- c(0) mse_ols &lt;- c(0) for(i in 1:200){ set.seed(i) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] OLS &lt;- lm(Salary~., data = train) pols &lt;- predict(OLS, test) cart &lt;- rpart(Salary~., data = train) pcart &lt;- predict(cart, test) bags &lt;- randomForest(Salary ~., mtry = 19, data = train) pbag &lt;- predict(bags, test) rf &lt;- randomForest(Salary ~., data = train) prf &lt;- predict(rf, test) boost &lt;- gbm(Salary~., distribution =&quot;gaussian&quot;, n.trees = 1000, data = train) # without a grid search pboost &lt;- predict(boost, test, n.trees = 100) mse_ols[i] &lt;- mean((test$Salary - pols)^2) mse_cart[i] &lt;- mean((test$Salary - pcart)^2) mse_bag[i] &lt;- mean((test$Salary - pbag)^2) mse_rf[i] &lt;- mean((test$Salary - prf)^2) mse_boost[i] &lt;- mean((test$Salary - pboost)^2) } # Bootstrapping Results a &lt;- matrix(c(mean(mse_cart), mean(mse_bag), mean(mse_rf), mean(mse_boost), mean(mse_ols)), 5, 1) row.names(a) &lt;- c(&quot;mse_cart&quot;, &quot;mse_bag&quot;, &quot;mse_rf&quot;, &quot;mse_boost&quot;, &quot;mse_ols&quot;) a ## [,1] ## mse_cart 0.3172687 ## mse_bag 0.2206152 ## mse_rf 0.2057663 ## mse_boost 0.2451800 ## mse_ols 0.4584240 b &lt;- matrix(c(sqrt(var(mse_cart)), sqrt(var(mse_bag)), sqrt(var(mse_rf)), sqrt(var(mse_boost)), sqrt(var(mse_ols))), 5, 1) row.names(b) &lt;- c(&quot;mse_cart&quot;, &quot;mse_bag&quot;, &quot;mse_rf&quot;, &quot;mse_boost&quot;, &quot;mse_ols&quot;) b ## [,1] ## mse_cart 0.07308726 ## mse_bag 0.06279604 ## mse_rf 0.05981292 ## mse_boost 0.05929530 ## mse_ols 0.06907506 The random forest and boosting have similar performances. However, boosting and is not tuned in the algorithm. With the full grid search in the previous algorithm, boosting would be a better choice. Let’s have a classification example. 17.8.4 Classification A simulated data set containing sales of child car seats at 400 different stores from. We will predict the sale, a binary variable that will be 1 if the sale is higher than 8. See ISLR (ISLR_Car?) for the details. library(ISLR) df &lt;- Carseats str(df) ## &#39;data.frame&#39;: 400 obs. of 11 variables: ## $ Sales : num 9.5 11.22 10.06 7.4 4.15 ... ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Medium&quot;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 1 2 1 2 1 2 ... #Change SALES to a factor variable df$Sales &lt;- ifelse(Carseats$Sales &lt;= 8, 0, 1) str(df$Sales) ## num [1:400] 1 1 1 0 0 1 0 1 0 0 ... library(PASWR) library(ROCR) library(rpart) library(randomForest) df &lt;- df[complete.cases(df), ] df$d &lt;- as.factor(df$Sales) n &lt;- 50 B &lt;- 1000 AUC1 &lt;- c() AUC2 &lt;- c() AUC3 &lt;- c() AUC4 &lt;- c() for (i in 1:n) { set.seed(i) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] p = ncol(train)-1 # We used two different outcome structure: &quot;Sales&quot; and &quot;d&quot; # &quot;d&quot; is a factor and &quot;Sales&quot; is numeric # Factor variable is necessary for RF but GBM needs a numeric variable # That&#39;s sometimes annoying but wee need to be careful about the data model1 &lt;- rpart(Sales~., data=train[,-12], method = &quot;class&quot;) model2 &lt;- randomForest(d~., ntree = B, mtry = p, data = train[, -1]) #Bagged model3 &lt;- randomForest(d~., ntree = B, data = train[, -1]) # RF model4 &lt;- gbm(Sales~., data=train[,-12], n.trees = B, distribution = &quot;bernoulli&quot;) # Boosting without grid search phat1 &lt;- predict(model1, test[,-12], type = &quot;prob&quot;) phat2 &lt;- predict(model2, test[,-1], type = &quot;prob&quot;) phat3 &lt;- predict(model3, test[,-1], type = &quot;prob&quot;) phat4 &lt;- predict(model4, n.trees = B, test[,-12], type = &quot;response&quot;) #AUC1 pred_rocr1 &lt;- prediction(phat1[,2], test$Sales) auc_ROCR1 &lt;- performance(pred_rocr1, measure = &quot;auc&quot;) AUC1[i] &lt;- auc_ROCR1@y.values[[1]] #AUC2 pred_rocr2 &lt;- prediction(phat2[,2], test$d) auc_ROCR2 &lt;- performance(pred_rocr2, measure = &quot;auc&quot;) AUC2[i] &lt;- auc_ROCR2@y.values[[1]] #AUC3 pred_rocr3 &lt;- prediction(phat3[,2], test$d) auc_ROCR3 &lt;- performance(pred_rocr3, measure = &quot;auc&quot;) AUC3[i] &lt;- auc_ROCR3@y.values[[1]] #AUC4 pred_rocr4 &lt;- prediction(phat4, test$Sales) auc_ROCR4 &lt;- performance(pred_rocr4, measure = &quot;auc&quot;) AUC4[i] &lt;- auc_ROCR4@y.values[[1]] } model &lt;- c(&quot;Single-Tree&quot;, &quot;Bagging&quot;, &quot;RF&quot;, &quot;Boosting&quot;) AUCs &lt;- c(mean(AUC1), mean(AUC2), mean(AUC3), mean(AUC4)) sd &lt;- c(sqrt(var(AUC1)), sqrt(var(AUC2)), sqrt(var(AUC3)), sqrt(var(AUC4))) data.frame(model, AUCs, sd) ## model AUCs sd ## 1 Single-Tree 0.7607756 0.03203628 ## 2 Bagging 0.8642527 0.02674844 ## 3 RF 0.8777902 0.02393084 ## 4 Boosting 0.9176397 0.01742109 The results are very telling: booster is a clear winner for prediction accuracy and stability. When we have these machine learning applications, one should always show the “baseline” prediction that we can judge the winner performance: A simple LPM would be a good baseline model: AUC5 &lt;- c() for (i in 1:100) { set.seed(i) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] model &lt;- lm(Sales ~ ., data= train[,-12]) phat5 &lt;- predict(model, test[, -12]) pred_rocr5 &lt;- prediction(phat5, test$Sales) auc_ROCR5 &lt;- performance(pred_rocr5, measure = &quot;auc&quot;) AUC5[i] &lt;- auc_ROCR5@y.values[[1]] } mean(AUC5) ## [1] 0.9546986 sqrt(var(AUC5)) ## [1] 0.0117673 I choose this example to show that we cannot assume that our complex algorithms will always be better than a simple OLS. We judge the success of prediction not only its own AUC and stability, but also how much it improves over a benchmark. 17.8.5 AdaBoost.M1 Let’s apply AdaBoost to our example to see if we can have any improvements library(JOUSBoost) library(ISLR) df &lt;- Carseats #Change SALES to a factor variable df$Sales &lt;- ifelse(Carseats$Sales &lt;= 8, -1, 1) #adaboost requires -1,1 coding str(df$Sales) ## num [1:400] 1 1 1 -1 -1 1 -1 1 -1 -1 ... # adaboost requires X as a matrix # so factor variables must be coded as numerical # With `one-hot()` library(mltools) library(data.table) df_new &lt;- one_hot(as.data.table(df)) Now, we are ready: rnd = seq(100, 500, 50) MAUC &lt;- c() for (r in 1:length(rnd)) { AUC &lt;- c() for (i in 1:20) { set.seed(i) ind &lt;- sample(nrow(df_new), nrow(df_new), replace = TRUE) train &lt;- df_new[ind,] test &lt;- df_new[-ind,] ada &lt;- adaboost(as.matrix(train[, -&quot;Sales&quot;]), train$Sales, tree_depth = 1, n_rounds = rnd[r]) phat &lt;- predict(ada, test, type = &quot;prob&quot;) pred_rocr &lt;- prediction(phat, test$Sales) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUC[i] &lt;- auc_ROCR@y.values[[1]] } MAUC[r] &lt;- mean(AUC) } mean(MAUC) ## [1] 0.9218254 sqrt(var(MAUC)) ## [1] 0.001441398 It’s slightly better than the gradient boosting (gbm) but not much from LPM. We should apply a better grid for the rounds of iterations. 17.8.6 Classification with XGBoost Before jumping into an example, let’s first understand about the most frequently used hyperparameters in xgboost. You can refer to its official documentation for more details. We will classify them in three groups: Booster type: Booster = gbtree is the default. It could be set to gblinear or dart. The first one uses a linear model and the second one refers to Dropout Additive Regression Trees. When constructing a gradient boosting machine, the first few trees at the beginning dominate the model performance relative to trees added later. Thus, the idea of “dropout” is to build an ensemble by randomly dropping trees in the boosting sequence. Tuning parameters (note that when gblinear is used, only nround, lambda, and alpha are used): nrounds = 100 (default). It controls the maximum number of iterations (or trees for classification). eta = 0.3. It controls the learning rate. Typically, it lies between 0.01 - 0.3. gamma = 0. It controls regularization (or prevents overfitting - a higher difference between the train and test prediction performance). It can be used as it brings improvements when shallow (low max_depth) trees are employed. max_depth = 6. It controls the depth of the tree. min_child_weight = 1. It blocks the potential feature interactions to prevent overfitting. (The minimum number of instances required in a child node.) subsample = 1. It controls the number of observations supplied to a tree. Generally, it lies between 0.01 - 0.3. (remember bagging). colsample_bytree = 1. It control the number of features (variables) supplied to a tree. Both subsample and colsample_bytree can be use to build a “random forest” type learner. lambda = 0, equivalent to Ridge regression alpha = 1, equivalent to Lasso regression (more useful on high dimensional data sets). When both are set different than zero, it becomes an “Elastic Net”, which we will see later. Evaluation parameters: objective = “reg:squarederror” for linear regression, “binary:logistic” binary classification (it returns class probabilities). See the official guide for more options. eval_metric = no default. Depending on objective selected, it could be one of those: mae, Logloss, AUC, RMSE, error - (#wrong cases/#all cases), mlogloss - multiclass. Before executing a full-scale grid search, see what default parameters provide you. That’s your “base” model’s prediction accuracy, which can improve from. If the result is not giving you a desired accuracy, as we did in Chapter 13.3.3, set eta = 0.1 and the other parameters at their default values. Using xgb.cv function get best n_rounds and build a model with these parameters. See how much improvement you will get in its accuracy. Then apply the full-scale grid search. We will use the same data (“Adult”) as we used in Chapter 11. library(xgboost) library(mltools) library(data.table) train &lt;- read.csv(&quot;adult_train.csv&quot;, header = FALSE) varNames &lt;- c(&quot;Age&quot;, &quot;WorkClass&quot;, &quot;fnlwgt&quot;, &quot;Education&quot;, &quot;EducationNum&quot;, &quot;MaritalStatus&quot;, &quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;CapitalGain&quot;, &quot;CapitalLoss&quot;, &quot;HoursPerWeek&quot;, &quot;NativeCountry&quot;, &quot;IncomeLevel&quot;) names(train) &lt;- varNames data &lt;- train tbl &lt;- table(data$IncomeLevel) tbl ## ## &lt;=50K &gt;50K ## 24720 7841 # we remove some outliers - See Ch.11 ind &lt;- which(data$NativeCountry==&quot; Holand-Netherlands&quot;) data &lt;- data[-ind, ] #Converting chr to factor with `apply()` family df &lt;- data df[sapply(df, is.character)] &lt;- lapply(df[sapply(df, is.character)], as.factor) str(df) ## &#39;data.frame&#39;: 32560 obs. of 15 variables: ## $ Age : int 39 50 38 53 28 37 49 52 31 42 ... ## $ WorkClass : Factor w/ 9 levels &quot; ?&quot;,&quot; Federal-gov&quot;,..: 8 7 5 5 5 5 5 7 5 5 ... ## $ fnlwgt : int 77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ... ## $ Education : Factor w/ 16 levels &quot; 10th&quot;,&quot; 11th&quot;,..: 10 10 12 2 10 13 7 12 13 10 ... ## $ EducationNum : int 13 13 9 7 13 14 5 9 14 13 ... ## $ MaritalStatus: Factor w/ 7 levels &quot; Divorced&quot;,&quot; Married-AF-spouse&quot;,..: 5 3 1 3 3 3 4 3 5 3 ... ## $ Occupation : Factor w/ 15 levels &quot; ?&quot;,&quot; Adm-clerical&quot;,..: 2 5 7 7 11 5 9 5 11 5 ... ## $ Relationship : Factor w/ 6 levels &quot; Husband&quot;,&quot; Not-in-family&quot;,..: 2 1 2 1 6 6 2 1 2 1 ... ## $ Race : Factor w/ 5 levels &quot; Amer-Indian-Eskimo&quot;,..: 5 5 5 3 3 5 3 5 5 5 ... ## $ Sex : Factor w/ 2 levels &quot; Female&quot;,&quot; Male&quot;: 2 2 2 2 1 1 1 2 1 2 ... ## $ CapitalGain : int 2174 0 0 0 0 0 0 0 14084 5178 ... ## $ CapitalLoss : int 0 0 0 0 0 0 0 0 0 0 ... ## $ HoursPerWeek : int 40 13 40 40 40 40 16 45 50 40 ... ## $ NativeCountry: Factor w/ 41 levels &quot; ?&quot;,&quot; Cambodia&quot;,..: 39 39 39 39 6 39 23 39 39 39 ... ## $ IncomeLevel : Factor w/ 2 levels &quot; &lt;=50K&quot;,&quot; &gt;50K&quot;: 1 1 1 1 1 1 1 2 2 2 ... As required by the xgboost package, we need a numeric \\(Y\\) and all the factor variables have to be one-hot coded df$Y &lt;- ifelse(data$IncomeLevel==&quot; &lt;=50K&quot;, 0, 1) #Remove `IncomeLevel` df &lt;- df[, -15] anyNA(df) # no NA&#39;s ## [1] FALSE # Initial Split 90-10% split set.seed(321) ind &lt;- sample(nrow(df), nrow(df)*0.90, replace = FALSE) train &lt;- df[ind, ] test &lt;- df[-ind, ] # One-hot coding using R&#39;s `model.matrix` ty &lt;- train$Y tsy &lt;- test$Y hot_tr &lt;- model.matrix(~.+0, data = train[,-which(names(train) == &quot;Y&quot;)]) hot_ts &lt;- model.matrix(~.+0, data = test[,-which(names(train) == &quot;Y&quot;)]) # Preparing efficient matrix ttrain &lt;- xgb.DMatrix(data = hot_tr, label = ty) ttest &lt;- xgb.DMatrix(data = hot_ts, label = tsy) Now we are ready to set our first xgb.sv with default parameters params &lt;- list(booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot; ) set.seed(112) cvb &lt;- xgb.cv( params = params, nrounds = 100, data = ttrain, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F ) ## [1] train-logloss:0.541285+0.000640 test-logloss:0.542411+0.001768 ## Multiple eval metrics are present. Will use test_logloss for early stopping. ## Will train until test_logloss hasn&#39;t improved in 20 rounds. ## ## [11] train-logloss:0.290701+0.000486 test-logloss:0.302696+0.003658 ## [21] train-logloss:0.264326+0.000814 test-logloss:0.285655+0.004132 ## [31] train-logloss:0.251203+0.001082 test-logloss:0.280880+0.004269 ## [41] train-logloss:0.243382+0.001291 test-logloss:0.279297+0.004772 ## [51] train-logloss:0.237065+0.001390 test-logloss:0.278460+0.004780 ## [61] train-logloss:0.230541+0.001288 test-logloss:0.278528+0.004913 ## [71] train-logloss:0.225721+0.001117 test-logloss:0.279118+0.005197 ## Stopping. Best iteration: ## [59] train-logloss:0.231852+0.000732 test-logloss:0.278273+0.004699 theb &lt;- cvb$best_iteration theb ## [1] 59 model_default &lt;- xgb.train (params = params, data = ttrain, nrounds = theb, watchlist = list(val=ttest,train=ttrain), print_every_n = 10, maximize = F , eval_metric = &quot;auc&quot;) ## [1] val-auc:0.898067 train-auc:0.895080 ## [11] val-auc:0.922919 train-auc:0.925884 ## [21] val-auc:0.927905 train-auc:0.936823 ## [31] val-auc:0.928464 train-auc:0.942277 ## [41] val-auc:0.929252 train-auc:0.946379 ## [51] val-auc:0.928459 train-auc:0.949633 ## [59] val-auc:0.928224 train-auc:0.951403 And the prediction: phat &lt;- predict (model_default, ttest) # AUC library(ROCR) pred_rocr &lt;- prediction(phat, tsy) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.9282243 # ROCR perf &lt;- performance(pred_rocr,&quot;tpr&quot;,&quot;fpr&quot;) plot(perf, colorize=TRUE) abline(a = 0, b = 1) You can go back to Chapter 11.3.2 and see that XGBoost is better than kNN in this example without even a proper grid search. https://web.archive.org/web/20121010030839/http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf↩︎ In a sparse matrix, cells containing 0 are not stored in memory. Therefore, in a dataset mainly made of 0, the memory size is reduced.↩︎ "],["causal-effect-1.html", "Chapter 18 Causal Effect 18.1 Random experiment 18.2 IV 18.3 DiffD 18.4 RD 18.5 Synthetic control 18.6 Double/Debiased Lasso/Methods", " Chapter 18 Causal Effect via Potential outcome Framework 18.1 Random experiment 18.2 IV 18.3 DiffD 18.4 RD 18.5 Synthetic control 18.6 Double/Debiased Lasso/Methods TBC "],["heterogeneous-treatment-effects.html", "Chapter 19 Heterogeneous Treatment Effects 19.1 Causal Tree 19.2 Causal Forest", " Chapter 19 Heterogeneous Treatment Effects 19.1 Causal Tree 19.2 Causal Forest "],["model-selection-and-sparsity.html", "Chapter 20 Model selection and Sparsity 20.1 Model selection 20.2 Dropping a variable in a regression 20.3 out-sample prediction accuracy 20.4 Sparsity", " Chapter 20 Model selection and Sparsity 20.1 Model selection 20.2 Dropping a variable in a regression We can assume that the outcome \\(y_i\\) is determined by the following function: \\[ y_{i}=\\beta_0+\\beta_1 x_{i}+\\varepsilon_{i}, ~~~~ i=1, \\ldots, n \\] where \\(\\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right)\\), \\(\\mathbf{Cov}\\left(\\varepsilon_{i}, \\varepsilon_{j}\\right)=0\\) for \\(i\\neq j.\\) Although unrealistic, for now we assume that \\(x_i\\) is fixed (non-stochastic) for simplicity in notations. That means in each sample we have same \\(x_i\\). We can write this function as \\[ y_{i}=f(x_i)+\\varepsilon_{i}, ~~~~ i=1, \\ldots, n \\] As usual, \\(f(x_i)\\) is the deterministic part (DGM) and \\(\\varepsilon_i\\) is the random part in the function that together determine the value of \\(y_i\\). Again, we are living in two universes: the population and a sample. Since none of the elements in population is known to us, we can only assume what \\(f(x)\\) would be. Based on a sample and the assumption about DGM, we choose an estimator of \\(f(x)\\), \\[ \\hat{f}(x) = \\hat{\\beta}_0+\\hat{\\beta}_1 x_{i}, \\] which is BLUE of \\(f(x)\\), when it is estimated with OLS given the assumptions about \\(\\varepsilon_i\\) stated above. Since the task of this estimation is to satisfy the unbiasedness condition, i.e. \\(\\mathbf{E}[\\hat{f}(x)]=f(x)\\), it can be achieved only if \\(\\mathbf{E}[\\hat{\\beta_0}]=\\beta_0\\) and \\(\\mathbf{E}[\\hat{\\beta_1}]=\\beta_1\\). At the end of this process, we can understand the effect of \\(x\\) on \\(y\\), signified by the unbiased slope coefficient \\(\\hat{\\beta_1}\\). This is not as an easy job as it sounds in this simple example. Finding an unbiased estimator of \\(\\beta\\) is the main challenge in the field of econometrics. In prediction, on the other hand, our main task is not to find unbiased estimator of \\(f(x)\\). We just want to predict \\(y_0\\) given \\(x_0\\). The subscript \\(0\\) tells us that we want to predict \\(y\\) for a specific value of \\(x\\). Hence we can write it as, \\[ y_{0}=\\beta_0+\\beta_1 x_{0}+\\varepsilon_{0}, \\] In other words, when \\(x_0=5\\), for example, \\(y_0\\) will be determined by \\(f(x_0)\\) and the random error, \\(\\varepsilon_0\\), which has the same variance, \\(\\sigma^2\\), as \\(\\varepsilon_i\\). Hence, when \\(x_0=5\\), although \\(f(x_0)\\) is fixed, \\(y_0\\) will vary because of its random part, \\(\\varepsilon_0\\). This in an irreducible uncertainty in predicting \\(y_0\\) given \\(f(x_0)\\). We do not know about the population. Therefore, we do not know what \\(f(x_0)\\) is. We can have a sample from the population and build a model \\(\\hat{f}(x)\\) so that \\(\\hat{f}(x_0)\\) would be as close to \\(f(x_0)\\) as possible. But this introduces another layer of uncertainty in predicting \\(y_0\\). Since each sample is random and different, \\(\\hat{f}(x_0)\\) will be a function of the sample: \\(\\hat{f}(x_0, S_m)\\). Of course, we will have one sample in practice. However, if this variation is high, it would be highly likely that our predictions, \\(\\hat{f}(x_0, S_m)\\), would be far off from \\(f(x_0)\\). We can use an unbiased estimator for prediction, but as we have seen before, we may be able to improve MSPE if we allow some bias in \\(\\hat{f}(x)\\). To see this potential trade-off, we look at the decomposition of MSPE with a simplified notation: \\[ \\mathbf{MSPE}=\\mathbf{E}\\left[(y_0-\\hat{f})^{2}\\right]=\\mathbf{E}\\left[(f+\\varepsilon-\\hat{f})^{2}\\right] \\] \\[ \\mathbf{MSPE}=\\mathbf{E}\\left[(f+\\varepsilon-\\hat{f}+\\mathbf{E}[\\hat{f}]-\\mathbf{E}[\\hat{f}])^{2}\\right] \\] We have seen this before. Since we calculate MSPE for \\(x_i = x_0\\), we call it the conditional MSPE, which can be expressed as \\(\\mathbf{MSPE}=\\mathbf{E}\\left[(y_0-\\hat{f})^{2}|x=x_0\\right]\\). We will see unconditional MSPE, which is the average of all possible data points later in last two sections. The simplification will follow the same steps, and we will have: \\[ \\mathbf{MSPE}=(f-\\mathbf{E}[\\hat{f}])^{2}+\\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right]+\\mathbf{E}\\left[\\varepsilon^{2}\\right] \\] Let’s look at the first term first: \\[ \\left(f-\\mathbf{E}[\\hat{f}]\\right)^{2}=\\left(\\beta_0+\\beta_1 x_{0}-\\mathbf{E}[\\hat{\\beta}_0]-x_{0}\\mathbf{E}[\\hat{\\beta}_1]\\right)^2=\\left((\\beta_0-\\mathbf{E}[\\hat{\\beta}_0])+x_{0}(\\beta_1-\\mathbf{E}[\\hat{\\beta}_1])\\right)^2. \\] Hence it shows the bias (squared) in parameters. The second term is the variance of \\(\\hat{f}(x)\\): \\[ \\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right]=\\mathbf{Var}[\\hat{f}(x)]=\\mathbf{Var}[\\hat{\\beta}_0+\\hat{\\beta}_1 x_{0}]=\\mathbf{Var}[\\hat{\\beta}_0]+x_{0}^2\\mathbf{Var}[\\hat{\\beta}_1]+2x_{0}\\mathbf{Cov}[\\hat{\\beta}_0,\\hat{\\beta}_1] \\] As expected, the model’s variance is the sum of the variances of estimators and their covariance. Again, the variance can be thought of variation of \\(\\hat{f}(x)\\) from sample to sample. With the irreducible prediction error \\(\\mathbf{E}[\\varepsilon^{2}]=\\sigma^2\\), \\[ \\mathbf{MSPE}=(\\mathbf{bias})^{2}+\\mathbf{Var}(\\hat{f})+\\sigma^2. \\] Suppose that our OLS estimators are unbiased and that \\(\\mathbf{Cov}[\\hat{\\beta}_0,\\hat{\\beta}_1]=0\\). In that case, \\[ \\mathbf{MSPE}_{OLS} =\\mathbf{Var}(\\hat{\\beta}_{0})+x_{0}^2\\mathbf{Var}(\\hat{\\beta}_{1})+\\sigma^2 \\] Before going further, let’s summarize the meaning of this measure. The mean squared prediction error of unbiased \\(\\hat{f}(x_0)\\), or how much \\(\\hat{f}(x_0)\\) deviates from \\(y_0\\) is defined by two factors: First, \\(y_0\\) itself varies around \\(f(x_0)\\) by \\(\\sigma^2\\). This is irreducible. Second, \\(\\hat{f}(x_0)\\) varies from sample to sample. The model’s variance is the sum of variations in estimated coefficients from sample to sample, which can be reducible. Suppose that \\(\\hat{\\beta}_{1}\\) has a large variance. Hence, we can ask what would happen if we dropped the variable: \\[ \\mathbf{MSPE}_{Biased~OLS} = \\mathbf{Bias}^2+\\mathbf{Var}(\\hat{\\beta}_{0})+\\sigma^2 \\] When we take the difference: \\[ \\mathbf{MSPE}_{OLS} -\\mathbf{MSPE}_{Biased~OLS} =x_{0}^2\\mathbf{Var}(\\hat{\\beta}_{1}) - \\mathbf{Bias}^2 \\] This expression shows that dropping a variable would decrease the expected prediction error if: \\[ x_{0}^2\\mathbf{Var}(\\hat{\\beta}_{1}) &gt; \\mathbf{Bias}^2 ~~\\Rightarrow~~ \\mathbf{MSPE}_{Biased~OLS} &lt; \\mathbf{MSPE}_{OLS} \\] This option, omitting a variable, is unthinkable if our task is to obtain an unbiased estimator of \\({f}(x)\\), but improves the prediction accuracy if the condition above is satisfied. Let’s expand this example into a two-variable case: \\[ y_{i}=\\beta_0+\\beta_1 x_{1i}+\\beta_2 x_{2i}+\\varepsilon_{i}, ~~~~ i=1, \\ldots, n. \\] Thus, the bias term becomes \\[ \\left(f-\\mathbf{E}[\\hat{f}]\\right)^{2}=\\left((\\beta_0-\\mathbf{E}[\\hat{\\beta}_0])+x_{10}(\\beta_1-\\mathbf{E}[\\hat{\\beta}_1])+x_{20}(\\beta_2-\\mathbf{E}[\\hat{\\beta}_2])\\right)^2. \\] And let’s assume that \\(\\mathbf{Cov}[\\hat{\\beta}_0,\\hat{\\beta}_1]=\\mathbf{Cov}[\\hat{\\beta}_0,\\hat{\\beta}_2]=0\\), but \\(\\mathbf{Cov}[\\hat{\\beta}_1,\\hat{\\beta}_2] \\neq 0\\). Hence, the variance of \\(\\hat{f}(x)\\): \\[ \\mathbf{Var}[\\hat{f}(x)]=\\mathbf{Var}[\\hat{\\beta}_0+\\hat{\\beta}_1 x_{10}+\\hat{\\beta}_2 x_{20}]=\\mathbf{Var}[\\hat{\\beta}_0]+x_{10}^2\\mathbf{Var}[\\hat{\\beta}_1]+x_{20}^2\\mathbf{Var}[\\hat{\\beta}_2]+\\\\2x_{10}x_{20}\\mathbf{Cov}[\\hat{\\beta}_1,\\hat{\\beta}_2]. \\] This two-variable example shows that as the number of variables rises, the covariance between variables inflates the model’s variance further. This fact captured by Variance Inflation Factor (VIF) in econometrics is a key point in high-dimensional models for two reasons: First, dropping a variable highly correlated with other variables would reduce the model’s variance substantially. Second, a highly correlated variable also has limited new information among other variables. Hence dropping a highly correlated variable (with a high variance) would have a less significant effect on the prediction accuracy while reducing the model’s variance substantially. Suppose that we want to predict \\(y_0\\) for \\(\\left[x_{10},~ x_{20}\\right]\\) and \\(\\mathbf{Var}[\\hat{\\beta}_2] \\approx 10~\\text{x}~\\mathbf{Var}[\\hat{\\beta}_1]\\). Hence, we consider dropping \\(x_2\\). To evaluate the effect of this decision on MSPE, we take the difference between two MSPE’s: \\[ \\mathbf{MSPE}_{OLS} -\\mathbf{MSPE}_{Biased~OLS} =x_{20}^2\\mathbf{Var}(\\hat{\\beta}_{2}) + 2x_{10}x_{20}\\mathbf{Cov}[\\hat{\\beta}_1,\\hat{\\beta}_2] - \\mathbf{Bias}^2 \\] Thus, dropping \\(x_2\\) would decrease the prediction error if \\[ x_{20}^2\\mathbf{Var}(\\hat{\\beta}_{2}) + 2x_{10}x_{20}\\mathbf{Cov}[\\hat{\\beta}_1,\\hat{\\beta}_2]&gt; \\mathbf{Bias}^2 ~~\\Rightarrow~~ \\mathbf{MSPE}_{Biased~OLS} &lt; \\mathbf{MSPE}_{OLS} \\] We know from Elementary Econometrics that \\(\\mathbf{Var}(\\hat{\\beta}_j)\\) increases by \\(\\sigma^2\\), decreases by the \\(\\mathbf{Var}(x_j)\\), and rises by the correlation between \\(x_j\\) and other \\(x\\)’s. Let’s look at \\(\\mathbf{Var}(\\hat{\\beta}_j)\\) closer: {% raw %} \\[ \\mathbf{Var}(\\hat{\\beta}_{j}) = \\frac{\\sigma^{2}}{\\mathbf{Var}(x_{j})} \\cdot \\frac{1}{1-R_{j}^{2}} \\] {% endraw %} where \\(R_j^2\\) is \\(R^2\\) in the regression on \\(x_j\\) on the remaining \\((k-2)\\) regressors (\\(x\\)’s). The second term is called the variance-inflating factor (VIF). As usual, a higher variability in a particular \\(x\\) leads to proportionately less variance in the corresponding coefficient estimate. Note that, however, as \\(R_j^2\\) get closer to one, that is, as the correlation between \\(x_j\\) with other regressors approaches to unity, \\(\\mathbf{Var}(\\hat{\\beta}_j)\\) goes to infinity. The variance of \\(\\varepsilon_i\\), \\(\\sigma^2\\), indicates how much \\(y_i\\)’s deviate from the \\(f(x)\\). Since \\(\\sigma^2\\) is typically unknown, we estimate it from the sample as \\[ \\widehat{\\sigma}^{2}=\\frac{1}{(n-k+1)} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}(x)\\right)^{2} \\] Remember that we have multiple samples, hence if our estimator is unbiased, we can prove that \\(\\mathbf{E}(\\hat{\\sigma}^2)=\\sigma^2\\). The proof is not important now. However, \\(\\mathbf{Var}(\\hat{\\beta}_j)\\) becomes \\[ \\mathbf{Var}\\left(\\hat{\\beta}_{j}\\right)=\\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}(x)\\right)^{2}}{(n-k+1)\\mathbf{Var}\\left(x_{j}\\right)} \\cdot \\frac{1}{1-R_{j}^{2}}, \\] It is clear now that a greater sample size, \\(n\\), results in a proportionately less variance in the coefficient estimates. On the other hand, as the number of regressors, \\(k\\), goes up, the variance goes up. In large \\(n\\) and small \\(k\\), the trade-off by dropping a variable would be insignificant, but as \\(k/n\\) rises, the trade-off becomes more important. Let’s have a simulation example to conclude this section. Here are the steps for our simulation: There is a random variable, \\(y\\), that we want to predict. \\(y_{i}=f(x_i)+\\varepsilon_{i}\\). DGM is \\(f(x_i)=\\beta_0+\\beta_1 x_{1i}+\\beta_2 x_{2i}\\) \\(\\varepsilon_{i} \\sim N(0, \\sigma^2)\\). The steps above define the population. We will withdraw \\(M\\) number of samples from this population. Using each sample (\\(S_m\\), where \\(m=1, \\ldots, M\\)), we will estimate two models: unbiased \\(\\hat{f}(x)_{OLS}\\) and biased \\(\\hat{f}(x)_{Biased~OLS}\\) Using these models we will predict \\(y&#39;_i\\) from a different sample (\\(T\\)) drawn from the same population. We can call it the “unseen” dataset or the “test” dataset, which contains out-of-sample data points, \\((y&#39;_i, x_{1i}, x_{2i})\\)., where \\(i=1, \\ldots, n\\). Before we start, we need to be clear how we define MSPE in our simulation. Since we will predict every \\(y&#39;_i\\) with corresponding predictors \\((x_{1i}, x_{2i})\\) in test set \\(T\\) by each \\(\\hat{f}(x_{1i}, x_{2i}, S_m))\\) estimated by each sample, we calculate the following unconditional MSPE: \\[ \\mathbf{MSPE}=\\mathbf{E}_{S}\\mathbf{E}_{S_{m}}\\left[(y&#39;_i-\\hat{f}(x_{1i}, x_{2i}, S_m))^{2}\\right]=\\\\\\mathbf{E}_S\\left[\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}^{\\prime}-\\hat{f}(x_{1i}, x_{2i}, S_m)\\right)^{2}\\right],~~~~~~~~~ m=1, \\ldots, M \\] We first calculate MSPE for all data points in the test set using \\(\\hat{f}(x_{1T}, x_{2T}, S_m)\\), and then take the average of \\(M\\) samples. Below, we will show the sensitivity of trade-off by the size of irreducible error. The simulation below plots \\(diff= \\mathbf{MSPE}_{OLS}-\\mathbf{MSPE}_{Biased~OLS}\\) against \\(\\sigma\\). # Function for X - fixed at repeated samples # Argument l is used for correlation and with 0.01 # Correlation between x_1 and x_2 is 0.7494 xfunc &lt;- function(n, l){ set.seed(123) x_1 &lt;- rnorm(n, 0, 25) x_2 &lt;- l*x_1+rnorm(n, 0, 0.2) X &lt;- data.frame(&quot;x_1&quot; = x_1, &quot;x_2&quot; = x_2) return(X) } # Note that we can model dependencies with copulas in R # More specifically by using mvrnorn() function. However, here # We want one variable with a higher variance. which is easier to do manaully # More: https://datascienceplus.com/modelling-dependence-with-copulas/ # Function for test set - with different X&#39;s but same dist. unseen &lt;- function(n, sigma, l){ set.seed(1) x_11 &lt;- rnorm(n, 0, 25) x_22 &lt;- l*x_11+rnorm(n, 0, 0.2) f &lt;- 0 + 2*x_11 + 2*x_22 y_u &lt;- f + rnorm(n, 0, sigma) un &lt;- data.frame(&quot;y&quot; = y_u, &quot;x_1&quot; = x_11, &quot;x_2&quot; = x_22) return(un) } # Function for simulation (M - number of samples) sim &lt;- function(M, n, sigma, l){ X &lt;- xfunc(n, l) # Repeated X&#39;s in each sample un &lt;- unseen(n, sigma, l) # Out-of sample (y, x_1, x_2) # containers MSPE_ols &lt;- rep(0, M) MSPE_b &lt;- rep(0, M) coeff &lt;- matrix(0, M, 3) coeff_b &lt;- matrix(0, M, 2) yhat &lt;- matrix(0, M, n) yhat_b &lt;- matrix(0, M, n) # loop for samples for (i in 1:M) { f &lt;- 0 + 2*X$x_1 + 2*X$x_2 # DGM y &lt;- f + rnorm(n, 0, sigma) samp &lt;- data.frame(&quot;y&quot; = y, X) ols &lt;- lm(y~., samp) # Unbaised OLS ols_b &lt;- lm(y~x_1, samp) #Biased OLS coeff[i,] &lt;- ols$coefficients coeff_b[i,] &lt;- ols_b$coefficients yhat[i,] &lt;- predict(ols, un) yhat_b[i,] &lt;- predict(ols_b, un) MSPE_ols[i] &lt;- mean((un$y-yhat[i])^2) MSPE_b[i] &lt;- mean((un$y-yhat_b[i])^2) } d = mean(MSPE_ols)-mean(MSPE_b) output &lt;- list(d, MSPE_b, MSPE_ols, coeff, coeff_b, yhat, yhat_b) return(output) } # Sensitivity of (MSPE_biased)-(MSPE_ols) # different sigma for the irreducible error sigma &lt;- seq(1, 20, 1) MSPE_dif &lt;- rep(0, length(sigma)) for (i in 1: length(sigma)) { MSPE_dif[i] &lt;- sim(1000, 100, sigma[i], 0.01)[[1]] } plot(sigma, MSPE_dif, col=&quot;red&quot;, main = &quot;Difference in MSPE vs. sigma&quot;, cex = 0.9, cex.main= 0.8, cex.lab = 0.7, cex.axis = 0.8) The simulation shows that the biased \\(\\hat{f}(x)\\) is getting a better precision in prediction as the “noise” in the data gets higher. The reason can be understood if we look at \\(\\mathbf{Var}(\\hat{\\beta}_{2}) + 2\\mathbf{Cov}[\\hat{\\beta}_1,\\hat{\\beta}_2]\\) closer: \\[ \\mathbf{Var}\\left(\\hat{\\beta}_{2}\\right)=\\frac{\\sigma^{2}}{\\mathbf{Var}\\left(x_{2}\\right)} \\cdot \\frac{1}{1-r_{1,2}^{2}}, \\] where \\(r_{1,2}^{2}\\) is the coefficient of correlation between \\(x_1\\) and \\(x_2\\). And, \\[ \\mathbf{Cov}\\left(\\hat{\\beta}_{1},\\hat{\\beta}_{2}\\right)=\\frac{-r_{1,2}^{2}\\sigma^{2}}{\\sqrt{\\mathbf{Var}\\left(x_{1}\\right)\\mathbf{Var}\\left(x_{2}\\right)}} \\cdot \\frac{1}{1-r_{1,2}^{2}}, \\] Hence, \\[ \\begin{aligned} \\mathbf{MSPE}_{O L S}-&amp; \\mathbf{MSPE}_{Biased~OLS}=\\mathbf{V} \\mathbf{a r}\\left(\\hat{\\beta}_{2}\\right)+2 \\mathbf{C} \\mathbf{o} \\mathbf{v}\\left[\\hat{\\beta}_{1}, \\hat{\\beta}_{2}\\right]-\\mathbf{Bias}^{2}=\\\\ &amp; \\frac{\\sigma^{2}}{1-r_{1,2}^{2}}\\left(\\frac{1}{\\mathbf{V a r}\\left(x_{2}\\right)}+\\frac{-2 r_{1,2}^{2}}{\\sqrt{\\mathbf{V a r}\\left(x_{1}\\right) \\mathbf{V a r}\\left(x_{2}\\right)}}\\right)-\\mathbf{Bias}^{2} \\end{aligned} \\] Given the bias due to the omitted variable \\(x_2\\), this expression shows the difference as a function of \\(\\sigma^2\\) and \\(r_{1,2}^{2}\\) and explains why the biased-OLS estimator have increasingly better predictions. As a final experiment, let’s have the same simulation that shows the relationship between correlation and trade-off. To create different correlations between \\(x_1\\) and \\(x_2\\), we use the xfunc() we created earlier. The argument \\(l\\) is used to change the the correlation and can be seen below. In our case, when \\(l=0.01\\) \\(r_{1,2}^{2}=0.7494\\). # Function for X for correlation X &lt;- xfunc(100, 0.001) cor(X) ## x_1 x_2 ## x_1 1.00000000 0.06838898 ## x_2 0.06838898 1.00000000 X &lt;- xfunc(100, 0.0011) cor(X) ## x_1 x_2 ## x_1 1.00000000 0.08010547 ## x_2 0.08010547 1.00000000 # We use this in our simulation X &lt;- xfunc(100, 0.01) cor(X) ## x_1 x_2 ## x_1 1.0000000 0.7494025 ## x_2 0.7494025 1.0000000 Now the simulation with different levels of correlation: # Sensitivity of (MSPE_biased)-(MSPE_ols) # different levels of correlation when sigma^2=7 l &lt;- seq(0.001, 0.011, 0.0001) MSPE_dif &lt;- rep(0, length(l)) for (i in 1: length(l)) { MSPE_dif[i] &lt;- sim(1000, 100, 7, l[i])[[1]] } plot(l, MSPE_dif, col=&quot;red&quot;, main= &quot;Difference in MSPE vs Correlation b/w X&#39;s&quot;, cex=0.9, cex.main= 0.8, cex.lab = 0.7, cex.axis = 0.8) As the correlation between \\(x\\)’s goes up, \\(\\mathbf{MSPE}_{OLS}-\\mathbf{MSPE}_{Biased~OLS}\\) rises. Later we will have a high-dimensional dataset (large \\(k\\)) to show the importance of correlation. We will leave the calculation of bias and how the sample size affects trade-off to labs. 20.3 out-sample prediction accuracy As a side note: when we compare the models in terms their out-sample prediction accuracy, we usually use the root MSPE (RMSPE), which gives us the prediction error in original units. When we calculate empirical in-sample MSPE with one sample, we can asses its out-of-sample prediction performance by the Mallows \\(C_P\\) statistics, which just substitutes the feasible estimator of \\(\\sigma^2\\) into the overfitting penalty. That is, for a linear model with \\(p + 1\\) coefficients fit by OLS, \\[ C_{p}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}(x_i)\\right)^{2}+\\frac{2 \\widehat{\\sigma}^{2}}{n}(p+1), \\] which becomes a good proxy for the our-of-sample error. That is, a small value of \\(C_p\\) means that the model is relatively precise. For comparing models, we really care about differences in empirical out-sample MSPE’s: \\[ \\Delta C_{p}=\\mathbf{MSPE}_{1}-\\mathbf{MSPE}_{2}+\\frac{2}{n} \\widehat{\\sigma}^{2}\\left(p_{1}-p_{2}\\right), \\] where we use \\(\\hat{\\sigma}^2\\) from the largest model. https://online.stat.psu.edu/stat462/node/197/ How are we going to find the best predictor? In addition to \\(C_p\\), we can also use Akaike Information Criterion (AIC), which also has the form of “in-sample performance plus penalty”. AIC can be applied whenever we have a likelihood function, whereas \\(C_p\\) can be used when we use squared errors. We will see later AIC and BIC (Bayesian Information Criteria) in this book. With these measures, we can indirectly estimate the test (out-of-sample) error by making an adjustment to the training (in-sample) error to account for the bias due to overfitting. Therefore, these methods are ex-post tools to penalize the overfitting. On the other hand, we can directly estimate the test error (out-sample) and choose the model that minimizes it. We can do it by directly validating the model using a cross-validation approach. Therefore, cross-validation methods provide ex-ante penalization for overfitting and are the main tools in selecting predictive models in machine learning applications as they have almost no assumptions. 20.4 Sparsity "],["classification-2.html", "Chapter 21 Classification 21.1 Nonparametric Classifier - kNN 21.2 mnist Dataset 21.3 Linear classifiers (again) 21.4 k-Nearest Neighbors 21.5 kNN with caret 21.6 Tuning in Classification 21.7 Confusion matrix 21.8 Performance measures 21.9 ROC Curve 21.10 AUC - Area Under the Curve 21.11 Classification Example 21.12 LPM 21.13 Logistic Regression 21.14 kNN", " Chapter 21 Classification 21.1 Nonparametric Classifier - kNN We complete this section, Nonparametric Estimations, with a nonparametric classifier and compare its performance with parametric classifiers, LPM and Logistic. 21.2 mnist Dataset Reading hand-written letters and numbers is not a big deal nowadays. For example, In Canada Post, computers read postal codes and robots sorts them for each postal code groups. This application is mainly achieved by machine learning algorithms. In order to understand how, let’s use a real dataset, Mnist. Here is the description of the dataset by Wikipedia: The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST’s training dataset, while the other half of the training set and the other half of the test set were taken from NIST’s testing dataset. There have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.23%. These images are converted into \\(28 \\times 28 = 784\\) pixels and, for each pixel, there is a measure that scales the darkness in that pixel between 0 (white) and 255 (black). Hence, for each digitized image, we have an indicator variable \\(Y\\) between 0 and 9, and we have 784 variables that identifies each pixel in the digitized image. Let’s download the data. (More details about the data). #loading the data library(tidyverse) library(dslabs) #Download the data to your directory. It&#39;s big! #mnist &lt;- read_mnist() #save(mnist, file = &quot;mnist.Rdata&quot;) load(&quot;mnist.Rdata&quot;) str(mnist) ## List of 2 ## $ train:List of 2 ## ..$ images: int [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ labels: int [1:60000] 5 0 4 1 9 2 1 3 1 4 ... ## $ test :List of 2 ## ..$ images: int [1:10000, 1:784] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ labels: int [1:10000] 7 2 1 0 4 1 4 9 5 9 ... The data is given as a list and already divided into train and test sets. We have 60,000 images in the train set and 10,000 images in the test set. For the train set, we have two nested sets: images, which contains all 784 features for 60,000 images. Hence, it’s a \\(60000 \\times 784\\) matrix. And, labels contains the labes (from 0 to 9) for each image. The digitizing can be understood from this image better: Each image has \\(28 \\times 28\\) = 784 pixels. For each image, the pixels are features with a label that shows the true number between 0 and 9. This methods is called as “flattening”, which is a technique that is used to convert multi-dimensional image into a one-dimension array (vector). For now, we will use a smaller version of this data set given in the dslabs package, which is a random sample of 1,000 images (only for 2 and 7 digits), 800 in the training set and 200 in the test set, with only two features: the proportion of dark pixels that are in the upper left quadrant, x_1, and the lower right quadrant, x_2. data(&quot;mnist_27&quot;) str(mnist_27) ## List of 5 ## $ train :&#39;data.frame&#39;: 800 obs. of 3 variables: ## ..$ y : Factor w/ 2 levels &quot;2&quot;,&quot;7&quot;: 1 2 1 1 2 1 2 2 2 1 ... ## ..$ x_1: num [1:800] 0.0395 0.1607 0.0213 0.1358 0.3902 ... ## ..$ x_2: num [1:800] 0.1842 0.0893 0.2766 0.2222 0.3659 ... ## $ test :&#39;data.frame&#39;: 200 obs. of 3 variables: ## ..$ y : Factor w/ 2 levels &quot;2&quot;,&quot;7&quot;: 1 2 2 2 2 1 1 1 1 2 ... ## ..$ x_1: num [1:200] 0.148 0.283 0.29 0.195 0.218 ... ## ..$ x_2: num [1:200] 0.261 0.348 0.435 0.115 0.397 ... ## $ index_train: int [1:800] 40334 33996 3200 38360 36239 38816 8085 9098 15470 5096 ... ## $ index_test : int [1:200] 46218 35939 23443 30466 2677 54248 5909 13402 11031 47308 ... ## $ true_p :&#39;data.frame&#39;: 22500 obs. of 3 variables: ## ..$ x_1: num [1:22500] 0 0.00352 0.00703 0.01055 0.01406 ... ## ..$ x_2: num [1:22500] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ p : num [1:22500] 0.703 0.711 0.719 0.727 0.734 ... ## ..- attr(*, &quot;out.attrs&quot;)=List of 2 ## .. ..$ dim : Named int [1:2] 150 150 ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;x_1&quot; &quot;x_2&quot; ## .. ..$ dimnames:List of 2 ## .. .. ..$ x_1: chr [1:150] &quot;x_1=0.0000000&quot; &quot;x_1=0.0035155&quot; &quot;x_1=0.0070310&quot; &quot;x_1=0.0105465&quot; ... ## .. .. ..$ x_2: chr [1:150] &quot;x_2=0.000000000&quot; &quot;x_2=0.004101417&quot; &quot;x_2=0.008202834&quot; &quot;x_2=0.012304251&quot; ... 21.3 Linear classifiers (again) A linear classifier (like LPM and Logistic) is one where a “hyperplane” is formed by taking a linear combination of the features. Hyperplane represents a decision boundary chosen by our classifier to separate the data points in different class labels. let’s start with LPM: \\[\\begin{equation} \\operatorname{Pr}\\left(Y=1 | X_{1}=x_{1}, X_{2}=x_{2}\\right)=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2} \\tag{21.1} \\end{equation}\\] # LPM requires numerical 1 and 0 y10 = ifelse(mnist_27$train$y == 7, 1, 0) train &lt;- data.frame(mnist_27$train, y10) plot(train$x_1, train$x_2, col = train$y10 + 1, cex = 0.5) Here, the black dots are 2 and red dots are 7. Note that if we use 0.5 as a decision rule such that it separates pairs (\\(x_1\\), \\(x_2\\)) for which \\(\\operatorname{Pr}\\left(Y=1 | X_{1}=x_{1}, X_{2}=x_{2}\\right) &lt; 0.5\\) then we can have a hyperplane as \\[ \\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{1}+\\hat{\\beta}_{2} x_{2}=0.5 \\Longrightarrow x_{2}=\\left(0.5-\\hat{\\beta}_{0}\\right) / \\hat{\\beta}_{2}-\\hat{\\beta}_{1} / \\hat{\\beta}_{2} x_{1}. \\] If we incorporate this into our plot for the train data: model &lt;- lm(y10 ~ x_1 + x_2, train) tr &lt;- 0.5 a &lt;- tr - model$coefficients[1] a &lt;- a / model$coefficients[3] b &lt;- -model$coefficients[2] / model$coefficients[3] plot(train$x_1, train$x_2, col = train$y10 + 1, cex = 0.72) abline(a, b, col = &quot;blue&quot;, lwd = 2.8) Play with the (discriminating) threshold and see how the hyperplane moves. When we change it to different numbers between 0 and 1, the number of correct and wrong predictions, a separation of red and black dots located in different sides, changes as well. Moreover the decision boundary is linear. That’s why LPM is called a linear classifier. Would including interactions and polynomials (nonlinear parts) would place the line such a way that separation of these dots (2s and 7s) would be better? Let’s see if adding a polynomial to our LPM improves this. model2 &lt;- lm(y10 ~ x_1 + I(x_1 ^ 2) + x_2, train) summary(model2) ## ## Call: ## lm(formula = y10 ~ x_1 + I(x_1^2) + x_2, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.14744 -0.28816 0.03999 0.28431 1.06759 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.09328 0.06571 1.419 0.1562 ## x_1 4.81884 0.55310 8.712 &lt; 2e-16 *** ## I(x_1^2) -2.75520 1.40760 -1.957 0.0507 . ## x_2 -1.18864 0.17252 -6.890 1.14e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3891 on 796 degrees of freedom ## Multiple R-squared: 0.3956, Adjusted R-squared: 0.3933 ## F-statistic: 173.7 on 3 and 796 DF, p-value: &lt; 2.2e-16 tr &lt;- 0.5 s &lt;- model2$coefficients a = tr / s[3] b = s[1] / s[3] d = s[2] / s[3] e = s[4] / s[3] x22 = a - b - d * train$x_1 - e * (train$x_1 ^ 2) plot(train$x_1, train$x_2, col = train$y10 + 1, cex = 0.72) lines(train$x_1[order(x22)], x22[order(x22)], lwd = 2.8) The coefficient of the polynomial is barely significant and very negligible in magnitude. And in fact the classification seems worse than the previous one. Would a logistic regression give us a better line? We don’t need to estimate it, but we can obtain the decision boundary for the logistic regression. Remember, \\[ P(Y=1 | x)=\\frac{\\exp \\left(w_{0}+\\sum_{i} w_{i} x_{i}\\right)}{1+\\exp \\left(w_{0}+\\sum_{i} w_{i} x_{i}\\right)} \\] And, \\[ P(Y=0 | x)=1-P(Y=1 | x)= \\frac{1}{1+\\exp \\left(w_{0}+\\sum_{i} w_{i} x_{i}\\right)} \\] if we take the ratio of success over failure, \\(P/1-P\\), \\[ \\frac{P}{1-P}=\\exp \\left(w_{0}+\\sum_{i} w_{i} x_{i}\\right) \\] If this ratio is higher than 1, we think that the probability for \\(Y=1\\) is higher than the probability for \\(Y=0\\). And this only happens when \\(P&gt;0.5\\). Hence, the condition to classify the observation as \\(Y=1\\) is: \\[ \\frac{P}{1-P}=\\exp \\left(w_{0}+\\sum_{i} w_{i} x_{i}\\right) &gt; 1 \\] If we take the log of both sides, \\[ w_{0}+\\sum_{i} w_{i} X_{i}&gt;0 \\] From here, the hyperplane function in our case becomes, \\[ \\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{1}+\\hat{\\beta}_{2} x_{2}=0 \\Longrightarrow x_{2}=-\\hat{\\beta}_{0} / \\hat{\\beta}_{2}-\\hat{\\beta}_{1} / \\hat{\\beta}_{2} x_{1}. \\] We see that the decision boundary is again linear. Therefore, LPM and logistic regressions are called as linear classifiers, which are good only if the problem on hand is linearly separable. Would it be possible to have a nonlinear boundary condition so that we can get a better classification for our predicted probabilities? 21.4 k-Nearest Neighbors k-nearest neighbors (kNN) is a nonparametric method used for classification (or regression), which estimate \\(p(x_1, x_2)\\) by using a method similar to bin smoothing. In kNN classification, the output is a class membership. An object is assigned to the class most common among its k-nearest neighbors. In kNN regressions, the output is the average of the values of k-nearest neighbors, which we’ve seen in bin smoothing applications. Suppose we have to classify (identify) the red dot as 7 or 2. Since it’s a nonparametric approach, we have to define bins. If the number of observations in bins set to 1 (\\(k = 1\\)), then we need to find one observation that is nearest to the red dot. How? Since we know to coordinates (\\(x_1, x_2\\)) of that red dot, we can find its nearest neighbors by some distance functions among all points (observations) in the data. A popular choice is the Euclidean distance given by \\[ d\\left(x, x^{\\prime}\\right)=\\sqrt{\\left(x_{1}-x_{1}^{\\prime}\\right)^{2}+\\ldots+\\left(x_{n}-x_{n}^{\\prime}\\right)^{2}}. \\] Other measures are also available and can be more suitable in different settings including the Manhattan, Chebyshev and Hamming distance. The last one is used if the features are binary. In our case the features are continuous so we can use the Euclidean distance. We now have to calculate this measure for every point (observation) in our data. In our graph we have 10 points, and we have to have 10 distance measures from the red dot. Usually, in practice, we calculate all distance measures between each point, which becomes a symmetric matrix with \\(n\\)x\\(n\\) dimensions. For example, for two dimensional space, we can calculate the distances as follows x1 &lt;- c(2, 2.1, 4, 4.3) x2 &lt;- c(3, 3.3, 5, 5.1) EDistance &lt;- function(x, y){ dx &lt;- matrix(0, length(x), length(x)) dy &lt;- matrix(0, length(x), length(x)) for (i in 1:length(x)) { dx[i,] &lt;- (x[i] - x)^2 dy[i,] &lt;- (y[i] - y)^2 dd &lt;- sqrt(dx^2 + dy^2) } return(dd) } EDistance(x1, x2) ## [,1] [,2] [,3] [,4] ## [1,] 0.00000000 0.09055385 5.65685425 6.88710389 ## [2,] 0.09055385 0.00000000 4.62430535 5.82436263 ## [3,] 5.65685425 4.62430535 0.00000000 0.09055385 ## [4,] 6.88710389 5.82436263 0.09055385 0.00000000 plot(x1, x2, col = &quot;red&quot;, lwd = 3) #segments(x1[1], x2[1], x1[2:4], x2[2:4], col = &quot;blue&quot; ) #segments(x1[2], x2[2], x1[c(1, 3:4)], x2[c(1, 3:4)], col = &quot;green&quot; ) #segments(x1[3], x2[3], x1[c(1:2, 4)], x2[c(1:2, 4)], col = &quot;orange&quot; ) segments(x1[4], x2[4], x1[1:3], x2[1:3], col = &quot;darkgreen&quot; ) The matrix shows all distances for four points and, as we expect, it is symmetric. The green lines show the distance from the last point (\\(x = 4.3,~ y = 5.1\\)) to all other points. Using this matrix, we can easily find the k-nearest neighbors for any point. When \\(k=1\\), the observation that has the shortest distance is going to be the one to predict what the red dot could be. This is shown in the figure below: If we define the bin as \\(k=3\\), we look for the 3 nearest points to the red dot and then take an average of the 1s (7s) and 0s (2s) associated with these points. Here is an example: Using \\(k\\) neighbors to estimate the probability of \\(Y=1\\) (the dot is 7), that is \\[\\begin{equation} \\hat{P}_{k}(Y=1 | X=x)=\\frac{1}{k} \\sum_{i \\in \\mathcal{N}_{k}(x, D)} I\\left(y_{i}=1\\right) \\tag{10.11} \\end{equation}\\] With this predicted probability, we classify the red dot to the class with the most observations in the \\(k\\) nearest neighbors (we assign a class at random to one of the classes tied for highest). Here is the rule in our case: \\[ \\hat{C}_{k}(x)=\\left\\{\\begin{array}{ll}{1} &amp; {\\hat{p}_{k 0}(x)&gt;0.5} \\\\ {0} &amp; {\\hat{p}_{k 1}(x)&lt;0.5}\\end{array}\\right. \\] Suppose our red dot has \\(x=(x_1,x_2)=(4,3)\\) \\[ \\begin{aligned} \\hat{P}\\left(Y=\\text { Seven } | X_{1}=4, X_{2}=3\\right)=\\frac{2}{3} \\\\ \\hat{P}\\left(Y=\\text { Two} | X_{1}=4, X_{2}=3\\right)=\\frac{1}{3} \\end{aligned} \\] Hence, \\[ \\hat{C}_{k=4}\\left(x_{1}=4, x_{2}=3\\right)=\\text { Seven } \\] As it’s clear from this application, \\(k\\) is our hyperparameter and we need to tune it as to have the best predictive kNN algorithm. The following section will show its application. But before that, we need to understand how decision boundaries can be found in kNN set.seed(1) x1 &lt;- runif(50) x2 &lt;- runif(50) library(deldir) tesselation &lt;- deldir(x1, x2) tiles &lt;- tile.list(tesselation) plot(tiles, pch = 19, close = TRUE, fillcol = hcl.colors(4, &quot;Sunset&quot;), xlim = c(-0.2:1.1)) These are called Voronoi cells associated with 1-NN, which is the set of polygons whose edges are the perpendicular bisectors of the lines joining the neighboring points. Thus, the decision boundary is the result of fusing adjacent Voronoi cells that are associated with same class. In the example above, it’s the boundary of unions of each colors. Finding the boundaries that trace each adjacent Vorono regions can be done with additional several steps. To see all in an application, we will use knn3() from the Caret package. We will not train a model but only see how the separation between classes will be nonlinear and different for different \\(k\\). library(tidyverse) library(caret) library(dslabs) #With k = 50 model1 &lt;- knn3(y ~ ., data = mnist_27$train, k = 2) x_1 &lt;- mnist_27$true_p$x_1 x_2 &lt;- mnist_27$true_p$x_2 df &lt;- data.frame(x_1, x_2) #This is whole data 22500 obs. p_hat &lt;- predict(model1, df, type = &quot;prob&quot;) # Predicting probabilities in each bin p_7 &lt;- p_hat[,2] #Selecting the p_hat for 7 df &lt;- data.frame(x_1, x_2, p_7) my_colors &lt;- c(&quot;black&quot;, &quot;red&quot;) p1 &lt;- ggplot() + geom_point(data = mnist_27$train, aes(x = x_1, y = x_2, colour = factor(y)), shape = 21, size = 1, stroke = 1) + stat_contour(data = df, aes(x = x_1, y = x_2, z = p_7), breaks=c(0.5), color=&quot;blue&quot;) + scale_color_manual(values = my_colors) plot(p1) #With k = 400 model2 &lt;- knn3(y ~ ., data = mnist_27$train, k = 400) p_hat &lt;- predict(model2, df, type = &quot;prob&quot;) # Prediciting probabilities in each bin p_7 &lt;- p_hat[,2] #Selecting the p_hat for 7 df &lt;- data.frame(x_1, x_2, p_7) p1 &lt;- ggplot() + geom_point(data = mnist_27$train, aes(x = x_1, y = x_2, colour = factor(y)), shape = 21, size = 1, stroke = 1) + stat_contour(data = df, aes(x = x_1, y = x_2, z = p_7), breaks=c(0.5), color=&quot;blue&quot;) + scale_color_manual(values = my_colors) plot(p1) One with \\(k=2\\) shows signs for overfitting, the other one with \\(k=400\\) indicates oversmoothing or underfitting. We need to tune \\(k\\) such a way that it will be best in terms of prediction accuracy. 21.5 kNN with caret There are many different learning algorithms developed by different authors and often with different parametric structures. The caret, Classification And Regression Training package tries to consolidate these differences and provide consistency. It currently includes 237 (and growing) different methods which are summarized in the caret package manual (Kuhn_2019?). Here, we will use mnset_27 to illustrate how we can use caret for kNN. For now, we will use the caret’s train() function to find the optimal k in kNN, which is basically an automated version of cross-validation that we will see in the next chapter. 21.5.1 mnist_27 Since, our dataset, mnist_27, is already split into train and test sets, we do not need to do it again. Here is the starting point: library(caret) #Training/Model building model_knn &lt;- train(y ~ ., method = &quot;knn&quot;, data = mnist_27$train) model_knn ## k-Nearest Neighbors ## ## 800 samples ## 2 predictor ## 2 classes: &#39;2&#39;, &#39;7&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 800, 800, 800, 800, 800, 800, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 5 0.8075980 0.6135168 ## 7 0.8157975 0.6300494 ## 9 0.8205824 0.6396302 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 9. By default, the cross validation is performed by taking 25 bootstrap samples comprised of 25% of the observations. Moreover, the default is to try \\(k=5,7,9\\). We can to expand it: #Training/Model building with our own grid set.seed(2008) model_knn1 &lt;- train( y ~ ., method = &quot;knn&quot;, data = mnist_27$train, tuneGrid = data.frame(k = seq(9, 71, 2)) ) ggplot(model_knn1, highlight = TRUE) model_knn1$bestTune ## k ## 10 27 model_knn1$finalModel ## 27-nearest neighbor model ## Training set outcome distribution: ## ## 2 7 ## 379 421 We can change its tuning to cross-validation: #Training/Model building with 10-k cross validation cv &lt;- trainControl(method = &quot;cv&quot;, number = 10, p = 0.9) model_knn2 &lt;- train(y ~ ., method = &quot;knn&quot;, data = mnist_27$train, tuneGrid = data.frame(k=seq(9,71,2)), trControl = cv) ggplot(model_knn2, highlight = TRUE) model_knn2$bestTune ## k ## 11 29 It seems like \\(k=27\\) (\\(k=29\\) with CV) gives us the best performing prediction model. We can see their prediction performance on the test set: caret::confusionMatrix(predict(model_knn1, mnist_27$test, type = &quot;raw&quot;), mnist_27$test$y) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 2 7 ## 2 92 19 ## 7 14 75 ## ## Accuracy : 0.835 ## 95% CI : (0.7762, 0.8836) ## No Information Rate : 0.53 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.6678 ## ## Mcnemar&#39;s Test P-Value : 0.4862 ## ## Sensitivity : 0.8679 ## Specificity : 0.7979 ## Pos Pred Value : 0.8288 ## Neg Pred Value : 0.8427 ## Prevalence : 0.5300 ## Detection Rate : 0.4600 ## Detection Prevalence : 0.5550 ## Balanced Accuracy : 0.8329 ## ## &#39;Positive&#39; Class : 2 ## caret::confusionMatrix(predict(model_knn2, mnist_27$test, type = &quot;raw&quot;), mnist_27$test$y) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 2 7 ## 2 91 18 ## 7 15 76 ## ## Accuracy : 0.835 ## 95% CI : (0.7762, 0.8836) ## No Information Rate : 0.53 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.6682 ## ## Mcnemar&#39;s Test P-Value : 0.7277 ## ## Sensitivity : 0.8585 ## Specificity : 0.8085 ## Pos Pred Value : 0.8349 ## Neg Pred Value : 0.8352 ## Prevalence : 0.5300 ## Detection Rate : 0.4550 ## Detection Prevalence : 0.5450 ## Balanced Accuracy : 0.8335 ## ## &#39;Positive&#39; Class : 2 ## What are these measures? What is a “Confusion Matrix”? We will see them in the next section. But for now, let’s use another example. 21.5.2 Adult dataset This dataset provides information on income earning and attributes that may effect it. Information on the dataset is given at its website (Kohavi_1996?): Extraction from 1994 US. Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE&gt;16) &amp;&amp; (AGI&gt;100) &amp;&amp; (AFNLWGT&gt;1)&amp;&amp; (HRSWK&gt;0)). The prediction task is to determine whether a person makes over 50K a year. # Download adult income data # SET YOUR WORKING DIRECTORY FIRST # url.train &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&quot; # url.test &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test&quot; # url.names &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&quot; # download.file(url.train, destfile = &quot;adult_train.csv&quot;) # download.file(url.test, destfile = &quot;adult_test.csv&quot;) # download.file(url.names, destfile = &quot;adult_names.txt&quot;) # Read the training set into memory train &lt;- read.csv(&quot;adult_train.csv&quot;, header = FALSE) str(train) ## &#39;data.frame&#39;: 32561 obs. of 15 variables: ## $ V1 : int 39 50 38 53 28 37 49 52 31 42 ... ## $ V2 : chr &quot; State-gov&quot; &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; ... ## $ V3 : int 77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ... ## $ V4 : chr &quot; Bachelors&quot; &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; ... ## $ V5 : int 13 13 9 7 13 14 5 9 14 13 ... ## $ V6 : chr &quot; Never-married&quot; &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; ... ## $ V7 : chr &quot; Adm-clerical&quot; &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; ... ## $ V8 : chr &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; ... ## $ V9 : chr &quot; White&quot; &quot; White&quot; &quot; White&quot; &quot; Black&quot; ... ## $ V10: chr &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; ... ## $ V11: int 2174 0 0 0 0 0 0 0 14084 5178 ... ## $ V12: int 0 0 0 0 0 0 0 0 0 0 ... ## $ V13: int 40 13 40 40 40 40 16 45 50 40 ... ## $ V14: chr &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; ... ## $ V15: chr &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ... # Read the test set into memory test &lt;- read.csv(&quot;adult_test.csv&quot;, header = FALSE) The data doesn’t have the variable names. That’s bad because we don’t know which one is which. Check the adult_names.txt file. The list of variables is given in that file. Thanks to Matthew Baumer (Baumer_2015?), we can write them manually: varNames &lt;- c(&quot;Age&quot;, &quot;WorkClass&quot;, &quot;fnlwgt&quot;, &quot;Education&quot;, &quot;EducationNum&quot;, &quot;MaritalStatus&quot;, &quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;CapitalGain&quot;, &quot;CapitalLoss&quot;, &quot;HoursPerWeek&quot;, &quot;NativeCountry&quot;, &quot;IncomeLevel&quot;) names(train) &lt;- varNames names(test) &lt;- varNames str(train) ## &#39;data.frame&#39;: 32561 obs. of 15 variables: ## $ Age : int 39 50 38 53 28 37 49 52 31 42 ... ## $ WorkClass : chr &quot; State-gov&quot; &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; ... ## $ fnlwgt : int 77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ... ## $ Education : chr &quot; Bachelors&quot; &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; ... ## $ EducationNum : int 13 13 9 7 13 14 5 9 14 13 ... ## $ MaritalStatus: chr &quot; Never-married&quot; &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; ... ## $ Occupation : chr &quot; Adm-clerical&quot; &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; ... ## $ Relationship : chr &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; ... ## $ Race : chr &quot; White&quot; &quot; White&quot; &quot; White&quot; &quot; Black&quot; ... ## $ Sex : chr &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; ... ## $ CapitalGain : int 2174 0 0 0 0 0 0 0 14084 5178 ... ## $ CapitalLoss : int 0 0 0 0 0 0 0 0 0 0 ... ## $ HoursPerWeek : int 40 13 40 40 40 40 16 45 50 40 ... ## $ NativeCountry: chr &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; ... ## $ IncomeLevel : chr &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ... Since the dataset is large we are not going to use the test set but split the train set into our own test and train sets. Note that, however, if we had used the original test set, we would have had to make some adjustments/cleaning before using it. For example, if you look at Age variable, it seems as a factor variable. It is an integer in the training set. We have to change it first. Moreover, our \\(Y\\) has two levels in the train set, it has 3 levels in the test set. We have to go over each variable and make sure that the test and train sets have the same features and class types. This task is left to you if you want to use the original train and test sets. A final tip: remove the first row in the original test set! #Caret needs some preparations! table(train$IncomeLevel) ## ## &lt;=50K &gt;50K ## 24720 7841 # This is b/c we will use the same data for LPM train$Y &lt;- ifelse(train$IncomeLevel == &quot; &lt;=50K&quot;, 0, 1) train &lt;- train[,-15] # kNN needs Y to be a factor variable train$Y &lt;- as.factor(train$Y) levels(train$Y)[levels(train$Y) == &quot;0&quot;] &lt;- &quot;Less&quot; levels(train$Y)[levels(train$Y) == &quot;1&quot;] &lt;- &quot;More&quot; levels(train$Y) ## [1] &quot;Less&quot; &quot;More&quot; #kNN set.seed(3033) train_df &lt;- caret::createDataPartition(y = train$Y, p = 0.7, list = FALSE) training &lt;- train[train_df, ] testing &lt;- train[-train_df, ] #Training/Model building with 10-k cross validation cv &lt;- caret::trainControl(method = &quot;cv&quot;, number = 10, p = 0.9) model_knn3 &lt;- caret::train( Y ~ ., method = &quot;knn&quot;, data = training, tuneGrid = data.frame(k = seq(9, 41 , 2)), trControl = cv ) ggplot(model_knn3, highlight = TRUE) Now we are going to use the test set to see the model’s performance. caret::confusionMatrix(predict(model_knn3, testing, type = &quot;raw&quot;), testing$Y) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Less More ## Less 7311 1871 ## More 105 481 ## ## Accuracy : 0.7977 ## 95% CI : (0.7896, 0.8056) ## No Information Rate : 0.7592 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.256 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9858 ## Specificity : 0.2045 ## Pos Pred Value : 0.7962 ## Neg Pred Value : 0.8208 ## Prevalence : 0.7592 ## Detection Rate : 0.7485 ## Detection Prevalence : 0.9400 ## Balanced Accuracy : 0.5952 ## ## &#39;Positive&#39; Class : Less ## Next, as you can guess, we will delve into these performance measures. Learning algorithm may not be evaluated only by its predictive capacity. We may want to interpret the results by identifying the important predictors and their importance. There is always a trade-off between interpretability and predictive accuracy. Here is a an illustration. We will talk about this later in the book. 21.6 Tuning in Classification What metrics are we going to use when we train our classification models? In kNN, for example, our hyperparameter is \\(k\\), the number of observations in each bin. In our applications with mnist_27 and Adult datasets, \\(k\\) was determined by a metric called as accuracy. What is it? If the choice of \\(k\\) depends on what metrics we use in tuning, can we improve our prediction performance by using a different metric? Moreover, the accuracy is calculated from the confusion table. Yet, the confusion table will be different for a range of discriminating thresholds used for labeling predicted probabilities. These are important questions in classification problems. We will begin answering them in this chapter. 21.7 Confusion matrix In general, whether it is for training or not, measuring the performance of a classification model is an important issue and has to be well understood before fitting or training a model. To evaluate a model’s fit, we can look at its predictive accuracy. In classification problems, this requires predicting \\(Y\\), as either 0 or 1, from the predicted value of \\(p(x)\\), such as \\[ \\hat{Y}=\\left\\{\\begin{array}{ll}{1,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&gt;\\frac{1}{2}} \\\\ {0,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&lt;\\frac{1}{2}}\\end{array}\\right. \\] From this transformation of \\(\\hat{p}(x)\\) to \\(\\hat{Y}\\), the overall predictive accuracy can be summarized with a matrix, \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {Y=1} &amp; {Y=0} \\\\ {\\hat{Y}=1} &amp; {\\text { TP }_{}} &amp; {\\text { FP }_{}} \\\\ {\\hat{Y}=0} &amp; {\\text { FN }_{}} &amp; {\\text { TN }_{}}\\end{array} \\] where, TP, FP, FN, TN are True positives, False Positives, False Negatives, and True Negatives, respectively. This table is also know as Confusion Table or confusion matrix. The name, confusion, is very intuitive because it is easy to see how the system is confusing two classes. There are many metrics that can be calculated from this table. Let’s use an example given in Wikipedia \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {Y=Cat} &amp; {Y=Dog} \\\\ {\\hat{Y}=Cat} &amp; {\\text { 5 }_{}} &amp; {\\text { 2 }_{}} \\\\ {\\hat{Y}=Dog} &amp; {\\text { 3 }_{}} &amp; {\\text { 3 }_{}}\\end{array} \\] According to this confusion matrix, there are 8 actual cats and 5 actual dogs (column totals). The learning algorithm, however, predicts only 5 cats and 3 dogs correctly. The model predicts 3 cats as dogs and 2 dogs as cats. All correct predictions are located in the diagonal of the table, so it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal. In predictive analytics, this table (matrix) allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy (\\((TP+TN)/n\\)) is not a reliable metric for the real performance of a classifier, when the dataset is unbalanced in terms of numbers of observations in each class. It can be seen how misleading the use of \\((TP+TN)/n\\) could be, if there were 95 cats and only 5 dogs in our example. If we choose accuracy as the performance measure in our training, our learning algorithm might classify all the observations as cats, because the overall accuracy would be 95%. In that case, however, all the dog would be misclassified as cats. 21.8 Performance measures Which metrics should we be using in training our classification models? These questions are more important when the classes are not in balance. Moreover, in some situation, false predictions would be more important then true predictions. In a situation that you try to predict, for example, cancer, minimizing false negatives (the model misses cancer patients) would be more important than minimizing false positives (the model wrongly predicts cancer). When we have an algorithm to predict spam emails, however, false positives would be the target to minimize rather than false negatives. Here is the full picture of various metrics using the same confusion table from Wikipedia: Let’s summarize some of the metrics and their use with examples for detecting cancer: Accuracy: the number of correct predictions (with and without cancer) relative to the number of observations (patients). This can be used when the classes are balanced with not less than a 60-40% split. \\((TP+TN)/n\\). Balanced Accuracy: when the class balance is worse than 60-40% split, \\((TP/P + TN/N)/2\\). Precision: the percentage positive predictions that are correct. That is, the proportion of patients that we predict as having cancer, actually have cancer, \\(TP/(TP+FP)\\). Sensitivity: the percentage of positives that are predicted correctly. That is, the proportion of patients that actually have cancer was correctly predicted by the algorithm as having cancer, \\(TP/(TP+FN)\\). This measure is also called as True Positive Rate or as Recall. Specificity: the percentage of negatives that are predicted correctly. Proportion of patients that do not have cancer, are predicted by the model as non-cancerous, This measure is also called as True Positive Rate = \\(TN/(TN+FP)\\). Here is the summary: \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {Y=Cat} &amp; {Y=Dog} \\\\ {\\hat{Y}=Cat} &amp; {\\text {TPR or Sensitivity }_{}} &amp; {\\text { FPR or Fall-out }_{}} \\\\ {\\hat{Y}=Dog} &amp; {\\text { FNR or Miss Rate }_{}} &amp; {\\text { TNR or Specificity }_{}}\\end{array} \\] Kappa is also calculated in most cases. It is an interesting measure because it compares the actual performance of prediction with what it would be if a random prediction was carried out. For example, suppose that your model predicts \\(Y\\) with 95% accuracy. How good your prediction power would be if a random choice would also predict 70% of \\(Y\\)s correctly? Let’s use an example: \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {Y=Cat} &amp; {Y=Dog} \\\\ {\\hat{Y}=Cat} &amp; {\\text { 22 }_{}} &amp; {\\text { 9 }_{}} \\\\ {\\hat{Y}=Dog} &amp; {\\text { 7 }_{}} &amp; {\\text { 13 }_{}}\\end{array} \\] In this case the accuracy is \\((22+13)/51 = 0.69\\) But how much of it is due the model’s performance itself? In other words, the distribution of cats and dogs can also give a predictive clue such that a certain level of prediction accuracy can be achieved by chance without any learning algorithm. For the TP cell in the table, this can be calculated as the difference between observed accuracy (OA) and expected accuracy (EA), \\[ \\mathrm{(OA-EA)_{TP}}=\\mathrm{Pr}(\\hat{Y}=Cat)[\\mathrm{Pr}(Y=Cat |\\hat{Y}= Cat)-\\mathrm{P}(Y=Cat)], \\] Remember from your statistics class, if the two variables are independent, the conditional probability of \\(X\\) given \\(Y\\) has to be equal to the marginal probability of \\(X\\). Therefore, inside the brackets, the difference between the conditional probability, which reflects the probability of predicting cats due to the model, and the marginal probability of observing actual cats reflects the true level of predictive power of the model by removing the randomness in prediction. \\[ \\mathrm{(OA-EA)_{TN}}=\\mathrm{Pr}(\\hat{Y}=Dog)[\\mathrm{Pr}(Y=Dog |\\hat{Y}= Dog)-\\mathrm{P}(Y=Dog)], \\] If we use the joint and marginal probability definitions, these can be written as: \\[ OA-EA=\\frac{m_{i j}}{n}-\\frac{m_{i} m_{j}}{n^{2}} \\] Here is the calculation of Kappa for our example: Total, \\(n = 51\\), \\(OA-EA\\) for \\(TP\\) = \\(22/51-31 \\times (29/51^2) = 0.0857\\) \\(OA-EA\\) for \\(TN\\) = \\(13/51-20 \\times (21/51^2) = 0.0934\\) And we normalize it by \\(1-EA = 1- 31 \\times (29/51^2) + 20 \\times (21/51^2) = 0.51\\), which is the value if the prediction was 100% successful. Hence, Kappa: \\((0.0857+0.0934) / (1 - 0.51) = 0.3655\\) Finally, Jouden’s J statistics also as known as Youden’s index or Informedness, is a single statistics that captures the performance of prediction. It’s simply \\(J=TPR+TNR-1\\) and ranges between 0 and 1 indicating useless and perfect prediction performance, respectively. This metric is also related to Receiver Operating Characteristics (ROC) curve analysis, which is the subject of next section. 21.9 ROC Curve Our outcome variable is categorical (\\(Y = 1\\) or \\(0\\)). Most classification algorithms calculate the predicted probability of success (\\(Y = 1\\)). If the probability is larger than a fixed cut-off threshold (discriminating threshold), then we assume that the model predicts success (Y = 1); otherwise, we assume that it predicts failure. As a result of such a procedure, the comparison of the observed and predicted values summarized in a confusion table depends on the threshold. The predictive accuracy of a model as a function of threshold can be summarized by Area Under Curve (AUC) of Receiver Operating Characteristics (ROC). The ROC curve, which is is a graphical plot that illustrates the diagnostic ability of a binary classifier, indicates a trade-off between True Positive Rate (TPR) and False Positive Rate (FPR). Hence, the success of a model comes with its predictions that increases TPR without raising FPR. The ROC curve was first used during World War II for the analysis of radar signals before it was employed in signal detection theory. Here is a visualization: Let’s start with an example, where we have 100 individuals, 50 with \\(y_i=1\\) and 50 with \\(y_i=0\\), which is well-balanced. If we use a discriminating threshold (0%) that puts everybody into Category 1 or a threshold (100%) that puts everybody into Category 2, that is, \\[ \\hat{Y}=\\left\\{\\begin{array}{ll}{1,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&gt;0 \\%} \\\\ {0,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)\\leq0 \\%}\\end{array}\\right. \\] and, \\[ \\hat{Y}=\\left\\{\\begin{array}{ll}{1,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&gt;100 \\%} \\\\ {0,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)\\leq100 \\%}\\end{array}\\right. \\] this would have led to the following confusing tables, respectively: \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {Y=1} &amp; {Y=0} \\\\ {\\hat{Y}=1} &amp; {\\text { 50 }_{}} &amp; {\\text { 50 }_{}} \\\\ {\\hat{Y}=0} &amp; {\\text { 0 }_{}} &amp; {\\text { 0 }_{}}\\end{array} \\] \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {Y=1} &amp; {Y=0} \\\\ {\\hat{Y}=1} &amp; {\\text { 0 }_{}} &amp; {\\text { 0 }_{}} \\\\ {\\hat{Y}=0} &amp; {\\text { 50 }_{}} &amp; {\\text { 50 }_{}}\\end{array} \\] In the first case, \\(TPR = 1\\) and \\(FPR = 1\\); and in the second case, \\(TPR = 0\\) and \\(FPR = 0\\). So when we calculate all possible confusion tables with different values of thresholds ranging from 0% to 100%, we will have the same number of (\\(TPR, FPR\\)) points each corresponding to one threshold. The ROC curve is the curve that connects these points. Let’s use an example with the Boston Housing Market dataset to illustrate ROC: library(MASS) data(Boston) # Create our binary outcome data &lt;- Boston[, -14] #Dropping &quot;medv&quot; data$dummy &lt;- c(ifelse(Boston$medv &gt; 25, 1, 0)) # Use logistic regression for classification model &lt;- glm(dummy ~ ., data = data, family = &quot;binomial&quot;) summary(model) ## ## Call: ## glm(formula = dummy ~ ., family = &quot;binomial&quot;, data = data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.312511 4.876070 1.090 0.275930 ## crim -0.011101 0.045322 -0.245 0.806503 ## zn 0.010917 0.010834 1.008 0.313626 ## indus -0.110452 0.058740 -1.880 0.060060 . ## chas 0.966337 0.808960 1.195 0.232266 ## nox -6.844521 4.483514 -1.527 0.126861 ## rm 1.886872 0.452692 4.168 3.07e-05 *** ## age 0.003491 0.011133 0.314 0.753853 ## dis -0.589016 0.164013 -3.591 0.000329 *** ## rad 0.318042 0.082623 3.849 0.000118 *** ## tax -0.010826 0.004036 -2.682 0.007314 ** ## ptratio -0.353017 0.122259 -2.887 0.003884 ** ## black -0.002264 0.003826 -0.592 0.554105 ## lstat -0.367355 0.073020 -5.031 4.88e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 563.52 on 505 degrees of freedom ## Residual deviance: 209.11 on 492 degrees of freedom ## AIC: 237.11 ## ## Number of Fisher Scoring iterations: 7 And our prediction (in-sample): # Classified Y&#39;s by TRUE and FALSE yHat &lt;- model$fitted.values &gt; 0.5 conf_table &lt;- table(yHat, data$dummy) #let&#39;s change the order of cells ctt &lt;- as.matrix(conf_table) ct &lt;- matrix(0, 2, 2) ct[1,1] &lt;- ctt[2,2] ct[2,2] &lt;- ctt[1,1] ct[1,2] &lt;- ctt[2,1] ct[2,1] &lt;- ctt[1,2] rownames(ct) &lt;- c(&quot;Yhat = 1&quot;, &quot;Yhat = 0&quot;) colnames(ct) &lt;- c(&quot;Y = 1&quot;, &quot;Y = 0&quot;) ct ## Y = 1 Y = 0 ## Yhat = 1 100 16 ## Yhat = 0 24 366 It would be much easier if we create our own function to rotate a matrix/table: rot &lt;- function(x){ t &lt;- apply(x, 2, rev) tt &lt;- apply(t, 1, rev) return(t(tt)) } ct &lt;- rot(conf_table) rownames(ct) &lt;- c(&quot;Yhat = 1&quot;, &quot;Yhat = 0&quot;) colnames(ct) &lt;- c(&quot;Y = 1&quot;, &quot;Y = 0&quot;) ct ## ## yHat Y = 1 Y = 0 ## Yhat = 1 100 16 ## Yhat = 0 24 366 Now we calculate our TPR, FPR, and J-Index: #TPR TPR &lt;- ct[1,1]/(ct[1,1]+ct[2,1]) TPR ## [1] 0.8064516 #FPR FPR &lt;- ct[1,2]/(ct[1,2]+ct[2,2]) FPR ## [1] 0.04188482 #J-Index TPR-FPR ## [1] 0.7645668 These rates are calculated for the threshold of 0.5. We can have all pairs of \\(TPR\\) and \\(FPR\\) for all possible discrimination thresholds. What’s the possible set? We will use our \\(\\hat{P}\\) values for this. #We create an ordered grid from our fitted values summary(model$fitted.values) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000000 0.004205 0.035602 0.245059 0.371758 0.999549 phat &lt;- model$fitted.values[order(model$fitted.values)] length(phat) ## [1] 506 #We need to have containers for the pairs of TPR and FPR TPR &lt;- c() FPR &lt;- c() #Now the loop for (i in 1:length(phat)) { yHat &lt;- model$fitted.values &gt; phat[i] conf_table &lt;- table(yHat, data$dummy) ct &lt;- as.matrix(conf_table) if(sum(dim(ct))&gt;3){ #here we ignore the thresholds 0 and 1 TPR[i] &lt;- ct[2,2]/(ct[2,2]+ct[1,2]) FPR[i] &lt;- ct[2,1]/(ct[1,1]+ct[2,1]) } } plot(FPR, TPR, col= &quot;blue&quot;, type = &quot;l&quot;, main = &quot;ROC&quot;, lwd = 3) abline(a = 0, b = 1, col=&quot;red&quot;) Several things we observe on this curve. First, there is a trade-off between TPF and FPR. Approximately, after 70% of TPR, an increase in TPF can be achieved by increasing FPR, which means that if we care more about the possible lowest FPR, we can fix the discriminating rate at that point. Second, we can identify the best discriminating threshold that makes the distance between TPR and FPR largest. In other words, we can identify the threshold where the marginal gain on TPR would be equal to the marginal cost of FPR. This can be achieved by the Jouden’s J statistics, \\(J=TPR+TNR-1\\), which identifies the best discriminating threshold. Note that \\(TNR= 1-FPR\\). Hence \\(J = TPR-FPR\\). # Youden&#39;s J Statistics J &lt;- TPR - FPR # The best discriminating threshold phat[which.max(J)] ## 231 ## 0.1786863 #TPR and FPR at this threshold TPR[which.max(J)] ## [1] 0.9354839 FPR[which.max(J)] ## [1] 0.1361257 J[which.max(J)] ## [1] 0.7993582 This simple example shows that the best (in-sample) fit can be achieved by \\[ \\hat{Y}=\\left\\{\\begin{array}{ll}{1,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&gt;17.86863 \\%} \\\\ {0,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)\\leq17.86863 \\%}\\end{array}\\right. \\] 21.10 AUC - Area Under the Curve Finally, we measure the predictive accuracy by the area under the ROC curve. An area of 1 represents a perfect performance; an area of 0.5 represents a worthless prediction. This is because an area of 0.5 suggests its performance is no better than random chance. For example, an accepted rough guide for classifying the accuracy of a diagnostic test in medical procedures is 0.90-1.00 = Excellent (A) 0.80-0.90 = Good (B) 0.70-0.80 = Fair (C) 0.60-0.70 = Poor (D) 0.50-0.60 = Fail (F) Since the formula and its derivation is beyond the scope of this chapter, we will use the package ROCR to calculate it. library(ROCR) data$dummy &lt;- c(ifelse(Boston$medv &gt; 25, 1, 0)) model &lt;- glm(dummy ~ ., data = data, family = &quot;binomial&quot;) phat &lt;- model$fitted.values phat_df &lt;- data.frame(phat, &quot;Y&quot; = data$dummy) pred_rocr &lt;- prediction(phat_df[,1], phat_df[,2]) perf &lt;- performance(pred_rocr,&quot;tpr&quot;,&quot;fpr&quot;) plot(perf, colorize=TRUE) abline(a = 0, b = 1) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUC &lt;- auc_ROCR@y.values[[1]] AUC ## [1] 0.9600363 This ROC curve is the same as the one that we developed earlier. When we train a model, in each run (different train and test sets) we will obtain a different AUC. Differences in AUC across train and validation sets creates an uncertainty about AUC. Consequently, the asymptotic properties of AUC for comparing alternative models has become a subject of discussions in the literature. Another important point is that, while AUC represents the entire area under the curve, our interest would be on a specific location of TPR or FPR. Hence it’s possible that, for any given two competing algorithms, while one prediction algorithm has a higher overall AUC, the other one could have a better AUC in that specific location. This issue can be seen in the following figure taken from Bad practices in evaluation methodology relevant to class-imbalanced problems by Jan Brabec and Lukas Machlica (Brab_2018?). For example, in the domain of network traffic intrusion-detection, the imbalance ratio is often higher than 1:1000, and the cost of a false alarm for an applied system is very high. This is due to increased analysis and remediation costs of infected devices. In such systems, the region of interest on the ROC curve is for false positive rate at most 0.0001. If AUC was computed in the usual way over the complete ROC curve then 99.99% of the area would be irrelevant and would represent only noise in the final outcome. We demonstrate this phenomenon in Figure 1. If AUC has to be used, we suggest to discuss the region of interest, and eventually compute the area only at this region. This is even more important if ROC curves are not presented, but only AUCs of the compared algorithms are reported. Most of the challenges in classification problems are related to class imbalances in the data. We look at this issue in Cahpter 39. 21.11 Classification Example We can conclude this section with a classification example. We will use Adult dataset. The information on the dataset is given at the Machine Learning Repository at UCI (Kohavi_1996?): The prediction task is to determine whether a person makes over $50K a year. This question would be similar to the question of whether the person makes less than 50K. However, we need to be careful in defining which class will be positive or negative. Suppose we have \\(Y\\), 0 and 1, and we define 1 as a positive class: \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {Y=1+} &amp; {Y=0-} \\\\ {\\hat{Y}=1+} &amp; {\\text { TP }_{}} &amp; {\\text { FP }_{}} \\\\ {\\hat{Y}=0-} &amp; {\\text { FN }_{}} &amp; {\\text { TN }_{}}\\end{array} \\] Now suppose we define 1 as a negative class: \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {Y=0+} &amp; {Y=1-} \\\\ {\\hat{Y}=0+} &amp; {\\text { TP }_{}} &amp; {\\text { FP }_{}} \\\\ {\\hat{Y}=1-} &amp; {\\text { FN }_{}} &amp; {\\text { TN }_{}}\\end{array} \\] Of course this is just a notational difference and nothing changes in calculations. But some performance measures, especially, sensitivity (TPR) and fall-out (FPR) will be different. We are going to use the original train set again to avoid some data cleaning jobs that we mentioned in Chapter 5. # Download adult income data # url.train &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&quot; # url.names &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&quot; # download.file(url.train, destfile = &quot;adult_train.csv&quot;) # download.file(url.names, destfile = &quot;adult_names.txt&quot;) # Read the training set into memory df &lt;- read.csv(&quot;adult_train.csv&quot;, header = FALSE) varNames &lt;- c(&quot;Age&quot;, &quot;WorkClass&quot;, &quot;fnlwgt&quot;, &quot;Education&quot;, &quot;EducationNum&quot;, &quot;MaritalStatus&quot;, &quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;CapitalGain&quot;, &quot;CapitalLoss&quot;, &quot;HoursPerWeek&quot;, &quot;NativeCountry&quot;, &quot;IncomeLevel&quot;) names(df) &lt;- varNames data &lt;- df In each machine learning application, the data preparation stage (i.e. cleaning the data, organizing the columns and rows, checking out the columns’ names, checking the types of each feature, identifying and handling the missing observations, etc) is a very important step and should be dealt with a good care. First, let’s see if the data balanced or not: tbl &lt;- table(data$IncomeLevel) tbl ## ## &lt;=50K &gt;50K ## 24720 7841 tbl[2] / tbl[1] ## &gt;50K ## 0.3171926 There are multiple variables that are chr in the data. str(data) ## &#39;data.frame&#39;: 32561 obs. of 15 variables: ## $ Age : int 39 50 38 53 28 37 49 52 31 42 ... ## $ WorkClass : chr &quot; State-gov&quot; &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; ... ## $ fnlwgt : int 77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ... ## $ Education : chr &quot; Bachelors&quot; &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; ... ## $ EducationNum : int 13 13 9 7 13 14 5 9 14 13 ... ## $ MaritalStatus: chr &quot; Never-married&quot; &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; ... ## $ Occupation : chr &quot; Adm-clerical&quot; &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; ... ## $ Relationship : chr &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; ... ## $ Race : chr &quot; White&quot; &quot; White&quot; &quot; White&quot; &quot; Black&quot; ... ## $ Sex : chr &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; ... ## $ CapitalGain : int 2174 0 0 0 0 0 0 0 14084 5178 ... ## $ CapitalLoss : int 0 0 0 0 0 0 0 0 0 0 ... ## $ HoursPerWeek : int 40 13 40 40 40 40 16 45 50 40 ... ## $ NativeCountry: chr &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; ... ## $ IncomeLevel : chr &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ... table(data$WorkClass) ## ## ? Federal-gov Local-gov Never-worked ## 1836 960 2093 7 ## Private Self-emp-inc Self-emp-not-inc State-gov ## 22696 1116 2541 1298 ## Without-pay ## 14 table(data$NativeCountry) ## ## ? Cambodia ## 583 19 ## Canada China ## 121 75 ## Columbia Cuba ## 59 95 ## Dominican-Republic Ecuador ## 70 28 ## El-Salvador England ## 106 90 ## France Germany ## 29 137 ## Greece Guatemala ## 29 64 ## Haiti Holand-Netherlands ## 44 1 ## Honduras Hong ## 13 20 ## Hungary India ## 13 100 ## Iran Ireland ## 43 24 ## Italy Jamaica ## 73 81 ## Japan Laos ## 62 18 ## Mexico Nicaragua ## 643 34 ## Outlying-US(Guam-USVI-etc) Peru ## 14 31 ## Philippines Poland ## 198 60 ## Portugal Puerto-Rico ## 37 114 ## Scotland South ## 12 80 ## Taiwan Thailand ## 51 18 ## Trinadad&amp;Tobago United-States ## 19 29170 ## Vietnam Yugoslavia ## 67 16 We can see that there is only one observation in Holand-Netherlands. This is a problem because it will be either in the training set or the test set. Therefore, when you estimate without taking care of it, it will give this error: Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : factor NativeCountry has new levels Holand-Netherlands We will see later how to take care of these issues in a loop with several error handling options. But now, let’s drop this observation: ind &lt;- which(data$NativeCountry ==&quot; Holand-Netherlands&quot;) data &lt;- data[-ind, ] Although some packages like lm() and glm() can use character variables, we should take care of them properly before any type of data analysis. Here is an example: df &lt;- data #converting by a loop for (i in 1:ncol(df)) { if (is.character(df[, i])) df[, i] &lt;- as.factor(df[, i]) } df &lt;- data #Converting with `apply()` family df[sapply(df, is.character)] &lt;- lapply(df[sapply(df, is.character)], as.factor) The job is to use LPM, Logistic, and kNN models to see which one could be a better predictive model for the data. In LPM and Logistic, we do not (yet) have any parameter to tune for a better prediction. Although we could use a degree of polynomials for selected features, we will set aside that option for now. We will later see regularization methods for parametric models, which will make LPM and logistic models “trainable”. In kNN, \\(k\\) is the hyperparameter to train the model. There are several key points to keep in mind in this classification practice: What performance metric(s) are we going to use for comparing the alternative models? How are we going to transform the predicted probabilities to classes (0’s and 1’s) so that we can have the confusion matrix? Let’s start with LPM first. 21.12 LPM anyNA(data) ## [1] FALSE # Our LPM requires data$Y &lt;- ifelse(data$IncomeLevel==&quot; &lt;=50K&quot;, 0, 1) data &lt;- data[, -15] Now, we are ready. We will use ROC and AUC for comparing the models. library(ROCR) AUC &lt;- c() t = 100 # number of times we loop for (i in 1:t) { set.seed(i) shuffle &lt;- sample(nrow(data), nrow(data), replace = FALSE) k &lt;- 5 testind &lt;- shuffle[1:(nrow(data) / k)] trainind &lt;- shuffle[-testind] trdf &lt;- data[trainind, ] #80% of the data tsdf &lt;- data[testind, ] #20% of data set a side #LPM model1 &lt;- glm(Y ~ ., data = trdf, family = &quot;gaussian&quot;) phat &lt;- predict(model1, tsdf) phat[phat &lt; 0] &lt;- 0 phat[phat &gt; 1] &lt;- 1 # ROC &amp; AUC (from ROCR) phat_df &lt;- data.frame(phat, &quot;Y&quot; = tsdf$Y) pred_rocr &lt;- prediction(phat_df[, 1], phat_df[, 2]) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUC[i] &lt;- auc_ROCR@y.values[[1]] } plot(AUC, col = &quot;grey&quot;) abline(a = mean(AUC), b = 0, col = &quot;red&quot;) mean(AUC) ## [1] 0.8936181 sqrt(var(AUC)) ## [1] 0.003810335 Let’s see the ROC curve from the last run. # ROC from the last run by `ROCR` perf &lt;- performance(pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) plot(perf, colorize = TRUE) abline(a = 0, b = 1) # And our &quot;own&quot; ROC phator &lt;- phat[order(phat)] phator[phator &lt; 0] &lt;- 0 phator[phator &gt; 1] &lt;- 1 phator &lt;- unique(phator) TPR &lt;- c() FPR &lt;- c() for (i in 1:length(phator)) { yHat &lt;- phat &gt; phator[i] conf_table &lt;- table(yHat, tsdf$Y) ct &lt;- as.matrix(conf_table) if (sum(dim(ct)) &gt; 3) { #here we ignore the min and max thresholds TPR[i] &lt;- ct[2, 2] / (ct[2, 2] + ct[1, 2]) FPR[i] &lt;- ct[2, 1] / (ct[1, 1] + ct[2, 1]) } } # Flat and vertical sections are omitted plot(FPR, TPR, col = &quot;blue&quot;, type = &quot;l&quot;, main = &quot;ROC&quot;) abline(a = 0, b = 1, col = &quot;red&quot;) What’s the confusion table at the “best” discriminating threshold? The answer is the one where the difference between TPR and FPR is maximized: Youden’s J Statistics. Note that this answers would be different if we have different weights in TPR and FPR. We may also have different targets, maximum FPR, for example. # Youden&#39;s J Statistics J &lt;- TPR - FPR # The best discriminating threshold opt_th &lt;- phator[which.max(J)] opt_th ## [1] 0.318723 #TPR and FPR at this threshold TPR[which.max(J)] ## [1] 0.8494898 FPR[which.max(J)] ## [1] 0.2024676 J[which.max(J)] ## [1] 0.6470222 And the confusion table (from the last run): yHat &lt;- phat &gt; opt_th conf_table &lt;- table(yHat, tsdf$Y) # Function to rotate the table rot &lt;- function(x){ t &lt;- apply(x, 2, rev) tt &lt;- apply(t, 1, rev) return(t(tt)) } # Better looking table ct &lt;- rot(conf_table) rownames(ct) &lt;- c(&quot;Yhat = 1&quot;, &quot;Yhat = 0&quot;) colnames(ct) &lt;- c(&quot;Y = 1&quot;, &quot;Y = 0&quot;) ct ## ## yHat Y = 1 Y = 0 ## Yhat = 1 1332 1001 ## Yhat = 0 236 3943 Note that the optimal threshold is almost the ratio of cases in the data around 31%. We will come back to this issue later. 21.13 Logistic Regression TBA FROM ORIGINAL CHAPTER Both LPM and Logistic methods are linear classifiers. We can add polynomials and interactions manually to capture possible nonlinearities in the data but that would be an impossible job as the number of features would grow exponentially. This brings us to a nonparametric classifier, kNN. 21.14 kNN We will train kNN with the choice of \\(k\\) and use AUC as our performance criteria in choosing \\(k\\). 21.14.1 kNN 10-fold CV There are several packages in R for kNN applications: knn() from the class package and knn3() in the caret package. We will use knn3() in the caret package. Since kNN use distances, we should scale the numerical variables first to make their magnitudes on the same scale. TBA FROM ORIGINAL CHAPTER Now, it can also be used with knn() from the class package. Note that kNN gets unstable as the number of variables increases. We can see it by calculating test AUC multiple times by adding an outer loop to our algorithm. 21.14.2 kNN with caret TBA FROM ORIGINAL CHAPTER We now know two things: (1) how good the prediction is with kNN; (2) how good it is relative to other “base” or “benchmark” models. These two questions must be answered every time to evaluate the prediction performance of a machine learning algorithm. Although we didn’t calculate the test AUC in our own kNN algorithm, we can accept that kNN performance is good with AUC that is close to 90%. However, it is not significantly better than LPM and Logistic "],["time-series.html", "Chapter 22 Time Series 22.1 ARIMA models 22.2 Hyndman-Khandakar algorithm 22.3 TS Plots 22.4 Box-Cox transformation 22.5 Stationarity 22.6 Modeling ARIMA 22.7 Grid search for ARIMA 22.8 Hyperparameter tuning with time-series data: 22.9 Speed", " Chapter 22 Time Series (PART) Time Series {.unnumbered} Forecasting {.unnumbered} Time series forecasting is a task that involves using a model to predict future values of a time series based on its past values. The data consists of sequences of values that are recorded at regular intervals over a period of time, such as daily stock prices or monthly weather data. Time series forecasting can be approached using a variety of machine learning techniques, including linear regression, decision trees, and neural networks. One key difference between time series forecasting and other types of machine learning tasks is the presence of temporal dependencies in the data. In time series data, the value at a particular time point is often influenced by the values that came before it, which means that the order in which the data points are presented is important. This can make time series forecasting more challenging, as the model must take into account the relationships between past and future values in order to make accurate predictions. One of the most accessible and comprehensive source on forecasting using R is Forecasting: Principles and Practice (FPP3) by Rob J Hyndman and George Athanasopoulos. The book now has the \\(3^{rd}\\) edition that uses the tsibble and fable packages rather than the forecast package. This brings a better integration to the tidyverse collection of packages. A move from FPP2 to FPP3 brings a move from forecast to fable. The main difference is that fable is designed for tsibble objects and forecast is designed for ts objects 8. In this section, we will use the tsibble and fable packages along with the fpp3 package and cover five main topics: applications with ARIMA models, grid search for ARIMA, time series embedding, forecasting with random forests, and artificial neural network applications, RNN and LSTM. The time-series analysis and forecasting is a very deep and complex subject, which is beyond the scope of this book to cover in detail. FPP3 is free and very accessible even for those without a strong background on time-series forecasting. Therefore, this section assumes that some major concepts, like stationarity, time series decomposition, and exponential smoothing, are already understood by further readings of FPP3. 22.1 ARIMA models ARIMA (Autoregressive Integrated Moving Average) is a main statistical model for time series forecasting. It is a linear parametric model that can be used to analyze and forecast data that exhibit temporal dependencies, such as seasonality and autocorrelation. The model is comprised of three components: Autoregressive (AR) component, which models the dependencies between the current value and the past values in the data. Integrated (I) component, which refers to the degree of differencing that is applied to the time series data. The degree of differencing is the number of times that the data is differenced in order to make it stationary. The stationarity means that the mean, variance, and covariance are constant over time. Moving average (MA) component, which models the dependencies between the current and the past forecast errors. The MA component of an ARIMA model is used to capture the short-term fluctuations in data that are not captured by the AR component. For example, if the time series data exhibits random noise or sudden spikes, the MA component can help to smooth out these fluctuations and improve the forecast accuracy. The ARIMA model can be written as ARIMA(p,d,q), where p is the order of the autoregressive component, d is the degree of differencing, and q is the order of the moving average component. The values of p, d, and q are chosen based on the characteristics of the time series data to achieve maximum forecasting accuracy. To use the ARIMA model, the time series data must first be preprocessed to remove any trend and seasonality, and to ensure that the data is stationary. The model is then fit to the preprocessed data, and forecasts are generated based on the fitted model. The mathematical foundation of the ARIMA model is based on the concept of autoregressive (AR) and moving average (MA) processes. An autoregressive process is a type of stochastic process in which the current value of a time series depends on a linear combination of past values of the series. An autoregressive process can be represented mathematically as: \\[ X_{t} = c + \\sum_{i=1}^{p}(\\phi_{i} X_{t-i}) + \\epsilon_{t}, \\] where \\(X_{t}\\) is the value of the time series at time \\(t\\), \\(c\\) is a constant, \\(\\phi_{i}\\) is the autoregressive coefficient for lag \\(i\\), and \\(\\epsilon_{t}\\) is white noise (a sequence of random variables with a mean of zero and a constant variance). A moving average process is a type of stochastic process in which the current value of a time series depends on a linear combination of past errors or residuals (the difference between the actual value and the forecasted value). A moving average process can be represented mathematically as: \\[ X_{t} = c + \\sum_{i=1}^{q}(\\theta_{i} \\epsilon_{t-i}) + \\epsilon_{t}, \\] where \\(\\theta_{i}\\) is the moving average coefficient for lag \\(i\\), and \\(\\epsilon_{t}\\) is again white noise. The ARIMA model, which is a combination of autoregressive and moving average processes, can be represented mathematically as: \\[ X_{t} = c + \\sum_{i=1}^{p}(\\phi_{i} X_{t-i}) + \\sum_{i=1}^{q}(\\theta_{i} \\epsilon_{t-i}) + \\epsilon_{t} \\] It is possible to write any stationary \\(\\operatorname{AR}(p)\\) model as an MA(\\(\\infty\\)) model by using repeated substitution. Here is the example for an \\(\\mathrm{AR}(1)\\) model without a constant: \\[ X_{t} = \\phi_{1} X_{t-1} + \\epsilon_{t} ~~~ \\text{and} ~~~ X_{t-1} = \\phi_{1} X_{t-2} + \\epsilon_{t-1}\\\\ X_{t}=\\phi_1\\left(\\phi_1 X_{t-2}+\\epsilon_{t-1}\\right)+\\epsilon_t\\\\ =\\phi_1^2 X_{t-2}+\\phi_1 \\epsilon_{t-1}+\\epsilon_t\\\\ =\\phi_1^3 X_{t-3}+\\phi_1^2 \\epsilon_{t-2}+\\phi_1 \\epsilon_{t-1}+\\epsilon_t\\\\ \\vdots \\] With \\(-1&lt;\\phi_1&lt;1\\), the value of \\(\\phi_1^k\\) will get smaller as \\(k\\) gets bigger. Therefore, \\(\\mathrm{AR}(1)\\) becomes an MA \\((\\infty)\\) process: \\[ X_t=\\epsilon_t+\\phi_1 \\epsilon_{t-1}+\\phi_1^2 \\epsilon_{t-2}+\\phi_1^3 \\epsilon_{t-3}+\\cdots \\] The parameters of the ARIMA model (\\(c\\), \\(\\phi_{i}\\), \\(\\theta_{i}\\)) are estimated using maximum likelihood estimation (MLE), which involves finding the values of the parameters that maximize the likelihood of the observed data given the model. Once the model has been fit to the data, it can be used to make point forecasts (predictions for a specific time point) or interval forecasts (predictions with a range of possible values). Some common methods for selecting p and q include in the ARIMA(p,d,q): Autocorrelation function (ACF) plot, which shows the correlations between the time series data and lagged versions of itself. A high positive autocorrelation at a lag of p suggests that p may be a good value for p in ARIMA(p,d,q). Partial autocorrelation function (PACF) plot, which shows the correlations between the time series data and lagged versions of itself, after accounting for the correlations at all lower lags. A high positive autocorrelation at a lag of q suggests the value for q in ARIMA(p,d,q). There are several statistical measures that can be used to compare the goodness of fit of different ARIMA models, such as Akaike’s Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These measures can be used to select the model with the lowest value, which is generally considered to be the best model. It is important to note that determining the values of p and q is an iterative process, and we may need to try different values and evaluate the results in order to find the best fit for our data. 22.2 Hyndman-Khandakar algorithm The Hyndman-Khandakar algorithm (Hyndman &amp; Khandakar, 2008) combines several steps for modeling (and estimation) of the ARIMA model: unit root tests, minimization of the AICc, and MLE to obtain an ARIMA model. The arguments to ARIMA() in the fable package provide for many variations for modeling ARIMA. The modeling procedure to a set of (non-seasonal) time series data for ARIMA is defined in FPP3 as follows: Plot the data to identify any outliers. If the data shows variation that increases or decreases with the level of the series, transform the data (Box-Cox transformation) to stabilize the variance. Check if the data are non-stationary. And, make them stationary, if they are not. Start with an ARIMA \\((p, d, 0)\\) or ARIMA \\((0, d, q)\\) depending of what ACF/PACF indicates. Try your chosen model(s), and use the AICc to search for a better model. However, after step 5, the residuals from the chosen model are supposed to be white noise. Otherwise, the model has to be modified. Once the residuals look like white noise, the ARIMA model is ready for forecasting. We will show all these steps by using the epidemic curve of COVID-19 in Toronto covering 266 days between the March \\(1^{st}\\) and the November \\(21^{st}\\) of 2020. An epidemic curve (or epi curve) is a visual display of the onset of illness among cases associated with an outbreak. The data contain the first wave and the first part of the second wave. It is from Ontario Data Catalogue sorted by Episode Date, which is the date when the first symptoms were started. Our data set also contains the mobility data is from Facebook, all_day_bing_tiles_visited_relative_change, which reflects positive or negative changes in movement relative to baseline. 22.3 TS Plots Let’s first load the data and convert it to tsibble. library(tsibble) library(fpp3) load(&quot;dftoronto.RData&quot;) day &lt;- seq.Date( from = as.Date(&quot;2020/03/01&quot;), to = as.Date(&quot;2020/11/21&quot;), by = 1 ) tdata &lt;- tibble(Day = day, mob = data$mob, cases = data$cases) toronto &lt;- tdata %&gt;% as_tsibble(index = Day) toronto ## # A tsibble: 266 x 3 [1D] ## Day mob cases ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-03-01 -0.0172 4 ## 2 2020-03-02 -0.0320 6 ## 3 2020-03-03 -0.0119 10 ## 4 2020-03-04 0.0186 7 ## 5 2020-03-05 0.0223 7 ## 6 2020-03-06 -0.00626 10 ## 7 2020-03-07 0.0261 8 ## 8 2020-03-08 0.0273 10 ## 9 2020-03-09 -0.0158 18 ## 10 2020-03-10 -0.0521 29 ## # ℹ 256 more rows Note the [1D] in the header indicating daily data. Dealing with daily and sub-daily data with ts class is not an easy process. The tsibble class handles such data with no problem. More details on tsibbles can be found at Tidy time series data using tsibbles. Although there are better plotting option cosmetically, we will stick to what fpp3 simply offers: a &lt;- toronto %&gt;% autoplot(mob, col = &#39;blue&#39;) + labs( title = &quot;Mobility Index&quot;, subtitle = &quot;Toronto 2020&quot;, x = &quot;Days&quot;, y = &quot;Index&quot; ) b &lt;- toronto %&gt;% autoplot(cases, col = &#39;red&#39;) + labs( title = &quot;Covid-19 Cases&quot;, subtitle = &quot;Toronto 2020&quot;, x = &quot;Days&quot;, y = &quot;Cases&quot; ) require(gridExtra) grid.arrange(b, a, ncol = 2) 22.4 Box-Cox transformation We would like to make the size of the variation about the same across the whole series. A proper variance-stabilizing transformation makes the forecasting model simpler and better. For example, Proietti and Lutkepohl (2012) find that the Box–Cox transformation produces forecasts which are significantly better than the untransformed data at the one-step-ahead horizon (See Does the Box–Cox transformation help in forecasting macroeconomic time series?). lmbd &lt;- toronto %&gt;% features(cases, features = guerrero) %&gt;% pull(lambda_guerrero) toronto %&gt;% autoplot(box_cox(cases, lambda = lmbd), col = &quot;red&quot;) + labs(y = &quot;&quot;, title = latex2exp::TeX(paste0( &quot;Cases - Transformed with $\\\\lambda$ = &quot;, round(lmbd, 2) ))) The option guerrero computes the optimal \\(\\lambda\\) value for a Box-Cox transformation using the Guerrero method. Note that, since the number of tests performed in a given day changes the numbers of cases, we should use “positivity rates”, which is the percentage of positive results in all COVID-19 tests given any day, instead of case numbers. We ignore this problem for now. 22.5 Stationarity A time series is called stationary if a shift in time does not cause a change in the shape of the distribution: the mean, variance, and covariance. Stationarity is an important assumption in many time series forecasting methods, because non-stationary data have statistical properties that change over time making the current patterns and trends ungeneralizable for the future. There are several tests that can be used to determine whether a time series is stationary or not, including the Dickey-Fuller and KPSS (Kwiatkowski-Phillips-Schmidt-Shin) tests. If a time series is found to be non-stationary, it may be necessary to transform the data in some way before applying a forecasting method in order to obtain reliable forecasts. The main method is differencing, which involves taking the difference between consecutive values in the series. Let’s first formally test all these series and see what we get: # number of first differences toronto %&gt;% features(cases, unitroot_ndiffs) ## # A tibble: 1 × 0 # Formal KPSS test on level toronto %&gt;% features(cases, unitroot_kpss) ## # A tibble: 1 × 0 # Formal KPSS test on the first difference toronto %&gt;% mutate(diffcases = difference(cases)) %&gt;% features(diffcases, unitroot_kpss) ## # A tibble: 1 × 0 It seems that the first difference can make the cases series stationary. The null in this test suggests that the series are stationary, and the p-value indicates that the null is rejected. So, it seems that the test after first differencing gives us a green light! However, ACF’s are telling us that seasonal differencing would be needed: level &lt;- toronto %&gt;% ACF(cases) %&gt;% autoplot() + labs(subtitle = &quot;Covid-19 Cases&quot;) fdiff &lt;- toronto %&gt;% ACF(difference(cases)) %&gt;% autoplot() + labs(subtitle = &quot;First-difference&quot;) diffbc &lt;- toronto %&gt;% ACF(difference(box_cox(cases, lmbd))) %&gt;% autoplot() + labs(subtitle = &quot;First-difference Box-Cox&quot;) ddiff &lt;- toronto %&gt;% ACF(difference(difference(box_cox(cases, lmbd)))) %&gt;% autoplot() + labs(subtitle = &quot;Double-difference Box-Cox&quot;) require(gridExtra) grid.arrange(level, fdiff, diffbc, ddiff, ncol = 2, nrow = 2) From ACF’s, there seems to be a weekly seasonal pattern at 7, 14, and 21, which are Sundays. We know that reported Covid-19 cases on Sundays tend to be lower than the rest of the week at least during the first wave. We can also test if we need seasonal differencing: toronto %&gt;% features(cases, unitroot_nsdiffs) ## # A tibble: 1 × 1 ## nsdiffs ## &lt;int&gt; ## 1 0 # with Box-Cox toronto %&gt;% features(box_cox(cases, lmbd), unitroot_nsdiffs) ## # A tibble: 1 × 1 ## nsdiffs ## &lt;int&gt; ## 1 0 The feature unitroot_nsdiffs returns 0 for both original and transformed series indicating no seasonal difference is required. We will stick to this “advice” because of two reasons. First, an unnecessary differencing would create more problems than a solution. Second, we can also modify ARIMA to incorporate seasonalllty in the data, which we will see shortly. Yet, out of curiosity, let’s remove the “seemingly” weekly seasonality and see what happens to ACF’s. Since, the order of differencing is not important, we first applied the seasonal differencing then applied the first difference: toronto %&gt;% gg_tsdisplay(difference(box_cox(cases, lmbd), 7) %&gt;% difference(), plot_type = &#39;partial&#39;, lag = 36) + labs(title = &quot;Seasonal &amp; first differenced&quot;, y = &quot;&quot;) We can calculate the strength of the trend (T) and seasonality (S) in the time series, \\(y_t=T_t+S_t+R_t\\), by \\[ F_{Trend}=\\max \\left(0,1-\\frac{\\operatorname{Var}\\left(R_t\\right)}{\\operatorname{Var}\\left(T_t+R_t\\right)}\\right),\\\\ F_{Seasonality}=\\max \\left(0,1-\\frac{\\operatorname{Var}\\left(R_t\\right)}{\\operatorname{Var}\\left(S_t+R_t\\right)}\\right), \\] where \\(R_t\\) is the remainder component: t &lt;- toronto %&gt;% features(cases, feat_stl) t(t[1:2]) ## [,1] ## trend_strength 0.9843102 ## seasonal_strength_week 0.5142436 Relative to \\(F_{Trend}\\), the seasonality is not robust in the data. So, our decision is to go with a simple first-differencing with Box-Cox transformation. However, we will look at the final predictive performance if the transformation provides any benefit. 22.6 Modeling ARIMA In his post, Forecasting COVID-19, Rob J Hyndman makes the following comment in March 2020: (…) the COVID-19 pandemic, it is easy to see why forecasting its effect is difficult. While we have a good understanding of how it works in terms of person-to-person infections, we have limited and misleading data. The current numbers of confirmed cases are known to be vastly underestimated due to the limited testing available. There are almost certainly many more cases of COVID-19 that have not been diagnosed than those that have. Also, the level of under-estimation varies enormously between countries. In a country like South Korea with a lot of testing, the numbers of confirmed cases are going to be closer to the numbers of actual cases than in the US where there has been much less testing. So we simply cannot easily model the spread of the pandemic using the data that is available. The second problem is that the forecasts of COVID-19 can affect the thing we are trying to forecast because governments are reacting, some better than others. A simple model using the available data will be misleading unless it can incorporate the various steps being taken to slow transmission. In summary, fitting simple models to the available data is pointless, misleading and dangerous. With our selection of the data, we do not intent to create another debate on forecasting COVID-19. There are hundreds of different forecasting models currently operational in a hub, The COVID-19 Forecast Hub, that can be used live. We will start with an automated algorithm ARIMA() that will allow a seasonal parameters: \\[ \\text { ARIMA }(p, d, q) \\times(P, D, Q) S \\] The first term is the non-seasonal part of ARIMA with \\(p=\\) AR order, \\(d=\\) non-seasonal differencing, \\(q=\\) MA order. The secon term is seasonal part of the model with \\(P=\\) seasonal AR order, \\(D=\\) seasonal differencing, \\(Q\\) = seasonal MA order, and \\(S=\\) seasonal pattern, which defines the number of time periods until the pattern repeats again. In our case, low values tend always to occur in some particular days, Sundays. Therefore, we may think that \\(\\mathrm{S}=7\\) is the span of the periodic seasonal behavior in our data. We can think of a seasonal first order autoregressive model, AR(1), would use \\(X_{t-7}\\) to predict \\(X_t\\). Likewise, a seasonal second order autoregressive model would use \\(X_{t-7}\\) and \\(X_{t-14}\\) to predict \\(X_t\\). A seasonal first order MA(1) model would use \\(\\epsilon_{t-7}\\) as a predictor. A seasonal second order MA(2) model would use \\(\\epsilon_{t-7}\\) and \\(\\epsilon_{t-14}\\). Let’s use our data first-differenced and transformed: toronto &lt;- toronto %&gt;% mutate(boxcases = box_cox(cases, lambda = lmbd)) toronto %&gt;% gg_tsdisplay(difference(boxcases), plot_type=&#39;partial&#39;) We look at the spikes and decays in ACF and PACF: a exponential decay in ACF is observed at seasonal spikes of 7, 14, and 21 as well as two spikes at 7 and 14 in PACF indicate seasonal AR(2). We will also add non-seasonal AR(2) due to 2 spikes in PACF at days 1 and 2. Here are our initial models: \\[ \\operatorname{ARIMA}(2,1,0)(2,1,0)_{7}\\\\ \\operatorname{ARIMA}(0,1,2)(0,1,3)_{7} \\] covfit &lt;- toronto %&gt;% model( AR2 = ARIMA(boxcases ~ pdq(2, 1, 0) + PDQ(3, 1, 0)), MA3 = ARIMA(boxcases ~ pdq(0, 1, 2) + PDQ(0, 1, 3)), auto = ARIMA(boxcases, stepwise = FALSE, approx = FALSE) ) t(cbind( &quot;AR2&quot; = covfit$AR2, &quot;MA3&quot; = covfit$MA3, &quot;auto&quot; = covfit$auto )) ## [,1] ## AR2 ARIMA(2,1,0)(3,1,0)[7] ## MA3 ARIMA(0,1,2)(0,1,3)[7] ## auto NULL model glance(covfit) %&gt;% arrange(AICc) %&gt;% dplyr::select(.model:BIC) ## # A tibble: 2 × 6 ## .model sigma2 log_lik AIC AICc BIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MA3 0.468 -277. 567. 567. 588. ## 2 AR2 0.534 -285. 582. 583. 604. covfit %&gt;% dplyr::select(MA3) %&gt;% report() ## Series: boxcases ## Model: ARIMA(0,1,2)(0,1,3)[7] ## ## Coefficients: ## ma1 ma2 sma1 sma2 sma3 ## -0.4340 0.1330 -0.8617 -0.0573 -0.0809 ## s.e. 0.0648 0.0612 0.0827 0.0733 0.0600 ## ## sigma^2 estimated as 0.4684: log likelihood=-277.29 ## AIC=566.58 AICc=566.92 BIC=587.9 The ARIMA() function uses unitroot_nsdiffs() to determine \\(D\\) when it is not specified. Earlier, we run this function that suggested no seasonal differencing. All other parameters are determined by minimizing the AICc (Akaike’s Information Criterion with a correction for finite sample sizes), which is similar to Akaike’s Information Criterion (AIC), but it includes a correction factor to account for the fact that the sample size may be small relative to the number of parameters in the model. This correction helps to reduce the bias in the AIC estimate and make it more accurate for small sample sizes. When the sample size is large, AIC and AICc are nearly equivalent and either one can be used. Although AICc values across the models are not comparable (for “auto”, as it has no seasonal differencing), it seems that our manually constructed ARIMA, \\(\\operatorname{ARIMA}(0,1,2)(0,1,3)_{7}\\) could also be an option. This brings the possibility of a grid search to our attention. Before that, however, let’s check their residuals: rbind( augment(covfit) %&gt;% filter(.model == &quot;auto&quot;) %&gt;% features(.innov, ljung_box, lag = 24, dof = 5), augment(covfit) %&gt;% filter(.model == &quot;MA3&quot;) %&gt;% features(.innov, ljung_box, lag = 24, dof = 5), augment(covfit) %&gt;% filter(.model == &quot;AR2&quot;) %&gt;% features(.innov, ljung_box, lag = 24, dof = 5) ) ## # A tibble: 3 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 auto NA NA ## 2 MA3 27.3 0.0971 ## 3 AR2 21.1 0.331 covfit %&gt;%dplyr::select(MA3) %&gt;% gg_tsresiduals(lag=36) There are several significant spikes in the ACF. But, the model passes the Ljung-Box test at the 5 percent significance level. Meanwhile, a model without white noise errors can still be used for forecasting, but the prediction intervals may not be accurate due to the correlated residuals. Sometimes, we cannot find a model that passes this test. In practice, we may have to look at the tradeoff between prediction accuracy and reliable confidence intervals. If the difference is too high, we may chose the best model with the highest prediction accuracy. Before looking at a cross-validation approach for model selection in ARIMA modeling, let use our model to predict a week ahead (2020-11-22 to 2020-11-28): fc &lt;- covfit %&gt;% forecast(h = 7) fc ## # A fable: 21 x 4 [1D] ## # Key: .model [3] ## .model Day boxcases .mean ## &lt;chr&gt; &lt;date&gt; &lt;dist&gt; &lt;dbl&gt; ## 1 AR2 2020-11-22 N(12, 0.53) 12.1 ## 2 AR2 2020-11-23 N(13, 0.68) 13.3 ## 3 AR2 2020-11-24 N(13, 0.87) 12.8 ## 4 AR2 2020-11-25 N(13, 1.1) 12.8 ## 5 AR2 2020-11-26 N(12, 1.3) 12.2 ## 6 AR2 2020-11-27 N(12, 1.5) 12.3 ## 7 AR2 2020-11-28 N(12, 1.7) 11.5 ## 8 MA3 2020-11-22 N(12, 0.48) 12.4 ## 9 MA3 2020-11-23 N(13, 0.63) 13.2 ## 10 MA3 2020-11-24 N(13, 0.87) 13.1 ## # ℹ 11 more rows fc %&gt;% autoplot(toronto, level = NULL) + xlab(&quot;Days&quot;) + ylab(&quot;Transformed Cases with Box-Cox&quot;) a &lt;- forecast(covfit, h = 7) %&gt;% filter(.model == &#39;auto&#39;) %&gt;% autoplot(toronto) + labs(title = &quot;COVID-19 Forecasting - Auto&quot;, y = &quot;Box-Cox Tranformed Cases&quot;) b &lt;- forecast(covfit, h = 7) %&gt;% filter(.model == &#39;MA3&#39;) %&gt;% autoplot(toronto) + labs(title = &quot;COVID-19 Forecasting - MA3&quot;, y = &quot;Box-Cox Transformed Cases&quot;) require(gridExtra) grid.arrange(a, b, ncol = 2) We have predicted values for coming 7 days but we do not have realized values. Hence, we cannot compare these models in terms of their accuracy. We can look at the forecast accuracy of these models by using a training set containing all data up to 2020-11-14. When we forecast the remaining seven days in the data, we can calculate the prediction accuracy. train &lt;- toronto %&gt;% filter_index( ~ &quot;2020-11-14&quot;) fit &lt;- train %&gt;% model( AR2 = ARIMA(boxcases ~ pdq(2, 1, 0) + PDQ(3, 1, 0)), MA3 = ARIMA(boxcases ~ pdq(0, 1, 2) + PDQ(0, 1, 3)), auto = ARIMA(boxcases, stepwise = FALSE, approx = FALSE) ) %&gt;% mutate(mixed = (auto + AR2 + MA3) / 3) Although mixing several different ARIMA models does not make sense, we can have an ensemble forecast mixing several different time series models in addition ARIMA modeling. A nice discussion can be found in this post at Stackoverflow. And, now the accuracy measures: fc &lt;- fit %&gt;% forecast(h = 7) fc %&gt;% autoplot(toronto, level = NULL) accuracy(fc, toronto) ## # A tibble: 4 × 10 ## .model .type ME RMSE MAE MPE MAPE MASE RMSSE ACF1 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AR2 Test -1.57 1.88 1.57 -11.6 11.6 1.35 1.30 0.359 ## 2 MA3 Test -1.61 1.91 1.61 -11.9 11.9 1.38 1.32 0.501 ## 3 auto Test NaN NaN NaN NaN NaN NaN NaN NA ## 4 mixed Test NaN NaN NaN NaN NaN NaN NaN NA In all measures, the model “auto” (ARIMA with the Hyndman-Khandakar algorithm) is better than others. Finally, it is always good to check ARIMA (or any time series forecasting) against the base benchmark. bfit &lt;- train %&gt;% model(ave = MEAN(boxcases), lm = TSLM(boxcases ~ trend() + season())) bfc &lt;- bfit %&gt;% forecast(h = 7) bfc %&gt;% autoplot(toronto, level = NULL) accuracy(bfc, toronto) ## # A tibble: 2 × 10 ## .model .type ME RMSE MAE MPE MAPE MASE RMSSE ACF1 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ave Test 4.59 4.72 4.59 31.8 31.8 3.94 3.26 0.507 ## 2 lm Test 2.07 2.32 2.07 14.1 14.1 1.77 1.60 0.516 The results shows our ARIMA model is doing much better job relative to a time-series linear model or a simple average. As we discussed earlier in this book, there are basically two ways to select a best fitting predictive model: ex-post and ex-ante tools to penalize the overfitting. With AIC (Akaike Information Criterion) and BIC (Bayesian Information Criteria) measures, we can indirectly estimate the test (out-of-sample) error by making an adjustment to the training (in-sample) error to account for the bias due to overfitting. Therefore, these methods are ex-post tools to penalize the overfitting. The Hyndman-Khandakar algorithm uses this ex-post approach by selecting the best predictive ARIMA model with minimum AICc among alternatives. We can directly estimate the test error (out-sample) and choose the model that minimizes it. Instead of selecting a model with AICc, we can do it by tuning the parameters of ARIMA with a cross-validation approach so that the tuned model achieves the highest predictive accuracy. 22.7 Grid search for ARIMA Before we apply a cross validation approach to choose the model that has the minimum test error (out-sample), we would like to do a grid search for a seasonal ARIMA with \\(d=1\\), \\(D=1\\), and \\(S=7\\). We will report two outcomes: AICc and RMSE (root mean squared error). #In-sample grid-search p &lt;- 0:3 q &lt;- 0:3 P &lt;- 0:3 Q &lt;- 0:2 comb &lt;- as.matrix(expand.grid(p, q, P, Q)) # We remove the unstable grids comb &lt;- as.data.frame(comb[-1,]) ind &lt;- which(comb$Var1 == 0 &amp; comb$Var2 == 0, arr.ind = TRUE) comb &lt;- comb[-ind,] row.names(comb) &lt;- NULL colnames(comb) &lt;- c(&quot;p&quot;, &quot;q&quot;, &quot;P&quot;, &quot;Q&quot;) aicc &lt;- c() RMSE &lt;- c() for (k in 1:nrow(comb)) { tryCatch({ fit &lt;- toronto %&gt;% model(ARIMA(boxcases ~ 0 + pdq(comb[k, 1], 1, comb[k, 2]) + PDQ(comb[k, 3], 1, comb[k, 4]))) wtf &lt;- fit %&gt;% glance res &lt;- fit %&gt;% residuals() aicc[k] &lt;- wtf$AICc RMSE[k] &lt;- sqrt(mean((res$.resid) ^ 2)) }, error = function(e) { }) } cbind(comb[which.min(aicc), ], &quot;AICc&quot; = min(aicc, na.rm = TRUE)) ## p q P Q AICc ## 75 3 3 0 1 558.7746 cbind(comb[which.min(RMSE), ], &quot;RMSE&quot; = min(RMSE, na.rm = TRUE)) ## p q P Q RMSE ## 120 3 3 3 1 0.6478857 Although we set the ARIMA without a constant, we could extend the grid with a constant. We can also add a line (ljung_box) that extracts and reports the Ljung-Box test for each model. We can then select the one that has a minimum AICc and passes the test. We may not need this grid search as the Hyndman-Khandakar algorithm for automatic ARIMA modelling is able to do it for us very effectively (except for the Ljung-Box test for each model). We should note that the Hyndman-Khandakar algorithm selects the best ARIMA model for forecasting with the minimum AICc. In practice, we can apply a similar grid search with cross validation for selecting the best model that has the minimum out-of-sample prediction error without checking if it passes the Ljung-Box test or not. Here is a simple example: #In-sample grid-search p &lt;- 0:3 q &lt;- 0:3 P &lt;- 0:3 Q &lt;- 0:2 comb &lt;- as.matrix(expand.grid(p, q, P, Q)) # We remove the unstable grids comb &lt;- as.data.frame(comb[-1,]) ind &lt;- which(comb$Var1 == 0 &amp; comb$Var2 == 0, arr.ind = TRUE) comb &lt;- comb[-ind, ] row.names(comb) &lt;- NULL colnames(comb) &lt;- c(&quot;p&quot;, &quot;q&quot;, &quot;P&quot;, &quot;Q&quot;) train &lt;- toronto %&gt;% filter_index( ~ &quot;2020-11-14&quot;) RMSE &lt;- c() for (k in 1:nrow(comb)) { tryCatch({ amk &lt;- train %&gt;% model(ARIMA(boxcases ~ 0 + pdq(comb[k, 1], 1, comb[k, 2]) + PDQ(comb[k, 3], 1, comb[k, 4]))) %&gt;% forecast(h = 7) %&gt;% accuracy(toronto) RMSE[k] &lt;- amk$RMSE }, error = function(e) { }) } cbind(comb[which.min(RMSE), ], &quot;RMSE&quot; = min(RMSE, na.rm = TRUE)) ## p q P Q RMSE ## 12 0 3 0 0 0.7937723 g &lt;- which.min(RMSE) toronto %&gt;% model(ARIMA(boxcases ~ 0 + pdq(comb[g, 1], 1, comb[g, 2]) + PDQ(comb[g, 3], 1, comb[g, 4]))) %&gt;% forecast(h = 7) %&gt;% autoplot(toronto, level = NULL) We will not apply h-step-ahead rolling-window cross-validations for ARIMA, which can be found in the post, Time series cross-validation using fable, by Hyndman (2021). However, when we have multiple competing models, we may not want to compare their predictive accuracy by looking at their error rates using only few out-of-sample observations. If we use rolling windows or continuously expanding windows, we can effectively create a large number of days tested within the data. 22.8 Hyperparameter tuning with time-series data: While we have a dedicated section (Section VII) on forecasting with times series data, we will complete this chapter by looking at the fundamental differences between time-series and cross-sectional in terms of grid search. We will use the EuStockMarkets data set pre-loaded in R. The data contains the daily closing prices of major European stock indices: Germany DAX (Ibis), Switzerland SMI, France CAC, and UK FTSE. The data are sampled in business time, i.e., weekends and holidays are omitted. We will focus on the FTSE. Below, the data and its plot: #Data data &lt;- as.data.frame(EuStockMarkets) day_index &lt;- seq(1, nrow(data), by = 1) data &lt;- cbind(data, day_index) head(data) ## DAX SMI CAC FTSE day_index ## 1 1628.75 1678.1 1772.8 2443.6 1 ## 2 1613.63 1688.5 1750.5 2460.2 2 ## 3 1606.51 1678.6 1718.0 2448.2 3 ## 4 1621.04 1684.1 1708.1 2470.4 4 ## 5 1618.16 1686.6 1723.1 2484.7 5 ## 6 1610.61 1671.6 1714.3 2466.8 6 plot( data$day_index, data$FTSE, col = &quot;orange&quot;, cex.main = 0.80, cex.axis = 0.75, type = &quot;l&quot; ) We can use smoothing methods to detect trends in the presence of noisy data especially in cases where the shape of the trend is unknown. A decomposition would show the components of the data: trend, seasonal fluctuations, and the noise, which is unpredictable and remainder of after the trend (and seasonality) is removed Here is an illustration for the FTSE with additive decomposition: tsd &lt;- EuStockMarkets dctsd &lt;- decompose(tsd[, 4]) plot(dctsd, col = &quot;red&quot;) Separating the trend from the noise will enable us to predict the future values better. Having learnt how to model a learning algorithm, we can also train loess() to extract the trend in FTSE. Several smoothing lines are illustrated below to visualize the differences: plot( data$day_index, data$FTSE, type = &quot;l&quot;, col = &quot;red&quot;, cex.main = 0.80, cex.axis = 0.75, lwd = 2 ) lines(data$day_index, predict(lm(FTSE ~ day_index, data)), lwd = 1, col = &quot;green&quot;) lines(data$day_index, predict(loess( data$FTSE ~ data$day_index, degree = 1, span = 0.01 )), lwd = 2, col = &quot;grey&quot;) lines(data$day_index, predict(loess( data$FTSE ~ data$day_index, degree = 1, span = 0.1 )), lwd = 2, col = &quot;blue&quot;) lines(data$day_index, predict(loess( data$FTSE ~ data$day_index, degree = 1, span = 0.9 )), lwd = 2, col = &quot;yellow&quot;) It seems that a linear trend is not appropriate as it underfits to predict. Although a smoothing method like loess() would be a good choice, but which loess() would be a good fit? One way of validating time series data is to keep the time order in the data when we use k-fold cross validation so that in each fold the training data takes place before the test data. This type of cross validation is called as h-step-ahead rolling cross-validation. (There is also a method called as sliding-window-cross-validation). Below we can see an illustration of this kind of cross validation: We are going to split the data without a random shuffle: span &lt;- seq(from = 0.05, to = 1, by = 0.05) # *****h-step-rolling-CV******** h &lt;- 10 opt &lt;- c() #CV loop nvalid &lt;- round(nrow(data) / h) #This gives the 10 cutoff points in rows cut &lt;- c(1) for (j in 1:h) { cut &lt;- c(cut, nvalid * j) } for (i in 1:h) { if (i &lt; h) { train &lt;- data[(cut[1]:cut[i + 1]),] } else{ train &lt;- data[cut[1]:cut[i],] } if (i + 2 &lt; h) valid &lt;- data[(cut[i + 1]:cut[i + 2]),] RMSPE &lt;- c(rep(0), length(span)) #Matrix to store RMSPE for (s in 1:length(span)) { model &lt;- loess( FTSE ~ day_index, control = loess.control(surface = &quot;direct&quot;), degree = 2, span = span[s], data = train ) fit &lt;- predict(model, valid$day_index) RMSPE[s] &lt;- sqrt(mean((valid$FTSE - fit) ^ 2)) } opt[i] &lt;- which(RMSPE == min(RMSPE), arr.ind = TRUE) } #Hyperparameters opt_span &lt;- mean(span[opt]) opt_span ## [1] 0.43 plot( data$day_index, data$FTSE, type = &quot;l&quot;, col = &quot;gray&quot;, cex.main = 0.80, cex.axis = 0.75 ) lines(data$day_index, predict(loess( data$FTSE ~ data$day_index, degree = 2, span = opt_span )), lwd = 2, col = &quot;red&quot;) Note that we did not start this algorithm with the initial split for testing. For the full train-validate-test routine the initial split has to be added into this cross-validation script. Moreover, we started the validation after the first 10% split. We can also decide on this starting point. For example, we can change the code and decide to train the model after 30% training set. That flexibility is specially important if we apply Day Forward-Chaining Nested Cross-Validation, which is the same method but rolling windows are the days. The following figure helps demonstrate this method: Although it is designed for a day-chained cross validation, we can replace days with weeks, months or 21-day windows. In fact, our algorithm that uses 10% splits can be considered a 10% Split Forward-Chaining Nested Cross-Validation. We will see multiple applications with special methods unique to time series data in Section VII. 22.9 Speed Before concluding this section, notice that as the sample size rises, the learning algorithms take longer to complete, specially for cross sectional data. That’s why there are some other cross-validation methods that use only a subsample randomly selected from the original sample to speed up the validation and the test procedures. You can think how slow the conventional cross validation would be if the dataset has 1-2 million observations, for example. There are some methods to accelerate the training process. One method is to increase the delta (the increments) in our grid and identify the range of hyperparameters where the RMSPE becomes the lowest. Then we reshape our grid with finer increments targeting that specific range. Another method is called a random grid search. In this method, instead of the exhaustive enumeration of all combinations of hyperparameters, we select them randomly. This can be found in Random Search for Hyper-Parameter Optimization by James Bergstra and Yoshua Bengio (Berg_2012?). To accelerate the grid search, we can also use parallel processing so that each loop will be assigned to a separate core in capable computers. We will see several application using these options later in the book. Both methods are covered in Chapter 14. Finally, we did not use functions in our algorithms. We should create functions for each major process in algorithms and compile them in one clean “source” script. There is a paper, &lt;https://robjhyndman.com/publications/tsibble/), by Wang et al. (2020) describing tsibble and the package in more details↩︎ "],["forecast.html", "Chapter 23 Forecast 23.1 Time Series Embedding 23.2 VAR for Recursive Forecasting 23.3 Embedding for Direct Forecast 23.4 Random Forest 23.5 Univariate 23.6 Multivariate 23.7 Rolling and expanding windows", " Chapter 23 Forecast 23.1 Time Series Embedding In general, forecasting models use either direct or recursive forecasting, or their combinations (See Taieb and Hyndman, 2012). The difference between these two methods is related to discussion on prediction accuracy and forecasting variance. Recursive forecasting requires a parametric model and would face increasing forecasting error when the underlying model is not linear. Direct forecasting, however, can be achieved by a nonparametric predictive algorithm, while it may have a higher variance as the forecast horizon gets longer. Multi-period recursive forecasting use a single time series model, like AR(1). With iterative substitutions of the estimated model, any forecast period of \\(h\\) can be computed. Let’s start with a simple AR(1) to see recursive forecasting: \\[ x_{t+1}=\\alpha_0+\\phi_1 x_t+\\epsilon_{t} \\] If we use this AR(1) to have a 3-period forecast: \\[ \\hat{x}_{t+1}=\\hat{\\alpha}_0+\\hat{\\phi}_1 x_t, \\\\ \\hat{x}_{t+2}=\\hat{\\alpha}_0+\\hat{\\phi}_1 \\hat{x}_{t+1}, \\\\ \\hat{x}_{t+3}=\\hat{\\alpha}_0+\\hat{\\phi}_1 \\hat{x}_{t+2} \\] With iterative substitutions: \\[ \\hat{x}_{t+1}=\\hat{\\alpha}_0+\\hat{\\phi}_1 x_t ~~~~ 1^{st} ~ \\text{Period}\\\\ \\hat{x}_{t+2}=\\hat{\\alpha}_0+\\hat{\\alpha}_0\\hat{\\alpha}_1+\\hat{\\phi}^2_1 x_{t} ~~~~ 2^{nd} ~ \\text{Period}\\\\ \\hat{x}_{t+3}=\\hat{\\alpha}_0+\\hat{\\alpha}_0\\hat{\\alpha}_1+\\hat{\\alpha}_0\\hat{\\alpha}^2_1+\\hat{\\phi}^3_1 x_t~~~~ 3^{rd} ~ \\text{Period} \\] Of course, we can generalize it for \\(h\\) periods: \\[ \\hat{x}_{t+h}=\\hat{\\alpha}_0 \\sum_{i=1}^h \\hat{\\phi}_1^{i-1}+\\hat{\\phi}_1^h x_t \\] The estimated coefficients (\\(\\hat{\\alpha}_0\\), \\(\\hat{\\phi}_1\\)) are the same; hence, we need only one model for any period. Alternatively, we can apply the direct multi-period forecasting, where a separate predictive model for each forecasting horizon between \\(h\\) and \\(t\\) is estimated. Here is the example with AR(1): \\[ x_{t+1}=\\alpha_0+\\alpha_1 x_t+\\epsilon_{t}, \\\\ x_{t+2}=\\beta_0+\\beta_1 x_t+\\epsilon_{t}, \\\\ x_{t+3}=\\omega_0+\\omega_1 x_t+\\epsilon_{t}. \\\\ \\] And, the 3-period direct forecasts with three different models: \\[ \\hat{x}_{t+1}=\\hat{\\alpha}_0+\\hat{\\alpha}_1 x_t ~~~~ 1^{st} ~ \\text{Period}\\\\ \\hat{x}_{t+2}=\\hat{\\beta}_0+\\hat{\\beta}_1 x_{t} ~~~~ 2^{nd} ~ \\text{Period}\\\\ \\hat{x}_{t+3}=\\hat{\\omega}_0+\\hat{\\omega}_1x_t~~~~ 3^{rd} ~ \\text{Period} \\] 23.2 VAR for Recursive Forecasting The problem with a multi-period recursive forecasting becomes clear when we have multivariate model: \\[ y_{t+1}=\\beta_0+\\beta_1 y_t+\\beta_2x_t+\\epsilon_{t} \\] If we want a 2-period forecast, \\[ \\hat{y}_{t+2}=\\hat{\\beta}_0+\\hat{\\beta}_1 \\hat{y}_{t+1}+\\hat{\\beta}_2 \\hat{x}_{t+1}, \\] Hence, \\(\\hat{x}_{t+1}\\) has to be estimated. This can be done with a Vector Autorregressive (VAR) framework. A VAR model consists of multiple equations, one per variable. Each equation includes a constant and lags of all of the variables in the system. \\[ \\begin{aligned} &amp; y_{t}=c_1+\\beta_{1} y_{t-1}+\\beta_{2} x_{t-1}+\\varepsilon_{t} \\\\ &amp; x_{t}=c_2+\\phi_{1} x_{t-1}+\\phi_{2} y_{t-1}+e_{t} \\end{aligned} \\] Each model is estimated using the principle of ordinary least squares, given that series are stationary. Forecasts in VAR are calculated with recursive iterations. Therefore, the set of equations generates forecasts for each variable. To decide the number of lags in each equation, the BIC is used. Let’s have our COVID-19 data and include the mobility to our forecasting model. library(tsibble) library(fpp3) load(&quot;dftoronto.RData&quot;) day &lt;- seq.Date( from = as.Date(&quot;2020/03/01&quot;), to = as.Date(&quot;2020/11/21&quot;), by = 1 ) tdata &lt;- tibble(Day = day, mob = data$mob, cases = data$cases) toronto &lt;- tdata %&gt;% as_tsibble(index = Day) toronto ## # A tsibble: 266 x 3 [1D] ## Day mob cases ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-03-01 -0.0172 4 ## 2 2020-03-02 -0.0320 6 ## 3 2020-03-03 -0.0119 10 ## 4 2020-03-04 0.0186 7 ## 5 2020-03-05 0.0223 7 ## 6 2020-03-06 -0.00626 10 ## 7 2020-03-07 0.0261 8 ## 8 2020-03-08 0.0273 10 ## 9 2020-03-09 -0.0158 18 ## 10 2020-03-10 -0.0521 29 ## # ℹ 256 more rows We will estimate the recursive forecasts for 1 to 14 days ahead. # We need make series stationary trdf &lt;- toronto %&gt;% mutate(diffcases = difference(cases), diffmob = difference(mob)) # VAR with BIC fit &lt;- trdf[-1, ] %&gt;% model(VAR(vars(diffcases, diffmob), ic = &quot;bic&quot;)) glance(fit) ## # A tibble: 1 × 6 ## .model sigma2 log_lik AIC AICc BIC ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;VAR(vars(diffcases, diffmob), ic = \\&quot;bic\\… &lt;dbl[…]&gt; -854. 1755. 1760. 1841. fit %&gt;% report() ## Series: diffcases, diffmob ## Model: VAR(5) ## ## Coefficients for diffcases: ## lag(diffcases,1) lag(diffmob,1) lag(diffcases,2) lag(diffmob,2) ## -0.4074 -105.6524 -0.0703 11.0374 ## s.e. 0.0639 28.3643 0.0695 29.9761 ## lag(diffcases,3) lag(diffmob,3) lag(diffcases,4) lag(diffmob,4) ## 0.0528 10.8093 -0.0123 -4.8989 ## s.e. 0.0701 31.8601 0.0713 30.0019 ## lag(diffcases,5) lag(diffmob,5) ## 0.0227 6.1099 ## s.e. 0.0640 29.2678 ## ## Coefficients for diffmob: ## lag(diffcases,1) lag(diffmob,1) lag(diffcases,2) lag(diffmob,2) ## 0e+00 -0.314 0e+00 -0.4688 ## s.e. 1e-04 0.057 1e-04 0.0603 ## lag(diffcases,3) lag(diffmob,3) lag(diffcases,4) lag(diffmob,4) ## 1e-04 -0.2931 -1e-04 -0.2664 ## s.e. 1e-04 0.0641 1e-04 0.0603 ## lag(diffcases,5) lag(diffmob,5) ## 3e-04 -0.4059 ## s.e. 1e-04 0.0588 ## ## Residual covariance matrix: ## diffcases diffmob ## diffcases 811.6771 -0.1648 ## diffmob -0.1648 0.0033 ## ## log likelihood = -853.64 ## AIC = 1755.28 AICc = 1760.38 BIC = 1840.73 fit %&gt;% forecast(h = 14) %&gt;% autoplot(trdf[-c(1:200), ]) We should have transformed both series by the Box-Cox transformation, but we ignored it above. 23.3 Embedding for Direct Forecast For direct forecasting, we need to rearrange the data in a way that we can estimate 7 models for forecasting ahead each day of 7 days. We will use embed() function to show what we mean with rearranging data for AR(3), for example: Y &lt;- 1:10 Y &lt;- embed(Y, 3) colnames(Y) = c(&quot;Y(t)&quot;, &quot;Y(t-1)&quot;, &quot;Y(t-2)&quot;) Y ## Y(t) Y(t-1) Y(t-2) ## [1,] 3 2 1 ## [2,] 4 3 2 ## [3,] 5 4 3 ## [4,] 6 5 4 ## [5,] 7 6 5 ## [6,] 8 7 6 ## [7,] 9 8 7 ## [8,] 10 9 8 Now, the key point is that there is no a temporal dependence between each row so that shuffling this data after re-structuring it admissible. Let’s have an AR(1) example on this simulated data # Stationary data rho &lt; 1 but = 0.85 n &lt;- 10000 rho &lt;- 0.85 y &lt;- c(0, n) set.seed(345) eps &lt;- rnorm(n, 0, 1) for (j in 1:(n - 1)) { y[j + 1] &lt;- y[j] * rho + eps[j] } ylagged &lt;- y[2:n] par(mfrow = c(1, 2)) plot(ylagged, y[1:(n - 1)], col = &quot;lightpink&quot;, ylab = &quot;y&quot;, xlab = &quot;y(t-1)&quot;) plot(y[1:500], type = &quot;l&quot;, col = &quot;red&quot;, ylab = &quot;y&quot;, xlab = &quot;t&quot; ) We will use an AR(1) estimation with OLS after embedding: head(y) ## [1] 0.0000000 -0.7849082 -0.9466863 -0.9661413 -1.1118166 -1.0125757 y_em &lt;- embed(y, 2) colnames(y_em) &lt;- c(&quot;yt&quot;, &quot;yt_1&quot;) head(y_em) ## yt yt_1 ## [1,] -0.7849082 0.0000000 ## [2,] -0.9466863 -0.7849082 ## [3,] -0.9661413 -0.9466863 ## [4,] -1.1118166 -0.9661413 ## [5,] -1.0125757 -1.1118166 ## [6,] -1.4942098 -1.0125757 And estimation of AR(1) with OLS: y_em &lt;- as.data.frame(y_em) ar1 &lt;- lm(yt ~ yt_1 - 1, y_em) ar1 ## ## Call: ## lm(formula = yt ~ yt_1 - 1, data = y_em) ## ## Coefficients: ## yt_1 ## 0.8496 Now, let’s shuffle y_em: # Shuffle ind &lt;- sample(nrow(y_em), nrow(y_em), replace = FALSE) y_em_sh &lt;- y_em[ind, ] ar1 &lt;- lm(yt ~ yt_1 - 1, y_em_sh) ar1 ## ## Call: ## lm(formula = yt ~ yt_1 - 1, data = y_em_sh) ## ## Coefficients: ## yt_1 ## 0.8496 This application shows the temporal independence across the observations in the rearranged data give that model (AR) is correctly specified. This is important because we can use conventional machine learning applications on time series data, like random forests, which we see in the next chapter. This re-arrangement can also be applied to multivariate data sets: tsdf &lt;- matrix(c(1:10, 21:30), nrow = 10) colnames(tsdf) &lt;- c(&quot;Y&quot;, &quot;X&quot;) first &lt;- embed(tsdf, 3) colnames(first) &lt;- c(&quot;y(t)&quot;,&quot;x(t)&quot;,&quot;y(t-1)&quot;,&quot;x(t-1)&quot;, &quot;y(t-2)&quot;, &quot;x(t-2)&quot;) head(first) ## y(t) x(t) y(t-1) x(t-1) y(t-2) x(t-2) ## [1,] 3 23 2 22 1 21 ## [2,] 4 24 3 23 2 22 ## [3,] 5 25 4 24 3 23 ## [4,] 6 26 5 25 4 24 ## [5,] 7 27 6 26 5 25 ## [6,] 8 28 7 27 6 26 Now, we need to have three models for three forecasting horizons. Here are these models: \\[ \\hat{y}_{t+1}=\\hat{\\alpha}_0+\\hat{\\alpha}_1 y_t + \\hat{\\alpha}_2 y_{t-1}+ \\hat{\\alpha}_3 x_t + \\hat{\\alpha}_4 x_{t-1}+ \\hat{\\alpha}_5 x_{t-2} ~~~~ 1^{st} ~ \\text{Period}\\\\ \\hat{y}_{t+2}=\\hat{\\beta}_0+\\hat{\\beta}_1 y_t + \\hat{\\beta}_2 y_{t-1}+ \\hat{\\beta}_3 x_t + \\hat{\\beta}_4 x_{t-1}+ \\hat{\\beta}_5 x_{t-2} ~~~~ 2^{nd} ~ \\text{Period}\\\\ \\hat{y}_{t+3}=\\hat{\\omega}_0+\\hat{\\omega}_1 y_t + \\hat{\\omega}_2 y_{t-1}+ \\hat{\\omega}_3 x_t + \\hat{\\omega}_4 x_{t-1}+ \\hat{\\omega}_5 x_{t-2} ~~~~ 3^{rd} ~ \\text{Period} \\] Each one of these models requires a different rearrangement in the data. Here are the required arrangement for each model: ## y(t) x(t) y(t-1) x(t-1) y(t-2) x(t-2) ## [1,] 3 23 2 22 1 21 ## [2,] 4 24 3 23 2 22 ## [3,] 5 25 4 24 3 23 ## [4,] 6 26 5 25 4 24 ## [5,] 7 27 6 26 5 25 ## [6,] 8 28 7 27 6 26 ## y(t) x(t-1) y(t-2) x(t-2) y(t-3) x(t-3) ## [1,] 4 23 2 22 1 21 ## [2,] 5 24 3 23 2 22 ## [3,] 6 25 4 24 3 23 ## [4,] 7 26 5 25 4 24 ## [5,] 8 27 6 26 5 25 ## [6,] 9 28 7 27 6 26 ## y(t) x(t-2) y(t-3) x(t-3) y(t-4) x(t-4) ## [1,] 5 23 2 22 1 21 ## [2,] 6 24 3 23 2 22 ## [3,] 7 25 4 24 3 23 ## [4,] 8 26 5 25 4 24 ## [5,] 9 27 6 26 5 25 ## [6,] 10 28 7 27 6 26 We already rearranged the data for the first model. if we remove the first row in y(t) and the last row in the remaining set, we can get the data for the second model: cbind(first[-1,1], first[-nrow(first),-1]) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 4 23 2 22 1 21 ## [2,] 5 24 3 23 2 22 ## [3,] 6 25 4 24 3 23 ## [4,] 7 26 5 25 4 24 ## [5,] 8 27 6 26 5 25 ## [6,] 9 28 7 27 6 26 ## [7,] 10 29 8 28 7 27 We will use our COVID-19 data and a simple linear regression as an example of direct forecasting: # Preparing data df &lt;- data.frame(dcases = trdf$diffcases, dmob = trdf$diffmob) df &lt;- df[complete.cases(df),] rownames(df) &lt;- NULL df &lt;- as.matrix(df) head(df) ## dcases dmob ## [1,] 2 -0.01480 ## [2,] 4 0.02013 ## [3,] -3 0.03049 ## [4,] 0 0.00367 ## [5,] 3 -0.02854 ## [6,] -2 0.03232 Now we need to decide on two parameters: the window size, that is, how many lags will be included in each row; and how many days we will forecast. The next section will use more advance functions for re-arranging the data and apply the direct forecasting with random forests. For now, let’s use a 3-day window and a 3-day forecast horizon: h = 3 w = 3 fh &lt;- c() # storage for forecast # Start with first dt &lt;- embed(df, w) y &lt;- dt[, 1] X &lt;- dt[, -1] for (i in 1:h) { fit &lt;- lm(y ~ X) l &lt;- length(fit$fitted.values) fh[i] &lt;- fit$fitted.values[l] y &lt;- y[-1] X &lt;- X[-nrow(X), ] } fh ## [1] 10.288416 -11.587090 0.302522 plot(1:266, trdf$diffcases, col = &quot;red&quot;, type = &quot;l&quot;) lines(267:269, fh, col = &quot;blue&quot;) We haven’t used training and test sets above. If we apply a proper splitting, we can even set the window size as our hyperparameter to minimize the forecast error: # We set the last 7 days as our test set train &lt;- df[1:258,] test &lt;- df[-c(1:258),] h = 7 w &lt;- 3:14 # a grid for window size fh &lt;- matrix(0, length(w), h) rownames(fh) &lt;- w colnames(fh) &lt;- 1:7 for (s in 1:length(w)) { dt &lt;- embed(train, w[s]) y &lt;- dt[, 1] X &lt;- dt[, -1] for (i in 1:h) { fit &lt;- lm(y ~ X) fh[s, i] &lt;- last(fit$fitted.values) y &lt;- y[-1] X &lt;- X[-nrow(X), ] } } fh ## 1 2 3 4 5 6 ## 3 -4.292862 -6.288479 5.2727764 10.692206 22.133103 -0.5252184 ## 4 -5.014668 -1.626752 8.2861736 23.982849 4.611554 -0.2773355 ## 5 -1.125996 1.634917 20.7212780 6.767507 5.115816 -0.5577792 ## 6 1.533541 14.584416 5.6832803 8.066816 4.937718 -6.8419291 ## 7 13.228621 1.612629 7.3973443 7.980486 -1.484987 -5.3696924 ## 8 2.812780 3.308271 7.6799879 1.589578 -1.265470 -9.6077196 ## 9 -5.430448 1.811491 0.7675925 1.698785 -7.123733 -16.9647249 ## 10 -5.488847 -4.382922 0.8842250 -4.199708 -14.615359 -13.8839491 ## 11 -11.104866 -4.133680 -5.3274242 -11.510596 -11.935885 -18.5728995 ## 12 -11.656935 -8.289153 -11.9044832 -9.515252 -16.534428 -16.8239307 ## 13 -18.314269 -13.292359 -9.2157517 -14.330746 -15.341226 -13.0680709 ## 14 -23.661938 -10.963027 -13.9621680 -12.855445 -11.683527 -12.3975126 ## 7 ## 3 -19.79742 ## 4 -19.62517 ## 5 -26.29534 ## 6 -23.77712 ## 7 -20.07199 ## 8 -27.04771 ## 9 -25.44710 ## 10 -30.22356 ## 11 -29.91304 ## 12 -25.62393 ## 13 -25.15019 ## 14 -27.72488 Rows in fh show the 7-day forecast for each window size. We can see which window size is the best: rmspe &lt;- c() for (i in 1:nrow(fh)) { rmspe[i] &lt;- sqrt(mean((fh[i, ] - test) ^ 2)) } rmspe ## [1] 33.45400 35.28827 31.67333 29.69115 31.57618 28.99568 28.53882 28.70796 ## [9] 27.16182 28.59872 28.77714 28.99870 which.min(rmspe) ## [1] 9 We used the last 7 days in our data as our test set. A natural question would be whether we could shuffle the data and use any 7 days as our test set? The answer is yes, because we do not need to follow a temporal order in the data after rearranging it with embedding. This is important because we can add a bootstrapping loop to our grid search above and get better tuning for finding the best window size. We incorporate all these ideas with our random forest application in the next chapter. 23.4 Random Forest We will utilize embedding methods for direct forecasting with Random Forests. We choose the random forests algorithm because it does not need an explicit tuning by a grid search. In the practice, however, we can still search for the number of trees and the number of variables randomly sampled as candidates at each split. Let’s get our COVID-19 data: library(tsibble) library(fpp3) load(&quot;toronto2.rds&quot;) day &lt;- seq.Date( from = as.Date(&quot;2020/03/01&quot;), to = as.Date(&quot;2020/11/21&quot;), by = 1 ) tdata &lt;- tibble(Day = day, data[, -1]) toronto2 &lt;- tdata %&gt;% as_tsibble(index = Day) toronto2 ## # A tsibble: 266 x 8 [1D] ## Day cases mob delay male age temp hum ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-03-01 4 -0.0172 36.8 0.75 55 -4.2 65.5 ## 2 2020-03-02 6 -0.0320 8.5 1 45 3.8 84 ## 3 2020-03-03 10 -0.0119 15 0.7 54 2.3 90 ## 4 2020-03-04 7 0.0186 25.7 0.286 50 3.35 71 ## 5 2020-03-05 7 0.0223 21 0.429 48.6 1.2 63.5 ## 6 2020-03-06 10 -0.00626 13.1 0.5 36 0.04 75 ## 7 2020-03-07 8 0.0261 10.4 0.5 46.2 -1.65 54 ## 8 2020-03-08 10 0.0273 11.6 0.9 50 6.3 56 ## 9 2020-03-09 18 -0.0158 8.89 0.611 35.6 12.5 55 ## 10 2020-03-10 29 -0.0521 9.69 0.448 41.7 5.15 79 ## # ℹ 256 more rows As before, the data contain the first wave and the initial part of the second wave in Toronto for 2020. It is from Ontario Data Catalogue sorted by episode dates (Day), which is the date when the first symptoms were started. The mobility data is from Facebook, all_day_bing_tiles_visited_relative_change, which is reflects positive or negative change in movement relative to baseline. The other variables related to tests are delay, which is the time between test results and the episode date, the gender distribution of people is given by male, age shows the average age among tested people any given day. The last two variables, temp and hum, show the daily maximum day temperature and the average outdoor humidity during the day, respectively. Except for age all other variables are non-stationary. We will take their first difference and make the series stationary before we proceed. df &lt;- toronto2 %&gt;% mutate( dcases = difference(cases), dmob = difference(mob), ddelay = difference(delay), dmale = difference(male), dtemp = difference(temp), dhum = difference(hum) ) dft &lt;- df[, -c(2:5, 7, 8)] #removing levels dft &lt;- dft[-1, c(1, 3:7, 2)] # reordering the columns First, we will use a univariate setting for a single-window forecasting, which is the last 7 days. 23.5 Univariate We will not have a grid search on the random forest algorithm, which could be added to the following script: library(randomForest) h = 7 w &lt;- 3:21 # a grid for window size fh &lt;- matrix(0, length(w), h) rownames(fh) &lt;- w colnames(fh) &lt;- 1:h for (s in 1:length(w)) { dt &lt;- as.data.frame(embed(as.matrix(dft[, 2]), w[s])) test_ind = nrow(dt) - (h) train &lt;- dt[1:test_ind,] test &lt;- dt[-c(1:test_ind),] y &lt;- train[, 1] X &lt;- train[, -1] for (i in 1:h) { fit &lt;- randomForest(X, y) fh[s,] &lt;- predict(fit, test[, -1]) y &lt;- y[-1] X &lt;- X[-nrow(X),] } } fh ## 1 2 3 4 5 6 7 ## 3 -15.880433333 9.121167 19.74952 -6.609067 -5.722785 15.3516667 -2.087685 ## 4 -9.544133333 -6.810200 23.81880 -4.161529 -1.214533 0.5277333 5.875767 ## 5 -2.893600000 -6.885967 32.69350 -8.538333 -2.237667 -5.7720778 11.219167 ## 6 0.070366667 -8.057467 22.55983 -12.094967 14.929700 -11.2058667 17.507433 ## 7 -2.908866667 -15.868133 30.45953 -13.805133 19.191067 -23.7922000 14.290767 ## 8 7.417066667 -22.705167 39.25673 -9.230933 13.765733 -15.4590667 27.734367 ## 9 -3.620766667 -33.568367 68.65190 -20.472133 12.023133 -25.9268333 19.220533 ## 10 -5.063400000 -33.309933 68.74610 -23.970333 14.040067 -25.0332000 12.531533 ## 11 -0.009233333 -31.799500 60.88173 -23.562800 19.142833 -19.2536000 13.502400 ## 12 0.805100000 -28.058533 55.59913 -26.090367 13.274300 -14.7545000 10.550400 ## 13 -2.226166667 -30.789467 59.84473 -25.233567 15.489200 -11.8885333 15.370867 ## 14 -1.639966667 -29.185000 58.61637 -21.431733 14.705067 -9.6757333 13.182233 ## 15 -5.260733333 -33.169933 48.44817 -21.982467 14.593733 -12.0262667 9.309300 ## 16 -5.429300000 -37.097867 61.44953 -21.993267 11.335233 -11.8728667 10.263967 ## 17 -10.734700000 -33.021967 67.64857 -25.555300 16.965033 -10.5087667 10.809200 ## 18 -6.696733333 -32.629567 59.85407 -23.227433 13.093067 -10.5785667 11.307200 ## 19 -11.170300000 -36.225467 54.98873 -25.408567 16.683433 -14.8820000 12.992233 ## 20 -3.524700000 -33.719100 61.33873 -23.823233 13.677867 -12.6076333 16.435500 ## 21 -5.546933333 -31.462433 53.45857 -22.071567 9.086700 -13.1858667 15.337400 We can now see RMSPE for each row (window size): actual &lt;- test[, 1] rmspe &lt;- c() for (i in 1:nrow(fh)) { rmspe[i] &lt;- sqrt(mean((fh[i,] - actual) ^ 2)) } rmspe ## [1] 42.27364 44.57501 44.73248 44.35358 44.75067 51.39679 53.18043 51.53383 ## [9] 50.71377 48.11244 50.52535 50.43200 48.67944 51.68634 51.56775 50.45623 ## [17] 50.10037 51.65413 49.68998 which.min(rmspe) ## [1] 1 And, if we plot several series of our forecast with different window sizes: plot( actual, type = &quot;l&quot;, col = &quot;red&quot;, ylim = c(-80, 50), ylab = &quot;Actual (red) vs. Forecasts&quot;, xlab = &quot;Last 7 days&quot;, main = &quot;7-Day Foerecasts&quot;, lwd = 3 ) lines(fh[1,], type = &quot;l&quot;, col = &quot;blue&quot;) lines(fh[2,], type = &quot;l&quot;, col = &quot;green&quot;) lines(fh[5,], type = &quot;l&quot;, col = &quot;orange&quot;) lines(fh[12,], type = &quot;l&quot;, col = &quot;black&quot;) legend( &quot;bottomright&quot;, title = &quot;Lags&quot;, legend = c(&quot;3-day&quot;, &quot;4-day&quot;, &quot;7-day&quot;, &quot;14-day&quot;), col = c(&quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;), lty = c(1, 1, 1, 1, 1), bty = &quot;o&quot;, cex = 0.75 ) As the window size gets larger, the forecast becomes increasingly smooth missing the short term dynamics. Another observation is that, although “blue” (3-day window) has the minimum RMSPE, it is not able to capture ups and downs relative to 7-day or 14-day windows. 23.6 Multivariate Can we increase the prediction accuracy with additional predictors? library(randomForest) h = 7 w &lt;- 3:14 # a grid for window size fh &lt;- matrix(0, length(w), h) rownames(fh) &lt;- w colnames(fh) &lt;- 1:h for (s in 1:length(w)) { dt &lt;- as.data.frame(embed(as.matrix(dft[, -1]), w[s])) test_ind = nrow(dt) - (h) train &lt;- dt[1:test_ind,] test &lt;- dt[-c(1:test_ind),] y &lt;- train[, 1] X &lt;- train[, -1] for (i in 1:h) { fit &lt;- randomForest(X, y) fh[s,] &lt;- predict(fit, test[, -1]) y &lt;- y[-1] X &lt;- X[-nrow(X),] } } fh ## 1 2 3 4 5 6 7 ## 3 -17.788533 -0.3141000 16.53653 -9.326433 -18.102767 10.969800 -3.163967 ## 4 -22.697100 -3.4509000 15.49413 -8.524767 -12.633133 2.173933 -3.292467 ## 5 -13.800000 -0.7464333 16.56037 -11.723300 -7.062600 -13.184300 2.813033 ## 6 -11.665500 1.9710333 20.05657 -10.684600 3.292100 -9.363967 6.406867 ## 7 -9.373733 -5.8512667 23.13793 -17.310433 6.720633 -16.444167 11.981633 ## 8 -10.233033 -12.3322000 22.61410 -9.796867 5.102200 -14.007967 12.177500 ## 9 -12.145600 -22.5280667 45.52763 -22.078400 7.680600 -21.096567 8.656400 ## 10 -11.179767 -19.4309000 47.87777 -23.726333 8.190267 -22.027567 12.756467 ## 11 -8.145300 -18.9786333 49.77097 -26.724600 14.152900 -21.407233 11.465667 ## 12 -10.947600 -18.4935667 47.21440 -23.789367 9.581900 -19.870133 8.059733 ## 13 -9.108533 -16.9865000 40.43617 -22.952300 9.804567 -19.631100 10.097000 ## 14 -10.174000 -20.3914333 43.31103 -26.960767 12.482533 -19.625367 5.761600 actual &lt;- test[, 1] rmspe &lt;- c() for (i in 1:nrow(fh)) { rmspe[i] &lt;- sqrt(mean((fh[i, ] - actual) ^ 2)) } rmspe ## [1] 42.25558 41.30589 38.97195 40.04286 40.51307 43.67209 44.61230 44.66197 ## [9] 44.42342 43.71028 42.97556 42.68597 which.min(rmspe) ## [1] 3 plot( actual, type = &quot;l&quot;, col = &quot;red&quot;, ylim = c(-80,+50), ylab = &quot;Actual (red) vs. Forecasts&quot;, xlab = &quot;Last 7 days&quot;, main = &quot;7-Day Foerecasts&quot;, lwd = 3 ) lines(fh[1,], type = &quot;l&quot;, col = &quot;blue&quot;) lines(fh[3,], type = &quot;l&quot;, col = &quot;green&quot;) lines(fh[5,], type = &quot;l&quot;, col = &quot;orange&quot;) lines(fh[12,], type = &quot;l&quot;, col = &quot;black&quot;) legend( &quot;bottomright&quot;, title = &quot;Lags&quot;, legend = c(&quot;3-day&quot;, &quot;5-day&quot;, &quot;7-day&quot;, &quot;14-day&quot;), col = c(&quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;black&quot;), lty = c(1, 1, 1, 1, 1), bty = &quot;o&quot;, cex = 0.75 ) It seems that additional predictors do increase the accuracy. Again, relative to the best model (5-day window) our 7-day window correctly captures most ups and downs in the forecast. Now, a visual inspection shows that all RMSPE’s are lower than the univariate forecasts. We would conclude that this is because of the new predictors, specially mobility, temperature, and humidity. As a side note, we need to test if those differences are statistical significant or not (i.e. Diebold-Mariano Test). 23.7 Rolling and expanding windows A seven-day window is not enough for a reliable judgment on the forecast accuracy. One way to deal with this issue is to use rolling or expanding windows to predict the next h days. The following example shows a 1-day-ahead forecast with varying lags for embedding. library(randomForest) l = 3:10 # lags for embedding ws = 150 # size of each rolling window rmspe &lt;- c() all_fh &lt;- vector(mode = &quot;list&quot;, length = length(l)) all_y &lt;- vector(mode = &quot;list&quot;, length = length(l)) for (s in 1:length(l)) { dt &lt;- as.data.frame(embed(as.matrix(dft[,-1]), l[s])) nwin &lt;- nrow(dt) - ws #number of windows fh &lt;- c() y &lt;- c() for (i in 1:nwin) { train &lt;- dt[i:(ws + i - 1),] # each loop, window moves one day forward test &lt;- dt[(ws + i),] set.seed(i + s) fit &lt;- randomForest(train[,-1], train[, 1]) fh[i] &lt;- predict(fit, test[,-1]) y[i] &lt;- test[, 1] # to use later for plotting } all_y[[s]] &lt;- y all_fh[[s]] &lt;- fh err &lt;- test[, 1] - fh rmspe[s] &lt;- sqrt(mean(err ^ 2)) } rmspe ## [1] 45.00315 45.02472 45.31378 44.83595 45.81351 47.01531 47.14842 46.94298 bst &lt;- which.min(rmspe) l[bst] # Winning lag in embedding ## [1] 6 To adjust the application above to an expanding-window forecast, we just need to change dt[i:(ws + i - 1), ] to dt[1:(ws + i - 1), ] in the script. Now, we can plot the results: par(mfrow = c(1, 2)) plot( all_y[[bst]], type = &quot;l&quot;, col = &quot;red&quot;, ylab = &quot;Actual (red) vs Predicted (Blue)&quot;, xlab = &quot;Days&quot;, main = &quot;1-Day-Ahead&quot; ) lines(all_fh[[bst]], col = &quot;blue&quot;) plot( all_y[[bst]][60:110], type = &quot;o&quot;, col = &quot;red&quot;, ylab = &quot;Actual (red) vs Predicted (Blue)&quot;, xlab = &quot;Days&quot;, main = &quot;Last 50 Days&quot; ) lines(all_fh[[bst]][60:110], col = &quot;blue&quot;) Getting the predicted values back to originals can be achieved by: \\[ \\begin{aligned} &amp; y_{t+1}=y_t+z_{t+1} \\\\ &amp; y_{t+2}=y_{t+1}+z_{t+2}=y_t+z_{t+1}+z_{t+2} \\end{aligned} \\] set.seed(321) y &lt;- rnorm(10) z &lt;- diff(y) # first differences back &lt;- cumsum(c(y[1], z)) cbind(y, back) ## y back ## [1,] 1.7049032 1.7049032 ## [2,] -0.7120386 -0.7120386 ## [3,] -0.2779849 -0.2779849 ## [4,] -0.1196490 -0.1196490 ## [5,] -0.1239606 -0.1239606 ## [6,] 0.2681838 0.2681838 ## [7,] 0.7268415 0.7268415 ## [8,] 0.2331354 0.2331354 ## [9,] 0.3391139 0.3391139 ## [10,] -0.5519147 -0.5519147 Since our algorithm predict the changes in observations, a simple sum would do the job for back transformation. For example, as a starting point, our algorithm predicts the change in \\(Y\\) from day 156 to 157 (window size 150 plus the best lag window, 6). When we add this predicted change to the actual \\(Y\\) at 156, it will give us the back-transformed forecast at day 157. y &lt;- df$cases # The first forecast is at ws (150) + l[best] (6) + 1, which is 157 # The first actual Y should start a day earlier # removing all Y&#39;s until ws+l[bst] y_a_day_before &lt;- y[-c(1:(ws + l[bst] - 1))] # This adds predicted changes to observed values a day earlier back_forecast &lt;- head(y_a_day_before,-1) + all_fh[[bst]] # Actual Y&#39;s in the test set starting at ws (150) + l[best] (6) + 1, which is 157 ytest &lt;- y[-c(1:(ws + l[bst]))] plot( ytest, type = &quot;l&quot;, col = &quot;blue&quot;, ylab = &quot;Actual Y (Blue) vs Forecast (Red)&quot;, xlab = &quot;Days&quot;, main = &quot;Back-transformed Forecast&quot; ) lines(back_forecast, type = &quot;l&quot;, col = &quot;red&quot;) It seems that, for most days, our algorithm simply forecasts the next day by using the value from the day before. If we change our algorithm to a 7-day-ahead forecast, this would be different. This is also a common problem when the predictive model has a poor forecasting power. Again, this is not due to our algorithm, but forecasting an epi curve with imperfect test data is almost impossible job, as we highlighted earlier. In practice, however, there are several ways that we can improve the scripts above. For example, we can consider the (rolling or expanding) window size as a hyperparameter. We can also have an explicit training for the Random Forest algorithm. We can have an ensemble forecasting by adding other predictive algorithms to the script, like boosting. Further, we can develop a base forecast that would give us a benchmark to see how much our algorithm improves against that base. Lastly, we could apply a transformation to the data in order to stabilize the variance in all variables. "],["support-vector-machine.html", "Chapter 24 Support Vector Machine 24.1 Optimal Separating Classifier 24.2 Nonlinear Boundary with Kernels 24.3 Application with SVM", " Chapter 24 Support Vector Machine In this section, we will delve into two of the most advanced learning algorithms, Support Vector Machines (SVM) and Neural Networks (NN), and explore their similarities and differences in terms of predictive power and explanatory capability. Both SVM and NN are parametric algorithms, but for different reasons. SVMs use a kernel method to embed non-linearity, while NNs use non-linear activation functions. Therefore, they have different types and numbers of parameters. Despite these differences, both SVM and NN can approximate non-linear decision functions and can tackle the same classification problem using the same dataset. Their performance accuracy is comparable when given comparable training. However, when given more training and computational power, NNs tend to outperform SVMs. One key difference between SVM and NN is the time required to train them on the same dataset. This is because the kernel method used by SVMs can be computationally expensive, while the activation function used by NNs can be more efficient. Overall, understanding the similarities and differences between SVM and NN can help us determine which algorithm is best suited for a particular problem. Up to this point we have seen “probabilistic” binary classifiers, such as kNN, CART, Ensemble models, and classification regressions (logistic , LPM), where probabilistic predictions are made on observations and then converted to binary predictions based a tuned discriminating threshold. Support-vector machines do not use probabilistic predictions to classify the outcomes, which is inspired from one of the oldest algorithms in machine learning introduced by Rosenblatt in 1958, the perceptron algorithm, for learning a linear classifier. Support Vector Machine (SVM) is a modern approach to linear separation. Here is the history and little introduction to SVM by Wikipedia: In machine learning, support-vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&amp;T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall. We will develop our discussion in this chapter on two cases: a linear class boundary (Optimal Separating Classifier) and a non-linear class boundary (Support-Vector Machines). First section has no practical importance as (when) we usually face non-linear class boundary problems in real life. However, it will help us build SVM step by step. We will use a simplifying assumption here to start with: the classes are perfectly linearly separable at the data points by using a single straight line. Thus we have two predictors: \\(x_1\\) and \\(x_2\\). Let’s look at an example: y &lt;- c(1,1,0,0,1,0,1,1,0,0) x1 &lt;- c(0.09,0.11, 0.17, 0.23, 0.33,0.5, 0.54,0.65,0.83,0.78) x2 &lt;- c(0.5,0.82, 0.24, 0.09,0.56, 0.40, 0.93, 0.82, 0.3, 0.72) data &lt;- data.frame(&quot;y&quot; = y, &quot;x1&quot; = x1, &quot;x2&quot; = x2) plot(data$x1, data$x2, col = (data$y+1), lwd = 4, xlab = &quot;x1&quot;, ylab = &quot;x2&quot;) Can we come up with a boundary (line) that separates blacks from reds? plot(data$x1, data$x2, col = (data$y+1), lwd = 4, xlab = &quot;x1&quot;, ylab = &quot;x2&quot;) abline(a = 0.29, b = 0.6, col = &quot;orange&quot;,lwd = 2) We call this line a hyperplane (well, in a 2-dimensional case it’s a line) that separates blacks from reds. Let’s mathematically define it: \\[ \\beta_{0}+X_{1} \\beta_{1}+X_{2} \\beta_{2} = 0 \\] Hence, the “line”: \\[ X_{2}=-\\hat{\\beta}_{0} / \\hat{\\beta}_{2}-\\hat{\\beta}_{1} / \\hat{\\beta}_{2} X_{1} . \\] And the classiciation rule after getting the “line” is simple \\[ \\beta_{0}+X_{1} \\beta_{1}+X_{2} \\beta_{2}&gt;0 \\text { (red) } \\text { or }&lt;0 \\text { (black) } \\] As soon as we come up with the line, the classification is simple. But, we have two questions to answer: (1) How are we going to derive the line from the data? (2) How can we decide which line among many alternatives, which give the same classification score on the training data, is the best in terms of generalization (a better prediction accuracy on different observations). There are many possible hyperplanes with the same classification score: plot(data$x1, data$x2, col = (data$y+1), lwd = 4, xlab = &quot;x1&quot;, ylab = &quot;x2&quot;) abline(a = 0.29, b = 0.6, col = &quot;blue&quot;, lwd = 2) abline(a = 0.20, b = 0.8, col = &quot;orange&quot;, lwd = 2) abline(a = 0.10, b = 1.05, col = &quot;green&quot;, lwd = 2) abline(a = 0.38, b = 0.47, col = &quot;brown&quot;, lwd = 2) The orange line in our example is \\[ -0.87-1.8X_{1}+3X_{2} = 0, \\\\ X_{2}=0.29 - 0.60 X_{1} \\] 24.1 Optimal Separating Classifier We start with a decision boundary separating the dataset and satisfying: \\[ \\mathbf{w} \\cdot \\mathbf{x}+b=0, \\] where \\(\\mathbf{w}\\) is the vector of weights (coefficients) and \\(b\\) is the intercept. We use \\(\\mathbf{w} \\cdot \\mathbf{x}\\) with a dot product, instead of \\(\\mathbf{w}^{T} \\mathbf{x}\\). We can select two others hyperplanes \\(\\mathcal{H}_{1}\\) and \\(\\mathcal{H}_{0}\\) which also separate the data and have the following equations : \\[ \\mathbf{w} \\cdot \\mathbf{x}+b=\\delta \\\\ \\mathbf{w} \\cdot \\mathbf{x}+b=-\\delta \\] We define the the decision boundary, which is equidistant from \\(\\mathcal{H}_{1}\\) and \\(\\mathcal{H}_{0}\\). For now, we can arbitrarily set \\(\\delta=1\\) to simplify the problem. \\[ \\mathbf{w} \\cdot \\mathbf{x}+b=1 \\\\ \\mathbf{w} \\cdot \\mathbf{x}+b=-1 \\] Here is our illustration: Plot shows the following lines: \\(\\mathcal{H}_{1}\\): \\(\\mathbf{w} \\cdot \\mathbf{x}+b=1\\); \\(\\mathcal{H}_{0}\\): \\(\\mathbf{w} \\cdot \\mathbf{x}+b=-1\\); Decision Boundary: \\(\\mathbf{w} \\cdot \\mathbf{x}+b=0.\\) The data points lying on \\(\\mathcal{H}_{1}\\) (2 reds) or \\(\\mathcal{H}_{0}\\) (2 blacks) are called support vectors and only these points influence the decision boundary! The margin (\\(m\\)), which is a perpendicular line (arrow), is defined as the perpendicular distance from the points on the dash lines (\\(\\mathcal{H}_{1}\\) and \\(\\mathcal{H}_{0}\\)) to the boundary (gray line). Since all these margins would be equidistant, both definitions of \\(m\\) would measure the same magnitude. Our job is to find the maximum margin. The model is invariant with respect to the training set changes, except the changes of support vectors. If we make a small error in estimating the boundary, the classification will likely stay correct. Moreover, the distance of an observation from the hyperplane can be seen as a measure of our confidence that the observation was correctly classified. 24.1.1 The Margin In order to understand how we can find the margin, we will use a bit vector algebra. Let’s start defining the vector normal Let \\(\\mathbf{u}=\\left\\langle u_{1}, u_{2}, u_{3}\\right\\rangle\\) and \\(\\mathbf{v}=\\left\\langle v_{1}, v_{2}, v_{3}\\right\\rangle\\) be two vectors with a common initial point. Then \\(\\mathbf{u}, \\mathbf{v}\\) and \\(\\mathbf{u}-\\mathbf{v}\\) form a triangle, as shown. By the Law of Cosines, \\[ \\|\\mathbf{u}-\\mathbf{v}\\|^{2}=\\|\\mathbf{u}\\|^{2}+\\|\\mathbf{v}\\|^{2}-2\\|\\mathbf{u}\\|\\|\\mathbf{v}\\| \\cos \\theta \\] where \\(\\theta\\) is the angle between \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). Note that \\(\\|\\mathbf{u}\\|\\) is representing the vector norm. Using the formula for the magnitude of a vector, we obtain \\[ \\left(u_{1}-v_{1}\\right)^{2}+\\left(u_{2}-v_{2}\\right)^{2}+\\left(u_{3}-v_{3}\\right)^{2}=\\left(u_{1}^{2}+u_{2}^{2}+u_{3}^{2}\\right)+\\left(v_{1}^{2}+v_{2}^{2}+v_{3}^{2}\\right)-2\\|\\mathbf{u}\\|\\|\\mathbf{v}\\| \\cos \\theta \\\\ u_{1} v_{1}+u_{2} v_{2}+u_{3} v_{3}=\\|\\mathbf{u}\\|\\|\\mathbf{v}\\| \\cos \\theta \\\\ \\mathbf{u} \\cdot \\mathbf{v}=\\|\\mathbf{u}\\|\\|\\mathbf{v}\\| \\cos \\theta\\text {. } \\] Suppose that two nonzero vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) have an angle between them, \\(\\theta=\\pi / 2\\). That is, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are perpendicular, or orthogonal. Then, we have \\[ \\mathbf{u} \\cdot \\mathbf{v}=|\\mathbf{u}||\\mathbf{v}| \\cos \\frac{\\pi}{2}=0 \\] In other words, if \\(\\mathbf{u} \\cdot \\mathbf{v}=0\\), then we must have \\(\\cos \\theta=0\\), where \\(\\theta\\) is the angle between them, which implies that \\(\\theta=\\pi / 2\\) (remember \\(\\operatorname{Cos} 90^{\\circ}=0\\)). In summary, \\(\\mathbf{u} \\cdot \\mathbf{v}=0\\) if and only if \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal. Using this fact, we can see that the vector \\(\\mathbf{w}\\) is perpendicular (a.k.a “normal”) to \\(\\mathcal{H}_{1}\\), \\(\\mathbf{w} \\cdot \\mathbf{x}+b=0.\\) Consider the points \\(x_{a}\\) and \\(x_{b}\\), which lie on \\(\\mathcal{H}_{1}\\). This gives us two equations: \\[ \\begin{aligned} &amp;\\mathbf{w} \\cdot \\mathbf{x}_a+b=1 \\\\ &amp;\\mathbf{w} \\cdot \\mathbf{x}_b+b=1 \\end{aligned} \\] Subtracting these two equations results in \\(\\mathbf{w} .\\left(\\mathbf{x}_{a}-\\mathbf{x}_{b}\\right)=0\\). Note that the vector \\(\\mathbf{x}_{a}-\\mathbf{x}_{b}\\) lies on \\(\\mathcal{H}_{1}\\). Since the dot product \\(\\mathbf{w} .\\left(\\mathbf{x}_{a}-\\mathbf{x}_{b}\\right)\\) is zero, \\(\\mathbf{w}\\) must be orthogonal to \\(\\mathbf{x}_{a}-\\mathbf{x}_{b},\\) thus, to \\(\\mathcal{H}_{1}\\) as well. This can be repeated for the decision boundary or \\(\\mathcal{H}_{0}\\) too. Let’s define a unit vector of \\(\\mathbf{w}\\) \\[ \\mathbf{u}=\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}, \\] where \\(\\|\\mathbf{w}\\| = \\sqrt{w_{1}^{2}+w_{2}^{2}} \\dots=\\sqrt{w_{1} w_{1}+w_{2} w_{2} \\dots} = \\mathbf{w}.\\mathbf{w}\\), which is called the magnitude (or length) of the vector. Since it is a unit vector (\\(\\|\\mathbf{u}\\|=1\\)) and it has the same direction as \\(\\mathbf{w}\\) it is also perpendicular to the hyperplane. If we multiply \\(\\mathbf{u}\\) by \\(m\\), which is the distance from either hyperplanes to the boundary, we get the vector \\(\\mathbf{k}=m \\mathbf{u}\\). We observed that \\(\\|\\mathbf{k}\\|=m\\) and \\(\\mathbf{k}\\) is perpendicular to \\(\\mathcal{H}_{1}\\) (since it has the same direction as \\(\\mathbf{u}\\)). Hence, \\(\\mathbf{k}\\) is the vector with the same magnitude and direction of \\(m\\) we were looking for. The rest will be relatively a simple algebra: \\[ \\mathbf{k}=m \\mathbf{u}=m \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|} \\] We start from a point, \\(\\mathbf{x}_{0}\\) on \\(\\mathcal{H}_{0}\\) and add \\(k\\) to find the point \\(\\mathbf{x^\\prime}=\\mathbf{x}_{0}+\\mathbf{k}\\) on the decision boundary, which means that \\(\\mathbf{w} \\cdot \\mathbf{x^\\prime}+b=0\\). \\[ \\begin{gathered} \\mathbf{w} \\cdot\\left(\\mathbf{x}_{0}+\\mathbf{k}\\right)+b=0, \\\\ \\mathbf{w} \\cdot\\left(\\mathbf{x}_{0}+m \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}\\right)+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}_{0}+m \\frac{\\mathbf{w} \\cdot \\mathbf{w}}{\\|\\mathbf{w}\\|}+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}_{0}+m \\frac{\\|\\mathbf{w}\\|^{2}}{\\|\\mathbf{w}\\|}+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}_{0}+m\\|\\mathbf{w}\\|+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}_{0}+b= -m\\|\\mathbf{w}\\|, \\\\ -1=-m\\|\\mathbf{w}\\|, \\\\ m\\|\\mathbf{w}\\|=1,\\\\ m=\\frac{1}{\\|\\mathbf{w}\\|}. \\end{gathered} \\] One can easily see that the bigger the norm is, the smaller the margin become. Thus, maximizing the margin is the same thing as minimizing the norm of \\(\\mathbf{w}\\). Among all possible hyperplanes meeting the constraints, if we choose the hyperplane with the smallest \\(\\|\\mathbf{w}\\|\\), it would be the one which will have the biggest margin9. Finally, the above derivation can be written to find the distance between the decision boundary and any point (\\(\\mathbf{x}\\)). Supposed that \\(\\mathbf{x^\\prime}\\) on the decision boundary: \\[ \\begin{gathered} \\mathbf{x^\\prime}=\\mathbf{x}-\\mathbf{k}, \\\\ \\mathbf{w} \\cdot \\mathbf{x^\\prime}+b=0, \\\\ \\mathbf{w} \\cdot\\left(\\mathbf{x}-m \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}\\right)+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}-m \\frac{\\mathbf{w} \\cdot \\mathbf{w}}{\\|\\mathbf{w}\\|}+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}-m \\frac{\\|\\mathbf{w}\\|^{2}}{\\|\\mathbf{w}\\|}+b=0, \\\\ \\mathbf{w} \\cdot\\mathbf{x}-m\\|\\mathbf{w}\\|+b=0, \\\\ m=\\frac{\\mathbf{w} \\cdot \\mathbf{x^\\prime}+b}{\\|\\mathbf{w}\\|}, \\\\ m=\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|} \\cdot \\mathbf{x}+\\frac{b}{\\|\\mathbf{w}\\|}, \\end{gathered} \\] which shows the distance between boundary and \\(\\mathcal{H}_{1}\\) is 1 as the result (\\(\\mathbf{w} \\cdot \\mathbf{x}+b=1\\)) reveals10. Given the following hyperplanes, \\[ \\mathbf{w} \\cdot \\mathbf{x}+b=1, \\\\ \\mathbf{w} \\cdot \\mathbf{x}+b=-1, \\] we can write our decision rules as \\[ \\mathbf{w} \\cdot \\mathbf{x}_i+b \\geq 1 ~~\\Longrightarrow ~~~ y_i = 1,\\\\ \\mathbf{w} \\cdot \\mathbf{x}_i+b \\leq-1 ~~\\Longrightarrow ~~~ y_i = -1. \\] And, when we combine them, we can get a unique constraint: \\[ y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1 ~~~~~\\text { for all} ~~~ i \\] Usually, it is confusing to have a fixed threshold, 1, in the constraint. To see the origin of this, we define our optimization problem as \\[ \\operatorname{argmax}\\left(\\mathbf{w}^{*}, b^{*}\\right)~~ m ~~~~~\\text {such that } ~~~~~ y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq m. \\] Since the hyperplane can be scaled any way we want: \\[ \\mathbf{w} \\cdot \\mathbf{x}_{i}+b = 0 ~~~ \\Rightarrow~~~ s\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) = 0\\\\\\text{where}~~~ s \\neq 0. \\] Hence, we can write \\[ \\frac{1}{\\|\\mathbf{w}\\|}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) =0. \\] Therefore, \\[ \\begin{gathered} \\frac{1}{\\|\\mathbf{w}\\|}\\mathbf{y}_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq m,\\\\ y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq m\\|\\mathbf{w}\\|\\\\ y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1 \\end{gathered} \\] Finally, we can write our optimization problem as \\[ \\operatorname{argmin}\\left(\\mathbf{w}^{*}, b^{*}\\right) ~~ \\|\\mathbf{w}\\| ~~~~\\text {such that } ~~~~ \\mathbf{y}_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq 1, \\] which can be re-written as you will see it in the literature: \\[ \\begin{array}{ll} \\underset{\\mathbf{w}, b}{\\operatorname{minimize}} &amp; \\frac{1}{2}\\|\\mathbf{w}\\|^{2} \\\\ \\text { subject to } &amp; y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1, \\quad i=1, \\ldots, n \\end{array} \\] where squaring the norm has the advantage of removing the square root and \\(1/2\\) helps solving the quadratic problem. All gives the same solution: \\[ \\hat{f}(x) = \\hat{\\mathbf{w}} \\cdot \\mathbf{x}_{i}+\\hat{b}, \\] which can be used for classifying new observation by \\(\\text{sign}\\hat{f}(x)\\). A Lagrangian function can be used to solve this optimization problem. We will not show the details of the solution process, but we will continue on with the non-separable case11. Let’s use svm() command from the e1071 package for an example: library(e1071) # Sample data - Perfectly separated set.seed(1) x &lt;- matrix(rnorm(20 * 2), ncol = 2) y &lt;- c(rep(-1, 10), rep(1, 10)) x[y == 1, ] &lt;- x[y == 1, ] + 2 dat &lt;- data.frame(x = x, y = as.factor(y)) # Support Vector Machine model mfit &lt;- svm(y ~ ., data = dat, kernel = &quot;linear&quot;, scale = FALSE) summary(mfit) ## ## Call: ## svm(formula = y ~ ., data = dat, kernel = &quot;linear&quot;, scale = FALSE) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 1 ## ## Number of Support Vectors: 4 ## ## ( 2 2 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 plot(mfit, dat, grid = 200, col = c(&quot;lightgray&quot;, &quot;lightpink&quot;)) Points indicated by an “x” are the support vectors. They directly affect the classification line. The points shown with an “o” don’t affect the calculation of the line. This principle distinguishes support vector method from other classification methods that use the entire data to fit the classification boundary. 24.1.2 The Non-Separable Case What if we have cases like, In the first plot, although the orange boundary would perfectly separates the classes, it would be less “generalizable” (i.e., more specific to the train data means more prediction errors) than the blue boundary. In the second plot, there doesn’t exist a linear boundary without an error. If we can tolerate a mistake, however, the blue line can be used as a separating boundary. In both cases the blue lines could be the solution with some kind of “tolerance” level. It turns out that, if we are able to introduce this “error tolerance” to our optimization problem described in the perfectly separable case, we can make the “Optimal Separating Classifier” as a trainable model by tuning the “error tolerance”, which can be our hyperparameter. This is exactly what we will do: \\[ \\operatorname{argmin}\\left(\\mathbf{w}^{*}, b^{*}\\right) \\|\\mathbf{w}\\| ~~~~~~~\\text {such that } ~~~~~~~ y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq 1 \\] \\[ \\operatorname{argmin}\\left(\\mathbf{w}^{*}, b^{*}\\right) \\|\\mathbf{w}\\| \\quad \\text { such that }\\left\\{\\begin{array}{l} y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq 1-\\epsilon_{i} ~~~~\\forall i \\\\ \\epsilon_{i} \\geq 0,~~~~ \\sum \\epsilon_{i} \\leq C \\end{array}\\right. \\] where \\(\\epsilon\\) is the “tolarence” for an error and \\(C\\) is a nonnegative hyperparameter. The first constraint can be written as \\(y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) + \\epsilon_{i} \\geq 1\\). Remember that, by the nature of this constraint, the points well inside their class boundary will not play a roll in shaping the tolerance level. This could be written another way: \\[ \\min _{\\mathbf{w} \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}, \\epsilon \\in \\mathbb{R}^{n}}\\left\\{\\frac{1}{2}\\|\\mathbf{w}\\|^{2}+C \\sum_{i=1}^{n} \\epsilon_{i}\\right\\}\\\\ \\text {subject to} \\\\ y_{i} \\cdot\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\epsilon_{i} ~~ \\text{and} ~~ \\epsilon_{i} \\geq 0, ~~\\forall i=1, \\cdots, n. \\] And as a maximization problem, \\[ \\operatorname{argmax}\\left(\\mathbf{w}^{*}, b^{*}\\right) ~m \\quad \\text { such that }\\left\\{\\begin{array}{l} y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq m(1-\\epsilon_{i}) ~~~~\\forall i \\\\ \\epsilon_{i} \\geq 0,~~~~ \\sum \\epsilon_{i} \\leq C \\end{array}\\right. \\] This approach is also called soft margin classification or support vector classifier in practice. Although this setting will relax the requirement of a perfect separation, it still requires a linear separation. set.seed(1) x &lt;- matrix(rnorm(20 * 2), ncol = 2) y &lt;- c(rep(-1, 10), rep(1, 10)) x[y == 1, ] &lt;- x[y == 1, ] + 1 dt &lt;- data.frame(x = x, y = as.factor(y)) # C = 10 mfit10 &lt;- svm( y ~ ., data = dt, kernel = &quot;linear&quot;, scale = FALSE, cost = 10 ) plot( mfit10, dat, grid = 200, col = c(&quot;lightgray&quot;, &quot;lightpink&quot;), main = &quot;C = 10&quot; ) # Tuning C tuned &lt;- tune(svm, y ~ ., data = dat, kernel = &quot;linear&quot;, ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))) (best &lt;- tuned$best.model) ## ## Call: ## best.tune(METHOD = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, ## 0.01, 0.1, 1, 5, 10, 100)), kernel = &quot;linear&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 0.1 ## ## Number of Support Vectors: 13 # Using tuned model on the whole data yhat &lt;- predict(best, dat) (misclass &lt;- table(predict = yhat, truth = dt$y)) ## truth ## predict -1 1 ## -1 10 1 ## 1 0 9 We will now look at how we can introduce non-linearity to the class boundaries. 24.2 Nonlinear Boundary with Kernels Many data sets are not linearly separable. Although, adding polynomial features and interactions can be used, a low polynomial degree cannot deal with very complex data sets. The support vector machine (SVM) is an extension of the “support vector classifier” that results from enlarging the feature space in a specific way, using kernels. SVM works well for complex but small- or medium-sized data sets. To demonstrate a nonlinear classification boundary, we will construct a new data set: set.seed (1) x &lt;- matrix(rnorm(200*2), ncol = 2) x[1:100, ] &lt;- x[1:100, ] + 2 x[101:150, ] &lt;- x[101:150, ] - 2 y &lt;- c(rep(1,150), rep(2,50)) dt &lt;- data.frame(x=x,y=as.factor(y)) plot(x[ ,1], x[ ,2], pch=16, col = y*2) Notice that the data is not linearly separable and isn’t all clustered together in a single group either. We can of course make our decision boundary nonlinear by adding the polynomials and interaction terms. Adding more terms, however, may expand the feature space to the point that leads to inefficient computations. We haven’t shown the explicit solution to the optimization problem we stated for a separable case \\[ \\begin{array}{ll} \\underset{\\mathbf{w}, b}{\\operatorname{minimize}} &amp; \\frac{1}{2}\\|\\mathbf{w}\\|^{2} \\\\ \\text { subject to } &amp; y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1, \\quad i=1, \\ldots, n \\end{array} \\] Which can be set in Lagrangian: \\[ \\min L=0.5\\|\\mathbf{w}\\|^{2}-\\sum \\alpha_i \\left[y_i \\left(\\mathbf{w} \\cdot \\mathbf{x}_i + b\\right)-1\\right],\\\\ \\min L=0.5\\|\\mathbf{w}\\|^{2}-\\sum \\alpha_i y_i \\left(\\mathbf{w} \\cdot \\mathbf{x}_i\\right) + b\\sum \\alpha_i y_i+\\sum \\alpha_i, \\] with respect to \\(\\mathbf{w},b\\). These are also called as “primal forms”. Hence the first order conditions are \\[ \\begin{aligned} &amp;\\frac{\\partial L}{\\partial \\mathbf{w}}=\\mathbf{w}-\\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i}=0 \\\\ &amp;\\frac{\\partial L}{\\partial b}=\\sum_{i=1}^{n} \\alpha_{i} y_{i}=0 \\end{aligned} \\] We solve the optimization problem by now solving for the dual of this original problem (substituting for \\(\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i}\\) and \\(\\sum_{i=1}^{n} \\alpha_{i} y_{i}=0\\) back into the original equation). Hence the “dual problem: \\[ \\max L\\left(\\alpha_{i}\\right)=\\sum_{i=1}^{n}\\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n}\\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right) \\] The solution to this involves computing the just the inner products of \\(x_{i}, x_{j}\\), which is the key point in SVM problems. \\[ \\alpha_{i}\\left[y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_i + b\\right)-1\\right]=0 ~~~~~\\forall i \\] From these we can see that, if \\(\\left(\\mathbf{w} \\cdot \\mathbf{x}_i + b\\right)&gt;1\\) (since \\(x_{i}\\) is not on the boundary of the slab), \\(\\alpha_{i}\\) will be \\(0.\\) Therefore, the most of the \\(\\alpha_{i}\\) ’s will be zero as we have a few support vectors (on the gutters or margin). This reduces the dimensionality of the solution! Notice that inner products provide some measure of “similarity”. The inner product between 2 vectors of unit length returns the cosine of the angle between them, which reveals how “far apart” they are. We have seen that if they are perpendicular (completely unlike) their inner product is 0; or, if they are parallel their inner product is 1 (completely similar). Now consider the function for only non zero \\(\\alpha\\) ’s. \\[ \\max L\\left(\\alpha_{i}\\right)=\\sum_{i=1}^{n}\\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right) \\] If two features \\(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\) are completely dissimilar (their dot product will be 0), they don’t contribute to \\(L\\). Or, if they are completely alike, their dot product will be 1. In this case, suppose that both \\(\\mathbf{x}_{i}\\) and \\(\\mathbf{x}_{j}\\) predict the same output value \\(y_{i}\\) (either \\(+1\\) or \\(-1\\) ). Then \\(y_{i} y_{j}\\) is always 1, and the value of \\(\\alpha_{i} \\alpha_{j} y_{i} y_{j} \\mathbf{x}_{i} \\mathbf{x}_{j}\\) will be positive. But this would decrease the value of \\(L\\) (since it would subtract from the first term sum). So, the algorithm downgrades similar feature vectors that make the same prediction. On the other hand, when \\(x_{i}\\), and \\(x_{j}\\) make opposite predictions (i.e., predicting different classes, one is \\(+1\\), the other \\(-1\\)) about the output value \\(y_{i}\\), but are otherwise very closely similar (i.e., their dot product is \\(1\\)), then the product \\(a_{i} a_{j} y_{i} y_{j} x_{i} x\\) will be negative. Since we are subtracting it, it adds to the sum maximizing \\(L\\). This is precisely the examples that algorithm is looking for: the critical ones that tell the two classes apart. What if the decision function is not linear as we have in the figure above? What transform would separate these? The idea in SVM is to obtain a nonlinear separation by mapping the data to a higher dimensional space. Remember the function we want to optimize: \\(L=\\sum \\alpha_{i}-1 / 2 \\sum \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right)\\) where \\(\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right)\\) is the dot product of the two feature vectors. We can transform them, for example, by \\(\\phi\\) that is a quadratic polynomial. As we discussed earlier, however, we don’t know the function explicitly. And worse, as we increase the degree of polynomial, the optimization becomes computational impossible. If there is a “kernel function” \\(K\\) such that \\(K\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right)=\\phi\\left(\\mathbf{x}_{i}\\right) \\cdot \\phi\\left(\\mathbf{x}_{j}\\right)\\), then we do not need to know or compute \\(\\phi\\) at all. That is, the kernel function defines inner products in the transformed space. Or, it defines similarity in the transformed space. The function we want to optimize becomes: \\[ \\max L\\left(\\alpha_{i}\\right)=\\sum_{i=1}^{n}\\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} K\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right) \\] The polynomial kernel \\(K\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right)=\\left(\\mathbf{x}_i \\cdot \\mathbf{x}_{j}+1\\right)^{p}\\), where \\(p\\) is a hyperparmater Examples for Non Linear SVMs \\[ \\begin{gathered} K(\\mathbf{x}, \\mathbf{y})=(\\mathbf{x} \\cdot \\mathbf{y}+1)^{p} \\\\ K(\\mathbf{x}, \\mathbf{y})=\\exp \\left\\{-\\|\\mathbf{x}-\\mathbf{y}\\|^{2} / 2 \\sigma^{2}\\right\\} \\\\ K(\\mathbf{x}, \\mathbf{y})=\\tanh (\\kappa \\mathbf{x} \\cdot \\mathbf{y}-\\delta) \\end{gathered} \\] The first one is polynomial (includes \\(\\mathrm{x} \\cdot \\mathrm{x}\\) as special case); the second one is radial basis function (Gaussian), the last one is sigmoid function. Here is the SVM application to our data: library (e1071) svmfit &lt;- svm(y~., data=dt, kernel = &quot;polynomial&quot;, cost = 1, degree = 2) plot(svmfit, dt, grid=200, col= c(&quot;pink&quot;, &quot;lightblue&quot;)) Or select the cost parameter by 10-fold CV among several values with radial kernel: tune.out &lt;- tune(svm, y~., data=dt, kernel=&quot;radial&quot;, ranges = list(cost = c(0.1, 1, 10, 100), gamma = c(0.5, 1, 2, 3, 4))) plot(tune.out$best.model,dt, grid=200, col= c(&quot;pink&quot;, &quot;lightblue&quot;)) With more than two features, we can’t plot decision boundary. We can, however, produce a ROC curve to analyze the results. As we know, SVM doesn’t give probabilities to belong to classes. We compute scores of the form \\(\\hat{f}(X)=\\varphi\\left(X_{i}\\right) \\hat{\\beta}\\) for each observation. Then use the scores as predicted values. Here is the application: library(ROCR) # Let&#39;s fit a SVM with radial kernel and plot a ROC curve: set.seed(1) train &lt;- sample(200, 100) train &lt;- sort(train, decreasing=TRUE) model &lt;- svm(y~., data = dt[train,], kernel = &quot;radial&quot;, cost = 1, gamma=0.5) fit &lt;- attributes(predict(model, dt[-train, ], decision.values=TRUE))$decision.values # AUC pred_rocr &lt;- prediction(fit, dt[-train,&quot;y&quot;]) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.9614225 # ROCR perf &lt;- performance(pred_rocr,&quot;tpr&quot;,&quot;fpr&quot;, main = &quot;SVM&quot;) plot(perf, colorize=TRUE) abline(a = 0, b = 1) # Let&#39;s also fit a Logistic model: logit &lt;- glm(y ~., data = dt[train, ], family = binomial(link = &#39;logit&#39;)) fit2 &lt;- predict(logit, dt[-train, ], type = &quot;response&quot;) pred_rocr &lt;- prediction(fit2, dt[-train,&quot;y&quot;]) perf &lt;- performance(pred_rocr,&quot;tpr&quot;,&quot;fpr&quot;, main = &quot;SVM&quot;) pred_rocr &lt;- prediction(fit2, dt[-train,&quot;y&quot;]) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.6274864 par(new = TRUE) plot(perf, colorize=TRUE) abline(a = 0, b = 1) 24.3 Application with SVM Let’s finsh this chapter with an example: train &lt;- read.csv(&quot;adult_train.csv&quot;, header = FALSE) varNames &lt;- c(&quot;Age&quot;, &quot;WorkClass&quot;, &quot;fnlwgt&quot;, &quot;Education&quot;, &quot;EducationNum&quot;, &quot;MaritalStatus&quot;, &quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;CapitalGain&quot;, &quot;CapitalLoss&quot;, &quot;HoursPerWeek&quot;, &quot;NativeCountry&quot;, &quot;IncomeLevel&quot;) names(train) &lt;- varNames data &lt;- train tbl &lt;- table(data$IncomeLevel) tbl ## ## &lt;=50K &gt;50K ## 24720 7841 # we remove some outliers - See Ch.11 ind &lt;- which(data$NativeCountry==&quot; Holand-Netherlands&quot;) data &lt;- data[-ind, ] #Converting chr to factor df &lt;- data df[sapply(df, is.character)] &lt;- lapply(df[sapply(df, is.character)], as.factor) When we use the whole data it takes very long time and memory. A much better way to deal with this issue is to not use all of the data. This is because, most data points will be redundant from the SVM’s perspective. Remember, SVM only benefits from having more data near the decision boundaries. Therefore, we can randomly select, say, 10% of the training data (it should be done multiple times to see its consistency), and understand what its performance looks like: # Initial Split 90-10% split set.seed(123) ind &lt;- sample(nrow(df), nrow(df) * 0.90, replace = FALSE) train &lt;- df[ind,] test &lt;- df[-ind,] # Using 10% of the train set.seed(321) ind &lt;- sample(nrow(train), nrow(train) * 0.10, replace = FALSE) dft &lt;- train[ind,] # You should check different kernels with a finer grid tuning &lt;- tune( svm, IncomeLevel ~ ., data = dft, kernel = &quot;radial&quot;, ranges = list( cost = c(0.1, 1, 10, 100), gamma = c(0.05, 0.5, 1, 2, 3, 4) ) ) tuning$best.model ## ## Call: ## best.tune(METHOD = svm, train.x = IncomeLevel ~ ., data = dft, ranges = list(cost = c(0.1, ## 1, 10, 100), gamma = c(0.05, 0.5, 1, 2, 3, 4)), kernel = &quot;radial&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 1 ## ## Number of Support Vectors: 1131 Now, let’s have our the tuned model tuned &lt;- svm(IncomeLevel~., data= dft, kernel=&quot;radial&quot;, cost =1) caret::confusionMatrix(reference = test$IncomeLevel, predict(tuned, newdata = test, type = &quot;class&quot;)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction &lt;=50K &gt;50K ## &lt;=50K 2328 392 ## &gt;50K 115 421 ## ## Accuracy : 0.8443 ## 95% CI : (0.8314, 0.8566) ## No Information Rate : 0.7503 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5311 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9529 ## Specificity : 0.5178 ## Pos Pred Value : 0.8559 ## Neg Pred Value : 0.7854 ## Prevalence : 0.7503 ## Detection Rate : 0.7150 ## Detection Prevalence : 0.8354 ## Balanced Accuracy : 0.7354 ## ## &#39;Positive&#39; Class : &lt;=50K ## Another (simpler) way to get AUC and ROC: # Getting phats tuned2 &lt;- svm(IncomeLevel~., data= dft, kernel=&quot;radial&quot;, cost =1, probability = TRUE) svm.prob &lt;- predict(tuned2, type=&quot;prob&quot;, newdata=test, probability = TRUE) phat &lt;- attr(svm.prob, &quot;probabilities&quot;)[,2] # AUC pred.rocr &lt;- prediction(phat, test$IncomeLevel) auc_ROCR &lt;- performance(pred.rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.9022007 # ROC perf &lt;- performance(pred.rocr, &quot;tpr&quot;,&quot;fpr&quot;) plot(perf, colorize = TRUE) abline(a = 0, b = 1) Unfortunately, there is no direct way to get information on predictors with SVM, in contrast to, for example, random forest or GBM. The package rminer provides some sort of information in variable importance, but the details are beyond the scope of this chapter. It’s application is given below. # library(rminer) # M &lt;- fit(IncomeLevel~., data=dft, model=&quot;svm&quot;, kpar=list(sigma=0), C=1) # (svm.imp &lt;- Importance(M, data=train)) Note that this result could have been different had we chosen \\(\\delta\\) different than 1.↩︎ The distance calculation can be generalized for any points such as \\(x_{1,0}\\) and \\(x_{2,0}\\): \\(d=\\frac{\\left|a x_{1,0}+b x_{2,0}+c\\right|}{\\sqrt{a^{2}+b^{2}}}.\\) See the multiple proofs at Wikipedia: https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_line↩︎ see Elements of Statistical Learning, Page 133 (Hastie et al., 2009).↩︎ "],["neural-networks.html", "Chapter 25 Neural Networks 25.1 Neural Network - the idea 25.2 Backpropagation 25.3 Neural Network - More inputs", " Chapter 25 Neural Networks Artificial neural networks (ANNs) are a type of machine learning model that are inspired by the structure and function of the human brain. They consist of interconnected units called artificial neurons or nodes, which are organized into layers. The concept of artificial neural networks dates back to the 1940s, when Warren McCulloch and Walter Pitts (1943) proposed a model of the neuron as a simple threshold logic unit. In the 1950s and 1960s, researchers began developing more complex models of neurons and exploring the use of neural networks for tasks such as pattern recognition and machine translation. However, these early efforts were largely unsuccessful due to the limited computational power of the time. It wasn’t until the 1980s and 1990s that significant progress was made in the development of artificial neural networks, thanks to advances in computer technology and the availability of larger and more diverse datasets. In 1986, Geoffrey Hinton and his team developed the backpropagation algorithm, which revolutionized the field by allowing neural networks to be trained more efficiently and accurately. Since then, artificial neural networks have been applied to a wide range of tasks, including image and speech recognition, natural language processing, and even playing games like chess and Go. They have also been used in a variety of fields, including finance, healthcare, and transportation. Today, artificial neural networks are an important tool in the field of machine learning, and continue to be an active area of research and development. There have been many influential works accomplished in the field of artificial neural networks (ANNs) over the years. Here are a few examples of some of the most important and influential works in the history of ANNs: Perceptrons by Frank Rosenblatt (1958): This paper introduced the concept of the perceptron, which is a type of ANN that can be trained to recognize patterns in data. The perceptron became a foundational concept in the field of machine learning and was a key catalyst for the development of more advanced ANNs. Backpropagation by Rumelhart, Hinton, and Williams (1986): This paper introduced the backpropagation algorithm, which is a method for training ANNs that allows them to learn and adapt over time. The backpropagation algorithm is still widely used today and has been a key factor in the success of ANNs in many applications. LeNet-5 by Yann LeCun et al. (1998): This paper described the development of LeNet-5, an ANN designed for recognizing handwritten digits. LeNet-5 was one of the first successful applications of ANNs in the field of image recognition and set the stage for many subsequent advances in this area. Deep Learning by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton (2015): This paper provided a comprehensive review of the field of deep learning, which is a type of ANN that uses many layers of interconnected neurons to process data. It has had a major impact on the development of deep learning and has helped to drive many of the recent advances in the field. 25.1 Neural Network - the idea Both Support Vector Machines and Neural Networks employ some kind of data transformation that moves them into a higher dimensional space. What the kernel function does for the SVM, the hidden layers do for neural networks. Let’s start with a predictive model with a single input (covariate). The simplest model could be a linear model: \\[ y \\approx \\alpha+\\beta x \\] Since this model could be a quite restrictive, we can have a more flexible one by a polynomial regression: \\[ y \\approx \\alpha+\\beta_1 x+\\beta_2 x^2+\\beta_3 x^3+\\ldots = \\alpha+\\sum_{m=1}^M \\beta_m x^m \\] The polynomial regression is based on fixed components, or bases: \\(x, x^2, x^3, \\ldots, x^M.\\) The artificial neural net replaces these fixed components with adjustable ones or bases: \\(f\\left(\\alpha_1+\\delta_1 x\\right)\\), \\(f\\left(\\alpha_2+\\delta_2 x\\right)\\), \\(\\ldots, f\\left(\\alpha_M+\\delta_M x\\right).\\) We can see the first simple ANN as nonlinear functions of linear combinations: \\[ y \\approx \\alpha+\\beta_1 f\\left(\\alpha_1+\\delta_1 x\\right)+\\beta_2 f\\left(\\alpha_2+\\delta_2 x\\right)+\\beta_3 f\\left(\\alpha_3+\\delta_3 x\\right)+\\ldots\\\\ = \\alpha+\\sum_{m=1}^M \\beta_m f\\left(\\alpha_m+\\delta_m x\\right) \\] where \\(f(.)\\) is an activation function – a fixed nonlinear function. Common examples of activation functions are The logistic (or sigmoid) function: \\(f(x)=\\frac{1}{1+e^{-x}}\\); The hyperbolic tangent function: \\(f(x)=\\tanh (x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\\); The Rectified Linear Unit (ReLU): \\(f(x)=\\max (0, x)\\); The full list of activation functions can be found at Wikipedia. Let us consider a realistic (simulated) sample: n &lt;- 200 set.seed(1) x &lt;- sort(runif(n)) y &lt;- sin(12*(x + 0.2))/(x + 0.2) + rnorm(n)/2 df &lt;- data.frame(y, x) plot(x, y, main=&quot;Simulated data&quot;, col= &quot;grey&quot;) We can fit a polynomial regression with \\(M = 3\\): ols &lt;- lm(y ~ x + I(x^2) + I(x^3)) plot(x, y, main=&quot;Polynomial: M = 3&quot;, col= &quot;grey&quot;) lines(x, predict(ols), col=&quot;blue&quot;, lwd = 3) Now, we can think of the line as weighted sum of fixed components: \\(\\alpha_1+\\beta_1 x+\\beta_2 x^2+\\beta_3 x^3\\). # Parts first &lt;- ols$coefficients[2]*x second &lt;- ols$coefficients[3]*x^2 third &lt;- ols$coefficients[4]*x^3 yhat &lt;- ols$coefficients[1] + first + second + third # Plots par(mfrow=c(1,4), oma = c(0,0,2,0)) plot(x, first, ylab = &quot;y&quot;, col = &quot;pink&quot;, main = &quot;x&quot;) plot(x, second, ylab = &quot;y&quot;, col = &quot;orange&quot;, main = expression(x^2)) plot(x, third, ylab = &quot;y&quot;, col = &quot;green&quot;, main = expression(x^3)) plot(x, y, ylab=&quot;y&quot;, col = &quot;grey&quot;, main = expression(y == alpha + beta[1]*x + beta[2]*x^2 + beta[3]*x^3)) lines(x, yhat, col = &quot;red&quot;, lwd = 3) mtext(&quot;Fixed Components&quot;, outer=TRUE, cex = 1.5, col=&quot;olivedrab&quot;) The artificial neural net replaces the fixed components in the polynomial regression with adjustable ones, \\(f\\left(\\alpha_1+\\delta_1 x\\right)\\), \\(f\\left(\\alpha_2+\\delta_2 x\\right)\\), \\(\\ldots, f\\left(\\alpha_M+\\delta_M x\\right)\\) that are more flexible. They are adjustable with tunable internal parameters. They can express several shapes, not just one (fixed) shape. Hence, adjustable components enable to capture complex models with fewer components (smaller M). Let’s replace those fixed components \\(x, x^2, x^3\\) in our polynomial regression with \\(f\\left(\\alpha_1+\\delta_1 x\\right)\\), \\(f\\left(\\alpha_2+\\delta_2 x\\right)\\), \\(f\\left(\\alpha_3+\\delta_3 x\\right).\\) library(neuralnet) set.seed(2) nn &lt;- neuralnet(y ~ x, data = df, hidden = 3, threshold = 0.05) yhat &lt;- compute(nn, data.frame(x))$net.result plot(x, y, main=&quot;Neural Networks: M = 3&quot;) lines(x, yhat, col=&quot;red&quot;, lwd = 3) Why did neural networks perform better than polynomial regression in the previous example? Again, adjustable components enable to capture complex models. Let’s delve little deeper. Here is the weight structure of \\[ y \\approx \\alpha+\\sum_{m=1}^3 \\beta_m f\\left(\\alpha_m+\\delta_m x\\right)\\\\ = \\alpha+\\beta_1 f\\left(\\alpha_1+\\delta_1 x\\right)+\\beta_2 f\\left(\\alpha_2+\\delta_2 x\\right)+\\beta_3 f\\left(\\alpha_3+\\delta_3 x\\right) \\] nn$weights ## [[1]] ## [[1]][[1]] ## [,1] [,2] [,3] ## [1,] 1.26253 6.59977 2.504890 ## [2,] -18.95937 -12.24665 -5.700564 ## ## [[1]][[2]] ## [,1] ## [1,] 2.407654 ## [2,] 13.032092 ## [3,] 19.923742 ## [4,] -32.173264 plot(nn, rep = &quot;best&quot;) We used sigmoid (logistic) activation functions \\[ \\text{Node 1:} ~~~f(x)=\\frac{1}{1+e^{-x}}=\\frac{1}{1+e^{-(1.26253-18.95937x)}}\\\\ \\text{Node 2:} ~~~f(x)=\\frac{1}{1+e^{-x}}=\\frac{1}{1+e^{-(6.599773-12.24665x)}}\\\\ \\text{Node 3:} ~~~f(x)=\\frac{1}{1+e^{-x}}=\\frac{1}{1+e^{-(2.504890-5.700564x)}} \\] We can calculate the value of each activation function by using our data, \\(x\\): X &lt;- cbind(1, x) # to 1st Node n1 &lt;- nn$weights[[1]][[1]][,1] f1 &lt;- nn$act.fct(X%*%n1) # to 2nd Node n2 &lt;- nn$weights[[1]][[1]][,2] f2 &lt;- nn$act.fct(X%*%n2) # to 3rd Node n3 &lt;- nn$weights[[1]][[1]][,3] f3 &lt;- nn$act.fct(X%*%n3) par(mfrow=c(1,3), oma = c(0,0,2,0)) plot(x, f1, col = &quot;pink&quot;, main = expression(f(alpha[1] + beta[1]*x))) plot(x, f2, col = &quot;orange&quot;, main = expression(f(alpha[2] + beta[2]*x))) plot(x, f3, col = &quot;green&quot;, main = expression(f(alpha[3] + beta[3]*x))) mtext(&quot;Flexible Components&quot;, outer=TRUE, cex = 1.5, col=&quot;olivedrab&quot;) Now we will go from these nodes to the “sink”: \\[ \\frac{1}{1+e^{-(1.26253-18.95937x)}} \\times 13.032092\\\\ \\frac{1}{1+e^{-(6.599773-12.24665x)}}\\times 19.923742\\\\ \\frac{1}{1+e^{-(2.504890-5.700564x)}}\\times -32.173264 \\] Finally, we will add these with a “bias”, the intercept: \\[ 2.407654 + \\\\ \\frac{1}{1+e^{-(1.26253-18.95937x)}} \\times 13.032092+\\\\ \\frac{1}{1+e^{-(6.599773-12.24665x)}}\\times 19.923742+\\\\ \\frac{1}{1+e^{-(2.504890-5.700564x)}}\\times -32.173264 \\] Here are the results: # From Nodes to sink (Y) f12 &lt;- f1*nn$weights[[1]][[2]][2] f22 &lt;- f2*nn$weights[[1]][[2]][3] f23 &lt;- f3*nn$weights[[1]][[2]][4] ## Results yhat &lt;- nn$weights[[1]][[2]][1] + f12 + f22 + f23 plot(x, y, main=&quot;ANN: M = 3&quot;) lines(x, yhat, col=&quot;red&quot;, lwd = 3) 25.2 Backpropagation In 1986, Rumelhart et al. found a way to train neural networks with the backpropagation algorithm. Today, we would call it a Gradient Descent using reverse-mode autodiff. Backpropagation is an algorithm used to train neural networks by adjusting the weights and biases of the network to minimize the cost function. Suppose we have a simple neural network as follows: The first layer is the source layer (with \\(X\\)). The second layer is called as hidden layer with three “neurons” each of which has an activation function (\\(A\\)). The last layer is the “sink” or output layer. First, let’s define a loss function, MSPE: \\[ \\text{MSPE}=\\frac{1}{n} \\sum_{i=1}^n\\left(y_i-\\hat{y}\\right)^2 \\] And we want to solve: \\[ \\omega^{\\star}=\\operatorname{argmin}\\left\\{\\frac{1}{n} \\sum_{i=1}^n\\left(y_i-\\hat{y}\\right)^2\\right\\} \\] To compute the gradient of the error with respect to a weight \\(w\\) or a bias \\(b\\), we use the chain rule: \\[ \\frac{\\partial \\text{MSPE}}{\\partial w} =\\frac{\\partial \\text{MSPE}}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w} \\] Remember, \\[ \\hat{y} = f(x)=\\frac{1}{1+e^{-z}}=\\frac{1}{1+e^{-(\\alpha+wx)}}, \\] where \\(w\\) is the weight or bias in the network. By repeating this process for each weight and bias in the network, we can calculate the error gradient and use it to adjust the weights and biases in order to minimize the error of the neural network. This can be done by using gradient descent (See Appendix 1), which is an iterative method for optimizing a differentiable objective function, typically by minimizing it. However, with a multilayer ANN, we use stochastic gradient descent (SGD), which is a faster method iteratively minimizing the loss function by taking small steps in the opposite direction of the gradient of the function at the current position. The gradient is calculated using a randomly selected subset of the data, rather than the entire data set, which is why it is called “stochastic.” One of the main advantages of SGD is that it can be implemented very efficiently and can handle large data sets very well. The gradient descent is explained in Chapter 38 in detail. We will use a plain gradient descent here to solve our ANN problem below as an example REMOVED NEED TO COPY paste and work on that 25.3 Neural Network - More inputs With a set of covariates \\(X=\\left(1, x_1, x_2, \\ldots, x_k\\right)\\), we have \\[ y \\approx \\alpha+\\sum_{m=1}^M \\beta_m f\\left(\\alpha_m+\\textbf{X} \\delta_m\\right)=\\\\ = \\alpha+\\beta_1 f\\left(\\alpha_1+\\delta_{11} x_{1i}+\\delta_{12} x_{2i} \\dots +\\delta_{1k} x_{ki}\\right)+\\dots\\\\ +\\beta_M f\\left(\\alpha_{M1}+\\delta_{M1} x_{1i}+\\delta_{M2} x_{2i} \\dots +\\delta_{Mk} x_{ki}\\right) \\] By adding nonlinear functions of linear combinations with \\(M&gt;1\\), we have seen that we can capture nonlinearity. With multiple features, we can now capture interaction effects and, hence, obtain a more flexible model. This can be seen in blue and orange arrows in the following picture: Let’s have an application using a Mincer equation and the data (SPS 1985 - cross-section data originating from the May 1985 Current Population Survey by the US Census Bureau) from the AER package. Before we start, there are few important pre-processing steps to complete. First, ANN are inefficient when the data are not scaled. The reason is backpropagation. Since ANN use gradient descent, the different scales in features will cause different step sizes. Scaling the data before feeding it to the model enables the steps in gradient descent updated at the same rate for all the features. Second, indicator predictors should be included in the input matrix by dummy-coding. Finally, the formula for the model needs to be constructed to initialize the algorithm. Let’s see all of these pre-processing steps below: library(AER) data(&quot;CPS1985&quot;) df &lt;- CPS1985 # Scaling and Dummy coding df[,sapply(df, is.numeric)] &lt;- scale((df[, sapply(df, is.numeric)])) ddf &lt;- model.matrix(~.-1, data= df, contrasts.arg = lapply(df[,sapply(df, is.factor)], contrasts, contrasts = FALSE)) ddf &lt;- as.data.frame(ddf) # formula to pass on ANN and lm() w.ind &lt;- which(colnames(ddf)==&quot;wage&quot;) frmnn &lt;- as.formula(paste(&quot;wage~&quot;, paste(colnames(ddf[-w.ind]), collapse=&#39;+&#39;))) frmln &lt;- as.formula(paste(&quot;wage~&quot;, &quot;I(experience^2)&quot;, &quot;+&quot;, paste(colnames(ddf[-w.ind]), collapse = &quot;+&quot;))) # Bootstrapping loops instead of CV mse.test &lt;- matrix(0, 10, 5) for(i in 1:10){ set.seed(i+1) trainid &lt;- unique(sample(nrow(ddf), nrow(ddf), replace = TRUE)) train &lt;- ddf[trainid,] test &lt;- ddf[-trainid,] # Models fit.lm &lt;- lm(frmln, data = train) fit.nn &lt;- neuralnet(frmnn, data = train, hidden = 1, threshold = 0.05, linear.output = FALSE) fit.nn2 &lt;- neuralnet(frmnn, data = train, hidden = 2, threshold = 0.05) fit.nn3 &lt;- neuralnet(frmnn, data = train, hidden = 3, threshold = 0.05) fit.nn4 &lt;- neuralnet(frmnn, data = train, hidden = 3, threshold = 0.05, act.fct = &quot;tanh&quot;, linear.output = FALSE) # Prediction errors mse.test[i,1] &lt;- mean((test$wage - predict(fit.lm, test))^2) mse.test[i,2] &lt;- mean((test$wage - predict(fit.nn, test))^2) mse.test[i,3] &lt;- mean((test$wage - predict(fit.nn2, test))^2) mse.test[i,4] &lt;- mean((test$wage - predict(fit.nn3, test))^2) mse.test[i,5] &lt;- mean((test$wage - predict(fit.nn4, test))^2) } colMeans(mse.test) ## [1] 0.7296417 0.8919442 0.9038211 1.0403616 0.8926576 This experiment alone shows that a linear Mincer equation (with I(expreince^2)) is a much better predictor than ANN. As the complexity of ANN rises with more neurons, the likelihood that ANN overfits goes up, which is the case in our experiment. In general, linear regression may be a good choice for simple, low-dimensional datasets with a strong linear relationship between the variables, while ANNs may be better suited for more complex, high-dimensional datasets with nonlinear relationships between variables. Overfitting can be a concern when using ANNs for prediction tasks. Overfitting occurs when a model is overly complex and has too many parameters relative to the size of the training data, which results in fitting the noise in the training data rather than the underlying pattern. As a result, the model may perform well on the training data but poorly on new, unseen data. One way to mitigate overfitting in ANNs is to use techniques such as regularization, which imposes constraints on the model to prevent it from becoming too complex. Another approach is to use techniques such as early stopping, which involves interrupting the training process when the model starts to overfit the training data. "],["deep-learning.html", "Chapter 26 Deep Learning", " Chapter 26 Deep Learning Simply, a Deep Neural Network (DNN), or Deep Learning, is an artificial neural network that has two or more hidden layers. Even greater flexibility is achieved via composition of activation functions: \\[ y \\approx \\alpha+\\sum_{m=1}^M \\beta_m f\\left(\\alpha_m^{(1)}+\\underbrace{\\sum_{p=1}^P f\\left(\\alpha_p^{(2)}+\\textbf{X} \\delta_p^{(2)}\\right)}_{\\text {it replaces } \\textbf{X}} \\delta_m^{(1)}\\right) \\] Before having an application, we should note the number of available packages that offer ANN implementations in R and with Python. For example, CRAN hosts more than 80 packages related to neural network modeling. Above, we just saw one example with neuralnet. The work by Mahdi et al, 2021 surveys and ranks these packages for their accuracy, reliability, and ease-of-use. mse.test &lt;- c() for(i in 1:10){ set.seed(i+1) trainid &lt;- unique(sample(nrow(ddf), nrow(ddf), replace = TRUE)) train &lt;- ddf[trainid,] test &lt;- ddf[-trainid,] # Models fit.nn22 &lt;- neuralnet(frmnn, data = train, hidden = c(3,3), threshold = 0.05) mse.test[i] &lt;- mean((test$wage - predict(fit.nn22, test))^2) } mean(mse.test) ## [1] 1.211114 The overfitting gets worse with an increased complexity! Here is the plot for our DNN: plot(fit.nn22, rep = &quot;best&quot;) A better plot could be obtained by using the NeuralNetTools package: library(NeuralNetTools) plotnet(fit.nn22) Training DNN is an important concept and we leave it to Chapter 25. As we see, deep neural networks can model complex non-linear relationships. With very complex problems, such as detecting hundreds of types of objects in high-resolution images, we need to train deeper NNs, perhaps with 10 layers or more each with hundreds of neurons. Therefore, training a fully-connected DNN is a very slow process facing a severe risk of overfitting with millions of parameters. Moreover, gradients problems make lower layers very hard to train. A solution to these problems came with a different NN architect such as convolutional Neural Networks (CNN or ConvNets) and Recurrent Neural Networks (RNN), which we will see in Chapter 25. Moreover, the interpretability of an artificial neural network (ANN), which is known to be a “blackbox” method, can be an issue regardless of the complexity of the network. However, it is generally easier to understand the decisions made by a simple ANN than by a more complex one. A simple ANN might have only a few layers and a relatively small number of neurons, making it easier to understand how the input data is processed and how the final output is produced. However, even a simple ANN can still be a black box in the sense that the specific calculations and decisions made by the individual neurons within the network are not fully visible or understood. On the other hand, a more complex ANN with many layers and a large number of neurons can be more difficult to interpret, as the internal workings of the network are more complex and harder to understand. In these cases, it can be more challenging to understand how the ANN is making its decisions or to identify any biases or errors in its output. Overall, the interpretability of an ANN depends on the complexity of the network and the specific task it is being used for. Simple ANNs may be more interpretable, but even they can be considered black boxes to some extent. Here are a few resources that provide information about the interpretability of artificial neural networks (ANNs): Interpretable Machine Learning by Christoph Molnar is a online book that provides an overview of interpretability in machine learning, including techniques for interpreting ANNs. Interpretability of Deep Neural Networks by Chakraborty is a survey paper that discusses the interpretability of deep neural networks and presents an overview of the various techniques and approaches that have been developed to improve their interpretability. Before concluding this section we apply DNN to a classification problem using the same data that we have in Chapter 14.4.4. library(ISLR) df &lt;- Carseats str(df) ## &#39;data.frame&#39;: 400 obs. of 11 variables: ## $ Sales : num 9.5 11.22 10.06 7.4 4.15 ... ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Medium&quot;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 1 2 1 2 1 2 ... #Change SALES to a factor variable df$Sales &lt;- as.factor(ifelse(Carseats$Sales &lt;= 8, 0, 1)) dff &lt;- df[, -1] # Scaling and Dummy coding dff[,sapply(dff, is.numeric)] &lt;- scale((dff[, sapply(dff, is.numeric)])) ddf &lt;- model.matrix(~.-1, data= dff, contrasts.arg = lapply(dff[, sapply(dff, is.factor)], contrasts, contrasts = FALSE)) ddf &lt;- data.frame(Sales = df$Sales, ddf) # Formula w.ind &lt;- which(colnames(ddf) == &quot;Sales&quot;) frm &lt;- as.formula(paste(&quot;Sales~&quot;, paste(colnames(ddf[-w.ind]), collapse = &#39;+&#39;))) library(ROCR) n &lt;- 10 AUC1 &lt;- c() AUC2 &lt;- c() for (i in 1:n) { set.seed(i) ind &lt;- unique(sample(nrow(ddf), nrow(ddf), replace = TRUE)) train &lt;- ddf[ind, ] test &lt;- ddf[-ind, ] # Models fit.ln &lt;- glm(frm, data = train, family = binomial(link = &quot;logit&quot;)) fit.dnn &lt;- neuralnet(frm, data = train, hidden = 2, threshold = 0.05, linear.output = FALSE, err.fct = &quot;ce&quot;) #Predictions phat.ln &lt;- predict(fit.ln, test, type = &quot;response&quot;) phat.dnn &lt;- predict(fit.dnn, test, type = &quot;repsonse&quot;)[,2] #AUC for predicting Y = 1 pred_rocr1 &lt;- ROCR::prediction(phat.ln, test$Sales) auc_ROCR1 &lt;- ROCR::performance(pred_rocr1, measure = &quot;auc&quot;) AUC1[i] &lt;- auc_ROCR1@y.values[[1]] pred_rocr2 &lt;- ROCR::prediction(phat.dnn, test$Sales) auc_ROCR2 &lt;- ROCR::performance(pred_rocr2, measure = &quot;auc&quot;) AUC2[i] &lt;- auc_ROCR2@y.values[[1]] } (c(mean(AUC1), mean(AUC2))) ## [1] 0.9471081 0.9186785 Again the results are not very convincing to use DNN in this example. Let’s have a more complex task with the Red Wine dataset from Kaggle (Cortez et.al, 2009). Our job us to use 11 attributes to classify each wine. dfr &lt;- read.csv(&quot;wineQualityReds.csv&quot;, header = TRUE) dfr &lt;- dfr[, -1] # removing the index table(dfr$quality) ## ## 3 4 5 6 7 8 ## 10 53 681 638 199 18 # Let&#39;s remove the outlier qualities: indo &lt;- which(dfr$quality == &quot;3&quot; | dfr$quality == &quot;8&quot;) dfr &lt;- dfr[-indo, ] dfr$quality &lt;- as.factor(dfr$quality) table(dfr$quality) ## ## 4 5 6 7 ## 53 681 638 199 Then scale the data and get the formula, # Scaling and Dummy coding dfr[, sapply(dfr, is.numeric)] &lt;- scale((dfr[, sapply(dfr, is.numeric)])) ddf &lt;- model.matrix( ~ quality - 1, data = dfr) w.ind &lt;- which(colnames(dfr) == &quot;quality&quot;) dfr &lt;- dfr[, -w.ind] # removing &#39;quality` df &lt;- cbind(ddf, dfr) frm &lt;- as.formula(paste( paste(colnames(ddf), collapse = &#39;+&#39;), &quot;~&quot;, paste(colnames(dfr), collapse = &#39;+&#39;) )) frm ## quality4 + quality5 + quality6 + quality7 ~ fixed.acidity + volatile.acidity + ## citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + ## total.sulfur.dioxide + density + pH + sulphates + alcohol And, our simple DNN application ind &lt;- sample(nrow(df), nrow(df) * .7) train &lt;- df[ind, ] test &lt;- df[-ind, ] fit.nn &lt;- neuralnet( frm, data = train, hidden = c(3, 2), threshold = 0.05, linear.output = FALSE, err.fct = &quot;ce&quot; ) plot(fit.nn, rep = &quot;best&quot;) And our prediction: library(utiml) phat &lt;- predict(fit.nn, test) head(phat) ## [,1] [,2] [,3] [,4] ## 11 0.04931942 0.6859165 0.2825146 0.007453298 ## 14 0.02015208 0.2300976 0.5716617 0.050629158 ## 16 0.08538068 0.8882124 0.0550048 0.009824072 ## 17 0.03592572 0.5136539 0.5273030 0.006685037 ## 22 0.03616818 0.5173671 0.5263112 0.006541895 ## 27 0.03092853 0.4318134 0.5389403 0.011412135 # Assigning label by selecting the highest phat label.hat &lt;- t(apply(phat, 1, function(x) as.numeric(x == max(x)))) head(label.hat) ## [,1] [,2] [,3] [,4] ## 11 0 1 0 0 ## 14 0 0 1 0 ## 16 0 1 0 0 ## 17 0 0 1 0 ## 22 0 0 1 0 ## 27 0 0 1 0 # Confusion Table pred &lt;- apply(phat, 1, which.max) fck &lt;- colnames(test)[1:4] predicted &lt;- fck[pred] act&lt;- apply(test[,1:4], 1, which.max) actual &lt;- fck[act] table(predicted, actual) ## actual ## predicted quality4 quality5 quality6 quality7 ## quality5 4 107 32 3 ## quality6 7 78 160 51 ## quality7 1 0 11 18 This is just an example and the results are not reflecting a trained model. Advance DNN applications with a proper training requires a longer time and more capable machines. We can do a grid search on different number of hidden layers and neurons. However, a large datasets and more complex DNNs need better applications, like, Keras that uses GPU with capable operating systems allowing a much better efficiency in training. So far, we have used the neuralnet package. There are several packages in R that are also capable of implementing and training artificial neural networks. The most suitable one for our needs will depend on our specific requirements and preferences. For a powerful and flexible package for building and training ANNs, neuralnet or deepnet may be good options. When we just need a simple and easy-to-use package for training feedforward networks and making predictions, nnet may be another good choice. If we want a general-purpose package that can handle a wide range of machine learning tasks, including ANNs, caret would be a good option. Deep neural networks (DNNs) are neural networks with many layers, which can be difficult to train because of the large number of parameters that need to be optimized. This can make the training process computationally intensive and prone to overfitting. Convolutional neural networks (CNNs), on the other hand, are specifically designed to process data that has a grid-like structure, such as an image. One key aspect of CNNs is that they use convolutional layers, which apply a set of filters to the input data and produce a set of transformed feature maps. These filters are able to detect specific features in the input data, such as edges, corners, or textures, and are able to share these features across the input data. This means that the number of parameters in a CNN is typically much smaller than in a DNN, which makes the model easier to train and less prone to overfitting. Overall, CNNs are well-suited for tasks such as image classification, object detection and, speech recognition. We will not cover the details of CNN here. There are several packages available in R for working with CNNs. Finally, in a deep neural network, “dropout” and “regularization” are techniques used to prevent overfitting. Dropout is a regularization technique that randomly drops out, or removes, a certain percentage of neurons from the network during training. This has the effect of reducing the complexity of the model, as it can’t rely on any one neuron or group of neurons to make predictions. Regularization is a general term that refers to any method used to prevent overfitting in a machine learning model. There are many types of regularization techniques, which add a penalty term to the parameters of the the activation functions. We will be back to ANN later in Section VII - Time Series. "],["graphical-network-analysis.html", "Chapter 27 Graphical Network Analysis 27.1 Fundementals 27.2 Covariance 27.3 Correlation 27.4 Precision Matrix 27.5 Semi-partial Correlation 27.6 Regularized Covariance Matrix 27.7 Multivariate Gaussian Distribution 27.8 High-dimensional data 27.9 Ridge (\\(\\ell_{2}\\)) and glasso (\\(\\ell_{1}\\))", " Chapter 27 Graphical Network Analysis A network represents a structure of relationships between objects. Graphical modeling presents a network structure in a graph, which consists of nodes and edges, by expressing conditional (in)dependence between the nodes. If we think of these nodes (objects) as variables and their relationship with each other as edges, a graphical model represents the probabilistic relationships among a set of variables. For example, the absence of edges (partial correlations) corresponds to conditional independence. Graphical models are becoming more popular in statistics because it helps us understand a very complex structure of relationships in networks, such as the dynamic structure of biological systems or social events. The central idea is that, since any pair of nodes may be joined by an edge, a missing edge represents some form of independency between the pair of variables. The complexity in network analysis comes from the fact that the independency may be either marginal or conditional on some or all of the other variables. Therefore, defining a graphical model requires identification of a type of graph needed for each particular case. In general, a graphical model could be designed with directed and undirected edges. In a directed graph, an arrow indicates the direction of dependency between nodes. In undirected graphs, however, the edges do not have directions. The field of graphical modeling is vast, hence it is beyond the scope of this book. Yet, we will look at the precision matrix, which has been shown that its regularization captures the network connections. Hence, the central theme of this section is the estimation of sparse standardized precision matrices, whose results can be illustrated by undirected graphs. 27.1 Fundementals In this chapter, we will cover several concepts related to statistical (in)dependence measured by correlations. 27.2 Covariance We start with a data matrix, which refers to the array of numbers: \\[ \\mathbf{X}=\\left(\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1 p} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2 p} \\\\ x_{31} &amp; x_{32} &amp; \\cdots &amp; x_{3 p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n 1} &amp; x_{n 2} &amp; \\cdots &amp; x_{n p} \\end{array}\\right) \\] An example would be set.seed(5) x &lt;- rnorm(30, sd=runif(30, 2, 50)) X &lt;- matrix(x, 10) X ## [,1] [,2] [,3] ## [1,] -1.613670 -4.436764 42.563842 ## [2,] -20.840548 36.237338 -36.942481 ## [3,] -100.484392 25.903897 -24.294407 ## [4,] 3.769073 -18.950442 -22.616651 ## [5,] -1.821506 -12.454626 -1.243431 ## [6,] 32.103933 3.693050 38.807102 ## [7,] 25.752668 22.861071 -18.452338 ## [8,] 59.864792 98.848864 -3.607105 ## [9,] 33.862342 34.853324 16.704375 ## [10,] 5.980194 62.755408 -21.841795 We start with defining the covariance matrix \\[ \\mathbf{S}=\\left(\\begin{array}{ccccc} s_{1}^{2} &amp; s_{12} &amp; s_{13} &amp; \\cdots &amp; s_{1 p} \\\\ s_{21} &amp; s_{2}^{2} &amp; s_{23} &amp; \\cdots &amp; s_{2 p} \\\\ s_{31} &amp; s_{32} &amp; s_{3}^{2} &amp; \\cdots &amp; s_{3 p} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ s_{p 1} &amp; s_{p 2} &amp; s_{p 3} &amp; \\cdots &amp; s_{p}^{2} \\end{array}\\right) \\] \\[ s_{j}^{2}=(1 / n) \\sum_{i=1}^{n}\\left(x_{i j}-\\bar{x}_{j}\\right)^{2} \\] is the variance of the \\(j\\)-th variable, \\[ \\begin{aligned} &amp;s_{j k}=(1 / n) \\sum_{i=1}^{n}\\left(x_{i j}-\\bar{x}_{j}\\right)\\left(x_{i k}-\\bar{x}_{k}\\right) \\end{aligned} \\] is the covariance between the \\(j\\)-th and \\(k\\)-th variables; and, \\[ \\bar{x}_{j}=(1 / n) \\sum_{i=1}^{n} x_{j i} \\] is the mean of the \\(j\\)-th variable. We can calculate the covariance matrix such as \\[ \\mathbf{S}=\\frac{1}{n} \\mathbf{X}_{c}^{\\prime} \\mathbf{X}_{c}, \\] where \\(\\mathbf{X}_{c}\\) is the centered matrix: \\[ \\mathbf{X}_{c}=\\left(\\begin{array}{cccc} x_{11}-\\bar{x}_{1} &amp; x_{12}-\\bar{x}_{2} &amp; \\cdots &amp; x_{1 p}-\\bar{x}_{p} \\\\ x_{21}-\\bar{x}_{1} &amp; x_{22}-\\bar{x}_{2} &amp; \\cdots &amp; x_{2 p}-\\bar{x}_{p} \\\\ x_{31}-\\bar{x}_{1} &amp; x_{32}-\\bar{x}_{2} &amp; \\cdots &amp; x_{3 p}-\\bar{x}_{p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n 1}-\\bar{x}_{1} &amp; x_{n 2}-\\bar{x}_{2} &amp; \\cdots &amp; x_{n p}-\\bar{x}_{p} \\end{array}\\right) \\] How? # More direct n &lt;- nrow(X) m &lt;- matrix(1, n, 1)%*%colMeans(X) Xc &lt;- X-m Xc ## [,1] [,2] [,3] ## [1,] -5.2709585 -29.3678760 45.6561309 ## [2,] -24.4978367 11.3062262 -33.8501919 ## [3,] -104.1416804 0.9727849 -21.2021184 ## [4,] 0.1117842 -43.8815539 -19.5243622 ## [5,] -5.4787951 -37.3857380 1.8488577 ## [6,] 28.4466449 -21.2380620 41.8993911 ## [7,] 22.0953790 -2.0700407 -15.3600493 ## [8,] 56.2075038 73.9177518 -0.5148158 ## [9,] 30.2050530 9.9222117 19.7966643 ## [10,] 2.3229057 37.8242961 -18.7495065 # Or C &lt;- diag(n) - matrix(1/n, n, n) XC &lt;- C %*% X Xc ## [,1] [,2] [,3] ## [1,] -5.2709585 -29.3678760 45.6561309 ## [2,] -24.4978367 11.3062262 -33.8501919 ## [3,] -104.1416804 0.9727849 -21.2021184 ## [4,] 0.1117842 -43.8815539 -19.5243622 ## [5,] -5.4787951 -37.3857380 1.8488577 ## [6,] 28.4466449 -21.2380620 41.8993911 ## [7,] 22.0953790 -2.0700407 -15.3600493 ## [8,] 56.2075038 73.9177518 -0.5148158 ## [9,] 30.2050530 9.9222117 19.7966643 ## [10,] 2.3229057 37.8242961 -18.7495065 # We can also use `scale` Xc &lt;- scale(X, center=TRUE, scale=FALSE) And, the covariance matrix # Covariance Matrix S &lt;- t(Xc) %*% Xc / (n-1) S ## [,1] [,2] [,3] ## [1,] 1875.3209 429.8712 462.4775 ## [2,] 429.8712 1306.9817 -262.8231 ## [3,] 462.4775 -262.8231 755.5193 # Check it cov(X) ## [,1] [,2] [,3] ## [1,] 1875.3209 429.8712 462.4775 ## [2,] 429.8712 1306.9817 -262.8231 ## [3,] 462.4775 -262.8231 755.5193 27.3 Correlation While covariance is a necessary step, we can capture the size and the direction of relationships between the variables: \\[ \\mathbf{R}=\\left(\\begin{array}{ccccc} 1 &amp; r_{12} &amp; r_{13} &amp; \\cdots &amp; r_{1 p} \\\\ r_{21} &amp; 1 &amp; r_{23} &amp; \\cdots &amp; r_{2 p} \\\\ r_{31} &amp; r_{32} &amp; 1 &amp; \\cdots &amp; r_{3 p} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ r_{p 1} &amp; r_{p 2} &amp; r_{p 3} &amp; \\cdots &amp; 1 \\end{array}\\right) \\] where \\[ r_{j k}=\\frac{s_{j k}}{s_{j} s_{k}}=\\frac{\\sum_{i=1}^{n}\\left(x_{i j}-\\bar{x}_{j}\\right)\\left(x_{i k}-\\bar{x}_{k}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i j}-\\bar{x}_{j}\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(x_{i k}-\\bar{x}_{k}\\right)^{2}}} \\] is the Pearson correlation coefficient between variables \\(\\mathbf{X}_{j}\\) and \\(\\mathbf{X}_{k}\\) We can calculate the correlation matrix \\[ \\mathbf{R}=\\frac{1}{n} \\mathbf{X}_{s}^{\\prime} \\mathbf{X}_{s} \\] where \\(\\mathbf{X}_{s}=\\mathbf{C X D}^{-1}\\) with \\(\\mathbf{C}=\\mathbf{I}_{n}-n^{-1} \\mathbf{1}_{n} \\mathbf{1}_{n}^{\\prime}\\) denoting a centering matrix, \\(\\mathbf{D}=\\operatorname{diag}\\left(s_{1}, \\ldots, s_{p}\\right)\\) denoting a diagonal scaling matrix. Note that the standardized matrix \\(\\mathbf{X}_{s}\\) has the form \\[ \\mathbf{X}_{s}=\\left(\\begin{array}{cccc} \\left(x_{11}-\\bar{x}_{1}\\right) / s_{1} &amp; \\left(x_{12}-\\bar{x}_{2}\\right) / s_{2} &amp; \\cdots &amp; \\left(x_{1 p}-\\bar{x}_{p}\\right) / s_{p} \\\\ \\left(x_{21}-\\bar{x}_{1}\\right) / s_{1} &amp; \\left(x_{22}-\\bar{x}_{2}\\right) / s_{2} &amp; \\cdots &amp; \\left(x_{2 p}-\\bar{x}_{p}\\right) / s_{p} \\\\ \\left(x_{31}-\\bar{x}_{1}\\right) / s_{1} &amp; \\left(x_{32}-\\bar{x}_{2}\\right) / s_{2} &amp; \\cdots &amp; \\left(x_{3 p}-\\bar{x}_{p}\\right) / s_{p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\left(x_{n 1}-\\bar{x}_{1}\\right) / s_{1} &amp; \\left(x_{n 2}-\\bar{x}_{2}\\right) / s_{2} &amp; \\cdots &amp; \\left(x_{n p}-\\bar{x}_{p}\\right) / s_{p} \\end{array}\\right) \\] How? # More direct n &lt;- nrow(X) sdx &lt;- 1/matrix(1, n, 1)%*%apply(X, 2, sd) m &lt;- matrix(1, n, 1)%*%colMeans(X) Xs &lt;- (X-m)*sdx Xs ## [,1] [,2] [,3] ## [1,] -0.121717156 -0.81233989 1.66102560 ## [2,] -0.565704894 0.31273963 -1.23151117 ## [3,] -2.404843294 0.02690804 -0.77135887 ## [4,] 0.002581324 -1.21380031 -0.71032005 ## [5,] -0.126516525 -1.03412063 0.06726369 ## [6,] 0.656890910 -0.58746247 1.52435083 ## [7,] 0.510227259 -0.05725905 -0.55881729 ## [8,] 1.297945627 2.04462654 -0.01872963 ## [9,] 0.697496131 0.27445664 0.72022674 ## [10,] 0.053640619 1.04625151 -0.68212986 # Or C &lt;- diag(n) - matrix(1/n, n, n) D &lt;- diag(apply(X, 2, sd)) Xs &lt;- C %*% X %*% solve(D) Xs ## [,1] [,2] [,3] ## [1,] -0.121717156 -0.81233989 1.66102560 ## [2,] -0.565704894 0.31273963 -1.23151117 ## [3,] -2.404843294 0.02690804 -0.77135887 ## [4,] 0.002581324 -1.21380031 -0.71032005 ## [5,] -0.126516525 -1.03412063 0.06726369 ## [6,] 0.656890910 -0.58746247 1.52435083 ## [7,] 0.510227259 -0.05725905 -0.55881729 ## [8,] 1.297945627 2.04462654 -0.01872963 ## [9,] 0.697496131 0.27445664 0.72022674 ## [10,] 0.053640619 1.04625151 -0.68212986 # Or Xs &lt;- scale(X, center=TRUE, scale=TRUE) # Finally, the correlation Matrix R &lt;- t(Xs) %*% Xs / (n-1) R ## [,1] [,2] [,3] ## [1,] 1.0000000 0.2745780 0.3885349 ## [2,] 0.2745780 1.0000000 -0.2644881 ## [3,] 0.3885349 -0.2644881 1.0000000 # Check it cor(X) ## [,1] [,2] [,3] ## [1,] 1.0000000 0.2745780 0.3885349 ## [2,] 0.2745780 1.0000000 -0.2644881 ## [3,] 0.3885349 -0.2644881 1.0000000 The correlations above are called “zero-order” or Pearson correlations. They only reflect pairwise correlations without controlling other variables. 27.4 Precision Matrix The inverse of covariance matrix, if it exists, is called the concentration matrix also knows as the precision matrix. Let us consider a \\(2 \\times 2\\) covariance matrix: \\[ \\left[\\begin{array}{cc} \\sigma^{2}(x) &amp; \\rho \\sigma(x) \\sigma(y) \\\\ \\rho \\sigma(x) \\sigma(y) &amp; \\sigma^{2}(y) \\end{array}\\right] \\] And, its inverse: \\[ \\frac{1}{\\sigma^{2}(x) \\sigma^{2}(y)-\\rho^{2} \\sigma^{2}(x) \\sigma^{2}(y)}\\left[\\begin{array}{cc} \\sigma^{2}(y) &amp; -\\rho \\sigma(x) \\sigma(y) \\\\ -\\rho \\sigma(x) \\sigma(y) &amp; \\sigma^{2}(x) \\end{array}\\right] \\] If call the precision matrix \\(D\\), the correlation coefficient will be \\[ -\\frac{d_{i j}}{\\sqrt{d_{i i}} \\sqrt{d_{j j}}}, \\] Or, \\[ \\frac{-\\rho \\sigma_{x} \\sigma_{y}}{\\sigma_{x}^{2} \\sigma_{y}^{2}\\left(1-e^{2}\\right)} \\times \\sqrt{\\sigma_{x}^{2}\\left(1-\\rho^{2}\\right)} \\sqrt{\\sigma_{y}^{2}\\left(1-\\rho^{2}\\right)}=-\\rho \\] That was for a \\(2 \\times 2\\) variance-covariance matrix. When we have more columns, the correlation coefficient reflects partial correlations. Here is an example: pm &lt;- solve(S) # precision matrix pm ## [,1] [,2] [,3] ## [1,] 0.0007662131 -0.0003723763 -0.0005985624 ## [2,] -0.0003723763 0.0010036440 0.0005770819 ## [3,] -0.0005985624 0.0005770819 0.0018907421 # Partial correlation of 1,2 -pm[1,2]/(sqrt(pm[1,1])*sqrt(pm[2,2])) ## [1] 0.4246365 # Or -cov2cor(solve(S)) ## [,1] [,2] [,3] ## [1,] -1.0000000 0.4246365 0.4973000 ## [2,] 0.4246365 -1.0000000 -0.4189204 ## [3,] 0.4973000 -0.4189204 -1.0000000 # Or ppcor::pcor(X) ## $estimate ## [,1] [,2] [,3] ## [1,] 1.0000000 0.4246365 0.4973000 ## [2,] 0.4246365 1.0000000 -0.4189204 ## [3,] 0.4973000 -0.4189204 1.0000000 ## ## $p.value ## [,1] [,2] [,3] ## [1,] 0.0000000 0.2546080 0.1731621 ## [2,] 0.2546080 0.0000000 0.2617439 ## [3,] 0.1731621 0.2617439 0.0000000 ## ## $statistic ## [,1] [,2] [,3] ## [1,] 0.000000 1.240918 1.516557 ## [2,] 1.240918 0.000000 -1.220629 ## [3,] 1.516557 -1.220629 0.000000 ## ## $n ## [1] 10 ## ## $gp ## [1] 1 ## ## $method ## [1] &quot;pearson&quot; 27.5 Semi-partial Correlation With partial correlation, we find the correlation between \\(X\\) and \\(Y\\) after controlling for the effect of \\(Z\\) on both \\(X\\) and \\(Y\\). If we want to hold \\(Z\\) constant for just \\(X\\) or just \\(Y\\), we use a semipartial correlation. While a partial correlation is computed between two residuals, a semipartial is computed between one residual and another variable. One interpretation of the semipartial is that the influence of a third variable is removed from one of two variables (hence, semipartial). This can be shown with the \\(R^2\\) formulation. Partial: \\[ r_{12.3}^{2}=\\frac{R_{1.23}^{2}-R_{1.3}^{2}}{1-R_{1.3}^{2}} \\] Semi-Partial: \\[ r_{1(2.3)}^{2}=R_{1.23}^{2}-R_{1.3}^{2} \\] Let’s see the difference between a slope coefficient, a semi-partial correlation, and a partial correlation by looking their definitions: Partial: \\[ r_{12,3}=\\frac{r_{12}-r_{13} r_{23}}{\\sqrt{1-r_{12}^{2}} \\sqrt{1-r_{23}^{2}}} \\] Regression: \\[ X_{1}=b_{1}+b_{2} X_{2}+b_{2} X_{3} \\] and \\[ b_{2}=\\frac{\\sum X_{3}^{2} \\sum X_{1} X_{2}-\\sum X_{1} X_{3} \\sum X_{2} X_{3}}{\\sum X_{2}^{2} \\sum X_{3}^{2}-\\left(\\sum X_{2} X_{3}\\right)^{2}} \\] With standardized variables: \\[ b_{2}=\\frac{r_{12}-r_{13} r_{23}}{1-r_{23}^{2}} \\] Semi-partial (or “part”) correlation: \\[ r_{1(2.3)}=\\frac{r_{1 2}-r_{1_{3}} r_{23}}{\\sqrt{1-r_{23}^{2}}} \\] The difference between the regression coefficient and the semi-partial coefficient is the square root in the denominator. Thus, the regression coefficient can exceed \\(|1.0|\\); the correlation cannot. In other words, semi-partial normalizes the coefficient between -1 and +1. The function spcor can calculate the pairwise semi-partial correlations for each pair of variables given others. ppcor::spcor(X) ## $estimate ## [,1] [,2] [,3] ## [1,] 1.0000000 0.3912745 0.4781862 ## [2,] 0.4095148 1.0000000 -0.4028191 ## [3,] 0.4795907 -0.3860075 1.0000000 ## ## $p.value ## [,1] [,2] [,3] ## [1,] 0.0000000 0.2977193 0.1929052 ## [2,] 0.2737125 0.0000000 0.2824036 ## [3,] 0.1914134 0.3048448 0.0000000 ## ## $statistic ## [,1] [,2] [,3] ## [1,] 0.000000 1.124899 1.440535 ## [2,] 1.187625 0.000000 -1.164408 ## [3,] 1.446027 -1.107084 0.000000 ## ## $n ## [1] 10 ## ## $gp ## [1] 1 ## ## $method ## [1] &quot;pearson&quot; 27.6 Regularized Covariance Matrix Due an increasing availability of high-dimensional data sets, graphical models have become powerful tools to discover conditional dependencies over a graph structure. However, there are two main challenges in identifying the relations in a network: first, the edges (relationships) may not be identified by Pearson or Spearman correlations as they often lead to spurious associations due to missing confounding factors. Second, although, applications with partial correlations might address this issue, traditional precision estimators are not well-defined in case of high-dimensional data. Why is a covariance matrix \\(S\\) singular when \\(n&lt;p\\) in \\(\\mathbf{X}\\)? Consider the \\(n \\times p\\) matrix of sample data, \\(\\mathbf{X}\\). Since we know that the rank of \\(\\mathbf{X}\\) is at most \\(\\min (n, p)\\). Hence, in \\[ \\mathbf{S}=\\frac{1}{n} \\mathbf{X}_{c}^{\\prime} \\mathbf{X}_{c}, \\] \\(\\operatorname{rank}(\\mathbf{X}_c)\\) will be \\(n\\). It is clear that the rank of \\(\\mathbf{S}\\) won’t be larger than the rank of \\(\\mathbf{X}_c\\). Since \\(\\mathbf{S}\\) is \\(p \\times p\\) and its rank is \\(n\\), \\(\\mathbf{S}\\) will be singular. That’s, if \\(n&lt;p\\) then \\(\\operatorname{rank}(\\mathbf{X})&lt;p\\) in which case \\(\\operatorname{rank}(\\mathbf{S})&lt;p\\). This brought several novel precision estimators in applications. Generally, these novel estimators overcome the undersampling by maximization of the log-likelihood augmented with a so-called penalty. A penalty discourages large values among the elements of the precision matrix estimate. This reduces the risk of overfitting but also yields a well-defined penalized precision matrix estimator. To solve the problem, as we have seen before in Section 6, penalized estimators adds a penalty to the likelihood functions ( \\(\\ell_2\\) in Ridge and \\(\\ell_1\\) in lasso) that makes the eigenvalues of \\(\\mathbf{S}\\) shrink in a particular manner to combat \\(p \\geq n\\). The graphical lasso (gLasso) is the \\(\\ell_1\\)-equivalent to graphical ridge. A nice feature of the \\(\\ell_1\\) penalty automatically induces sparsity and thus also select the edges in the underlying graph. The \\(\\ell_2\\) penalty in Ridge relies on an extra step that selects the edges after the regularized precision matrix with shrunken correlations is estimated. In this chapter we will see graphical ridge and lasso applications based on Gaussian graphical models that will provide sparse precision matrices in case of \\(n&lt;p\\). 27.7 Multivariate Gaussian Distribution Before understanding \\(\\ell_1\\) or \\(\\ell_2\\) regularization, we need to see the multivariate Gaussian distribution, its parameterization and maximum likelihood estimation (MLE) solutions. The multivariate Gaussian distribution of a random vector \\(\\mathbf{X} \\in \\mathbf{R}^{p}\\) is commonly expressed in terms of the parameters \\(\\mu\\) and \\(\\Sigma\\), where \\(\\mu\\) is an \\(p \\times 1\\) vector and \\(\\Sigma\\) is an \\(p \\times p\\), a nonsingular symmetric covariance matrix. Hence, we have the following form for the density function: \\[ f(x \\mid \\mu, \\Sigma)=\\frac{1}{(2 \\pi)^{p / 2}|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\}, \\] where \\(|\\Sigma|\\) is the determinant of the covariance matrix. The likelihood function is: \\[ \\mathcal{L}(\\mu, \\Sigma)=(2 \\pi)^{-\\frac{n p}{2}} \\prod_{i=1}^{n} \\operatorname{det}(\\Sigma)^{-\\frac{1}{2}} \\exp \\left(-\\frac{1}{2}\\left(x_{i}-\\mu\\right)^{\\mathrm{T}} \\Sigma^{-1}\\left(x_{i}-\\mu\\right)\\right) \\] Since the estimate \\(\\bar{x}\\) does not depend on \\(\\Sigma\\), we can just substitute it for \\(\\mu\\) in the likelihood function, \\[ \\mathcal{L}(\\bar{x}, \\Sigma) \\propto \\operatorname{det}(\\Sigma)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}} \\Sigma^{-1}\\left(x_{i}-\\bar{x}\\right)\\right) \\] We seek the value of \\(\\Sigma\\) that maximizes the likelihood of the data (in practice it is easier to work with \\(\\log \\mathcal{L}\\) ). With the cyclical nature of trace, \\[ \\begin{aligned} \\mathcal{L}(\\bar{x}, \\Sigma) &amp; \\propto \\operatorname{det}(\\Sigma)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2} \\sum_{i=1}^{n}\\left(\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}} \\Sigma^{-1}\\left(x_{i}-\\bar{x}\\right)\\right)\\right) \\\\ &amp;=\\operatorname{det}(\\Sigma)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2} \\sum_{i=1}^{n} \\operatorname{tr}\\left(\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}} \\Sigma^{-1}\\right)\\right) \\\\ &amp;=\\operatorname{det}(\\Sigma)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2} \\operatorname{tr}\\left(\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}} \\Sigma^{-1}\\right)\\right) \\\\ &amp;=\\operatorname{det}(\\Sigma)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2} \\operatorname{tr}\\left(S \\Sigma^{-1}\\right)\\right) \\end{aligned} \\] where \\[ S=\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}} \\in \\mathbf{R}^{p \\times p} \\] And finally, we re-write the likelihood in the log form using the trace trick: \\[ \\ln \\mathcal{L}(\\mu, \\Sigma)=\\text { const }-\\frac{n}{2} \\ln \\operatorname{det}(\\Sigma)-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)\\left(x_{i}-\\mu\\right)^{\\mathrm{T}}\\right] \\] or, for a multivariate normal model with mean 0 and covariance \\(\\Sigma\\), the likelihood function in this case is given by \\[ \\ell(\\Omega ; S)=\\ln |\\Omega|-\\operatorname{tr}(S \\Omega) \\] where \\(\\Omega=\\Sigma^{-1}\\) is the so-called precision matrix (also sometimes called the concentration matrix), which we want to estimate, which we will denote \\(P\\). Indeed, one can naturally try to use the inverse of \\(S\\) for this. For an intuitive way to see the whole algebra, let’s start with the general normal density \\[ \\frac{1}{\\sqrt{2 \\pi}} \\frac{1}{\\sigma} \\exp \\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right) \\] The log-likelihood is \\[ \\mathcal{L}(\\mu, \\sigma)=\\text { A constant }-\\frac{n}{2} \\log \\left(\\sigma^{2}\\right)-\\frac{1}{2} \\sum_{i=1}^{n}\\left(\\frac{x_{i}-\\mu}{\\sigma}\\right)^{2}, \\] maximization of which is equivalent to minimizing \\[ \\mathcal{L}(\\mu, \\sigma)=n \\log \\left(\\sigma^{2}\\right)+\\sum_{i=1}^{n}\\left(\\frac{x_{i}-\\mu}{\\sigma}\\right)^{2} \\] We can look at the general multivariate normal (MVN) density \\[ (\\sqrt{2 \\pi})^{-d}|\\boldsymbol{\\Sigma}|^{-1 / 2} \\exp \\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{t} \\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right) \\] Note that \\(|\\boldsymbol{\\Sigma}|^{-1 / 2}\\), which is the reciprocal of the square root of the determinant of the covariance matrix \\(\\boldsymbol{\\Sigma}\\), does what \\(1 / \\sigma\\) does in the univariate case. Moreover, \\(\\boldsymbol{\\Sigma}^{-1}\\) does what \\(1 / \\sigma^{2}\\) does in the univariate case. The maximization of likelihood would lead to minimizing (analogous to the univariate case) \\[ n \\log |\\boldsymbol{\\Sigma}|+\\sum_{i=1}^{n}(\\mathbf{x}-\\boldsymbol{\\mu})^{t} \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}) \\] Again, \\(n \\log |\\mathbf{\\Sigma}|\\) takes the spot of \\(n \\log \\left(\\sigma^{2}\\right)\\) which was there in the univariate case. If the data is not high-dimensional, the estimations are simple. Let’s start with a data matrix of 10x6, where no need for regularization. n = 10 p = 6 X &lt;- matrix (rnorm(n*p), n, p) # Cov. &amp; Precision Matrices S &lt;- cov(X) pm &lt;- solve(S) # precision -pm[1,2]/(sqrt(pm[1,1])*sqrt(pm[2,2])) ## [1] -0.6059634 -cov2cor(pm) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -1.00000000 -0.6059634 0.1947369 -0.4292779 0.3289393 0.06004998 ## [2,] -0.60596342 -1.0000000 -0.3477352 -0.7813197 0.6814685 0.50178834 ## [3,] 0.19473691 -0.3477352 -1.0000000 -0.1892090 0.0775629 0.40192145 ## [4,] -0.42927791 -0.7813197 -0.1892090 -1.0000000 0.8678627 0.27524750 ## [5,] 0.32893932 0.6814685 0.0775629 0.8678627 -1.0000000 -0.39719667 ## [6,] 0.06004998 0.5017883 0.4019214 0.2752475 -0.3971967 -1.00000000 # ppcor pc &lt;- ppcor::pcor(X) pc$estimate ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.00000000 -0.6059634 0.1947369 -0.4292779 0.3289393 0.06004998 ## [2,] -0.60596342 1.0000000 -0.3477352 -0.7813197 0.6814685 0.50178834 ## [3,] 0.19473691 -0.3477352 1.0000000 -0.1892090 0.0775629 0.40192145 ## [4,] -0.42927791 -0.7813197 -0.1892090 1.0000000 0.8678627 0.27524750 ## [5,] 0.32893932 0.6814685 0.0775629 0.8678627 1.0000000 -0.39719667 ## [6,] 0.06004998 0.5017883 0.4019214 0.2752475 -0.3971967 1.00000000 # glasso glassoFast::glassoFast(S,rho=0) ## $w ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.60140347 -0.37332949 0.3171606 0.02177682 -0.13790645 -0.1327159 ## [2,] -0.37332949 0.50242861 -0.2405377 -0.25645323 0.01832675 0.2601432 ## [3,] 0.31716062 -0.24053766 0.5735265 -0.09239610 -0.28453786 0.1190710 ## [4,] 0.02177682 -0.25645323 -0.0923961 0.66021163 0.65364259 -0.3052422 ## [5,] -0.13790645 0.01832675 -0.2845379 0.65364259 1.10471906 -0.3465787 ## [6,] -0.13271587 0.26014323 0.1190710 -0.30524217 -0.34657873 0.7010335 ## ## $wi ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 4.3912715 4.562946 -0.7541076 2.932966 -1.5229061 -0.2098044 ## [2,] 4.5629462 12.914469 2.3082064 9.154998 -5.4111089 -3.0096715 ## [3,] -0.7541076 2.308206 3.4125042 1.139186 -0.3163017 -1.2392019 ## [4,] 2.9329659 9.154998 1.1391865 10.631981 -6.2528743 -1.4976793 ## [5,] -1.5229061 -5.411109 -0.3163017 -6.252874 4.8825940 1.4647642 ## [6,] -0.2098044 -3.009672 -1.2392019 -1.497679 1.4647642 2.7860635 ## ## $errflag ## [1] 0 ## ## $niter ## [1] 1 Rl &lt;- glassoFast::glassoFast(S,rho=0)$wi # -Rl[1,2]/(sqrt(Rl[1,1])*sqrt(Rl[2,2])) ## [1] -0.6059153 -cov2cor(Rl) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -1.00000000 -0.6059153 0.19480566 -0.4292445 0.32889151 0.05998241 ## [2,] -0.60591532 -1.0000000 -0.34769605 -0.7812911 0.68143226 0.50174762 ## [3,] 0.19480566 -0.3476961 -1.00000000 -0.1891260 0.07748893 0.40189255 ## [4,] -0.42924454 -0.7812911 -0.18912595 -1.0000000 0.86785527 0.27517959 ## [5,] 0.32889151 0.6814323 0.07748893 0.8678553 -1.00000000 -0.39714298 ## [6,] 0.05998241 0.5017476 0.40189255 0.2751796 -0.39714298 -1.00000000 27.8 High-dimensional data Now with a data matrix of 6x10: n = 6 p = 10 set.seed(1) X &lt;- matrix (rnorm(n*p), n, p) # Cov. &amp; Precision Matrices S &lt;- cov(X) S ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.889211221 -0.17223814 -0.36660043 0.35320957 -0.629545741 -0.27978848 ## [2,] -0.172238139 0.34416306 -0.09280183 -0.04282613 0.139236591 -0.26060435 ## [3,] -0.366600426 -0.09280183 1.46701338 -0.50796342 -0.024550727 -0.11504405 ## [4,] 0.353209573 -0.04282613 -0.50796342 1.24117592 -0.292005017 0.42646139 ## [5,] -0.629545741 0.13923659 -0.02455073 -0.29200502 0.553562287 0.26275658 ## [6,] -0.279788479 -0.26060435 -0.11504405 0.42646139 0.262756584 0.81429052 ## [7,] 0.143364328 -0.14895377 0.29598156 0.30839120 -0.275296303 0.04418159 ## [8,] -0.273835576 0.17201439 -0.31052657 -0.39667581 0.376175973 -0.02536104 ## [9,] -0.008919669 0.24390178 -0.50198614 0.52741301 0.008044799 -0.01297542 ## [10,] -0.304722895 0.33936685 -1.08854590 0.20441696 0.499437080 0.20218868 ## [,7] [,8] [,9] [,10] ## [1,] 0.14336433 -0.27383558 -0.008919669 -0.3047229 ## [2,] -0.14895377 0.17201439 0.243901782 0.3393668 ## [3,] 0.29598156 -0.31052657 -0.501986137 -1.0885459 ## [4,] 0.30839120 -0.39667581 0.527413006 0.2044170 ## [5,] -0.27529630 0.37617597 0.008044799 0.4994371 ## [6,] 0.04418159 -0.02536104 -0.012975416 0.2021887 ## [7,] 0.37576405 -0.40476558 0.046294293 -0.4691147 ## [8,] -0.40476558 0.46612332 -0.026813818 0.5588965 ## [9,] 0.04629429 -0.02681382 0.540956259 0.5036908 ## [10,] -0.46911465 0.55889647 0.503690786 1.3107637 try(solve(S), silent = FALSE) ## Error in solve.default(S) : ## system is computationally singular: reciprocal condition number = 1.04542e-18 The standard definition for the inverse of a matrix fails if the matrix is not square or singular. However, one can generalize the inverse using singular value decomposition. Any rectangular real matrix \\(\\mathbf{M}\\) can be decomposed as \\(\\mathbf{M=U \\Sigma V^{&#39;}}\\), where \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthogonal and \\(\\mathbf{D}\\) is a diagonal matrix containing only the positive singular values. The pseudoinverse, also known as Moore-Penrose or generalized inverse is then obtained as \\[ \\mathbf{M^+} = \\mathbf{V \\Sigma^{-1} U&#39;} \\] Don’t be confused due to notation: \\(\\Sigma\\) is not the covariance matrix here With using the method of generalized inverse by ppcor and corpcor: Si &lt;- corpcor::pseudoinverse(S) -Si[1,2]/(sqrt(Si[1,1])*sqrt(Si[2,2])) ## [1] -0.4823509 # ppcor pc &lt;- ppcor::pcor(X) ## Warning in ppcor::pcor(X): The inverse of variance-covariance matrix is ## calculated using Moore-Penrose generalized matrix invers due to its determinant ## of zero. ## Warning in sqrt((n - 2 - gp)/(1 - pcor^2)): NaNs produced pc$estimate ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.00000000 -0.48235089 -0.43471080 -0.6132218 0.59239395 -0.1515785108 ## [2,] -0.48235089 1.00000000 -0.85835176 -0.7984656 0.08341783 0.1922476120 ## [3,] -0.43471080 -0.85835176 1.00000000 -0.8107355 -0.06073205 -0.1395456329 ## [4,] -0.61322177 -0.79846556 -0.81073546 1.0000000 0.11814582 -0.3271223659 ## [5,] 0.59239395 0.08341783 -0.06073205 0.1181458 1.00000000 -0.4056046405 ## [6,] -0.15157851 0.19224761 -0.13954563 -0.3271224 -0.40560464 1.0000000000 ## [7,] 0.81227748 0.76456650 0.76563183 0.7861380 -0.07927500 0.2753626258 ## [8,] -0.74807903 -0.67387820 -0.64812735 -0.6321303 -0.04063566 -0.2660628754 ## [9,] 0.79435763 0.32542381 0.52481792 0.5106454 -0.08284875 0.5458020595 ## [10,] 0.01484899 -0.34289348 0.01425498 -0.2181704 -0.41275254 0.0006582396 ## [,7] [,8] [,9] [,10] ## [1,] 0.8122775 -0.74807903 0.79435763 0.0148489929 ## [2,] 0.7645665 -0.67387820 0.32542381 -0.3428934821 ## [3,] 0.7656318 -0.64812735 0.52481792 0.0142549759 ## [4,] 0.7861380 -0.63213032 0.51064540 -0.2181703890 ## [5,] -0.0792750 -0.04063566 -0.08284875 -0.4127525424 ## [6,] 0.2753626 -0.26606288 0.54580206 0.0006582396 ## [7,] 1.0000000 0.96888026 -0.84167300 0.2703213517 ## [8,] 0.9688803 1.00000000 0.84455999 -0.3746342510 ## [9,] -0.8416730 0.84455999 1.00000000 -0.0701428715 ## [10,] 0.2703214 -0.37463425 -0.07014287 1.0000000000 # corpcor with pseudo inverse corpcor::cor2pcor(S) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.00000000 -0.48235089 -0.43471080 -0.6132218 0.59239395 -0.1515785108 ## [2,] -0.48235089 1.00000000 -0.85835176 -0.7984656 0.08341783 0.1922476120 ## [3,] -0.43471080 -0.85835176 1.00000000 -0.8107355 -0.06073205 -0.1395456329 ## [4,] -0.61322177 -0.79846556 -0.81073546 1.0000000 0.11814582 -0.3271223659 ## [5,] 0.59239395 0.08341783 -0.06073205 0.1181458 1.00000000 -0.4056046405 ## [6,] -0.15157851 0.19224761 -0.13954563 -0.3271224 -0.40560464 1.0000000000 ## [7,] 0.81227748 0.76456650 0.76563183 0.7861380 -0.07927500 0.2753626258 ## [8,] -0.74807903 -0.67387820 -0.64812735 -0.6321303 -0.04063566 -0.2660628754 ## [9,] 0.79435763 0.32542381 0.52481792 0.5106454 -0.08284875 0.5458020595 ## [10,] 0.01484899 -0.34289348 0.01425498 -0.2181704 -0.41275254 0.0006582396 ## [,7] [,8] [,9] [,10] ## [1,] 0.8122775 -0.74807903 0.79435763 0.0148489929 ## [2,] 0.7645665 -0.67387820 0.32542381 -0.3428934821 ## [3,] 0.7656318 -0.64812735 0.52481792 0.0142549759 ## [4,] 0.7861380 -0.63213032 0.51064540 -0.2181703890 ## [5,] -0.0792750 -0.04063566 -0.08284875 -0.4127525424 ## [6,] 0.2753626 -0.26606288 0.54580206 0.0006582396 ## [7,] 1.0000000 0.96888026 -0.84167300 0.2703213517 ## [8,] 0.9688803 1.00000000 0.84455999 -0.3746342510 ## [9,] -0.8416730 0.84455999 1.00000000 -0.0701428715 ## [10,] 0.2703214 -0.37463425 -0.07014287 1.0000000000 However, we know from Chapter 29 that these solutions are not stable. Further, we also want to identify the sparsity in the precision matrix that differentiates the significant edges from insignificant ones for a network analysis . 27.9 Ridge (\\(\\ell_{2}\\)) and glasso (\\(\\ell_{1}\\)) A contemporary use for precision matrices is found in network reconstruction through graphical modeling (Network Analysis). In a multivariate normal model, \\(p_{i j}=p_{j i}=0\\) (the entries in the precision matrix) if and only if \\(X_{i}\\) and \\(X_{j}\\) are independent when condition ong all other variables. In real world applications, \\(P\\) (the precision matrix) is often relatively sparse with lots of zeros. With the close relationship between \\(P\\) and the partial correlations, the non-zero entries of the precision matrix can be interpreted the edges of a graph where nodes correspond to the variables. Regularization helps us find the sparsified partial correlation matrix. We first start with Ridge and rags2ridges (see, Introduction to rags2ridges), which is an R-package for fast and proper \\(\\ell_{2}\\)-penalized estimation of precision (and covariance) matrices also called ridge estimation. Their algorithm solves the following: \\[ \\ell(\\Omega ; S)=\\ln |\\Omega|-\\operatorname{tr}(S \\Omega)-\\frac{\\lambda}{2}\\|\\Omega-T\\|_{2}^{2} \\] where \\(\\lambda&gt;0\\) is the ridge penalty parameter, \\(T\\) is a \\(p \\times p\\) known target matrix and \\(\\|\\cdot\\|_{2}\\) is the \\(\\ell_{2}\\)-norm. Assume for now the target matrix is an all zero matrix and thus out of the equation. The core function of rags2ridges is ridgeP which computes this estimate in a fast manner. Let’s try some simulations: library(rags2ridges) p &lt;- 6 n &lt;- 20 X &lt;- createS(n = n, p = p, dataset = TRUE) # Cov. &amp; Precision Matrices S &lt;- cov(X) S ## A B C D E F ## A 0.45682789 -0.11564467 0.13200583 -0.01595920 0.09809975 0.01702341 ## B -0.11564467 0.55871526 -0.06301115 0.12714447 0.16007573 0.01767518 ## C 0.13200583 -0.06301115 0.85789870 -0.03128875 -0.05379863 0.13134788 ## D -0.01595920 0.12714447 -0.03128875 0.99469250 0.03927349 0.10959642 ## E 0.09809975 0.16007573 -0.05379863 0.03927349 0.91136419 0.02529372 ## F 0.01702341 0.01767518 0.13134788 0.10959642 0.02529372 1.27483389 try(solve(S), silent = FALSE) ## A B C D E F ## A 2.534157787 0.60434887 -0.37289623 -0.03330977 -0.39969959 0.006995153 ## B 0.604348874 2.09324877 0.02734562 -0.23919383 -0.42049199 -0.011003651 ## C -0.372896230 0.02734562 1.25338509 0.03989939 0.11121753 -0.130174434 ## D -0.033309770 -0.23919383 0.03989939 1.04638061 0.00537128 -0.090412788 ## E -0.399699586 -0.42049199 0.11121753 0.00537128 1.22116416 -0.024982169 ## F 0.006995153 -0.01100365 -0.13017443 -0.09041279 -0.02498217 0.806155504 P &lt;- rags2ridges::ridgeP(S, lambda = 0.0001) P ## A 6 x 6 ridge precision matrix estimate with lambda = 0.000100 ## A B C D E F ## A 2.533115451 0.60366542 -0.37265274 -0.033220588 -0.399324682 0.006973044 ## B 0.603665423 2.09268463 0.02745898 -0.239097683 -0.420206304 -0.011013671 ## C -0.372652744 0.02745898 1.25336484 0.039885330 0.111143219 -0.130167968 ## D -0.033220588 -0.23909768 0.03988533 1.046411061 0.005328985 -0.090412858 ## E -0.399324682 -0.42020630 0.11114322 0.005328985 1.221068947 -0.024975879 ## F 0.006973044 -0.01101367 -0.13016797 -0.090412858 -0.024975879 0.806196516 library(rags2ridges) p &lt;- 25 n &lt;- 20 X &lt;- createS(n = n, p = p, dataset = TRUE) # Cov. &amp; Precision Matrices S &lt;- cov(X) try(solve(S), silent = FALSE) ## Error in solve.default(S) : ## system is computationally singular: reciprocal condition number = 1.2598e-18 P &lt;- rags2ridges::ridgeP(S, lambda = 1.17) P[1:7, 1:7] ## A B C D E F ## A 2.743879476 -0.03541676 -0.01830371 0.008774811 -0.1056438 0.01539484 ## B -0.035416755 2.63060175 0.23945569 0.088696164 -0.2786984 -0.29657059 ## C -0.018303709 0.23945569 2.55818158 -0.092298329 0.1512445 0.08314785 ## D 0.008774811 0.08869616 -0.09229833 2.373307290 0.3717918 0.01829917 ## E -0.105643841 -0.27869839 0.15124449 0.371791841 2.3048669 -0.32627382 ## F 0.015394836 -0.29657059 0.08314785 0.018299166 -0.3262738 2.79070578 ## G -0.059760460 -0.18022734 -0.08924614 -0.149071791 -0.1574611 0.06178467 ## G ## A -0.05976046 ## B -0.18022734 ## C -0.08924614 ## D -0.14907179 ## E -0.15746109 ## F 0.06178467 ## G 2.61837378 What Lambda should we choose? One strategy for choosing \\(\\lambda\\) is selecting it to be stable yet precise (a bias-variance trade-off). Automatic k-fold cross-validation can be done with optPenalty.kCVauto() is well suited for this. opt &lt;- optPenalty.kCVauto(X, lambdaMin = 0.001, lambdaMax = 100) str(opt) ## List of 2 ## $ optLambda: num 0.721 ## $ optPrec : &#39;ridgeP&#39; num [1:25, 1:25] 2.7894 -0.0521 -0.0324 0.0211 -0.1459 ... ## ..- attr(*, &quot;lambda&quot;)= num 0.721 ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:25] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ... ## .. ..$ : chr [1:25] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ... op &lt;- opt$optLambda We know that Ridge will not provide a sparse solution. Yet, we need a sparse precision matrix for network analysis. The \\(\\ell_{2}\\) penalty of rags2ridges relies on an extra step that selects the edges after the precision matrix is estimated. The extra step is explained in their paper (van Wieringen, W.N. and Peeters, C.F.W., 2016): While some may argue this as a drawback (typically due to a lack of perceived simplicity), it is often beneficial to separate the “variable selection” and estimation. First, a separate post-hoc selection step allows for greater flexibility. Secondly, when co-linearity is present the L1 penalty is “unstable” in the selection between the items, i.e, if 2 covariances are co-linear only one of them will typically be selected in a unpredictable way whereas the L2 will put equal weight on both and “average” their effect. Ultimately, this means that the L2 estimate is typically more stable than the L1. At last point to mention here is also that the true underlying graph might not always be very sparse (or sparse at all). The function spasify() handles the the spasification by applying the FDR (False Discovery Rate) method: P &lt;- ridgeP(S, lambda = op) spar &lt;- sparsify(P, threshold = &quot;localFDR&quot;) ## Step 1... determine cutoff point ## Step 2... estimate parameters of null distribution and eta0 ## Step 3... compute p-values and estimate empirical PDF/CDF ## Step 4... compute q-values and local fdr ## Step 5... prepare for plotting ## ## - Retained elements: 0 ## - Corresponding to 0 % of possible edges ## spar ## $sparseParCor ## A 25 x 25 ridge precision matrix estimate with lambda = 0.721308 ## A B C D E F … ## A 1 0 0 0 0 0 … ## B 0 1 0 0 0 0 … ## C 0 0 1 0 0 0 … ## D 0 0 0 1 0 0 … ## E 0 0 0 0 1 0 … ## F 0 0 0 0 0 1 … ## … 19 more rows and 19 more columns ## ## $sparsePrecision ## A 25 x 25 ridge precision matrix estimate with lambda = 0.721308 ## A B C D E F … ## A 2.626295 0.000000 0.000000 0.000000 0.000000 0.000000 … ## B 0.000000 2.528829 0.000000 0.000000 0.000000 0.000000 … ## C 0.000000 0.000000 2.409569 0.000000 0.000000 0.000000 … ## D 0.000000 0.000000 0.000000 2.179168 0.000000 0.000000 … ## E 0.000000 0.000000 0.000000 0.000000 2.145443 0.000000 … ## F 0.000000 0.000000 0.000000 0.000000 0.000000 2.724853 … ## … 19 more rows and 19 more columns The steps are explained in their paper. After edge selections, GGMnetworkStats() can be utilized to get summary statistics of the resulting graph topology: fc &lt;- GGMnetworkStats(P) fc ## $degree ## A B C D E F G H I J K L M N O P Q R S T U V W X Y ## 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 ## ## $betweenness ## A B C D E F G H I J K L M N O P Q R S T U V W X Y ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## ## $closeness ## A B C D E F G ## 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 ## H I J K L M N ## 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 ## O P Q R S T U ## 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 ## V W X Y ## 0.04166667 0.04166667 0.04166667 0.04166667 ## ## $eigenCentrality ## A B C D E F G H I J K L M N O P Q R S T ## 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ## U V W X Y ## 0.2 0.2 0.2 0.2 0.2 ## ## $nNeg ## A B C D E F G H I J K L M N O P Q R S T U V W X Y ## 13 14 10 13 13 12 17 13 14 15 9 9 16 9 13 11 10 16 6 12 9 9 13 14 16 ## ## $nPos ## A B C D E F G H I J K L M N O P Q R S T U V W X Y ## 11 10 14 11 11 12 7 11 10 9 15 15 8 15 11 13 14 8 18 12 15 15 11 10 8 ## ## $chordal ## [1] TRUE ## ## $mutualInfo ## A B C D E F G H ## 0.1807197 0.4154956 0.3812980 0.4185954 0.5532431 0.3059883 0.4113305 0.3120867 ## I J K L M N O P ## 0.2574861 0.5072752 0.7451307 0.3550588 0.3779470 0.4719414 0.3452659 0.3017873 ## Q R S T U V W X ## 0.2619416 0.4744925 0.1594554 0.1324863 0.2547769 0.2546357 0.2225756 0.2803463 ## Y ## 0.1775594 ## ## $variance ## A B C D E F G H ## 0.4561861 0.5991397 0.6076539 0.6974337 0.8105009 0.4983632 0.6001093 0.4855818 ## I J K L M N O P ## 0.4370395 0.6095917 0.9474860 0.6172182 0.7700104 0.7586031 0.6285586 0.4753553 ## Q R S T U V W X ## 0.5676784 0.7166316 0.4281188 0.4160550 0.5350846 0.5846058 0.5149757 0.4866827 ## Y ## 0.4732289 ## ## $partialVar ## A B C D E F G H ## 0.3807646 0.3954400 0.4150120 0.4588907 0.4661042 0.3669923 0.3977332 0.3554061 ## I J K L M N O P ## 0.3378282 0.3670559 0.4497453 0.4327515 0.5276626 0.4732091 0.4450397 0.3515231 ## Q R S T U V W X ## 0.4368603 0.4458887 0.3650175 0.3644288 0.4147384 0.4531858 0.4122146 0.3676995 ## Y ## 0.3962399 While the \\(\\ell_{2}\\) penalty of graphical ridge relies on an extra step to select the edges after \\(P\\) is estimated, the graphical lasso (gLasso) is the \\(\\ell_{1}\\)-equivalent to graphical ridge, where the \\(\\ell_{1}\\) penalty automatically induces sparsity and select the edges in the underlying graph. The graphical lasso aims to solve the following regularized maximum likelihood problem: \\[ \\mathcal{L}(\\Omega)=\\operatorname{tr}(\\Omega S)-\\log |\\Omega|+\\lambda\\|\\Omega\\|_1 \\] gl &lt;- glasso::glasso(S, rho = 0.2641, approx = FALSE)[c(&#39;w&#39;, &#39;wi&#39;)] - cov2cor(gl$wi)[1:10, 1:10] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -1 0.00000000 0.00000000 0.0000000 0.00000000 0.0000000 ## [2,] 0 -1.00000000 -0.07439378 0.0000000 0.09011239 0.1280014 ## [3,] 0 -0.07439164 -1.00000000 0.0000000 0.00000000 0.0000000 ## [4,] 0 0.00000000 0.00000000 -1.0000000 -0.15354175 0.0000000 ## [5,] 0 0.09011181 0.00000000 -0.1535415 -1.00000000 0.1502234 ## [6,] 0 0.12800152 0.00000000 0.0000000 0.15022332 -1.0000000 ## [7,] 0 0.00000000 0.00000000 0.0000000 0.00000000 0.0000000 ## [8,] 0 0.00000000 0.00000000 0.0000000 0.10986140 0.0000000 ## [9,] 0 0.00000000 -0.09170730 0.0000000 0.26299733 0.0000000 ## [10,] 0 0.00000000 0.20156741 0.0000000 0.00000000 0.0000000 ## [,7] [,8] [,9] [,10] ## [1,] 0.000000000 0.000000000 0.00000000 0.00000000 ## [2,] 0.000000000 0.000000000 0.00000000 0.00000000 ## [3,] 0.000000000 0.000000000 -0.09170577 0.20156678 ## [4,] 0.000000000 0.000000000 0.00000000 0.00000000 ## [5,] 0.000000000 0.109861399 0.26299730 0.00000000 ## [6,] 0.000000000 0.000000000 0.00000000 0.00000000 ## [7,] -1.000000000 -0.007621682 0.00000000 0.05018338 ## [8,] -0.007621648 -1.000000000 0.00000000 0.00000000 ## [9,] 0.000000000 0.000000000 -1.00000000 0.00000000 ## [10,] 0.050183134 0.000000000 0.00000000 -1.00000000 The glasso package does not provide an option for tuning parameter selection. In practice, users apply can be done by cross-validation and eBIC. There are also multiple packages and function to plot the networks for a visual inspection. "],["decompositions.html", "Chapter 28 Decompositions 28.1 Matrix Decomposition 28.2 Eigenvectors and eigenvalues 28.3 Singular Value Decomposition 28.4 Rank(r) Approximations 28.5 Moore-Penrose inverse", " Chapter 28 Decompositions 28.1 Matrix Decomposition Matrix decomposition, also known as matrix factorization, is a process of breaking down a matrix into simpler components that can be used to simplify calculations, solve systems of equations, and gain insight into the underlying structure of the matrix. Matrix decomposition plays an important role in machine learning, particularly in the areas of dimensionality reduction, data compression, and feature extraction. For example, Principal Component Analysis (PCA) is a popular method for dimensionality reduction, which involves decomposing a high-dimensional data matrix into a lower-dimensional representation while preserving the most important information. PCA achieves this by finding the eigenvectors and eigenvalues of the covariance matrix of the data and then selecting the top eigenvectors as the new basis for the data. Singular Value Decomposition (SVD) is also commonly used in recommender systems to find latent features in user-item interaction data. SVD decomposes the user-item interaction matrix into three matrices: a left singular matrix, a diagonal matrix of singular values, and a right singular matrix. The left and right singular matrices represent user and item features, respectively, while the singular values represent the importance of those features. Rank optimization is another method that finds a low-rank approximation of a matrix that best fits a set of observed data. In other words, it involves finding a lower-rank approximation of a given matrix that captures the most important features of the original matrix. For example, SVD decomposes a matrix into a product of low-rank matrices, while PCA finds the principal components of a data matrix, which can be used to create a lower-dimensional representation of the data. In machine learning, rank optimization is often used in applications such as collaborative filtering, image processing, and data compression. By finding a low-rank approximation of a matrix, it is possible to reduce the amount of memory needed to store the matrix and improve the efficiency of algorithms that work with the matrix. We start with the eigenvalue decomposition (EVD), which is the foundation to many matrix decomposition methods 28.2 Eigenvectors and eigenvalues Eigenvalues and eigenvectors have many important applications in linear algebra and beyond. For example, in machine learning, principal component analysis (PCA) involves computing the eigenvectors and eigenvalues of the covariance matrix of a data set, which can be used to reduce the dimensionality of the data while preserving its important features. Almost all vectors change direction, when they are multiplied by a matrix, \\(\\mathbf{A}\\), except for certain vectors (\\(\\mathbf{v}\\)) that are in the same direction as \\(\\mathbf{A} \\mathbf{v}.\\) Those vectors are called “eigenvectors”. We can see how we obtain the eigenvalues and eigenvectors of a matrix \\(\\mathbf{A}\\). If \\[ \\mathbf{A} \\mathbf{v}=\\lambda \\mathbf{v} \\] Then, \\[ \\begin{aligned} &amp;\\mathbf{A} \\mathbf{v}-\\lambda \\mathbf{I} \\mathbf{v}=0 \\\\ &amp;(\\mathbf{A}-\\lambda \\mathbf{I}) \\mathbf{v}=0, \\end{aligned} \\] where \\(\\mathbf{I}\\) is the identity matrix. It turns out that this equation is equivalent to: \\[ \\operatorname{det}(\\mathbf{A}-\\lambda \\mathbf{I})=0, \\] because \\(\\operatorname{det}(\\mathbf{A}-\\lambda \\mathbf{I}) \\equiv(\\mathbf{A}-\\lambda \\mathbf{I}) \\mathbf{v}=0\\). The reason is that we want a non-trivial solution to \\((\\mathbf{A}-\\lambda \\mathbf{I}) \\mathbf{v}=0\\). Therefore, \\((\\mathbf{A}-\\lambda \\mathbf{I})\\) should be non-invertible. Otherwise, if it is invertible, we get \\(\\mathbf{v}=(\\mathbf{A}-\\lambda \\mathbf{I})^{-1} \\cdot 0=0\\), which is a trivial solution. Since a matrix is non-invertible if its determinant is 0 . Thus, \\(\\operatorname{det}(\\mathbf{A}-\\lambda \\mathbf{I})=0\\) for non-trivial solutions. We start with a square matrix, \\(\\mathbf{A}\\), like \\[ A =\\left[\\begin{array}{cc} 1 &amp; 2 \\\\ 3 &amp; -4 \\end{array}\\right] \\] \\[ \\begin{aligned} \\det (\\mathbf{A}-\\lambda \\mathbf{I})= &amp; \\left|\\begin{array}{cc} 1-\\lambda &amp; 2 \\\\ 3 &amp; -4-\\lambda \\end{array}\\right|=(1-\\lambda)(-4-\\lambda)-2 \\cdot 3 \\\\ &amp; =-4-\\lambda+4 \\lambda+\\lambda^2-6 \\\\ &amp; =\\lambda^2+3 \\lambda-10 \\\\ &amp; =(\\lambda-2)(\\lambda+5)=0 \\\\ &amp; \\therefore \\lambda_1=2, ~ \\lambda_2=-5 \\\\ &amp; \\end{aligned} \\] We have two eigenvalues. We now need to consider each eigenvalue indivudally $$ \\[\\begin{gathered} \\lambda_1=2 \\\\ (A 1-\\lambda I) \\mathbf{v}=0 \\\\ {\\left[\\begin{array}{cc} 1-\\lambda_1 &amp; 2 \\\\ 3 &amp; -4-\\lambda_1 \\end{array}\\right]\\left[\\begin{array}{l} v_1 \\\\ v_2 \\end{array}\\right]=\\left[\\begin{array}{l} 0 \\\\ 0 \\end{array}\\right]} \\\\ {\\left[\\begin{array}{cc} -1 &amp; 2 \\\\ 3 &amp; -6 \\end{array}\\right]\\left[\\begin{array}{l} v_1 \\\\ v_2 \\end{array}\\right]=\\left[\\begin{array}{l} 0 \\\\ 0 \\end{array}\\right]} \\end{gathered}\\] $$ Hence, \\[ \\begin{aligned} -v_1+2 v_2=0 \\\\ 3 v_1-6 v_2=0\\\\ v_1=2, ~ v_2=1 \\end{aligned} \\] And, $$ \\[\\begin{aligned} &amp; \\lambda_2=-5 \\\\ &amp; {\\left[\\begin{array}{cc} 1-\\lambda_2 &amp; 2 \\\\ 3 &amp; -4-\\lambda_2 \\end{array}\\right]\\left[\\begin{array}{l} v_1 \\\\ v_2 \\end{array}\\right]=\\left[\\begin{array}{l} 0 \\\\ 0 \\end{array}\\right]} \\\\ &amp; {\\left[\\begin{array}{cc} 6 &amp; 2 \\\\ 3 &amp; 1 \\end{array}\\right]\\left[\\begin{array}{l} v_1 \\\\ v_2 \\end{array}\\right]=\\left[\\begin{array}{l} 0 \\\\ 0 \\end{array}\\right]} \\end{aligned}\\] $$ Hence, $$ \\[\\begin{gathered} 6 v_1+2 v_2=0 \\\\ 3 v_1+v_2=0 \\\\ v_1=-1,~ v_2=3 \\end{gathered}\\] $$ We have two eigenvalues \\[ \\begin{aligned} &amp; \\lambda_1=2 \\\\ &amp; \\lambda_2=-5 \\end{aligned} \\] And two corresponding eigenvectors \\[ \\left[\\begin{array}{l} 2 \\\\ 1 \\end{array}\\right],\\left[\\begin{array}{c} -1 \\\\ 3 \\end{array}\\right] \\] for \\(\\lambda_1=2\\) \\[ \\left[\\begin{array}{cc} 1 &amp; 2 \\\\ 3 &amp; -4 \\end{array}\\right]\\left[\\begin{array}{l} 2 \\\\ 1 \\end{array}\\right]=\\left[\\begin{array}{l} 2+2 \\\\ 6-4 \\end{array}\\right]=\\left[\\begin{array}{l} 4 \\\\ 2 \\end{array}\\right]=2\\left[\\begin{array}{l} 2 \\\\ 1 \\end{array}\\right] \\] Let’s see the solution in R A &lt;- matrix(c(1, 3, 2, -4), 2, 2) eigen(A) ## eigen() decomposition ## $values ## [1] -5 2 ## ## $vectors ## [,1] [,2] ## [1,] -0.3162278 0.8944272 ## [2,] 0.9486833 0.4472136 The eigenvectors are typically normalized by dividing by its length \\(\\sqrt{v^{\\prime} v}\\), which is 5 in our case for \\(\\lambda_1=2\\). # For the ev (2, 1), for lambda c(2, 1) / sqrt(5) ## [1] 0.8944272 0.4472136 There some nice properties that we can observe in this application. # Sum of eigenvalues = sum of diagonal terms of A (Trace of A) ev &lt;- eigen(A)$values sum(ev) == sum(diag(A)) ## [1] TRUE # Product of eigenvalues = determinant of A round(prod(ev), 4) == round(det(A), 4) ## [1] TRUE # Diagonal matrix D has eigenvalues = diagonal elements D &lt;- matrix(c(2, 0, 0, 5), 2, 2) eigen(D)$values == sort(diag(D), decreasing = TRUE) ## [1] TRUE TRUE We can see that, if one of the eigenvalues is zero for a matrix, the determinant of the matrix will be zero. We willl return to this issue in Singluar Value Decomposition. Let’s finish this chapter with Diagonalization and Eigendecomposition. Suppose we have \\(m\\) linearly independent eigenvectors (\\(\\mathbf{v_i}\\) is eigenvector \\(i\\) in a column vector in \\(\\mathbf{V}\\)) of \\(\\mathbf{A}\\). \\[ \\mathbf{AV}=\\mathbf{A}\\left[\\mathbf{v_1} \\mathbf{v_2} \\cdots \\mathbf{v_m}\\right]=\\left[\\mathbf{A} \\mathbf{v_1} \\mathbf{A} \\mathbf{v_2} \\ldots \\mathbf{A} \\mathbf{v_m}\\right]=\\left[\\begin{array}{llll} \\lambda_1 \\mathbf{v_1} &amp; \\lambda_2\\mathbf{v_2} &amp; \\ldots &amp; \\lambda_m \\mathbf{v_m} \\end{array}\\right] \\] because \\[ \\mathbf{A} \\mathbf{v}=\\lambda \\mathbf{v} \\] $$ == $$ So that \\[ \\mathbf{A V=V \\Lambda} \\] Hence, \\[ \\mathbf{A}=\\mathbf{V} \\Lambda \\mathbf{V}^{-1} \\] Eigendecomposition (a.k.a. spectral decomposition) decomposes a matrix \\(\\mathbf{A}\\) into a multiplication of a matrix of eigenvectors \\(\\mathbf{V}\\) and a diagonal matrix of eigenvalues \\(\\mathbf{\\Lambda}\\). This can only be done if a matrix is diagonalizable. In fact, the definition of a diagonalizable matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) is that it can be eigendecomposed into \\(n\\) eigenvectors, so that \\(\\mathbf{V}^{-1} \\mathbf{A} \\mathbf{V}=\\Lambda\\). \\[ \\begin{align} \\mathbf{A}^2&amp;=(\\mathbf{V} \\Lambda \\mathbf{V}^{-1})(\\mathbf{V} \\Lambda \\mathbf{V}^{-1})\\\\ &amp;=\\mathbf{V} \\Lambda \\text{I} \\Lambda \\mathbf{V}^{-1}\\\\ &amp;=\\mathbf{V} \\Lambda^2 \\mathbf{V}^{-1}\\\\ \\end{align} \\] in general \\[ \\mathbf{A}^k=\\mathbf{V} \\Lambda^k \\mathbf{V}^{-1} \\] Example: A = matrix(sample(1:100, 9), 3, 3) A ## [,1] [,2] [,3] ## [1,] 83 62 81 ## [2,] 3 19 67 ## [3,] 52 16 72 eigen(A) ## eigen() decomposition ## $values ## [1] 158.2224+ 0.00000i 7.8888+29.87303i 7.8888-29.87303i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.8002326+0i -0.2924139-0.4927725i -0.2924139+0.4927725i ## [2,] 0.2739645+0i 0.7334935+0.0000000i 0.7334935+0.0000000i ## [3,] 0.5334522+0i -0.1085486+0.3491043i -0.1085486-0.3491043i V = eigen(A)$vectors Lam = diag(eigen(A)$values) # Prove that AV = VLam round(A %*% V, 4) == round(V %*% Lam, 4) ## [,1] [,2] [,3] ## [1,] TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE # And decomposition A == round(V %*% Lam %*% solve(V), 4) ## [,1] [,2] [,3] ## [1,] TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE And, matrix inverse with eigendecomposition: \\[ \\mathbf{A}^{-1}=\\mathbf{V} \\Lambda^{-1} \\mathbf{V}^{-1} \\] Example: A = matrix(sample(1:100, 9), 3, 3) A ## [,1] [,2] [,3] ## [1,] 41 58 30 ## [2,] 3 57 84 ## [3,] 60 65 89 V = eigen(A)$vectors Lam = diag(eigen(A)$values) # Inverse of A solve(A) ## [,1] [,2] [,3] ## [1,] -0.002356638 -0.019559485 0.01925501 ## [2,] 0.029065200 0.011259492 -0.02042419 ## [3,] -0.019638649 0.004962945 0.01317160 # And V %*% solve(Lam) %*% solve(V) ## [,1] [,2] [,3] ## [1,] -0.002356638+0i -0.019559485+0i 0.01925501+0i ## [2,] 0.029065200+0i 0.011259492-0i -0.02042419+0i ## [3,] -0.019638649+0i 0.004962945+0i 0.01317160-0i The inverse of \\(\\mathbf{\\Lambda}\\) is just the inverse of each diagonal element (the eigenvalues). But, this can only be done if a matrix is diagonalizable. So if \\(\\mathbf{A}\\) is not \\(n \\times n\\), then we can use \\(\\mathbf{A&#39;A}\\) or \\(\\mathbf{AA&#39;}\\), both symmetric now. Example: \\[ \\mathbf{A}=\\left(\\begin{array}{ll} 1 &amp; 2 \\\\ 2 &amp; 4 \\end{array}\\right) \\] As \\(\\det(\\mathbf{A})=0,\\) \\(\\mathbf{A}\\) is singular and its inverse is undefined. In other words, since \\(\\det(\\mathbf{A})\\) equals the product of the eigenvalues \\(\\lambda_j\\) of \\(\\mathrm{A}\\), the matrix \\(\\mathbf{A}\\) has an eigenvalue which is zero. To see this, consider the spectral (eigen) decomposition of \\(A\\) : \\[ \\mathbf{A}=\\sum_{j=1}^{p} \\theta_{j} \\mathbf{v}_{j} \\mathbf{v}_{j}^{\\top} \\] where \\(\\mathbf{v}_{\\mathrm{j}}\\) is the eigenvector belonging to \\(\\theta_{\\mathrm{j}}\\) The inverse of \\(\\mathbf{A}\\) is then: \\[ \\mathbf{A}^{-1}=\\sum_{j=1}^{p} \\theta_{j}^{-1} \\mathbf{v}_{j} \\mathbf{v}_{j}^{\\top} \\] A has eigenvalues 5 and 0. The inverse of \\(A\\) via the spectral decomposition is then undefined: \\[ \\mathbf{A}^{-1}=\\frac{1}{5} \\mathbf{v}_{1} \\mathbf{v}_{1}^{\\top}+ \\frac{1}{0} \\mathbf{v}_{1} \\mathbf{v}_{1}^{\\top} \\] 28.3 Singular Value Decomposition Singular Value Decomposition (SVD) is another type of decomposition. Different than eigendecomposition, which requires a square matrix, SVD allows us to decompose a rectangular matrix. This is more useful because the rectangular matrix usually represents data in practice. For any matrix \\(\\mathbf{A}\\), both \\(\\mathbf{A^{\\top} A}\\) and \\(\\mathbf{A A^{\\top}}\\) are symmetric. Therefore, they have \\(n\\) and \\(m\\) **orthogonal* eigenvectors, respectively. The proof is simple: Suppose we have a 2 x 2 symmetric matrix, \\(\\mathbf{A}\\), with two distinct eigenvalues (\\(\\lambda_1, \\lambda_2\\)) and two corresponding eigenvectors (\\(\\mathbf{v}_1\\) and \\(\\mathbf{v}_1\\)). Following the rule, \\[ \\begin{aligned} &amp; \\mathbf{A} \\mathbf{v}_1=\\lambda_1 \\mathbf{v}_1, \\\\ &amp; \\mathbf{A} \\mathbf{v}_2=\\lambda_2 \\mathbf{v}_2. \\\\ \\end{aligned} \\] Let’s multiply (inner product) the first one with \\(\\mathbf{v}_2^{\\top}\\): \\[ \\mathbf{v}_2^{\\top}\\mathbf{A} \\mathbf{v}_1=\\lambda_1 \\mathbf{v}_2^{\\top} \\mathbf{v}_1 \\] And, the second one with \\(\\mathbf{v}_1^{\\top}\\) \\[ \\mathbf{v}_1^{\\top}\\mathbf{A} \\mathbf{v}_2=\\lambda_2 \\mathbf{v}_1^{\\top} \\mathbf{v}_2 \\] If we take the transpose of both side of \\(\\mathbf{v}_2^{\\top}\\mathbf{A} \\mathbf{v}_1=\\lambda_1 \\mathbf{v}_2^{\\top} \\mathbf{v}_1\\), it will be \\[ \\mathbf{v}_1^{\\top}\\mathbf{A} \\mathbf{v}_2=\\lambda_1 \\mathbf{v}_1^{\\top} \\mathbf{v}_2 \\] And, subtract these last two: \\[ \\begin{aligned} &amp;\\mathbf{v}_1^{\\top}\\mathbf{A} \\mathbf{v}_2=\\lambda_2 \\mathbf{v}_1^{\\top} \\mathbf{v}_2 \\\\ &amp; \\mathbf{v}_1^{\\top}\\mathbf{A} \\mathbf{v}_2=\\lambda_1 \\mathbf{v}_1^{\\top} \\mathbf{v}_2 \\\\ &amp; \\hline 0=\\left(\\lambda_2 - \\lambda_1\\right) \\mathbf{v}_1^{\\top} \\mathbf{v}_2 \\end{aligned} \\] Since , \\(\\lambda_1\\) and \\(\\lambda_2\\) are distinct, \\(\\lambda_2- \\lambda_1\\) cannot be zero. Therefore, $ _1^{} _2 = 0$. As we saw in Chapter 15, the dot products of two vectors can be expressed geometrically \\[ \\begin{aligned} a \\cdot b=\\|a\\|\\|b\\| \\cos (\\theta),\\\\ \\cos (\\theta)=\\frac{a \\cdot b}{\\|a\\|\\|b\\|} \\end{aligned} \\] Hence, \\(\\cos (\\theta)\\) has to be zero for $ _1^{} _2 = 0$. Since \\(\\cos (90)=0\\), the two vectors are orthogonal. We start with the following eigendecomposition for \\(\\mathbf{A^{\\top}A}\\) and \\(\\mathbf{A A^{\\top}}\\): \\[ \\begin{aligned} \\mathbf{A^{\\top} A =V D V^{\\top}} \\\\ \\mathbf{A A^{\\top} =U D^{\\prime} U^{\\top}} \\end{aligned} \\] where \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix consisting of the eigenvectors of \\(\\mathbf{A}^{\\top}\\mathbf{A},\\) and, \\(\\mathbf{D}\\) is an \\(n \\times n\\) diagonal matrix with the eigenvalues of \\(\\mathbf{A^{\\top} A}\\) on the diagonal. The same decomposition for \\(\\mathbf{A A^{\\top}}\\), now \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix consisting of the eigenvectors of \\(\\mathbf{A A^{\\top}}\\), and \\(\\mathbf{D^{\\prime}}\\) is an \\(m \\times m\\) diagonal matrix with the eigenvalues of \\(\\mathbf{A A^{\\top}}\\) on the diagonal. It turns out that \\(\\mathbf{D}\\) and \\(\\mathbf{D^{\\prime}}\\) have the same non-zero diagonal entries except that the order might be different. We can write SVD for any real \\(m \\times n\\) matrix as \\[ \\mathbf{A=U \\Sigma V^{\\top}} \\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix whose columns are the eigenvectors of \\(\\mathbf{A A^{\\top}}\\), \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix whose columns are the eigenvectors of \\(\\mathbf{A^{\\top} A}\\), and \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) diagonal matrix of the form: \\[ \\mathbf{\\Sigma}=\\left(\\begin{array}{cccc} \\sigma_{1} &amp; &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_{n} &amp; \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp;0 \\\\ \\end{array}\\right) \\] with \\(\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{n}&gt;0\\) . The number of non-zero singular values is equal to the rank of \\(\\operatorname{rank}(\\mathbf{A})\\). In \\(\\mathbf{\\Sigma}\\) above, \\(\\sigma_{1}, \\ldots, \\sigma_{n}\\) are the square roots of the eigenvalues of \\(\\mathbf{A^{\\top} A}\\). They are called the singular values of \\(\\mathbf{A}\\). One important point is that, although \\(\\mathbf{U}\\) in \\(\\mathbf{U \\Sigma V^{\\top}}\\) is \\(m \\times m\\), when it is multiplied by \\(\\mathbf{\\Sigma}\\), it reduces to \\(n \\times n\\) due to zeros in \\(\\mathbf{\\Sigma}\\). Hence, we can actually select only those in \\(\\mathbf{U}\\) that are not going to be zeroed out due to that multiplication. When we take only \\(n \\times n\\) from \\(\\mathbf{U}\\) matrix, it is called “Economy SVD”, \\(\\mathbf{\\hat{U} \\hat{\\Sigma} V^{\\top}}\\), where all matrices will be \\(n \\times n\\). The singular value decomposition is very useful when our basic goal is to “solve” the system \\(\\mathbf{A} x=b\\) for all matrices \\(\\mathbf{A}\\) and vectors \\(b\\) with a numerically stable algorithm. Some important applications of the SVD include computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix. We will see some of them in the following chapters Here is an example: set.seed(104) A &lt;- matrix(sample(100, 12), 3, 4) A ## [,1] [,2] [,3] [,4] ## [1,] 77 24 32 78 ## [2,] 67 61 39 96 ## [3,] 34 94 42 28 svda &lt;- svd(A) svda ## $d ## [1] 199.83933 70.03623 16.09872 ## ## $u ## [,1] [,2] [,3] ## [1,] 0.5515235 0.5259321 0.6474699 ## [2,] 0.6841400 0.1588989 -0.7118312 ## [3,] 0.4772571 -0.8355517 0.2721747 ## ## $v ## [,1] [,2] [,3] ## [1,] 0.5230774 0.3246068 0.7091515 ## [2,] 0.4995577 -0.8028224 -0.1427447 ## [3,] 0.3221338 -0.1722864 0.2726277 ## [4,] 0.6107880 0.4694933 -0.6343518 # Singular values = sqrt(eigenvalues of t(A)%*%A)) ev &lt;- eigen(t(A) %*% A)$values round(sqrt(ev), 5) ## [1] 199.83933 70.03623 16.09872 0.00000 Note that this ““Economy SVD” using only the non-zero eigenvalues and their respective eigenvectors. Ar &lt;- svda$u %*% diag(svda$d) %*% t(svda$v) Ar ## [,1] [,2] [,3] [,4] ## [1,] 77 24 32 78 ## [2,] 67 61 39 96 ## [3,] 34 94 42 28 As we use SVD in the following chapter, its usefulness will be obvious. 28.4 Rank(r) Approximations One of the useful applications of singular value decomposition (SVD) is rank approximations, or matrix approximations. We can write \\(\\mathbf{A=U \\Sigma V^{\\top}}\\) as \\[ =\\sigma_{1} u_{1} v_{1}^{\\top}+\\sigma_{2} u_{2} v_{2}^{\\top}+\\ldots+\\sigma_{n} u_{n} v_{n}^{\\top}+ 0. \\] Each term in this equation is a Rank(1) matrix: \\(u_1\\) is \\(n \\times 1\\) column vector and \\(v_1\\) is \\(1 \\times n\\) row vector. Since these are the only orthogonal entries in the resulting matrix, the first term with \\(\\sigma_1\\) is a Rank(1) \\(n \\times n\\) matrix. All other terms have the same dimension. Since \\(\\sigma\\)’s are ordered, the first term is the carries the most information. So, Rank(1) approximation is taking only the first term and ignoring the others. Here is a simple example: #rank-one approximation A &lt;- matrix(c(1, 5, 4, 2), 2 , 2) A ## [,1] [,2] ## [1,] 1 4 ## [2,] 5 2 v1 &lt;- matrix(eigen(t(A) %*% (A))$vector[, 1], 1, 2) sigma &lt;- sqrt(eigen(t(A) %*% (A))$values[1]) u1 &lt;- matrix(eigen(A %*% t(A))$vector[, 1], 2, 1) # Rank(1) approximation of A Atilde &lt;- sigma * u1 %*% v1 Atilde ## [,1] [,2] ## [1,] -2.560369 -2.069843 ## [2,] -4.001625 -3.234977 And, Rank(2) approximation can be obtained by adding the first 2 terms. As we add more terms, we can get the full information in the data. But often times, we truncate the ranks at \\(r\\) by removing the terms with small \\(sigma\\). This is also called noise reduction. There are many examples on the Internet for real image compression, but we apply rank approximation to a heatmap from our own work. The heatmap shows moving-window partial correlations between daily positivity rates (Covid-19) and mobility restrictions for different time delays (days, “lags”) comt &lt;- readRDS(&quot;comt.rds&quot;) heatmap( comt, Colv = NA, Rowv = NA, main = &quot;Heatmap - Original&quot;, xlab = &quot;Lags&quot;, ylab = &quot;Starting days of 7-day rolling windows&quot; ) # Rank(2) with SVD fck &lt;- svd(comt) r = 2 comt.re &lt;- as.matrix(fck$u[, 1:r]) %*% diag(fck$d)[1:r, 1:r] %*% t(fck$v[, 1:r]) heatmap( comt.re, Colv = NA, Rowv = NA, main = &quot;Heatmap Matrix - Rank(2) Approx&quot;, xlab = &quot;Lags&quot;, ylab = &quot;Startting days of 7-day rolling windows&quot; ) This Rank(2) approximation reduces the noise in the moving-window partial correlations so that we can see the clear trend about the delay in the effect of mobility restrictions on the spread. We change the order of correlations in the original heatmap, and make it row-wise correlations: #XX&#39; and X&#39;X SVD wtf &lt;- comt %*% t(comt) fck &lt;- svd(wtf) r = 2 comt.re2 &lt;- as.matrix(fck$u[, 1:r]) %*% diag(fck$d)[1:r, 1:r] %*% t(fck$v[, 1:r]) heatmap( comt.re2, Colv = NA, Rowv = NA, main = &quot;Row Corr. - Rank(2)&quot;, xlab = &quot;Startting days of 7-day rolling windows&quot;, ylab = &quot;Startting days of 7-day rolling windows&quot; ) This is now worse than the original heatmap we had ealier. When we apply a Rank(2) approximation, however, we have a very clear picture: wtf &lt;- t(comt) %*% comt fck &lt;- svd(wtf) r = 2 comt.re3 &lt;- as.matrix(fck$u[, 1:r]) %*% diag(fck$d)[1:r, 1:r] %*% t(fck$v[, 1:r]) heatmap( comt.re3, Colv = NA, Rowv = NA, main = &quot;Column Corr. - Rank(2)&quot;, xlab = &quot;Lags&quot;, ylab = &quot;Lags&quot; ) There is a series of great lectures on SVD and other matrix approximations by Steve Brunton at YouTube https://www.youtube.com/watch?v=nbBvuuNVfco. 28.5 Moore-Penrose inverse The Singular Value Decomposition (SVD) can be used for solving Ordinary Least Squares (OLS) problems. In particular, the SVD of the design matrix \\(\\mathbf{X}\\) can be used to compute the coefficients of the linear regression model. Here are the steps: \\[ \\mathbf{y = X \\beta}\\\\ \\mathbf{y = U \\Sigma V&#39; \\beta}\\\\ \\mathbf{U&#39;y = U&#39;U \\Sigma V&#39; \\beta}\\\\ \\mathbf{U&#39;y = \\Sigma V&#39; \\beta}\\\\ \\mathbf{\\Sigma^{-1}}\\mathbf{U&#39;y = V&#39; \\beta}\\\\ \\mathbf{V\\Sigma^{-1}}\\mathbf{U&#39;y = \\beta}\\\\ \\] This formula for beta is computationally efficient and numerically stable, even for ill-conditioned or singular \\(\\mathbf{X}\\) matrices. Moreover, it allows us to compute the solution to the OLS problem without explicitly computing the inverse of \\(\\mathbf{X}^T \\mathbf{X}\\). Menawhile, the term \\[ \\mathbf{V\\Sigma^{-1}U&#39; = M^+} \\] is called “generalized inverse” or The Moore-Penrose Pseudoinverse. If \\(\\mathbf{X}\\) has full column rank, then the pseudoinverse is also the unique solution to the OLS problem. However, if \\(\\mathbf{X}\\) does not have full column rank, then its pseudoinverse may not exist or may not be unique. In this case, the OLS estimator obtained using the pseudoinverse will be a “best linear unbiased estimator” (BLUE), but it will not be the unique solution to the OLS problem. To be more specific, the OLS estimator obtained using the pseudoinverse will minimize the sum of squared residuals subject to the constraint that the coefficients are unbiased, i.e., they have zero expected value. However, there may be other linear unbiased estimators that achieve the same minimum sum of squared residuals. These alternative estimators will differ from the OLS estimator obtained using the pseudoinverse in the values they assign to the coefficients. In practice, the use of the pseudoinverse to estimate the OLS coefficients when \\(\\mathbf{X}\\) does not have full column rank can lead to numerical instability, especially if the singular values of \\(\\mathbf{X}\\) are very small. In such cases, it may be more appropriate to use regularization techniques such as ridge or Lasso regression to obtain stable and interpretable estimates. These methods penalize the size of the coefficients and can be used to obtain sparse or “shrunken” estimates, which can be particularly useful in high-dimensional settings where there are more predictors than observations. Here are some application of SVD and Pseudoinverse. library(MASS) ##Simple SVD and generalized inverse A &lt;- matrix(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1), 9, 4) a.svd &lt;- svd(A) ds &lt;- diag(1 / a.svd$d[1:3]) u &lt;- a.svd$u v &lt;- a.svd$v us &lt;- as.matrix(u[, 1:3]) vs &lt;- as.matrix(v[, 1:3]) (a.ginv &lt;- vs %*% ds %*% t(us)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 ## [2,] 0.25000000 0.25000000 0.25000000 -0.08333333 -0.08333333 -0.08333333 ## [3,] -0.08333333 -0.08333333 -0.08333333 0.25000000 0.25000000 0.25000000 ## [4,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 ## [,7] [,8] [,9] ## [1,] 0.08333333 0.08333333 0.08333333 ## [2,] -0.08333333 -0.08333333 -0.08333333 ## [3,] -0.08333333 -0.08333333 -0.08333333 ## [4,] 0.25000000 0.25000000 0.25000000 ginv(A) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 ## [2,] 0.25000000 0.25000000 0.25000000 -0.08333333 -0.08333333 -0.08333333 ## [3,] -0.08333333 -0.08333333 -0.08333333 0.25000000 0.25000000 0.25000000 ## [4,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 ## [,7] [,8] [,9] ## [1,] 0.08333333 0.08333333 0.08333333 ## [2,] -0.08333333 -0.08333333 -0.08333333 ## [3,] -0.08333333 -0.08333333 -0.08333333 ## [4,] 0.25000000 0.25000000 0.25000000 We can use SVD for solving a regular OLS on simulated data: #Simulated DGP x1 &lt;- rep(1, 20) x2 &lt;- rnorm(20) x3 &lt;- rnorm(20) u &lt;- matrix(rnorm(20, mean = 0, sd = 1), nrow = 20, ncol = 1) X &lt;- cbind(x1, x2, x3) beta &lt;- matrix(c(0.5, 1.5, 2), nrow = 3, ncol = 1) Y &lt;- X %*% beta + u #OLS betahat_OLS &lt;- solve(t(X) %*% X) %*% t(X) %*% Y betahat_OLS ## [,1] ## x1 0.6310514 ## x2 1.5498699 ## x3 1.7014166 #SVD X.svd &lt;- svd(X) ds &lt;- diag(1 / X.svd$d) u &lt;- X.svd$u v &lt;- X.svd$v us &lt;- as.matrix(u) vs &lt;- as.matrix(v) X.ginv_mine &lt;- vs %*% ds %*% t(us) # Compare X.ginv &lt;- ginv(X) round((X.ginv_mine - X.ginv), 4) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## [1,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [2,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [3,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [,15] [,16] [,17] [,18] [,19] [,20] ## [1,] 0 0 0 0 0 0 ## [2,] 0 0 0 0 0 0 ## [3,] 0 0 0 0 0 0 # Now OLS betahat_ginv &lt;- X.ginv %*% Y betahat_ginv ## [,1] ## [1,] 0.6310514 ## [2,] 1.5498699 ## [3,] 1.7014166 betahat_OLS ## [,1] ## x1 0.6310514 ## x2 1.5498699 ## x3 1.7014166 "],["pca-principle-component-analysis.html", "Chapter 29 PCA (Principle Component Analysis) 29.1 Factor Analysis", " Chapter 29 PCA (Principle Component Analysis) Having seen SVD and Eigenvalue decomposition, we can now look at Principle Component Analysis (PCA), which is a statistical procedure that allows us to summarize the information content in large data files. In other words, PCA is a popular technique used to reduce the dimensionality of high-dimensional data while retaining most of the information in the original data. PCA is a eigenvalue decomposition of a covariance matrix (of data matrix \\(\\mathbf{X}\\)). Since a covariance matrix is a square symmetric matrix, we can apply the eigenvalue decomposition, which reveals the unique orthogonal directions (variances) in the data so that their orthogonal linear combinations maximize the total variance. The goal is here a dimension reduction of the data matrix. Hence by selecting a few loading, we can reduce the dimension of the data but capture a substantial variation in the data at the same time. Principal components are the ordered (orthogonal) lines (vectors) that best account for the maximum variance in the data by their magnitude. To get the (unique) variances (direction and the magnitude) in data, we first obtain the mean-centered covariance matrix. When we use the covariance matrix of the data, we can use eigenvalue decomposition to identify the unique variation (eigenvectors) and their relative magnitudes (eigenvalues) in the data. Here is a simple procedure: \\(\\mathbf{X}\\) is the data matrix, \\(\\mathbf{B}\\) is the mean-centered data matrix, \\(\\mathbf{C}\\) is the covariance matrix (\\(\\mathbf{B}^T\\mathbf{B}\\)). Note that, if \\(\\mathbf{B}\\) is scaled, i.e. “z-scored”, \\(\\mathbf{B}^T\\mathbf{B}\\) gives correlation matrix. We will have more information on covariance and correlation in Chapter 32. The eigenvectors and values of \\(\\mathbf{C}\\) by \\(\\mathbf{C} = \\mathbf{VDV^{\\top}}\\). Thus, \\(\\mathbf{V}\\) contains the eigenvectors (loadings) and \\(\\mathbf{D}\\) contains eigenvalues. Using \\(\\mathbf{V}\\), the transformation of \\(\\mathbf{B}\\) with \\(\\mathbf{B} \\mathbf{V}\\) maps the data of \\(p\\) variables to a new space of \\(p\\) variables which are uncorrelated over the dataset. \\(\\mathbf{T} =\\mathbf{B} \\mathbf{V}\\) is called the principle component or score matrix. Since SVD of \\(\\mathbf{B} = \\mathbf{U} \\Sigma \\mathbf{V}^{\\top}\\), we can also get \\(\\mathbf{B}\\mathbf{V} = \\mathbf{T} = \\mathbf{U\\Sigma}\\). Hence the principle components are \\(\\mathbf{T} = \\mathbf{BV} = \\mathbf{U\\Sigma}\\). However, not all the principal components need to be kept. Keeping only the first \\(r\\) principal components, produced by using only the first \\(r\\) eigenvectors, gives the truncated transformation \\(\\mathbf{T}_{r} = \\mathbf{B} \\mathbf{V}_{r}\\). Obviously you choose those with higher variance in each directions by the order of eigenvalues. We can use \\(\\frac{\\lambda_{k}}{\\sum_{i=1} \\lambda_{k}}\\) to identify \\(r\\). Or cumulatively, we can see how much variation could be captured by \\(r\\) number of \\(\\lambda\\)s, which gives us an idea how many principle components to keep: \\[ \\frac{\\sum_{i=1}^{r} \\lambda_{k}}{\\sum_{i=1}^n \\lambda_{k}} \\] We use the factorextra package and the decathlon2 data for an example. library(&quot;factoextra&quot;) data(decathlon2) X &lt;- as.matrix(decathlon2[, 1:10]) head(X) ## X100m Long.jump Shot.put High.jump X400m X110m.hurdle Discus ## SEBRLE 11.04 7.58 14.83 2.07 49.81 14.69 43.75 ## CLAY 10.76 7.40 14.26 1.86 49.37 14.05 50.72 ## BERNARD 11.02 7.23 14.25 1.92 48.93 14.99 40.87 ## YURKOV 11.34 7.09 15.19 2.10 50.42 15.31 46.26 ## ZSIVOCZKY 11.13 7.30 13.48 2.01 48.62 14.17 45.67 ## McMULLEN 10.83 7.31 13.76 2.13 49.91 14.38 44.41 ## Pole.vault Javeline X1500m ## SEBRLE 5.02 63.19 291.7 ## CLAY 4.92 60.15 301.5 ## BERNARD 5.32 62.77 280.1 ## YURKOV 4.72 63.44 276.4 ## ZSIVOCZKY 4.42 55.37 268.0 ## McMULLEN 4.42 56.37 285.1 n &lt;- nrow(X) B &lt;- scale(X, center = TRUE) C &lt;- t(B) %*% B / (n - 1) head(C) ## X100m Long.jump Shot.put High.jump X400m ## X100m 1.0000000 -0.7377932 -0.3703180 -0.3146495 0.5703453 ## Long.jump -0.7377932 1.0000000 0.3737847 0.2682078 -0.5036687 ## Shot.put -0.3703180 0.3737847 1.0000000 0.5747998 -0.2073588 ## High.jump -0.3146495 0.2682078 0.5747998 1.0000000 -0.2616603 ## X400m 0.5703453 -0.5036687 -0.2073588 -0.2616603 1.0000000 ## X110m.hurdle 0.6699790 -0.5521158 -0.2701634 -0.2022579 0.5970140 ## X110m.hurdle Discus Pole.vault Javeline X1500m ## X100m 0.6699790 -0.3893760 0.01156433 -0.26635476 -0.17805307 ## Long.jump -0.5521158 0.3287652 0.07982045 0.28806781 0.17332597 ## Shot.put -0.2701634 0.7225179 -0.06837068 0.47558572 0.00959628 ## High.jump -0.2022579 0.4210187 -0.55129583 0.21051789 -0.15699017 ## X400m 0.5970140 -0.2545326 0.11156898 0.02350554 0.18346035 ## X110m.hurdle 1.0000000 -0.4213608 0.12118697 0.09655757 -0.10331329 #Check it head(cov(B)) ## X100m Long.jump Shot.put High.jump X400m ## X100m 1.0000000 -0.7377932 -0.3703180 -0.3146495 0.5703453 ## Long.jump -0.7377932 1.0000000 0.3737847 0.2682078 -0.5036687 ## Shot.put -0.3703180 0.3737847 1.0000000 0.5747998 -0.2073588 ## High.jump -0.3146495 0.2682078 0.5747998 1.0000000 -0.2616603 ## X400m 0.5703453 -0.5036687 -0.2073588 -0.2616603 1.0000000 ## X110m.hurdle 0.6699790 -0.5521158 -0.2701634 -0.2022579 0.5970140 ## X110m.hurdle Discus Pole.vault Javeline X1500m ## X100m 0.6699790 -0.3893760 0.01156433 -0.26635476 -0.17805307 ## Long.jump -0.5521158 0.3287652 0.07982045 0.28806781 0.17332597 ## Shot.put -0.2701634 0.7225179 -0.06837068 0.47558572 0.00959628 ## High.jump -0.2022579 0.4210187 -0.55129583 0.21051789 -0.15699017 ## X400m 0.5970140 -0.2545326 0.11156898 0.02350554 0.18346035 ## X110m.hurdle 1.0000000 -0.4213608 0.12118697 0.09655757 -0.10331329 Eigenvalues and vectors … #Eigens evalues &lt;- eigen(C)$values evalues ## [1] 3.7499727 1.7451681 1.5178280 1.0322001 0.6178387 0.4282908 0.3259103 ## [8] 0.2793827 0.1911128 0.1122959 evectors &lt;- eigen(C)$vectors evectors #Ordered ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.42290657 -0.2594748 -0.081870461 -0.09974877 0.2796419 -0.16023494 ## [2,] -0.39189495 0.2887806 0.005082180 0.18250903 -0.3355025 -0.07384658 ## [3,] -0.36926619 -0.2135552 -0.384621732 -0.03553644 0.3544877 -0.32207320 ## [4,] -0.31422571 -0.4627797 -0.003738604 -0.07012348 -0.3824125 -0.52738027 ## [5,] 0.33248297 -0.1123521 -0.418635317 -0.26554389 -0.2534755 0.23884715 ## [6,] 0.36995919 -0.2252392 -0.338027983 0.15726889 -0.2048540 -0.26249611 ## [7,] -0.37020078 -0.1547241 -0.219417086 -0.39137188 0.4319091 0.28217086 ## [8,] 0.11433982 0.5583051 -0.327177839 0.24759476 0.3340758 -0.43606610 ## [9,] -0.18341259 -0.0745854 -0.564474643 0.47792535 -0.1697426 0.42368592 ## [10,] -0.03599937 0.4300522 -0.286328973 -0.64220377 -0.3227349 -0.10850981 ## [,7] [,8] [,9] [,10] ## [1,] 0.03227949 -0.35266427 0.71190625 -0.03272397 ## [2,] -0.24902853 -0.72986071 0.12801382 -0.02395904 ## [3,] -0.23059438 0.01767069 -0.07184807 0.61708920 ## [4,] -0.03992994 0.25003572 0.14583529 -0.41523052 ## [5,] -0.69014364 0.01543618 -0.13706918 -0.12016951 ## [6,] 0.42797378 -0.36415520 -0.49550598 0.03514180 ## [7,] 0.18416631 -0.26865454 -0.18621144 -0.48037792 ## [8,] -0.12654370 0.16086549 -0.02983660 -0.40290423 ## [9,] 0.23324548 0.19922452 0.33300936 -0.02100398 ## [10,] 0.34406521 0.09752169 0.19899138 0.18954698 Now with prcomp(). First, eigenvalues: # With `prcomp()` Xpca &lt;- prcomp(X, scale = TRUE) #Eigenvalues Xpca$sdev ## [1] 1.9364846 1.3210481 1.2320016 1.0159725 0.7860272 0.6544393 0.5708855 ## [8] 0.5285666 0.4371645 0.3351059 They are the square root of the eigenvalues that we calculated before and they are ordered.# sqrt(evalues) And, the “loadings” (Eigenvectors): #Eigenvectors Xpca$rotation # 10x10 ## PC1 PC2 PC3 PC4 PC5 ## X100m -0.42290657 -0.2594748 0.081870461 0.09974877 -0.2796419 ## Long.jump 0.39189495 0.2887806 -0.005082180 -0.18250903 0.3355025 ## Shot.put 0.36926619 -0.2135552 0.384621732 0.03553644 -0.3544877 ## High.jump 0.31422571 -0.4627797 0.003738604 0.07012348 0.3824125 ## X400m -0.33248297 -0.1123521 0.418635317 0.26554389 0.2534755 ## X110m.hurdle -0.36995919 -0.2252392 0.338027983 -0.15726889 0.2048540 ## Discus 0.37020078 -0.1547241 0.219417086 0.39137188 -0.4319091 ## Pole.vault -0.11433982 0.5583051 0.327177839 -0.24759476 -0.3340758 ## Javeline 0.18341259 -0.0745854 0.564474643 -0.47792535 0.1697426 ## X1500m 0.03599937 0.4300522 0.286328973 0.64220377 0.3227349 ## PC6 PC7 PC8 PC9 PC10 ## X100m 0.16023494 -0.03227949 -0.35266427 0.71190625 0.03272397 ## Long.jump 0.07384658 0.24902853 -0.72986071 0.12801382 0.02395904 ## Shot.put 0.32207320 0.23059438 0.01767069 -0.07184807 -0.61708920 ## High.jump 0.52738027 0.03992994 0.25003572 0.14583529 0.41523052 ## X400m -0.23884715 0.69014364 0.01543618 -0.13706918 0.12016951 ## X110m.hurdle 0.26249611 -0.42797378 -0.36415520 -0.49550598 -0.03514180 ## Discus -0.28217086 -0.18416631 -0.26865454 -0.18621144 0.48037792 ## Pole.vault 0.43606610 0.12654370 0.16086549 -0.02983660 0.40290423 ## Javeline -0.42368592 -0.23324548 0.19922452 0.33300936 0.02100398 ## X1500m 0.10850981 -0.34406521 0.09752169 0.19899138 -0.18954698 loadings &lt;- Xpca$rotation The signs of eigenvectors are flipped and opposites of what we calculated with eigen() above. This is because the definition of an eigenbasis is ambiguous of sign. There are multiple discussions about the sign reversals in eignevectores. Let’s visualize the order: plot(Xpca$sdev) # Eigenvalues fviz_eig(Xpca) # Cumulative with &quot;factoextra&quot; # Or var &lt;- (Xpca$sdev) ^ 2 var_perc &lt;- var / sum(var) * 100 barplot( var_perc, xlab = &#39;PC&#39;, ylab = &#39;Percent Variance&#39;, names.arg = 1:length(var_perc), las = 1, ylim = c(0, max(var_perc)), col = &#39;lightgreen&#39; ) abline(h = mean(var_perc), col = &#39;red&#39;) Since we have ten variables, if each variable contributed equally, they would each contribute 10% to the total variance (red line). This criterion suggests we should also include principal component 4 (but barely) in our interpretation. And principle component scores \\(\\mathbf{T} = \\mathbf{X}\\mathbf{V}\\) (a.k.a score matrix) with prcomp(): pc &lt;- scale(X) %*% Xpca$rotation head(pc) ## PC1 PC2 PC3 PC4 PC5 PC6 ## SEBRLE 0.2727622 0.5264068 1.5556058 0.10384438 1.05453531 0.7177257 ## CLAY 0.8879389 2.0551314 0.8249697 1.81612193 -0.40100595 -1.5039874 ## BERNARD -1.3466138 1.3229149 0.9439501 -1.46516144 -0.17925232 0.5996203 ## YURKOV -0.9108536 -2.2390912 1.9063730 0.09501304 0.18735823 0.3754439 ## ZSIVOCZKY -0.1018764 -1.0694498 -2.0596722 0.07056229 -0.03232182 -0.9321431 ## McMULLEN 0.2353742 -0.9215376 -0.8028425 1.17942532 1.79598700 -0.3241881 ## PC7 PC8 PC9 PC10 ## SEBRLE -0.04935537 -0.02990462 0.63079187 0.07728655 ## CLAY -0.75968352 0.06536612 -0.05920672 0.15812336 ## BERNARD -0.75032098 0.49570997 -0.07483747 -0.03288604 ## YURKOV -0.29565551 -0.09332310 0.06769776 0.13791531 ## ZSIVOCZKY -0.30752133 -0.29476740 0.48055837 0.44234659 ## McMULLEN 0.02896393 0.53358562 -0.05116850 0.37610188 dim(pc) ## [1] 27 10 # which is also given by `prcomp()` head(Xpca$x) ## PC1 PC2 PC3 PC4 PC5 PC6 ## SEBRLE 0.2727622 0.5264068 1.5556058 0.10384438 1.05453531 0.7177257 ## CLAY 0.8879389 2.0551314 0.8249697 1.81612193 -0.40100595 -1.5039874 ## BERNARD -1.3466138 1.3229149 0.9439501 -1.46516144 -0.17925232 0.5996203 ## YURKOV -0.9108536 -2.2390912 1.9063730 0.09501304 0.18735823 0.3754439 ## ZSIVOCZKY -0.1018764 -1.0694498 -2.0596722 0.07056229 -0.03232182 -0.9321431 ## McMULLEN 0.2353742 -0.9215376 -0.8028425 1.17942532 1.79598700 -0.3241881 ## PC7 PC8 PC9 PC10 ## SEBRLE -0.04935537 -0.02990462 0.63079187 0.07728655 ## CLAY -0.75968352 0.06536612 -0.05920672 0.15812336 ## BERNARD -0.75032098 0.49570997 -0.07483747 -0.03288604 ## YURKOV -0.29565551 -0.09332310 0.06769776 0.13791531 ## ZSIVOCZKY -0.30752133 -0.29476740 0.48055837 0.44234659 ## McMULLEN 0.02896393 0.53358562 -0.05116850 0.37610188 Now you can think that if we use evectors that we calculated earlier with filliped signs, the data would be different. It’s similar to multiply the entire data with -1. So the data would not change in a sense that that captures the variation between observations and variables. That’s why the sign of eigenvalues are arbitraray. Now, with SVD: # With SVD Xsvd &lt;- svd(scale(X)) pc_2 &lt;- Xsvd$u %*% diag(Xsvd$d) dim(pc_2) ## [1] 27 10 head(pc_2) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.2727622 0.5264068 1.5556058 0.10384438 1.05453531 0.7177257 ## [2,] 0.8879389 2.0551314 0.8249697 1.81612193 -0.40100595 -1.5039874 ## [3,] -1.3466138 1.3229149 0.9439501 -1.46516144 -0.17925232 0.5996203 ## [4,] -0.9108536 -2.2390912 1.9063730 0.09501304 0.18735823 0.3754439 ## [5,] -0.1018764 -1.0694498 -2.0596722 0.07056229 -0.03232182 -0.9321431 ## [6,] 0.2353742 -0.9215376 -0.8028425 1.17942532 1.79598700 -0.3241881 ## [,7] [,8] [,9] [,10] ## [1,] -0.04935537 -0.02990462 0.63079187 0.07728655 ## [2,] -0.75968352 0.06536612 -0.05920672 0.15812336 ## [3,] -0.75032098 0.49570997 -0.07483747 -0.03288604 ## [4,] -0.29565551 -0.09332310 0.06769776 0.13791531 ## [5,] -0.30752133 -0.29476740 0.48055837 0.44234659 ## [6,] 0.02896393 0.53358562 -0.05116850 0.37610188 Here we can reduce the dimensionality by selecting only 4 PC (the first 4 PC’s are above the average, which explain more than 80% of the variation in the data - see the graph above) reduced &lt;- pc[, 1:4] dim(reduced) ## [1] 27 4 head(reduced) ## PC1 PC2 PC3 PC4 ## SEBRLE 0.2727622 0.5264068 1.5556058 0.10384438 ## CLAY 0.8879389 2.0551314 0.8249697 1.81612193 ## BERNARD -1.3466138 1.3229149 0.9439501 -1.46516144 ## YURKOV -0.9108536 -2.2390912 1.9063730 0.09501304 ## ZSIVOCZKY -0.1018764 -1.0694498 -2.0596722 0.07056229 ## McMULLEN 0.2353742 -0.9215376 -0.8028425 1.17942532 The individual columns of \\(\\mathbf{T}\\) successively inherit the maximum possible variance from \\(\\mathbf{X}\\), with each coefficient vector in \\(\\mathbf{V}\\) constrained to be a unit vector. In \\(\\mathbf{T}=\\mathbf{X V}\\), \\(\\mathbf{V}\\) is a \\(p \\times p\\) matrix of weights whose columns are the eigenvectors of \\(\\mathbf{X}^{\\top} \\mathbf{X}\\). The columns of \\(\\mathbf{V}\\) multiplied by the square root of corresponding eigenvalues, that is, eigenvectors scaled up by the variances, are called loadings in PCA and Factor analysis. Note that if we make a singular value decomposition for a covariance matrix \\[ \\begin{aligned} \\mathbf{X}^{T} \\mathbf{X} &amp;=\\mathbf{V} \\mathbf{\\Sigma}^{\\top} \\mathbf{U}^{\\top} \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{\\top} \\\\ &amp;=\\mathbf{V} \\mathbf{\\Sigma}^{\\top} \\mathbf{\\Sigma} \\mathbf{V}^{\\top} \\\\ &amp;=\\mathbf{V} \\hat{\\mathbf{\\Sigma}}^{2} \\mathbf{V}^{\\top} \\end{aligned} \\] where \\(\\hat{\\boldsymbol{\\Sigma}}\\) is the square diagonal matrix with the singular values of \\(\\mathbf{X}\\) and the excess zeros are chopped off so that it satisfies \\(\\hat{\\boldsymbol{\\Sigma}}^{2}=\\boldsymbol{\\Sigma}^{\\top} \\boldsymbol{\\Sigma}\\). Comparison with the eigenvector factorization of \\(\\mathbf{X}^{\\top} \\mathbf{X}\\) establishes that the right singular vectors \\(\\mathbf{V}\\) of \\(\\mathbf{X}\\) are equivalent to the eigenvectors of \\(\\mathbf{X}^{\\top} \\mathbf{X}\\), while the singular values \\(\\sigma_{(k)}\\) of \\(\\mathbf{X}\\) are equal to the square-root of the eigenvalues \\(\\lambda_{(k)}\\) of \\(\\mathbf{X}^{\\top} \\mathbf{X}\\). 29.1 Factor Analysis Factor analysis and Principal Component Analysis (PCA) both involve reducing the dimensionality of a dataset, but they are not the same. PCA is a mathematical technique that transforms a dataset of possibly correlated variables into a smaller set of uncorrelated variables known as principal components. The principal components are linear combinations of the original variables, and each principal component accounts for as much of the variation in the data as possible. Factor Analysis (FA) is a method for modeling observed variables, and their covariance structure, in terms of a smaller number of underlying latent (unobserved) “factors”. In FA the observed variables are modeled as linear functions of the “factors.” In PCA, we create new variables that are linear combinations of the observed variables. In both PCA and FA, the dimension of the data is reduced. The main difference between FA and PCA lies in their objectives. PCA aims to reduce the number of variables by identifying the most important components, while factor analysis aims to identify the underlying factors that explain the correlations among the variables. Therefore, PCA is more commonly used for data reduction or data compression, while factor analysis is more commonly used for exploring the relationships among variables. As shown below, a factor model can be represented by as a series of multiple regressions, where each \\(X_{i}\\) (\\(i = 1, \\cdots, p\\)) is a function of \\(m\\) number of unobservable common factors \\(f_{i}\\): \\[ \\begin{gathered} X_{1}=\\mu_{1}+\\beta_{11} f_{1}+\\beta_{12} f_{2}+\\cdots+\\beta_{1m} f_{m}+\\epsilon_{1} \\\\ X_{2}=\\mu_{2}+\\beta_{21} f_{1}+\\beta_{22} f_{2}+\\cdots+\\beta_{2 m} f_{m}+\\epsilon_{2} \\\\ \\vdots \\\\ X_{p}=\\mu_{p}+\\beta_{p 1} f_{1}+\\beta_{p 2} f_{2}+\\cdots+\\beta_{p m} f_{m}+\\epsilon_{p} \\end{gathered} \\] where \\(\\mathrm{E}\\left(X_i\\right)=\\mu_i\\), \\(\\epsilon_{i}\\) are called the specific factors. The coefficients, \\(\\beta_{i j},\\) are the factor loadings. We can expressed all of them in a matrix notation. \\[\\begin{equation} \\mathbf{X}=\\boldsymbol{\\mu}+\\mathbf{L f}+\\boldsymbol{\\epsilon} \\end{equation}\\] where \\[ \\mathbf{L}=\\left(\\begin{array}{cccc} \\beta_{11} &amp; \\beta_{12} &amp; \\ldots &amp; \\beta_{1 m} \\\\ \\beta_{21} &amp; \\beta_{22} &amp; \\ldots &amp; \\beta_{2 m} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ \\beta_{p 1} &amp; \\beta_{p 2} &amp; \\ldots &amp; \\beta_{p m} \\end{array}\\right) \\] There are multiple assumptions: \\(E\\left(\\epsilon_{i}\\right)=0\\) and \\(\\operatorname{var}\\left(\\epsilon_{i}\\right)=\\psi_{i}\\), which is called as “specific variance”, \\(E\\left(f_{i}\\right)=0\\) and \\(\\operatorname{var}\\left(f_{i}\\right)=1\\), \\(\\operatorname{cov}\\left(f_{i}, f_{j}\\right)=0\\) for \\(i \\neq j\\), \\(\\operatorname{cov}\\left(\\epsilon_{i}, \\epsilon_{j}\\right)=0\\) for \\(i \\neq j\\), \\(\\operatorname{cov}\\left(\\epsilon_{i}, f_{j}\\right)=0\\), Given these assumptions, the variance of \\(X_i\\) can be expressed as \\[ \\operatorname{var}\\left(X_{i}\\right)=\\sigma_{i}^{2}=\\sum_{j=1}^{m} \\beta_{i j}^{2}+\\psi_{i} \\] There are two sources of the variance in \\(X_i\\): \\(\\sum_{j=1}^{m} \\beta_{i j}^{2}\\), which is called the Communality for variable \\(i\\), and specific variance, \\(\\psi_{i}\\). Moreover, \\(\\operatorname{cov}\\left(X_{i}, X_{j}\\right)=\\sigma_{i j}=\\sum_{k=1}^{m} l_{i k} l_{j k}\\), \\(\\operatorname{cov}\\left(X_{i}, f_{j}\\right)=l_{i j}\\) The factor model for our variance-covariance matrix of \\(\\mathbf{X}\\) can then be expressed as: \\[ \\begin{equation} \\operatorname{var-cov}(\\mathbf{X}) = \\Sigma=\\mathbf{L L}^{\\prime}+\\mathbf{\\Psi} \\end{equation} \\] which is the sum of the shared variance with another variable, \\(\\mathbf{L} \\mathbf{L}^{\\prime}\\) (the common variance or communality) and the unique variance, \\(\\mathbf{\\Psi}\\), inherent to each variable (specific variance) We need to look at \\(\\mathbf{L L}^{\\prime}\\), where \\(\\mathbf{L}\\) is the \\(p \\times m\\) matrix of loadings. In general, we want to have \\(m \\ll p\\). The \\(i^{\\text {th }}\\) diagonal element of \\(\\mathbf{L L}^{\\prime}\\), the sum of the squared loadings, is called the \\(i^{\\text {th }}\\) communality. The communality values represent the percent of variability explained by the common factors. The sizes of the communalities and/or the specific variances can be used to evaluate the goodness of fit. To estimate factor loadings with PCA, we first calculate the principal components of the data, and then compute the factor loadings using the eigenvectors of the correlation matrix of the standardized data. When PCA is used, the matrix of estimated factor loadings, \\(\\mathbf{L},\\) is given by: \\[ \\widehat{\\mathbf{L}}=\\left[\\begin{array}{lll} \\sqrt{\\hat{\\lambda}_1} \\hat{\\mathbf{v}}_1 &amp; \\sqrt{\\hat{\\lambda}_2} \\hat{\\mathbf{v}}_2 &amp; \\ldots \\sqrt{\\hat{\\lambda}_m} \\hat{\\mathbf{v}}_m \\end{array}\\right] \\] where \\[ \\hat{\\beta}_{i j}=\\hat{\\mathbf{v}}_{i j} \\sqrt{\\hat{\\lambda}_j} \\] where \\(i\\) is the index of the original variable, \\(j\\) is the index of the principal component, eigenvector \\((i,j)\\) is the \\(i\\)-th component of the \\(j\\)-th eigenvector of the correlation matrix, eigenvalue \\((j)\\) is the \\(j\\)-th eigenvalue of the correlation matrix This method tries to find values of the loadings that bring the estimate of the total communality close to the total of the observed variances. The covariances are ignored. Remember, the communality is the part of the variance of the variable that is explained by the factors. So a larger communality means a more successful factor model in explaining the variable. Let’s have an example. The data set is called bfi and comes from the psych package. The data includes 25 self-reported personality items from the International Personality Item Pool, gender, education level, and age for 2800 subjects. The personality items are split into 5 categories: Agreeableness (A), Conscientiousness (C), Extraversion (E), Neuroticism (N), Openness (O). Each item was answered on a six point scale: 1 Very Inaccurate to 6 Very Accurate. library(psych) library(GPArotation) data(&quot;bfi&quot;) str(bfi) ## &#39;data.frame&#39;: 2800 obs. of 28 variables: ## $ A1 : int 2 2 5 4 2 6 2 4 4 2 ... ## $ A2 : int 4 4 4 4 3 6 5 3 3 5 ... ## $ A3 : int 3 5 5 6 3 5 5 1 6 6 ... ## $ A4 : int 4 2 4 5 4 6 3 5 3 6 ... ## $ A5 : int 4 5 4 5 5 5 5 1 3 5 ... ## $ C1 : int 2 5 4 4 4 6 5 3 6 6 ... ## $ C2 : int 3 4 5 4 4 6 4 2 6 5 ... ## $ C3 : int 3 4 4 3 5 6 4 4 3 6 ... ## $ C4 : int 4 3 2 5 3 1 2 2 4 2 ... ## $ C5 : int 4 4 5 5 2 3 3 4 5 1 ... ## $ E1 : int 3 1 2 5 2 2 4 3 5 2 ... ## $ E2 : int 3 1 4 3 2 1 3 6 3 2 ... ## $ E3 : int 3 6 4 4 5 6 4 4 NA 4 ... ## $ E4 : int 4 4 4 4 4 5 5 2 4 5 ... ## $ E5 : int 4 3 5 4 5 6 5 1 3 5 ... ## $ N1 : int 3 3 4 2 2 3 1 6 5 5 ... ## $ N2 : int 4 3 5 5 3 5 2 3 5 5 ... ## $ N3 : int 2 3 4 2 4 2 2 2 2 5 ... ## $ N4 : int 2 5 2 4 4 2 1 6 3 2 ... ## $ N5 : int 3 5 3 1 3 3 1 4 3 4 ... ## $ O1 : int 3 4 4 3 3 4 5 3 6 5 ... ## $ O2 : int 6 2 2 3 3 3 2 2 6 1 ... ## $ O3 : int 3 4 5 4 4 5 5 4 6 5 ... ## $ O4 : int 4 3 5 3 3 6 6 5 6 5 ... ## $ O5 : int 3 3 2 5 3 1 1 3 1 2 ... ## $ gender : int 1 2 2 2 1 2 1 1 1 2 ... ## $ education: int NA NA NA NA NA 3 NA 2 1 NA ... ## $ age : int 16 18 17 17 17 21 18 19 19 17 ... To get rid of missing observations and the last three variables, df &lt;- bfi[complete.cases(bfi[, 1:25]), 1:25] The first decision that we need make is the number of factors that we will need to extract. For \\(p=25\\), the variance-covariance matrix \\(\\Sigma\\) contains \\[ \\frac{p(p+1)}{2}=\\frac{25 \\times 26}{2}=325 \\] unique elements or entries. With \\(m\\) factors, the number of parameters in the factor model would be \\[ p(m+1)=25(m+1) \\] Taking \\(m=5\\), we have 150 parameters in the factor model. How do we choose \\(m\\)? Although it is common to look at the results of the principal components analysis, often in social sciences, the underlying theory within the field of study indicates how many factors to expect. scree(df) Let’s use the factanal() function of the build-in stats package, which performs maximum likelihood estimation. pa.out &lt;- factanal(df, factors = 5) pa.out ## ## Call: ## factanal(x = df, factors = 5) ## ## Uniquenesses: ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 ## 0.830 0.576 0.466 0.691 0.512 0.660 0.569 0.677 0.510 0.557 0.634 0.454 0.558 ## E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5 ## 0.468 0.592 0.271 0.337 0.478 0.507 0.664 0.675 0.744 0.518 0.752 0.726 ## ## Loadings: ## Factor1 Factor2 Factor3 Factor4 Factor5 ## A1 0.104 -0.393 ## A2 0.191 0.144 0.601 ## A3 0.280 0.110 0.662 ## A4 0.181 0.234 0.454 -0.109 ## A5 -0.124 0.351 0.580 ## C1 0.533 0.221 ## C2 0.624 0.127 0.140 ## C3 0.554 0.122 ## C4 0.218 -0.653 ## C5 0.272 -0.190 -0.573 ## E1 -0.587 -0.120 ## E2 0.233 -0.674 -0.106 -0.151 ## E3 0.490 0.315 0.313 ## E4 -0.121 0.613 0.363 ## E5 0.491 0.310 0.120 0.234 ## N1 0.816 -0.214 ## N2 0.787 -0.202 ## N3 0.714 ## N4 0.562 -0.367 -0.192 ## N5 0.518 -0.187 0.106 -0.137 ## O1 0.182 0.103 0.524 ## O2 0.163 -0.113 0.102 -0.454 ## O3 0.276 0.153 0.614 ## O4 0.207 -0.220 0.144 0.368 ## O5 -0.512 ## ## Factor1 Factor2 Factor3 Factor4 Factor5 ## SS loadings 2.687 2.320 2.034 1.978 1.557 ## Proportion Var 0.107 0.093 0.081 0.079 0.062 ## Cumulative Var 0.107 0.200 0.282 0.361 0.423 ## ## Test of the hypothesis that 5 factors are sufficient. ## The chi square statistic is 1490.59 on 185 degrees of freedom. ## The p-value is 1.22e-202 The first chunk provides the “uniqueness” (specific variance) for each variable, which range from 0 to 1 . The uniqueness explains the proportion of variability, which cannot be explained by a linear combination of the factors. That’s why it’s referred to as noise. This is the \\(\\hat{\\Psi}\\) in the equation above. A high uniqueness for a variable implies that the factors are not the main source of its variance. The next section reports the loadings ranging from \\(-1\\) to \\(1.\\) This is the \\(\\hat{\\mathbf{L}}\\) in the equation (31.2) above. Variables with a high loading are well explained by the factor. Note that R does not print loadings less than \\(0.1\\). The communalities for the \\(i^{t h}\\) variable are computed by taking the sum of the squared loadings for that variable. This is expressed below: \\[ \\hat{h}_i^2=\\sum_{j=1}^m \\hat{l}_{i j}^2 \\] A well-fit factor model has low values for uniqueness and high values for communality. One way to calculate the communality is to subtract the uniquenesses from 1. apply(pa.out$loadings ^ 2, 1, sum) # communality ## A1 A2 A3 A4 A5 C1 C2 C3 ## 0.1703640 0.4237506 0.5337657 0.3088959 0.4881042 0.3401202 0.4313729 0.3227542 ## C4 C5 E1 E2 E3 E4 E5 N1 ## 0.4900773 0.4427531 0.3659303 0.5459794 0.4422484 0.5319941 0.4079732 0.7294156 ## N2 N3 N4 N5 O1 O2 O3 O4 ## 0.6630751 0.5222584 0.4932099 0.3356293 0.3253527 0.2558864 0.4815981 0.2484000 ## O5 ## 0.2740596 1 - apply(pa.out$loadings ^ 2, 1, sum) # uniqueness ## A1 A2 A3 A4 A5 C1 C2 C3 ## 0.8296360 0.5762494 0.4662343 0.6911041 0.5118958 0.6598798 0.5686271 0.6772458 ## C4 C5 E1 E2 E3 E4 E5 N1 ## 0.5099227 0.5572469 0.6340697 0.4540206 0.5577516 0.4680059 0.5920268 0.2705844 ## N2 N3 N4 N5 O1 O2 O3 O4 ## 0.3369249 0.4777416 0.5067901 0.6643707 0.6746473 0.7441136 0.5184019 0.7516000 ## O5 ## 0.7259404 The table under the loadings reports the proportion of variance explained by each factor. Proportion Var shows the proportion of variance explained by each factor. The row Cumulative Var is the cumulative Proportion Var. Finally, the row SS loadings reports the sum of squared loadings. This can be used to determine a factor worth keeping (Kaiser Rule). The last section of the output reports a significance test: The null hypothesis is that the number of factors in the model is sufficient to capture the full dimensionality of the data set. Hence, in our example, we fitted not an appropriate model. Finally, we may compare estimated correlation matrix, \\(\\hat{\\Sigma}\\) and the observed correlation matrix: Lambda &lt;- pa.out$loadings Psi &lt;- diag(pa.out$uniquenesses) Sigma_hat &lt;- Lambda %*% t(Lambda) + Psi head(Sigma_hat) ## A1 A2 A3 A4 A5 C1 ## A1 1.00000283 -0.2265272 -0.2483489 -0.1688548 -0.2292686 -0.03259104 ## A2 -0.22652719 0.9999997 0.4722224 0.3326049 0.4275597 0.13835721 ## A3 -0.24834886 0.4722224 1.0000003 0.3686079 0.4936403 0.12936294 ## A4 -0.16885485 0.3326049 0.3686079 1.0000017 0.3433611 0.13864850 ## A5 -0.22926858 0.4275597 0.4936403 0.3433611 1.0000000 0.11450065 ## C1 -0.03259104 0.1383572 0.1293629 0.1386485 0.1145007 1.00000234 ## C2 C3 C4 C5 E1 E2 ## A1 -0.04652882 -0.04791267 0.02951427 0.03523371 0.02815444 0.0558511 ## A2 0.17883430 0.15482391 -0.12081021 -0.13814633 -0.18266118 -0.2297604 ## A3 0.16516097 0.14465370 -0.11034318 -0.14189597 -0.24413210 -0.2988190 ## A4 0.18498354 0.18859903 -0.18038121 -0.21194824 -0.14856332 -0.2229227 ## A5 0.12661287 0.12235652 -0.12717601 -0.17194282 -0.28325012 -0.3660944 ## C1 0.37253491 0.30456798 -0.37410412 -0.31041193 -0.03649401 -0.1130997 ## E3 E4 E5 N1 N2 N3 ## A1 -0.1173724 -0.1247436 -0.03147217 0.17738934 0.16360231 0.07593942 ## A2 0.3120607 0.3412253 0.22621235 -0.09257735 -0.08847947 -0.01009412 ## A3 0.3738780 0.4163964 0.26694168 -0.10747302 -0.10694310 -0.02531812 ## A4 0.2124931 0.3080580 0.18727470 -0.12916711 -0.13311708 -0.08204545 ## A5 0.3839224 0.4444111 0.27890083 -0.20296456 -0.20215265 -0.13177926 ## C1 0.1505643 0.0926584 0.24950915 -0.05014855 -0.02623443 -0.04635470 ## N4 N5 O1 O2 O3 O4 ## A1 0.03711760 0.01117121 -0.05551140 0.002021464 -0.08009528 -0.066085241 ## A2 -0.07360446 0.03103525 0.13218881 0.022852428 0.19161373 0.069742928 ## A3 -0.10710549 0.01476409 0.15276068 0.028155055 0.22602542 0.058986934 ## A4 -0.15287214 -0.01338970 0.03924192 0.059069402 0.06643848 -0.034070120 ## A5 -0.20800410 -0.08388175 0.16602866 -0.008940967 0.23912037 0.008904693 ## C1 -0.10426314 -0.05989091 0.18543197 -0.154255652 0.19444225 0.063216945 ## O5 ## A1 0.03042630 ## A2 -0.03205802 ## A3 -0.03274830 ## A4 0.03835946 ## A5 -0.05224734 ## C1 -0.15426539 Let’s check the differences: round(head(cor(df)) - head(Sigma_hat), 2) ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 ## A1 0.00 -0.12 -0.03 0.01 0.04 0.05 0.06 0.03 0.09 0.00 0.08 0.03 ## A2 -0.12 0.00 0.03 0.02 -0.03 -0.04 -0.05 0.03 -0.03 0.02 -0.04 -0.01 ## A3 -0.03 0.03 0.00 0.02 0.02 -0.02 -0.02 -0.02 -0.01 -0.01 0.03 0.01 ## A4 0.01 0.02 0.02 0.00 -0.02 -0.04 0.04 -0.06 0.01 -0.04 0.01 0.01 ## A5 0.04 -0.03 0.02 -0.02 0.00 0.02 -0.01 0.01 0.00 0.00 0.03 0.03 ## C1 0.05 -0.04 -0.02 -0.04 0.02 0.00 0.07 0.01 0.01 0.05 0.01 0.01 ## E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 ## A1 0.07 0.05 0.01 -0.01 -0.02 0.02 0.01 0.00 0.06 0.06 0.02 -0.02 ## A2 -0.06 -0.04 0.07 0.00 0.04 -0.03 -0.01 -0.01 -0.01 -0.01 -0.03 0.01 ## A3 0.01 -0.03 -0.01 0.02 0.01 -0.01 -0.02 -0.05 0.00 -0.02 0.00 -0.03 ## A4 -0.01 0.01 -0.02 0.02 -0.02 0.01 -0.02 0.00 0.02 -0.02 0.00 -0.02 ## A5 0.03 0.04 -0.01 0.00 0.00 -0.01 -0.01 0.00 0.00 0.00 0.00 0.00 ## C1 -0.02 0.06 0.02 -0.02 -0.01 0.02 0.01 0.01 -0.01 0.02 0.00 0.04 ## O5 ## A1 0.07 ## A2 -0.05 ## A3 -0.01 ## A4 -0.01 ## A5 0.00 ## C1 0.02 This matrix is also called as the residual matrix. For extracting and visualizing the results of factor analysis, we can use the factoextra package: https://cran.r-project.org/web/packages/factoextra/readme/README.html "],["smoothing.html", "Chapter 30 Smoothing 30.1 Using bins 30.2 Kernel smoothing 30.3 Locally weighted regression loess() 30.4 Smooth Spline Regression 30.5 Multivariate Loess", " Chapter 30 Smoothing The main reason for using smoothing methods is noise reduction, which makes patterns and trends in the data more noticeable and easier to analyze for the improved accuracy of predictions made from the data. You can think of smoothing as a process that reduces the effect of noise in the data. We can define \\(Y_i\\) with a following model: \\[ Y_{i}=f\\left(x_{i}\\right)+\\varepsilon_{i} \\] We do not want to (and cannot) predict \\(Y_i\\) as we do not know the random part, \\(\\epsilon_i\\), the “noise”. If we predict \\(f(x)\\) well, it would give us a good approximation about \\(Y_i\\). Nonparametric estimations can be helpful for recovering \\(f(x)\\). In general, the purposes of smoothing is two-fold: building a forecasting model by smoothing and learning the shape of the trend embedded in the data (\\(Y\\)). The mcycle dataset from the MASS package contains \\(n=133\\) pairs of time points (in ms - milliseconds) and observed head accelerations (in g - acceleration of gravity) that were recorded in a simulated motorcycle accident. We will have several smoothing methods to explore the relationship between time and acceleration. First, let’s visualize the relationship between time \\((X)\\) and acceleration \\((Y)\\) and see if we can assume that \\(f(x_i)\\) is a linear function of time: library(tidyverse) library(MASS) data(mcycle) head(mcycle) ## times accel ## 1 2.4 0.0 ## 2 2.6 -1.3 ## 3 3.2 -2.7 ## 4 3.6 0.0 ## 5 4.0 -2.7 ## 6 6.2 -2.7 plot(mcycle$times, mcycle$accel, cex.axis = 0.75, cex.main = 0.8) # linear regression lines(mcycle$times, predict(lm(accel ~ times, mcycle)), lwd = 2, col = &quot;red&quot;) The line does not appear to describe the trend very well. 30.1 Using bins As we have seen before, the main idea is to group data points into bins (equal size) in which the value of \\(f(x)\\) can be assumed to be constant. This assumption could be realistic because if we consider \\(f(x)\\) is almost constant in small windows of time. After deciding the window size (say, 10ms), we find out how many observations (\\(Y_i\\)) we have in those 10-ms windows. We can calculate the number of observations in a 10-ms window centered around \\(x_i\\) satisfying the following condition: \\[ \\left(x-\\frac{10}{2}&lt;x_{i}&lt;x+\\frac{10}{2}\\right) \\] When we apply condition such as this for each observation of \\(x_i\\), we create a moving 10-ms window. Note that the window is established by adding half of 10ms to \\(x_i\\) to get the forward half and subtracting it from \\(x_i\\) to get the backward half. When we identify all the observations in each window, we estimate \\(f(x)\\) as the average of the \\(Y_i\\) values in that window. If we define \\(A_0\\) as the set of indexes in each window and \\(N_0\\) as the number of observation in each window, with our data, computing \\(f(x)\\) can be expressed as \\[\\begin{equation} \\hat{f}\\left(x_{0}\\right)=\\frac{1}{N_{0}} \\sum_{i \\in A_{0}} Y_{i} \\tag{30.1} \\end{equation}\\] Here is its application to our data: #With ksmooth() Pay attention to &quot;box&quot; fit1 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;box&quot;, bandwidth = 7)) fit2 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;box&quot;, bandwidth = 10)) fit3 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;box&quot;, bandwidth = 21)) plot(mcycle$times, mcycle$accel, xlab = &quot;Time (ms)&quot;, ylab = &quot;Acceleration (g)&quot;) lines(mcycle$times, fit1$y, lwd = 2, col = &quot;blue&quot;) lines(mcycle$times, fit2$y, lwd = 2, col = &quot;red&quot;) lines(mcycle$times, fit3$y, lwd = 2, col = &quot;green&quot;) As you can see, even if we use a shorter bandwidth, the lines are quite wiggly. 30.2 Kernel smoothing We can take care of this by taking weighted averages that give the center points more weight than far away points. #With ksmooth() Pay attention to &quot;box&quot; fit1 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;normal&quot;, bandwidth = 7)) fit2 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;normal&quot;, bandwidth = 10)) fit3 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;normal&quot;, bandwidth = 21)) plot(mcycle$times, mcycle$accel, xlab = &quot;Time (ms)&quot;, ylab = &quot;Acceleration (g)&quot;) lines(mcycle$times, fit1$y, lwd = 2, col = &quot;blue&quot;) lines(mcycle$times, fit2$y, lwd = 2, col = &quot;red&quot;) lines(mcycle$times, fit3$y, lwd = 2, col = &quot;green&quot;) Now, they look smoother. There are several functions in R that implement bin smoothing. One example is ksmooth, shown above. As we have seen before, however, we typically prefer methods such as loess that improves on these methods fitting a constant. 30.3 Locally weighted regression loess() A limitation of the bin smoother approach by ksmooth() is that we need small windows for the approximately constant assumptions to hold. Now loess() permits us to consider larger window sizes. #With loess() fit1 &lt;- loess(accel ~ times, degree = 1, span = 0.1, mcycle) fit2 &lt;-loess(accel ~ times, degree = 1, span = 0.9, mcycle) summary(fit1) ## Call: ## loess(formula = accel ~ times, data = mcycle, span = 0.1, degree = 1) ## ## Number of Observations: 133 ## Equivalent Number of Parameters: 18.57 ## Residual Standard Error: 22.93 ## Trace of smoother matrix: 22.01 (exact) ## ## Control settings: ## span : 0.1 ## degree : 1 ## family : gaussian ## surface : interpolate cell = 0.2 ## normalize: TRUE ## parametric: FALSE ## drop.square: FALSE plot(mcycle$times, mcycle$accel, xlab = &quot;Time (ms)&quot;, ylab = &quot;Acceleration (g)&quot;) lines(mcycle$times, fit1$fitted, lwd = 2, col = &quot;blue&quot;) lines(mcycle$times, fit2$fitted, lwd = 2, col = &quot;red&quot;) It seems the “red” line is underfitting the data. We can make our windows even larger by fitting parabolas instead of lines. fit1 &lt;- loess(accel ~ times, degree = 1, span = 0.1, data = mcycle) fit2 &lt;-loess(accel ~ times, degree = 2, span = 0.1, data = mcycle) plot(mcycle$times, mcycle$accel, xlab = &quot;Time (ms)&quot;, ylab = &quot;Acceleration (g)&quot;) lines(mcycle$times, fit1$fitted, lwd = 2, col = &quot;blue&quot;) lines(mcycle$times, fit2$fitted, lwd = 2, col = &quot;green&quot;) 30.4 Smooth Spline Regression We can also use npreg package with ss() function for automated smooth splines library(npreg) fit3 &lt;- with(mcycle, npreg::ss(times, accel)) fit3 ## ## Call: ## npreg::ss(x = times, y = accel) ## ## Smoothing Parameter spar = 0.1585867 lambda = 8.337283e-07 ## Equivalent Degrees of Freedom (Df) 12.20781 ## Penalized Criterion (RSS) 62034.66 ## Generalized Cross-Validation (GCV) 565.4684 plot(fit3, xlab = &quot;Time (ms)&quot;, ylab = &quot;Acceleration (g)&quot;, col = &quot;orange&quot;) rug(mcycle$times) # add rug to plot for the precise location of each point The gray shaded area denotes a 95% Bayesian “confidence interval” for the unknown function. 30.5 Multivariate Loess When there are more than two predictors, it is always advisable to use additive models either GAM or MARS. Nevertheless, let’s try loess() with several variables. This dataset is from from St.Louis Federal Reserve. It has 6 variables (observed monthly): psavert, personal savings rate; pce, personal consumption expenditures (in billions of dollars); unemploy, number of unemployed (in thousands), uempmed median duration of unemployment (weeks), and pop total population (in thousands). Although we have a time variable, date, we create an index for time. data(economics, package = &quot;ggplot2&quot;) str(economics) ## spc_tbl_ [574 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ date : Date[1:574], format: &quot;1967-07-01&quot; &quot;1967-08-01&quot; ... ## $ pce : num [1:574] 507 510 516 512 517 ... ## $ pop : num [1:574] 198712 198911 199113 199311 199498 ... ## $ psavert : num [1:574] 12.6 12.6 11.9 12.9 12.8 11.8 11.7 12.3 11.7 12.3 ... ## $ uempmed : num [1:574] 4.5 4.7 4.6 4.9 4.7 4.8 5.1 4.5 4.1 4.6 ... ## $ unemploy: num [1:574] 2944 2945 2958 3143 3066 ... economics$index &lt;- 1:nrow(economics) fit1 &lt;- loess(uempmed ~ index, data = economics, span = 0.25) # 25% smoothing span RRSS_1 &lt;- sqrt(mean((fit1$residuals) ^ 2)) RRSS_1 ## [1] 1.192171 plot(economics$index, economics$uempmed, cex.axis = 0.75, cex.main = 0.8, xlab = &quot;Time index - months&quot;, ylab = &quot;Unemployment duration - weeks&quot;, col = &quot;grey&quot;) lines(economics$index, fit1$fitted, lwd = 1, col = &quot;red&quot;) #Now more predictors fit2 &lt;- loess(uempmed ~ pce + psavert + pop + index, data = economics, span = 2) RRSS_2 &lt;- sqrt(mean((fit2$residuals) ^ 2)) RRSS_2 ## [1] 58.61336 "],["imbalanced-data.html", "Chapter 31 Imbalanced Data 31.1 SMOTE 31.2 Fraud detection", " Chapter 31 Imbalanced Data Classification with imbalanced data is characterized by the uneven proportion of cases that are available for each class, and causes problems in many learning algorithms. An imbalance in the data is usually considered an issue when the distribution of classes is skewed more than 60-40% ratio. There are two simple methods to overcome imbalance data in classification problems: oversampling and undersampling, both of which are used to adjust the class distribution in a data set. While oversampling simply randomly replicates the minority class, undersampling randomly selects a subset of majority class and reduces the overall sample size. Thus, it can discard useful data. There are also more complex oversampling techniques, including the creation of artificial data points. The most common technique is known as SMOTE, Synthetic Minority Oversampling Technique Chawla et al. [-(Chawla_2002?). This method generates synthetic data based on the feature space similarities between existing minority instances. In order to create a synthetic instance, it finds the k-nearest neighbors of each minority instance, randomly selects one of them, and then calculate linear interpolations to produce a new minority instance in the neighborhood. The other methods is the adaptive synthetic sampling approach (ADASYN), which builds on the methodology of SMOTE, by shifting the importance of the classification boundary to those minority classes. In this chapter we will see some examples using only SMOTE. 31.1 SMOTE We will use Credit Card Fraud Detection dataset on Kaggle {-(Kaggle_Cred?)}. The dataset has about 300K anonymized credit card transfers labeled as fraudulent or genuine. The features are numerical and anonymized (V1, V2, … , V28). They are the principal components obtained with principal component analysis (PCA). The only features which have not been transformed with PCA are Time and Amount. Feature Time contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature Amount is the transaction Amount and Class is the response variable and it takes value 1 in case of fraud and 0 otherwise. The prediction problem is to label transactions as fraud or not. We will use only a subset of data with roughly 10K observations, representing transactions. library(tidyverse) library(ROCR) library(smotefamily) library(randomForest) head(creditcard10) ## # A tibble: 6 × 31 ## Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 77319 -0.278 0.924 1.40 0.833 0.0318 -0.619 0.592 0.0361 -0.751 ## 2 130219 0.916 -2.70 -3.26 -0.00660 -0.504 -1.14 1.26 -0.609 -1.18 ## 3 42328 -2.25 -1.03 0.937 0.198 1.01 -2.00 -0.754 0.691 -0.423 ## 4 51453 -0.386 0.766 0.850 0.195 0.850 0.188 0.702 0.0700 0.0516 ## 5 48711 -1.04 0.240 1.53 -0.0509 1.80 0.650 0.556 0.172 -0.288 ## 6 125697 1.52 -1.92 -2.33 -0.586 -0.468 -0.837 0.387 -0.440 -0.456 ## # ℹ 21 more variables: V10 &lt;dbl&gt;, V11 &lt;dbl&gt;, V12 &lt;dbl&gt;, V13 &lt;dbl&gt;, V14 &lt;dbl&gt;, ## # V15 &lt;dbl&gt;, V16 &lt;dbl&gt;, V17 &lt;dbl&gt;, V18 &lt;dbl&gt;, V19 &lt;dbl&gt;, V20 &lt;dbl&gt;, ## # V21 &lt;dbl&gt;, V22 &lt;dbl&gt;, V23 &lt;dbl&gt;, V24 &lt;dbl&gt;, V25 &lt;dbl&gt;, V26 &lt;dbl&gt;, ## # V27 &lt;dbl&gt;, V28 &lt;dbl&gt;, Amount &lt;dbl&gt;, Class &lt;dbl&gt; table(creditcard10$Class) ## ## 0 1 ## 28427 53 prop.table(table(creditcard10$Class)) ## ## 0 1 ## 0.998139045 0.001860955 The class balance is way off! The split is approximately 99.83% to 0.017%. df &lt;- creditcard10 plot(df$V1, df$V2, col = (df$Class+1) + 4, lwd = 0.5) ind &lt;- which(df$Class == 1, arr.ind = TRUE) plot(df$V1[ind], df$V2[ind], col = &quot;red&quot;, lwd = 0.7) The idea behind SMOTE is very simple: take a red point (fraud) say Jason, find the k nearest neighbors to Jason by using all features. Then we randomly pick one observation among these, let’s say, 5 neighbors. Suppose that person is Mervin. Now, also suppose that we have only two features: V1 and V2 Observations V1 V2 Jason -12.5 -5.0 Mervin -10.5 -2.5 Then the new synthetic point will be created by \\(V1 = -12.5 + r(-10.5-(-12.5)) = -12.5 +r2.0\\) and \\(V2 = -5 + r(-2.5-(-5)) = -5 +r2.5\\), where \\(r\\) is a random number between 0 and 1. If it’s 0.7, for example, the new synthetic observation will be added to data: Observations V1 V2 Jason -12.5 -5.0 Mervin -10.5 -2.5 Synthetic -11.1 -3.25 This is one synthetic observation created form a real observation, Tim. We can repeat it 10, 20 times and create many synthetic observations from Tim. And then we can repeat it for each real cases of fraud. library(smotefamily) df$Class &lt;- as.factor(df$Class) outc &lt;- SMOTE(X = df[, -31], target = df$Class, K = 4, dup_size = 10) over_df = outc$data table(over_df$class) ## ## 0 1 ## 28427 583 prop.table(table(over_df$class)) ## ## 0 1 ## 0.97990348 0.02009652 Or, with higher K and dup_size: library(smotefamily) df$Class &lt;- as.factor(df$Class) outc &lt;- SMOTE(X = df[, -31], target = df$Class, K = 4, dup_size = 50) over_df = outc$data table(over_df$class) ## ## 0 1 ## 28427 2703 prop.table(table(over_df$class)) ## ## 0 1 ## 0.91317058 0.08682942 And, here is the new plot with expanded “fraud” cases: ind &lt;- which(over_df$class == 1, arr.ind = TRUE) plot(over_df$V1[ind], over_df$V2[ind], col = &quot;red&quot;, lwd = 0.7) All together: plot(over_df$V1, over_df$V2, col = (as.numeric(over_df$class)+1) + 4, lwd = 0.5) 31.2 Fraud detection Here is what we will do with the fraud data in the following script: Apply SMOTE on training set to balance the class distribution Train a Random Forest model on re-balanced training set Test performance on (original) test set In addition, we will compare with and without balancing. library(ROCR) library(smotefamily) library(randomForest) rm(list = ls()) load(&quot;creditcard10.RData&quot;) df$Class &lt;- as.factor(df$Class) AUCb &lt;- c() AUCimb &lt;- c() n = 10 # Could be 50, since the data is large for RF B = 100 for (i in 1:n) { set.seed(i) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) ind &lt;- unique(ind) # Otherwise it oversamples 0&#39;s train &lt;- df[ind, ] test &lt;- df[-ind, ] # Balancing outdf &lt;- SMOTE(X = train[, -31], target = train$Class, K = 10, dup_size = 50) trainn &lt;- outdf$data trainn$class &lt;- as.factor(trainn$class) #SMOTE makes factor to &quot;chr&quot;! colnames(trainn)[31] &lt;- &quot;Class&quot; #SMOTE made it lower case! modelb &lt;- randomForest(Class~., ntree = B, data = trainn) phatb &lt;- predict(modelb, test, type = &quot;prob&quot;) # Without Balancing modelimb &lt;- randomForest(Class~., ntree = B, data = train) phatimb &lt;- predict(modelimb, test, type = &quot;prob&quot;) #AUCb pred_rocr1 &lt;- prediction(phatb[,2], test$Class) auc_ROCR1 &lt;- performance(pred_rocr1, measure = &quot;auc&quot;) AUCb[i] &lt;- auc_ROCR1@y.values[[1]] #AUCimb pred_rocr1 &lt;- prediction(phatimb[,2], test$Class) auc_ROCR1 &lt;- performance(pred_rocr1, measure = &quot;auc&quot;) AUCimb[i] &lt;- auc_ROCR1@y.values[[1]] } model &lt;- c(&quot;Balanced&quot;, &quot;Imbalanced&quot;) AUCs &lt;- c(mean(AUCb), mean(AUCimb)) sd &lt;- c(sqrt(var(AUCb)), sqrt(var(AUCimb))) data.frame(model, AUCs, sd) ## model AUCs sd ## 1 Balanced 0.9682024 0.02968400 ## 2 Imbalanced 0.9340113 0.03698929 Our results show improved AUC results and lower sd for our model built on the balanced data. "],["text-analysis.html", "Chapter 32 Text Analysis", " Chapter 32 Text Analysis "],["other-nonparametric-estimation-methods.html", "Chapter 33 Other Nonparametric Estimation methods 33.1 Other Nonparametric Estimation methods 33.2 Regression splines 33.3 MARS 33.4 GAM", " Chapter 33 Other Nonparametric Estimation methods 33.1 Other Nonparametric Estimation methods 33.2 Regression splines 33.3 MARS Multivariate Additive Regression Splines (MARS) 33.4 GAM Generalized Additive Models (GAM) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
