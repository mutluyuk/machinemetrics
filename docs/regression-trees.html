<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 16 Regression Trees | MachineMetrics</title>
  <meta name="description" content="Chapter 16 Regression Trees | MachineMetrics" />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 16 Regression Trees | MachineMetrics" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="/png/MachineMetrics.png" />
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 16 Regression Trees | MachineMetrics" />
  
  
  <meta name="twitter:image" content="/png/MachineMetrics.png" />

<meta name="author" content="Yigit Aydede and Mutlu Yuksel" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="shrinkage-models.html"/>
<link rel="next" href="ensemble-methods.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">.</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#why-this-book-is-different"><i class="fa fa-check"></i>Why this book is different?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-manuscript"><i class="fa fa-check"></i>Structure of Manuscript:</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-can-use-this-book"><i class="fa fa-check"></i>Who Can Use This Book?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> INTRODUCTION:</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#prediction-vs.-estimation"><i class="fa fa-check"></i><b>1.1</b> Prediction vs. Estimation:</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#where-can-you-use-the-covered-topics-in-social-sciences"><i class="fa fa-check"></i><b>1.2</b> Where can you use the covered topics in Social Sciences?:</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#translation-of-concepts-different-terminology"><i class="fa fa-check"></i><b>1.3</b> Translation of Concepts: Different Terminology</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#is-machine-learning-better"><i class="fa fa-check"></i><b>1.4</b> Is Machine Learning Better?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html"><i class="fa fa-check"></i><b>2</b> Statistical Models and Simulations</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#distinguishing-between-statistical-modeling-and-machine-learning-in-data-analysis"><i class="fa fa-check"></i><b>2.1</b> Distinguishing Between Statistical Modeling and Machine Learning in Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#goals-and-objectives"><i class="fa fa-check"></i><b>2.1.1</b> Goals and Objectives</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#prediction-vs.-inference"><i class="fa fa-check"></i><b>2.1.2</b> Prediction vs. Inference</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#conclusion"><i class="fa fa-check"></i><b>2.1.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#parametric-and-nonparametric-models"><i class="fa fa-check"></i><b>2.2</b> Parametric and Nonparametric Models:</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#predictive-vs.-causal-models"><i class="fa fa-check"></i><b>2.3</b> Predictive vs. Causal Models</a></li>
<li class="chapter" data-level="2.4" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#model-selection-and-approaches-in-data-modeling"><i class="fa fa-check"></i><b>2.4</b> Model Selection and Approaches in Data Modeling</a></li>
<li class="chapter" data-level="2.5" data-path="statistical-models-and-simulations.html"><a href="statistical-models-and-simulations.html#simulation"><i class="fa fa-check"></i><b>2.5</b> Simulation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>3</b> Counterfactual:</a>
<ul>
<li class="chapter" data-level="3.1" data-path="counterfactual.html"><a href="counterfactual.html#qualitative-and-quantitative-research-methods"><i class="fa fa-check"></i><b>3.1</b> Qualitative and Quantitative research methods:</a></li>
<li class="chapter" data-level="3.2" data-path="counterfactual.html"><a href="counterfactual.html#quantitative---research-methods"><i class="fa fa-check"></i><b>3.2</b> Quantitative - Research methods :</a></li>
<li class="chapter" data-level="3.3" data-path="counterfactual.html"><a href="counterfactual.html#data-and-visualization"><i class="fa fa-check"></i><b>3.3</b> Data and visualization</a></li>
<li class="chapter" data-level="3.4" data-path="counterfactual.html"><a href="counterfactual.html#correlation"><i class="fa fa-check"></i><b>3.4</b> Correlation</a></li>
<li class="chapter" data-level="3.5" data-path="counterfactual.html"><a href="counterfactual.html#effect-of-x-on-y-regression"><i class="fa fa-check"></i><b>3.5</b> Effect of X on Y / Regression</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="counterfactual.html"><a href="counterfactual.html#how-can-we-estimate-the-population-parameters-beta_0-and-beta_1"><i class="fa fa-check"></i><b>3.5.1</b> How can we estimate the population parameters, <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>?</a></li>
<li class="chapter" data-level="3.5.2" data-path="counterfactual.html"><a href="counterfactual.html#predicting-y"><i class="fa fa-check"></i><b>3.5.2</b> Predicting <span class="math inline">\(y\)</span></a></li>
<li class="chapter" data-level="3.5.3" data-path="counterfactual.html"><a href="counterfactual.html#mle"><i class="fa fa-check"></i><b>3.5.3</b> MLE</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="counterfactual.html"><a href="counterfactual.html#causal-effect"><i class="fa fa-check"></i><b>3.6</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="counterfactual.html"><a href="counterfactual.html#average-treatment-effectate"><i class="fa fa-check"></i><b>3.6.1</b> Average Treatment Effect(ATE)</a></li>
<li class="chapter" data-level="3.6.2" data-path="counterfactual.html"><a href="counterfactual.html#additional-treatment-effects"><i class="fa fa-check"></i><b>3.6.2</b> Additional Treatment Effects</a></li>
<li class="chapter" data-level="3.6.3" data-path="counterfactual.html"><a href="counterfactual.html#selection-bias-and-heteregeneous-treatment-effect-bias"><i class="fa fa-check"></i><b>3.6.3</b> Selection Bias and Heteregeneous Treatment Effect Bias:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>4</b> Learning</a>
<ul>
<li class="chapter" data-level="" data-path="learning.html"><a href="learning.html#learning-systems"><i class="fa fa-check"></i>Learning Systems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="error.html"><a href="error.html"><i class="fa fa-check"></i><b>5</b> Error</a>
<ul>
<li class="chapter" data-level="5.1" data-path="error.html"><a href="error.html#estimation-error---mse"><i class="fa fa-check"></i><b>5.1</b> Estimation error - MSE</a></li>
<li class="chapter" data-level="5.2" data-path="error.html"><a href="error.html#prediction-error--mspe"><i class="fa fa-check"></i><b>5.2</b> Prediction error- MSPE</a></li>
<li class="chapter" data-level="5.3" data-path="error.html"><a href="error.html#technical-points-about-mse-and-mspe"><i class="fa fa-check"></i><b>5.3</b> Technical points about MSE and MSPE</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>6</b> Bias-Variance Trade-off</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>6.1</b> Biased estimator as a predictor</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>7</b> Overfitting</a></li>
<li class="chapter" data-level="8" data-path="regression-v.s.-classification.html"><a href="regression-v.s.-classification.html"><i class="fa fa-check"></i><b>8</b> Regression v.s. Classification</a></li>
<li class="chapter" data-level="9" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html"><i class="fa fa-check"></i><b>9</b> Parametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="9.1" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#parametric-estimations"><i class="fa fa-check"></i><b>9.1</b> Parametric Estimations</a></li>
<li class="chapter" data-level="9.2" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#lpm"><i class="fa fa-check"></i><b>9.2</b> LPM</a></li>
<li class="chapter" data-level="9.3" data-path="parametric-estimations---basics.html"><a href="parametric-estimations---basics.html#logistic-regression"><i class="fa fa-check"></i><b>9.3</b> Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>10</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="10.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>10.1</b> Density estimations</a></li>
<li class="chapter" data-level="10.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regression"><i class="fa fa-check"></i><b>10.2</b> Kernel regression</a></li>
<li class="chapter" data-level="10.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#knn"><i class="fa fa-check"></i><b>10.3</b> Knn</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#adult-dataset"><i class="fa fa-check"></i><b>10.3.1</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>11</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="11.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-and-validation"><i class="fa fa-check"></i><b>11.1</b> Training and Validation</a></li>
<li class="chapter" data-level="11.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>11.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="11.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>11.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="11.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#grid-search"><i class="fa fa-check"></i><b>11.4</b> Grid Search</a></li>
<li class="chapter" data-level="11.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>11.5</b> Cross-validated grid search</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html"><i class="fa fa-check"></i><b>12</b> Optimization Algorithms - Basics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#brute-force-optimization"><i class="fa fa-check"></i><b>12.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="12.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#derivative-based-methods"><i class="fa fa-check"></i><b>12.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="12.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>12.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="12.4" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>12.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#one-variable"><i class="fa fa-check"></i><b>12.4.1</b> One-variable</a></li>
<li class="chapter" data-level="12.4.2" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>12.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="12.4.3" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#multivariable"><i class="fa fa-check"></i><b>12.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="optimization-algorithms---basics.html"><a href="optimization-algorithms---basics.html#optimization-with-r"><i class="fa fa-check"></i><b>12.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="prediction-intervals.html"><a href="prediction-intervals.html"><i class="fa fa-check"></i><b>13</b> Prediction Intervals</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction-intervals.html"><a href="prediction-intervals.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>13.1</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a>
<ul>
<li class="chapter" data-level="14.1" data-path="interpretability.html"><a href="interpretability.html#interpretable-vs-noninterpretable-models"><i class="fa fa-check"></i><b>14.1</b> Interpretable vs NonInterpretable Models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="shrinkage-models.html"><a href="shrinkage-models.html"><i class="fa fa-check"></i><b>15</b> Shrinkage Models</a>
<ul>
<li class="chapter" data-level="15.1" data-path="shrinkage-models.html"><a href="shrinkage-models.html#ridge"><i class="fa fa-check"></i><b>15.1</b> Ridge</a></li>
<li class="chapter" data-level="15.2" data-path="shrinkage-models.html"><a href="shrinkage-models.html#lasso"><i class="fa fa-check"></i><b>15.2</b> Lasso</a></li>
<li class="chapter" data-level="15.3" data-path="shrinkage-models.html"><a href="shrinkage-models.html#adaptive-lasso"><i class="fa fa-check"></i><b>15.3</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="15.4" data-path="shrinkage-models.html"><a href="shrinkage-models.html#sparsity"><i class="fa fa-check"></i><b>15.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regression-trees.html"><a href="regression-trees.html"><i class="fa fa-check"></i><b>16</b> Regression Trees</a>
<ul>
<li class="chapter" data-level="16.1" data-path="regression-trees.html"><a href="regression-trees.html#cart---classification-tree"><i class="fa fa-check"></i><b>16.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="16.2" data-path="regression-trees.html"><a href="regression-trees.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>16.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="16.3" data-path="regression-trees.html"><a href="regression-trees.html#pruning"><i class="fa fa-check"></i><b>16.3</b> Pruning</a></li>
<li class="chapter" data-level="16.4" data-path="regression-trees.html"><a href="regression-trees.html#classification-with-titanic"><i class="fa fa-check"></i><b>16.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="16.5" data-path="regression-trees.html"><a href="regression-trees.html#regression-tree"><i class="fa fa-check"></i><b>16.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>17</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="17.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>17.1</b> Bagging</a></li>
<li class="chapter" data-level="17.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>17.2</b> Random Forest</a></li>
<li class="chapter" data-level="17.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>17.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>17.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="17.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost"><i class="fa fa-check"></i><b>17.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="17.3.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#xgboost"><i class="fa fa-check"></i><b>17.3.3</b> XGBoost</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#ensemble-applications"><i class="fa fa-check"></i><b>17.4</b> Ensemble Applications</a></li>
<li class="chapter" data-level="17.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification"><i class="fa fa-check"></i><b>17.5</b> Classification</a></li>
<li class="chapter" data-level="17.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression"><i class="fa fa-check"></i><b>17.6</b> Regression</a></li>
<li class="chapter" data-level="17.7" data-path="ensemble-methods.html"><a href="ensemble-methods.html#exploration"><i class="fa fa-check"></i><b>17.7</b> Exploration</a></li>
<li class="chapter" data-level="17.8" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-applications"><i class="fa fa-check"></i><b>17.8</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="17.8.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#regression-1"><i class="fa fa-check"></i><b>17.8.1</b> Regression</a></li>
<li class="chapter" data-level="17.8.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>17.8.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="17.8.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting-vs.-others"><i class="fa fa-check"></i><b>17.8.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="17.8.4" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-1"><i class="fa fa-check"></i><b>17.8.4</b> Classification</a></li>
<li class="chapter" data-level="17.8.5" data-path="ensemble-methods.html"><a href="ensemble-methods.html#adaboost.m1"><i class="fa fa-check"></i><b>17.8.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="17.8.6" data-path="ensemble-methods.html"><a href="ensemble-methods.html#classification-with-xgboost"><i class="fa fa-check"></i><b>17.8.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="causal-effect-1.html"><a href="causal-effect-1.html"><i class="fa fa-check"></i><b>18</b> Causal Effect</a>
<ul>
<li class="chapter" data-level="18.1" data-path="causal-effect-1.html"><a href="causal-effect-1.html#random-experiment"><i class="fa fa-check"></i><b>18.1</b> Random experiment</a></li>
<li class="chapter" data-level="18.2" data-path="causal-effect-1.html"><a href="causal-effect-1.html#iv"><i class="fa fa-check"></i><b>18.2</b> IV</a></li>
<li class="chapter" data-level="18.3" data-path="causal-effect-1.html"><a href="causal-effect-1.html#diffd"><i class="fa fa-check"></i><b>18.3</b> DiffD</a></li>
<li class="chapter" data-level="18.4" data-path="causal-effect-1.html"><a href="causal-effect-1.html#rd"><i class="fa fa-check"></i><b>18.4</b> RD</a></li>
<li class="chapter" data-level="18.5" data-path="causal-effect-1.html"><a href="causal-effect-1.html#synthetic-control"><i class="fa fa-check"></i><b>18.5</b> Synthetic control</a></li>
<li class="chapter" data-level="18.6" data-path="causal-effect-1.html"><a href="causal-effect-1.html#doubledebiased-lassomethods"><i class="fa fa-check"></i><b>18.6</b> Double/Debiased Lasso/Methods</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html"><i class="fa fa-check"></i><b>19</b> Heterogeneous Treatment Effects</a>
<ul>
<li class="chapter" data-level="19.1" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-tree"><i class="fa fa-check"></i><b>19.1</b> Causal Tree</a></li>
<li class="chapter" data-level="19.2" data-path="heterogeneous-treatment-effects.html"><a href="heterogeneous-treatment-effects.html#causal-forest"><i class="fa fa-check"></i><b>19.2</b> Causal Forest</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html"><i class="fa fa-check"></i><b>20</b> Model selection and Sparsity</a>
<ul>
<li class="chapter" data-level="20.1" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#model-selection"><i class="fa fa-check"></i><b>20.1</b> Model selection</a></li>
<li class="chapter" data-level="20.2" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>20.2</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="20.3" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#out-sample-prediction-accuracy"><i class="fa fa-check"></i><b>20.3</b> out-sample prediction accuracy</a></li>
<li class="chapter" data-level="20.4" data-path="model-selection-and-sparsity.html"><a href="model-selection-and-sparsity.html#sparsity-1"><i class="fa fa-check"></i><b>20.4</b> Sparsity</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="classification-2.html"><a href="classification-2.html"><i class="fa fa-check"></i><b>21</b> Classification</a>
<ul>
<li class="chapter" data-level="21.1" data-path="classification-2.html"><a href="classification-2.html#nonparametric-classifier---knn"><i class="fa fa-check"></i><b>21.1</b> Nonparametric Classifier - kNN</a></li>
<li class="chapter" data-level="21.2" data-path="classification-2.html"><a href="classification-2.html#mnist-dataset"><i class="fa fa-check"></i><b>21.2</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="21.3" data-path="classification-2.html"><a href="classification-2.html#linear-classifiers-again"><i class="fa fa-check"></i><b>21.3</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="21.4" data-path="classification-2.html"><a href="classification-2.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>21.4</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="21.5" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret"><i class="fa fa-check"></i><b>21.5</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="21.5.1" data-path="classification-2.html"><a href="classification-2.html#mnist_27"><i class="fa fa-check"></i><b>21.5.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="21.5.2" data-path="classification-2.html"><a href="classification-2.html#adult-dataset-1"><i class="fa fa-check"></i><b>21.5.2</b> Adult dataset</a></li>
</ul></li>
<li class="chapter" data-level="21.6" data-path="classification-2.html"><a href="classification-2.html#tuning-in-classification"><i class="fa fa-check"></i><b>21.6</b> Tuning in Classification</a></li>
<li class="chapter" data-level="21.7" data-path="classification-2.html"><a href="classification-2.html#confusion-matrix"><i class="fa fa-check"></i><b>21.7</b> Confusion matrix</a></li>
<li class="chapter" data-level="21.8" data-path="classification-2.html"><a href="classification-2.html#performance-measures"><i class="fa fa-check"></i><b>21.8</b> Performance measures</a></li>
<li class="chapter" data-level="21.9" data-path="classification-2.html"><a href="classification-2.html#roc-curve"><i class="fa fa-check"></i><b>21.9</b> ROC Curve</a></li>
<li class="chapter" data-level="21.10" data-path="classification-2.html"><a href="classification-2.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>21.10</b> AUC - Area Under the Curve</a></li>
<li class="chapter" data-level="21.11" data-path="classification-2.html"><a href="classification-2.html#classification-example"><i class="fa fa-check"></i><b>21.11</b> Classification Example</a></li>
<li class="chapter" data-level="21.12" data-path="classification-2.html"><a href="classification-2.html#lpm-1"><i class="fa fa-check"></i><b>21.12</b> LPM</a></li>
<li class="chapter" data-level="21.13" data-path="classification-2.html"><a href="classification-2.html#logistic-regression-1"><i class="fa fa-check"></i><b>21.13</b> Logistic Regression</a></li>
<li class="chapter" data-level="21.14" data-path="classification-2.html"><a href="classification-2.html#knn-1"><i class="fa fa-check"></i><b>21.14</b> kNN</a>
<ul>
<li class="chapter" data-level="21.14.1" data-path="classification-2.html"><a href="classification-2.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>21.14.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="21.14.2" data-path="classification-2.html"><a href="classification-2.html#knn-with-caret-1"><i class="fa fa-check"></i><b>21.14.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>22</b> Time Series</a>
<ul>
<li class="chapter" data-level="22.1" data-path="time-series.html"><a href="time-series.html#arima-models"><i class="fa fa-check"></i><b>22.1</b> ARIMA models</a></li>
<li class="chapter" data-level="22.2" data-path="time-series.html"><a href="time-series.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>22.2</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="22.3" data-path="time-series.html"><a href="time-series.html#ts-plots"><i class="fa fa-check"></i><b>22.3</b> TS Plots</a></li>
<li class="chapter" data-level="22.4" data-path="time-series.html"><a href="time-series.html#box-cox-transformation"><i class="fa fa-check"></i><b>22.4</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="22.5" data-path="time-series.html"><a href="time-series.html#stationarity"><i class="fa fa-check"></i><b>22.5</b> Stationarity</a></li>
<li class="chapter" data-level="22.6" data-path="time-series.html"><a href="time-series.html#modeling-arima"><i class="fa fa-check"></i><b>22.6</b> Modeling ARIMA</a></li>
<li class="chapter" data-level="22.7" data-path="time-series.html"><a href="time-series.html#grid-search-for-arima"><i class="fa fa-check"></i><b>22.7</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="22.8" data-path="time-series.html"><a href="time-series.html#hyperparameter-tuning-with-time-series-data"><i class="fa fa-check"></i><b>22.8</b> Hyperparameter tuning with time-series data:</a></li>
<li class="chapter" data-level="22.9" data-path="time-series.html"><a href="time-series.html#speed"><i class="fa fa-check"></i><b>22.9</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="forecast.html"><a href="forecast.html"><i class="fa fa-check"></i><b>23</b> Forecast</a>
<ul>
<li class="chapter" data-level="23.1" data-path="forecast.html"><a href="forecast.html#time-series-embedding"><i class="fa fa-check"></i><b>23.1</b> Time Series Embedding</a></li>
<li class="chapter" data-level="23.2" data-path="forecast.html"><a href="forecast.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.2</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.3" data-path="forecast.html"><a href="forecast.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.3</b> Embedding for Direct Forecast</a></li>
<li class="chapter" data-level="23.4" data-path="forecast.html"><a href="forecast.html#random-forest-1"><i class="fa fa-check"></i><b>23.4</b> Random Forest</a></li>
<li class="chapter" data-level="23.5" data-path="forecast.html"><a href="forecast.html#univariate"><i class="fa fa-check"></i><b>23.5</b> Univariate</a></li>
<li class="chapter" data-level="23.6" data-path="forecast.html"><a href="forecast.html#multivariate"><i class="fa fa-check"></i><b>23.6</b> Multivariate</a></li>
<li class="chapter" data-level="23.7" data-path="forecast.html"><a href="forecast.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>23.7</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="support-vector-machine.html"><a href="support-vector-machine.html"><i class="fa fa-check"></i><b>24</b> Support Vector Machine</a>
<ul>
<li class="chapter" data-level="24.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>24.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="24.1.1" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-margin"><i class="fa fa-check"></i><b>24.1.1</b> The Margin</a></li>
<li class="chapter" data-level="24.1.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#the-non-separable-case"><i class="fa fa-check"></i><b>24.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="support-vector-machine.html"><a href="support-vector-machine.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>24.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="24.3" data-path="support-vector-machine.html"><a href="support-vector-machine.html#application-with-svm"><i class="fa fa-check"></i><b>24.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>25</b> Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>25.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="25.2" data-path="neural-networks.html"><a href="neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>25.2</b> Backpropagation</a></li>
<li class="chapter" data-level="25.3" data-path="neural-networks.html"><a href="neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>25.3</b> Neural Network - More inputs</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>26</b> Deep Learning</a></li>
<li class="chapter" data-level="27" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i><b>27</b> Graphical Network Analysis</a>
<ul>
<li class="chapter" data-level="27.1" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#fundementals"><i class="fa fa-check"></i><b>27.1</b> Fundementals</a></li>
<li class="chapter" data-level="27.2" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#covariance"><i class="fa fa-check"></i><b>27.2</b> Covariance</a></li>
<li class="chapter" data-level="27.3" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#correlation-1"><i class="fa fa-check"></i><b>27.3</b> Correlation</a></li>
<li class="chapter" data-level="27.4" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#precision-matrix"><i class="fa fa-check"></i><b>27.4</b> Precision Matrix</a></li>
<li class="chapter" data-level="27.5" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#semi-partial-correlation"><i class="fa fa-check"></i><b>27.5</b> Semi-partial Correlation</a></li>
<li class="chapter" data-level="27.6" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#regularized-covariance-matrix"><i class="fa fa-check"></i><b>27.6</b> Regularized Covariance Matrix</a></li>
<li class="chapter" data-level="27.7" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>27.7</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="27.8" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#high-dimensional-data"><i class="fa fa-check"></i><b>27.8</b> High-dimensional data</a></li>
<li class="chapter" data-level="27.9" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>27.9</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="decompositions.html"><a href="decompositions.html"><i class="fa fa-check"></i><b>28</b> Decompositions</a>
<ul>
<li class="chapter" data-level="28.1" data-path="decompositions.html"><a href="decompositions.html#matrix-decomposition"><i class="fa fa-check"></i><b>28.1</b> Matrix Decomposition</a></li>
<li class="chapter" data-level="28.2" data-path="decompositions.html"><a href="decompositions.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>28.2</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="28.3" data-path="decompositions.html"><a href="decompositions.html#singular-value-decomposition"><i class="fa fa-check"></i><b>28.3</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28.4" data-path="decompositions.html"><a href="decompositions.html#rankr-approximations"><i class="fa fa-check"></i><b>28.4</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="28.5" data-path="decompositions.html"><a href="decompositions.html#moore-penrose-inverse"><i class="fa fa-check"></i><b>28.5</b> Moore-Penrose inverse</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html"><i class="fa fa-check"></i><b>29</b> PCA (Principle Component Analysis)</a>
<ul>
<li class="chapter" data-level="29.1" data-path="pca-principle-component-analysis.html"><a href="pca-principle-component-analysis.html#factor-analysis"><i class="fa fa-check"></i><b>29.1</b> Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>30</b> Smoothing</a>
<ul>
<li class="chapter" data-level="30.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>30.1</b> Using bins</a></li>
<li class="chapter" data-level="30.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>30.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="30.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>30.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="30.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>30.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="30.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>30.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>31</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="31.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>31.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="31.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>31.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="text-analysis.html"><a href="text-analysis.html"><i class="fa fa-check"></i><b>32</b> Text Analysis</a></li>
<li class="chapter" data-level="33" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html"><i class="fa fa-check"></i><b>33</b> Other Nonparametric Estimation methods</a>
<ul>
<li class="chapter" data-level="33.1" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#other-nonparametric-estimation-methods-1"><i class="fa fa-check"></i><b>33.1</b> Other Nonparametric Estimation methods</a></li>
<li class="chapter" data-level="33.2" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#regression-splines"><i class="fa fa-check"></i><b>33.2</b> Regression splines</a></li>
<li class="chapter" data-level="33.3" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#mars"><i class="fa fa-check"></i><b>33.3</b> MARS</a></li>
<li class="chapter" data-level="33.4" data-path="other-nonparametric-estimation-methods.html"><a href="other-nonparametric-estimation-methods.html#gam"><i class="fa fa-check"></i><b>33.4</b> GAM</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/mutluyuk/machinemetrics" target="blank">2023 Initial Draft</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MachineMetrics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-trees" class="section level1 hasAnchor" number="16">
<h1><span class="header-section-number">Chapter 16</span> Regression Trees<a href="regression-trees.html#regression-trees" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Tree-based predictive models are one of the best and most used supervised learning methods. Unlike linear models, they handle non-linear relationships quite well. They can be applied for both classification or regression problems, which aspires its name: <strong>C</strong>lassification <strong>A</strong>nd <strong>R</strong>egression <strong>T</strong>rees.</p>
<p>The foundation of their models is based on a decision tree, which is a flowchart where each internal <strong>node</strong> represents a decision point (goes left or right), each <strong>branch</strong> represents those decisions, and each <strong>leaf</strong> at the end of a branch represents the outcome of the decision. Here is a simple decision tree about a gamble:</p>
<p><img src="png/DT.png" width="130%" height="130%" /></p>
<p>How can we use a decision tree in a learning algorithm? Let’s start with a classification problem:</p>
<div id="cart---classification-tree" class="section level2 hasAnchor" number="16.1">
<h2><span class="header-section-number">16.1</span> CART - Classification Tree<a href="regression-trees.html#cart---classification-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s start with a very simple example: suppose we have the following data:</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="regression-trees.html#cb327-1" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb327-2"><a href="regression-trees.html#cb327-2" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.09</span>, <span class="fl">0.11</span>, <span class="fl">0.17</span>, <span class="fl">0.23</span>, <span class="fl">0.33</span>, <span class="fl">0.5</span>, <span class="fl">0.54</span>, <span class="fl">0.62</span>, <span class="fl">0.83</span>, <span class="fl">0.88</span>)</span>
<span id="cb327-3"><a href="regression-trees.html#cb327-3" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.82</span>, <span class="fl">0.2</span>, <span class="fl">0.09</span>, <span class="fl">0.58</span>, <span class="fl">0.5</span>, <span class="fl">0.93</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>, <span class="fl">0.83</span>)</span>
<span id="cb327-4"><a href="regression-trees.html#cb327-4" tabindex="-1"></a></span>
<span id="cb327-5"><a href="regression-trees.html#cb327-5" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">x1 =</span> x1, <span class="at">x2 =</span> x2)</span>
<span id="cb327-6"><a href="regression-trees.html#cb327-6" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb327-7"><a href="regression-trees.html#cb327-7" tabindex="-1"></a>  data<span class="sc">$</span>x1,</span>
<span id="cb327-8"><a href="regression-trees.html#cb327-8" tabindex="-1"></a>  data<span class="sc">$</span>x2,</span>
<span id="cb327-9"><a href="regression-trees.html#cb327-9" tabindex="-1"></a>  <span class="at">col =</span> (data<span class="sc">$</span>y <span class="sc">+</span> <span class="dv">1</span>),</span>
<span id="cb327-10"><a href="regression-trees.html#cb327-10" tabindex="-1"></a>  <span class="at">lwd =</span> <span class="dv">4</span>,</span>
<span id="cb327-11"><a href="regression-trees.html#cb327-11" tabindex="-1"></a>  <span class="at">ylab =</span> <span class="st">&quot;x2&quot;</span>,</span>
<span id="cb327-12"><a href="regression-trees.html#cb327-12" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;x1&quot;</span></span>
<span id="cb327-13"><a href="regression-trees.html#cb327-13" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr2-1.png" width="672" /></p>
<p>What’s the best rule on <span class="math inline">\(x_2\)</span> to classify black (<span class="math inline">\(0\)</span>) and red balls (<span class="math inline">\(1\)</span>)? <strong>Find a cutoff point on <span class="math inline">\(x_2\)</span> such that the maximum number of observations is correctly classified</strong></p>
<p>To minimize the misclassification, we find that the cutoff point should be between <span class="math inline">\(\{0.6: 0.79\}\)</span>. Hence the rule is <span class="math inline">\(x_2 &lt; k\)</span>, where <span class="math inline">\(k \in\{0.6: 0.79\}.\)</span></p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="regression-trees.html#cb328-1" tabindex="-1"></a><span class="fu">plot</span>(data<span class="sc">$</span>x1, data<span class="sc">$</span>x2, <span class="at">col =</span> (data<span class="sc">$</span>y <span class="sc">+</span> <span class="dv">1</span>), <span class="at">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb328-2"><a href="regression-trees.html#cb328-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="fl">0.62</span>,</span>
<span id="cb328-3"><a href="regression-trees.html#cb328-3" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb328-4"><a href="regression-trees.html#cb328-4" tabindex="-1"></a>       <span class="at">lty =</span> <span class="dv">5</span>,</span>
<span id="cb328-5"><a href="regression-trees.html#cb328-5" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr3-1.png" width="672" /></p>
<p><img src="png/tree1.png" width="130%" height="130%" /></p>
<p>From this simple rule, we have two misclassified balls. We can add a new rule in the area below the horizontal blue line:</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="regression-trees.html#cb329-1" tabindex="-1"></a><span class="fu">plot</span>(data<span class="sc">$</span>x1, data<span class="sc">$</span>x2, <span class="at">col =</span> (data<span class="sc">$</span>y <span class="sc">+</span> <span class="dv">1</span>), <span class="at">lwd =</span> <span class="dv">4</span>)</span>
<span id="cb329-2"><a href="regression-trees.html#cb329-2" tabindex="-1"></a><span class="fu">abline</span>(</span>
<span id="cb329-3"><a href="regression-trees.html#cb329-3" tabindex="-1"></a>  <span class="at">h =</span> <span class="fl">0.62</span>,</span>
<span id="cb329-4"><a href="regression-trees.html#cb329-4" tabindex="-1"></a>  <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;darkgreen&quot;</span>),</span>
<span id="cb329-5"><a href="regression-trees.html#cb329-5" tabindex="-1"></a>  <span class="at">lty =</span> <span class="dv">5</span>,</span>
<span id="cb329-6"><a href="regression-trees.html#cb329-6" tabindex="-1"></a>  <span class="at">lwd =</span> <span class="dv">2</span></span>
<span id="cb329-7"><a href="regression-trees.html#cb329-7" tabindex="-1"></a>)</span>
<span id="cb329-8"><a href="regression-trees.html#cb329-8" tabindex="-1"></a><span class="fu">segments</span>(</span>
<span id="cb329-9"><a href="regression-trees.html#cb329-9" tabindex="-1"></a>  <span class="at">x0 =</span> <span class="fl">0.2</span>,</span>
<span id="cb329-10"><a href="regression-trees.html#cb329-10" tabindex="-1"></a>  <span class="at">y0 =</span> <span class="dv">0</span>,</span>
<span id="cb329-11"><a href="regression-trees.html#cb329-11" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fl">0.2</span>,</span>
<span id="cb329-12"><a href="regression-trees.html#cb329-12" tabindex="-1"></a>  <span class="at">y1 =</span> <span class="fl">0.62</span>,</span>
<span id="cb329-13"><a href="regression-trees.html#cb329-13" tabindex="-1"></a>  <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>,</span>
<span id="cb329-14"><a href="regression-trees.html#cb329-14" tabindex="-1"></a>  <span class="at">lty =</span> <span class="dv">5</span>,</span>
<span id="cb329-15"><a href="regression-trees.html#cb329-15" tabindex="-1"></a>  <span class="at">lwd =</span> <span class="dv">2</span></span>
<span id="cb329-16"><a href="regression-trees.html#cb329-16" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr4-1.png" width="672" /></p>
<p><img src="png/tree2.png" width="130%" height="130%" /></p>
<p>Using these two rules, we correctly classified all balls (<span class="math inline">\(Y\)</span>). We did the classification manually by looking at the graph. How can we do it by an algorithm?</p>
<p>First, we need to create an index that is going to measure the <strong>impurity</strong> in each node. Instead of counting misclassified <span class="math inline">\(y\)</span>’s, the <strong>impurity</strong> index will give us a continuous metric. The first index is the <strong>Gini Index</strong>, which can be defined at some node <span class="math inline">\(\mathcal{N}\)</span>:</p>
<p><span class="math display">\[
G(\mathcal{N}) = \sum_{k=1}^{K} p_{k}\left(1-p_{k}\right) = 1-\sum_{k=1}^{K} p_{k}^{2}
\]</span>
where, with <span class="math inline">\(p_k\)</span> is the fraction of items labeled with class <span class="math inline">\(k\)</span> in the node. If we have a binary outcome <span class="math inline">\((k=2)\)</span>, when <span class="math inline">\(p_k \in \{0, 1\}\)</span>, <span class="math inline">\(G(\mathcal{N})=0\)</span> and when <span class="math inline">\(p_k = 0.5,\)</span> <span class="math inline">\(G(\mathcal{N})=0.25\)</span>. The former implies the minimal impurity (diversity), the latter shows the maximal impurity. A small <span class="math inline">\(G\)</span> means that a node contains observations predominantly from a single class. As in the previous example, when we have a binary outcome with two classes, <span class="math inline">\(y_i \in \{0, 1\}\)</span>, this index can be written as:</p>
<p><span class="math display">\[
G(\mathcal{N})=\sum_{k=1}^{2} p_{k}\left(1-p_{k}\right)=2p\left(1-p\right)
\]</span></p>
<p>If we split the node into two leaves, <span class="math inline">\(\mathcal{N}_L\)</span> (left) and <span class="math inline">\(\mathcal{N}_R\)</span> (right), the <span class="math inline">\(G\)</span> will be:</p>
<p><span class="math display">\[
G\left(\mathcal{N}_{L}, \mathcal{N}_{R}\right)=p_{L} G\left(\mathcal{N}_{L}\right)+p_{R} G\left(\mathcal{N}_{R}\right)
\]</span></p>
<p>Where <span class="math inline">\(p_L\)</span>, <span class="math inline">\(p_R\)</span> are the proportion of observations in <span class="math inline">\(\mathcal{N}_L\)</span> and <span class="math inline">\(\mathcal{N}_R\)</span>.</p>
<p>Remember, we are trying to find the rule that gives us the best cutoff point (split). Now we can write the rule:</p>
<p><span class="math display">\[
\Delta=G(\mathcal{N})-G\left(\mathcal{N}_{L}, \mathcal{N}_{R}\right)&gt;\epsilon
\]</span></p>
<p>When the impurity is reduced substantially, the difference will be some positive number (<span class="math inline">\(\epsilon\)</span>). Hence, we find the cutoff point on a single variable that minimizes the impurity (maximizes <span class="math inline">\(\Delta\)</span>).</p>
<p>Let’s use a dataset<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>, which reports about heart attacks and fatality (our binary variable).</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="regression-trees.html#cb330-1" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb330-2"><a href="regression-trees.html#cb330-2" tabindex="-1"></a><span class="co">#Data</span></span>
<span id="cb330-3"><a href="regression-trees.html#cb330-3" tabindex="-1"></a><span class="co">#myocarde = read.table(&quot;http://freakonometrics.free.fr/myocarde.csv&quot;,head=TRUE, sep=&quot;;&quot;)</span></span>
<span id="cb330-4"><a href="regression-trees.html#cb330-4" tabindex="-1"></a>myocarde <span class="ot">&lt;-</span> <span class="fu">read_delim</span>(</span>
<span id="cb330-5"><a href="regression-trees.html#cb330-5" tabindex="-1"></a>  <span class="st">&quot;myocarde.csv&quot;</span>,</span>
<span id="cb330-6"><a href="regression-trees.html#cb330-6" tabindex="-1"></a>  <span class="at">delim =</span> <span class="st">&quot;;&quot;</span> ,</span>
<span id="cb330-7"><a href="regression-trees.html#cb330-7" tabindex="-1"></a>  <span class="at">escape_double =</span> <span class="cn">FALSE</span>,</span>
<span id="cb330-8"><a href="regression-trees.html#cb330-8" tabindex="-1"></a>  <span class="at">trim_ws =</span> <span class="cn">TRUE</span>,</span>
<span id="cb330-9"><a href="regression-trees.html#cb330-9" tabindex="-1"></a>  <span class="at">show_col_types =</span> <span class="cn">FALSE</span></span>
<span id="cb330-10"><a href="regression-trees.html#cb330-10" tabindex="-1"></a>)</span>
<span id="cb330-11"><a href="regression-trees.html#cb330-11" tabindex="-1"></a>myocarde <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(myocarde)</span>
<span id="cb330-12"><a href="regression-trees.html#cb330-12" tabindex="-1"></a><span class="fu">str</span>(myocarde)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    71 obs. of  8 variables:
##  $ FRCAR: num  90 90 120 82 80 80 94 80 78 100 ...
##  $ INCAR: num  1.71 1.68 1.4 1.79 1.58 1.13 2.04 1.19 2.16 2.28 ...
##  $ INSYS: num  19 18.7 11.7 21.8 19.7 14.1 21.7 14.9 27.7 22.8 ...
##  $ PRDIA: num  16 24 23 14 21 18 23 16 15 16 ...
##  $ PAPUL: num  19.5 31 29 17.5 28 23.5 27 21 20.5 23 ...
##  $ PVENT: num  16 14 8 10 18.5 9 10 16.5 11.5 4 ...
##  $ REPUL: num  912 1476 1657 782 1418 ...
##  $ PRONO: chr  &quot;SURVIE&quot; &quot;DECES&quot; &quot;DECES&quot; &quot;SURVIE&quot; ...</code></pre>
<p>The variable definitions are as follows: <code>FRCAR</code> (heart rate), <code>INCAR</code> (heart index), <code>INSYS</code> (stroke index), <code>PRDIA</code> (diastolic pressure), <code>PAPUL</code> (pulmonary arterial pressure), <code>PVENT</code> (ventricular pressure), <code>REPUL</code> (lung resistance), <code>PRONO</code>, which is our outcome variable (death “DECES”, survival “SURVIE”). We are ready to calculate <span class="math inline">\(G\)</span>-index:</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="regression-trees.html#cb332-1" tabindex="-1"></a><span class="co"># Recode PRONO</span></span>
<span id="cb332-2"><a href="regression-trees.html#cb332-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(myocarde<span class="sc">$</span>PRONO <span class="sc">==</span> <span class="st">&quot;SURVIE&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb332-3"><a href="regression-trees.html#cb332-3" tabindex="-1"></a></span>
<span id="cb332-4"><a href="regression-trees.html#cb332-4" tabindex="-1"></a><span class="co"># Find G(N) without L and R</span></span>
<span id="cb332-5"><a href="regression-trees.html#cb332-5" tabindex="-1"></a>G <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">mean</span>(y) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(y))</span>
<span id="cb332-6"><a href="regression-trees.html#cb332-6" tabindex="-1"></a>G</span></code></pre></div>
<pre><code>## [1] 0.4832375</code></pre>
<p>This is the level of “impurity” in our data. Now, we need to pick one variable and find a cutoff point in the variable. Then, we will calculate the same <span class="math inline">\(G\)</span> for both left and right of that point. The goal is the find the best cutoff point that reduces the “impurity”. Let’s pick <code>FRCAR</code> arbitrarily for now. Later we will see how to find the variable that the first split (left and right) should start from so that the reduction in “impurity” will be maximized.</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="regression-trees.html#cb334-1" tabindex="-1"></a><span class="co"># Let&#39;s pick FRCAR to start</span></span>
<span id="cb334-2"><a href="regression-trees.html#cb334-2" tabindex="-1"></a>x_1 <span class="ot">&lt;-</span> myocarde<span class="sc">$</span>FRCAR</span>
<span id="cb334-3"><a href="regression-trees.html#cb334-3" tabindex="-1"></a></span>
<span id="cb334-4"><a href="regression-trees.html#cb334-4" tabindex="-1"></a><span class="co"># Put x and y in table</span></span>
<span id="cb334-5"><a href="regression-trees.html#cb334-5" tabindex="-1"></a>tab <span class="ot">=</span> <span class="fu">table</span>(y, x_1)</span>
<span id="cb334-6"><a href="regression-trees.html#cb334-6" tabindex="-1"></a>tab</span></code></pre></div>
<pre><code>##    x_1
## y   60 61 65 67 70 75 78 79 80 81 82 84 85 86 87 90 92 94 95 96 99 100 102 103
##   0  1  0  1  0  1  1  0  1  4  0  0  0  1  0  2  2  2  1  3  0  0   1   1   1
##   1  0  2  1  1  0  3  1  0  7  1  3  1  0  4  0  4  2  1  1  1  1   3   0   0
##    x_1
## y   105 108 110 116 118 120 122 125
##   0   1   0   2   1   1   1   0   0
##   1   0   1   1   0   1   0   1   1</code></pre>
<p>We are ready to calculate</p>
<p><span class="math display">\[
G\left(\mathcal{N}_{L}, \mathcal{N}_{R}\right)=p_{L} G\left(\mathcal{N}_{L}\right)+p_{R} G\left(\mathcal{N}_{R}\right),
\]</span>
when <span class="math inline">\(x = 60\)</span>, for example.</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="regression-trees.html#cb336-1" tabindex="-1"></a><span class="co"># x = 60, for example to see if (GL + GR &gt; GN)</span></span>
<span id="cb336-2"><a href="regression-trees.html#cb336-2" tabindex="-1"></a>GL <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">mean</span>(y[x_1 <span class="sc">&lt;=</span> <span class="dv">60</span>]) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(y[x_1 <span class="sc">&lt;=</span> <span class="dv">60</span>]))</span>
<span id="cb336-3"><a href="regression-trees.html#cb336-3" tabindex="-1"></a>GR <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">mean</span>(y[x_1 <span class="sc">&gt;</span> <span class="dv">60</span>]) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(y[x_1 <span class="sc">&gt;</span> <span class="dv">60</span>]))</span>
<span id="cb336-4"><a href="regression-trees.html#cb336-4" tabindex="-1"></a>pL <span class="ot">&lt;-</span> <span class="fu">length</span>(x_1[x_1 <span class="sc">&lt;=</span> <span class="dv">60</span>]) <span class="sc">/</span> <span class="fu">length</span>(x_1) <span class="co">#Proportion of obs. on Left</span></span>
<span id="cb336-5"><a href="regression-trees.html#cb336-5" tabindex="-1"></a>pR <span class="ot">&lt;-</span> <span class="fu">length</span>(x_1[x_1 <span class="sc">&gt;</span> <span class="dv">60</span>]) <span class="sc">/</span> <span class="fu">length</span>(x_1) <span class="co">#Proportion of obs. on Right</span></span></code></pre></div>
<p>How much did we improve <span class="math inline">\(G\)</span>?</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="regression-trees.html#cb337-1" tabindex="-1"></a>delta  <span class="ot">=</span> G <span class="sc">-</span> pL <span class="sc">*</span> GL <span class="sc">-</span> pR <span class="sc">*</span> GR</span>
<span id="cb337-2"><a href="regression-trees.html#cb337-2" tabindex="-1"></a>delta</span></code></pre></div>
<pre><code>## [1] 0.009998016</code></pre>
<p>We need to go trough each number on <span class="math inline">\(x_1\)</span> and identify the point that maximizes delta. A function can do that:</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="regression-trees.html#cb339-1" tabindex="-1"></a>GI <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb339-2"><a href="regression-trees.html#cb339-2" tabindex="-1"></a>  GL <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">mean</span>(y[x_1 <span class="sc">&lt;=</span> x]) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(y[x_1 <span class="sc">&lt;=</span> x]))</span>
<span id="cb339-3"><a href="regression-trees.html#cb339-3" tabindex="-1"></a>  GR <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">mean</span>(y[x_1 <span class="sc">&gt;</span> x]) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(y[x_1 <span class="sc">&gt;</span> x]))</span>
<span id="cb339-4"><a href="regression-trees.html#cb339-4" tabindex="-1"></a>  pL <span class="ot">&lt;-</span> <span class="fu">length</span>(x_1[x_1 <span class="sc">&lt;=</span> x]) <span class="sc">/</span> <span class="fu">length</span>(x_1)</span>
<span id="cb339-5"><a href="regression-trees.html#cb339-5" tabindex="-1"></a>  pR <span class="ot">&lt;-</span> <span class="fu">length</span>(x_1[x_1 <span class="sc">&gt;</span> x]) <span class="sc">/</span> <span class="fu">length</span>(x_1)</span>
<span id="cb339-6"><a href="regression-trees.html#cb339-6" tabindex="-1"></a>  del <span class="ot">=</span> G <span class="sc">-</span> pL <span class="sc">*</span> GL <span class="sc">-</span> pR <span class="sc">*</span> GR</span>
<span id="cb339-7"><a href="regression-trees.html#cb339-7" tabindex="-1"></a>  <span class="fu">return</span>(del)</span>
<span id="cb339-8"><a href="regression-trees.html#cb339-8" tabindex="-1"></a>}</span>
<span id="cb339-9"><a href="regression-trees.html#cb339-9" tabindex="-1"></a></span>
<span id="cb339-10"><a href="regression-trees.html#cb339-10" tabindex="-1"></a><span class="co"># Let&#39;s test it</span></span>
<span id="cb339-11"><a href="regression-trees.html#cb339-11" tabindex="-1"></a><span class="fu">GI</span>(<span class="dv">60</span>)</span></code></pre></div>
<pre><code>## [1] 0.009998016</code></pre>
<p>It works! Now, we can use this function in a loop that goes over each unique <span class="math inline">\(x\)</span> and calculate their delta.</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="regression-trees.html#cb341-1" tabindex="-1"></a>xm <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">unique</span>(x_1))</span>
<span id="cb341-2"><a href="regression-trees.html#cb341-2" tabindex="-1"></a>delta <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb341-3"><a href="regression-trees.html#cb341-3" tabindex="-1"></a></span>
<span id="cb341-4"><a href="regression-trees.html#cb341-4" tabindex="-1"></a><span class="co"># Since we don&#39;t split at the last number</span></span>
<span id="cb341-5"><a href="regression-trees.html#cb341-5" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(xm) <span class="sc">-</span> <span class="dv">1</span>) {</span>
<span id="cb341-6"><a href="regression-trees.html#cb341-6" tabindex="-1"></a>  delta[i] <span class="ot">&lt;-</span> <span class="fu">GI</span>(xm[i])</span>
<span id="cb341-7"><a href="regression-trees.html#cb341-7" tabindex="-1"></a>}</span>
<span id="cb341-8"><a href="regression-trees.html#cb341-8" tabindex="-1"></a></span>
<span id="cb341-9"><a href="regression-trees.html#cb341-9" tabindex="-1"></a>delta</span></code></pre></div>
<pre><code>##  [1] 9.998016e-03 4.978782e-04 1.082036e-05 1.041714e-03 8.855953e-05
##  [6] 7.363859e-04 2.295303e-03 2.546756e-04 1.142757e-03 2.551599e-03
## [11] 9.862318e-03 1.329134e-02 8.257492e-03 2.402430e-02 1.160767e-02
## [16] 1.634414e-02 1.352527e-02 1.229951e-02 3.109723e-03 5.692941e-03
## [21] 9.212475e-03 1.919591e-02 1.244092e-02 6.882353e-03 2.747959e-03
## [26] 6.282533e-03 1.547312e-03 1.082036e-05 4.978782e-04 9.671419e-03
## [31] 4.766628e-03</code></pre>
<p>Let’s see the cutoff point that gives us the highest delta.</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="regression-trees.html#cb343-1" tabindex="-1"></a><span class="fu">max</span>(delta)</span></code></pre></div>
<pre><code>## [1] 0.0240243</code></pre>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="regression-trees.html#cb345-1" tabindex="-1"></a>xm[<span class="fu">which.max</span>(delta)]</span></code></pre></div>
<pre><code>## [1] 86</code></pre>
<p>Although this is a simple and an imperfect algorithm, it shows us how we can build a learning system based on a decision tree. On one variable, <code>FRCAR</code>, and with only one split we improved the Gini index by 2.5%. Obviously this is not good enough. Can we do more splitting?</p>
<p>Since we now have two nodes (Left and Right at <span class="math inline">\(x_1 = 86\)</span>), we can consider each of them as one node and apply the same formula to both left and right nodes. As you can guess, this may give us a zero-<span class="math inline">\(G\)</span>, as we end up with splitting at every <span class="math inline">\(x_{1i}\)</span>. We can prevent this overfitting by <strong>pruning</strong>, which we will see later.</p>
<p>Wouldn’t it be a good idea if we check all seven variables and start with the one that has a significant improvements in delta when we split? We can do it easily with a loop:</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="regression-trees.html#cb347-1" tabindex="-1"></a><span class="co"># Adjust our function a little: add &quot;tr&quot;, the cutoff</span></span>
<span id="cb347-2"><a href="regression-trees.html#cb347-2" tabindex="-1"></a>GI <span class="ot">&lt;-</span> <span class="cf">function</span>(x, tr) {</span>
<span id="cb347-3"><a href="regression-trees.html#cb347-3" tabindex="-1"></a>  G <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">mean</span>(y) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(y))</span>
<span id="cb347-4"><a href="regression-trees.html#cb347-4" tabindex="-1"></a>  GL <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">mean</span>(y[x <span class="sc">&lt;=</span> tr]) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(y[x <span class="sc">&lt;=</span> tr]))</span>
<span id="cb347-5"><a href="regression-trees.html#cb347-5" tabindex="-1"></a>  GR <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">mean</span>(y[x <span class="sc">&gt;</span> tr]) <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(y[x <span class="sc">&gt;</span> tr]))</span>
<span id="cb347-6"><a href="regression-trees.html#cb347-6" tabindex="-1"></a>  pL <span class="ot">&lt;-</span> <span class="fu">length</span>(x[x <span class="sc">&lt;=</span> tr]) <span class="sc">/</span> <span class="fu">length</span>(x)</span>
<span id="cb347-7"><a href="regression-trees.html#cb347-7" tabindex="-1"></a>  pR <span class="ot">&lt;-</span> <span class="fu">length</span>(x[x <span class="sc">&gt;</span> tr]) <span class="sc">/</span> <span class="fu">length</span>(x)</span>
<span id="cb347-8"><a href="regression-trees.html#cb347-8" tabindex="-1"></a>  del <span class="ot">=</span> G <span class="sc">-</span> pL <span class="sc">*</span> GL <span class="sc">-</span> pR <span class="sc">*</span> GR</span>
<span id="cb347-9"><a href="regression-trees.html#cb347-9" tabindex="-1"></a>  <span class="fu">return</span>(del)</span>
<span id="cb347-10"><a href="regression-trees.html#cb347-10" tabindex="-1"></a>}</span>
<span id="cb347-11"><a href="regression-trees.html#cb347-11" tabindex="-1"></a></span>
<span id="cb347-12"><a href="regression-trees.html#cb347-12" tabindex="-1"></a><span class="co"># The loop that applies GI on every x</span></span>
<span id="cb347-13"><a href="regression-trees.html#cb347-13" tabindex="-1"></a>d <span class="ot">&lt;-</span> myocarde[, <span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>]</span>
<span id="cb347-14"><a href="regression-trees.html#cb347-14" tabindex="-1"></a>split <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb347-15"><a href="regression-trees.html#cb347-15" tabindex="-1"></a>maxdelta <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb347-16"><a href="regression-trees.html#cb347-16" tabindex="-1"></a></span>
<span id="cb347-17"><a href="regression-trees.html#cb347-17" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(d)) {</span>
<span id="cb347-18"><a href="regression-trees.html#cb347-18" tabindex="-1"></a>  xm <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">unique</span>(d[, j]))</span>
<span id="cb347-19"><a href="regression-trees.html#cb347-19" tabindex="-1"></a>  delta <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb347-20"><a href="regression-trees.html#cb347-20" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(xm) <span class="sc">-</span> <span class="dv">1</span>) {</span>
<span id="cb347-21"><a href="regression-trees.html#cb347-21" tabindex="-1"></a>    delta[i] <span class="ot">&lt;-</span> <span class="fu">GI</span>(d[, j], xm[i])</span>
<span id="cb347-22"><a href="regression-trees.html#cb347-22" tabindex="-1"></a>  }</span>
<span id="cb347-23"><a href="regression-trees.html#cb347-23" tabindex="-1"></a>  maxdelta[j] <span class="ot">&lt;-</span> <span class="fu">max</span>(delta)</span>
<span id="cb347-24"><a href="regression-trees.html#cb347-24" tabindex="-1"></a>  split[j] <span class="ot">&lt;-</span> xm[<span class="fu">which.max</span>(delta)]</span>
<span id="cb347-25"><a href="regression-trees.html#cb347-25" tabindex="-1"></a>}</span>
<span id="cb347-26"><a href="regression-trees.html#cb347-26" tabindex="-1"></a></span>
<span id="cb347-27"><a href="regression-trees.html#cb347-27" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">variables =</span> <span class="fu">colnames</span>(d), <span class="at">delta =</span> maxdelta)</span></code></pre></div>
<pre><code>##   variables      delta
## 1     FRCAR 0.02402430
## 2     INCAR 0.26219024
## 3     INSYS 0.28328013
## 4     PRDIA 0.13184706
## 5     PAPUL 0.09890283
## 6     PVENT 0.04612125
## 7     REPUL 0.26790701</code></pre>
<p>This is good. We can identify that <code>INSYS</code> should be our first variable to split, as it has the highest delta.</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="regression-trees.html#cb349-1" tabindex="-1"></a><span class="fu">round</span>(split[<span class="fu">which.max</span>(maxdelta)], <span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] 19</code></pre>
<p>We now know where to split on <code>INSYS</code>, which is 19. After splitting <code>INSYS</code> left and right, we move on to the next variable to split, which would be the second best: <code>REBUL</code>.</p>
<p>For a better interpretability, we can rank the importance of each variable by <strong>their gain in Gini</strong>. We can approximately order them by looking at our delta:</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="regression-trees.html#cb351-1" tabindex="-1"></a>dm <span class="ot">&lt;-</span> <span class="fu">matrix</span>(maxdelta, <span class="dv">7</span>, <span class="dv">1</span>)</span>
<span id="cb351-2"><a href="regression-trees.html#cb351-2" tabindex="-1"></a><span class="fu">rownames</span>(dm) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">names</span>(myocarde[<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>]))</span>
<span id="cb351-3"><a href="regression-trees.html#cb351-3" tabindex="-1"></a>dm <span class="ot">&lt;-</span> dm[<span class="fu">order</span>(dm[, <span class="dv">1</span>]), ]</span>
<span id="cb351-4"><a href="regression-trees.html#cb351-4" tabindex="-1"></a></span>
<span id="cb351-5"><a href="regression-trees.html#cb351-5" tabindex="-1"></a><span class="fu">barplot</span>(</span>
<span id="cb351-6"><a href="regression-trees.html#cb351-6" tabindex="-1"></a>  dm,</span>
<span id="cb351-7"><a href="regression-trees.html#cb351-7" tabindex="-1"></a>  <span class="at">horiz =</span> <span class="cn">TRUE</span>,</span>
<span id="cb351-8"><a href="regression-trees.html#cb351-8" tabindex="-1"></a>  <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>,</span>
<span id="cb351-9"><a href="regression-trees.html#cb351-9" tabindex="-1"></a>  <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.3</span>),</span>
<span id="cb351-10"><a href="regression-trees.html#cb351-10" tabindex="-1"></a>  <span class="at">cex.names =</span> <span class="fl">0.5</span>,</span>
<span id="cb351-11"><a href="regression-trees.html#cb351-11" tabindex="-1"></a>  <span class="at">cex.axis =</span> <span class="fl">0.8</span>,</span>
<span id="cb351-12"><a href="regression-trees.html#cb351-12" tabindex="-1"></a>  <span class="at">main =</span> <span class="st">&quot;Variable Importance at the 1st Split&quot;</span></span>
<span id="cb351-13"><a href="regression-trees.html#cb351-13" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr16-1.png" width="672" /></p>
<p>The package <code>rpart</code> (<strong>R</strong>ecursive <strong>PART</strong>itioning) implements all these steps that we experimented above.</p>
</div>
<div id="rpart---recursive-partitioning" class="section level2 hasAnchor" number="16.2">
<h2><span class="header-section-number">16.2</span> <code>rpart</code> - Recursive Partitioning<a href="regression-trees.html#rpart---recursive-partitioning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As in our case, when the response variable is categorical, the resulting tree is called <strong>classification tree</strong>. The default criterion, which is maximized in each split is the <strong>Gini coefficient</strong>. The method-argument can be switched according to the type of the response variable. It is <code>class</code> for categorical, <code>anova</code> for numerical, <code>poisson</code> for count data and <code>exp</code> for survival data. If the outcome variable is a factor variable, as in our case, we do not have to specify the method.</p>
<p>The tree is built by the following process in <code>rpart</code>: first the single variable is found that <strong>best splits</strong> the data into two groups. After the data is separated, this process is applied separately to each sub-group. This goes on recursively until the subgroups either reach a <strong>minimum size</strong> or until no improvement can be made.</p>
<p>Details can be found in this <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">vignette</a> <span class="citation">(<a href="#ref-Atkinson_2022"><strong>Atkinson_2022?</strong></a>)</span>.</p>
<p>Here, we apply <code>rpart</code> to our data without any modification to its default arguments:</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="regression-trees.html#cb352-1" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb352-2"><a href="regression-trees.html#cb352-2" tabindex="-1"></a>tree <span class="ot">=</span> <span class="fu">rpart</span>(PRONO <span class="sc">~</span> ., <span class="at">data =</span> myocarde, <span class="at">method =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb352-3"><a href="regression-trees.html#cb352-3" tabindex="-1"></a></span>
<span id="cb352-4"><a href="regression-trees.html#cb352-4" tabindex="-1"></a><span class="co"># Plot it</span></span>
<span id="cb352-5"><a href="regression-trees.html#cb352-5" tabindex="-1"></a><span class="fu">library</span>(rpart.plot) <span class="co"># You can use plot() but prp() is much better</span></span>
<span id="cb352-6"><a href="regression-trees.html#cb352-6" tabindex="-1"></a><span class="fu">prp</span>(</span>
<span id="cb352-7"><a href="regression-trees.html#cb352-7" tabindex="-1"></a>  tree,</span>
<span id="cb352-8"><a href="regression-trees.html#cb352-8" tabindex="-1"></a>  <span class="at">type =</span> <span class="dv">2</span>,</span>
<span id="cb352-9"><a href="regression-trees.html#cb352-9" tabindex="-1"></a>  <span class="at">extra =</span> <span class="dv">1</span>,</span>
<span id="cb352-10"><a href="regression-trees.html#cb352-10" tabindex="-1"></a>  <span class="at">split.col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb352-11"><a href="regression-trees.html#cb352-11" tabindex="-1"></a>  <span class="at">split.border.col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb352-12"><a href="regression-trees.html#cb352-12" tabindex="-1"></a>  <span class="at">box.col =</span> <span class="st">&quot;pink&quot;</span></span>
<span id="cb352-13"><a href="regression-trees.html#cb352-13" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr17-1.png" width="672" /></p>
<p>This shows that the left node (<code>DECES</code>) cannot be significantly improved by a further split on <code>REPUL</code>. But the right node (<code>SURVIE</code>) can be improved.</p>
<p>Note that we haven’t trained our model explicitly. There are two ways to <strong>control</strong> the growth of a tree:</p>
<ol style="list-style-type: decimal">
<li>We can limit the growth of our tree by using its control parameters and by checking if the split is worth it, which is, as a default, what <code>rpart</code> is doing with 10-fold cross-validation.</li>
<li>We can grow the tree without any limitation and then <code>prune</code> it.</li>
</ol>
<p>Since we use the default control parameters with 10-fold CV, our first tree was grown by the first strategy. Before going further, let’s spend some time on the main arguments of <code>rpart()</code>:</p>
<p><code>rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)</code></p>
<p>The <code>control</code> argument controls how the tree grows. We briefly describe its arguments based on <a href="https://www.mayo.edu/research/documents/rpartminipdf/doc-10027257">An Introduction to Recursive Partitioning Using the RPART Routines</a> by Atkinson et.al. <span class="citation">(<a href="#ref-Atkinson_2000"><strong>Atkinson_2000?</strong></a>)</span>:</p>
<p><code>rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, maxdepth = 30, ...)</code></p>
<ul>
<li><code>minsplit</code>: The minimum number of observations in a node for which the routine will even try to compute a split. The default is 20.</li>
<li><code>minbucket</code>: The minimum number of observations in a terminal node: This defaults to <code>minsplit</code>/3.</li>
<li><code>cp</code>: The threshold complexity parameter. Default is 0.01.</li>
<li><code>maxcompete</code>: The number of alternative splits in addition to the best that will be printed.</li>
<li><code>maxsurrogate</code>: The maximum number of surrogate variables to retain at each node.</li>
<li><code>usesurrogate</code>: If the value is 0, then a subject (observation) who is missing the primary split variable does not progress further down the tree.</li>
<li><code>xval</code>: The number of cross-validations to be done. Default is 10.</li>
<li><code>maxdepth</code>: The maximum depth of any node of the final tree</li>
</ul>
<p>Remember, <code>rpart</code> does not drop the subject if it has a missing observation on a predictor. When the observation missing on the primary split on that variable, <code>rpart</code> find a surrogate for the variable so that it can carry out the split.</p>
<p>We can see the the growth of the tree by looking at its CV table:</p>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="regression-trees.html#cb353-1" tabindex="-1"></a><span class="fu">printcp</span>(tree)</span></code></pre></div>
<pre><code>## 
## Classification tree:
## rpart(formula = PRONO ~ ., data = myocarde, method = &quot;class&quot;)
## 
## Variables actually used in tree construction:
## [1] INSYS REPUL
## 
## Root node error: 29/71 = 0.40845
## 
## n= 71 
## 
##         CP nsplit rel error  xerror    xstd
## 1 0.724138      0   1.00000 1.00000 0.14282
## 2 0.034483      1   0.27586 0.62069 0.12640
## 3 0.010000      2   0.24138 0.65517 0.12863</code></pre>
<p>The <code>rel error</code> of each iteration of the tree is the fraction of mislabeled elements in the iteration relative to the fraction of mislabeled elements in the root. Hence it’s 100% (1.00000 in the table) in the root node. In other words, <code>rel error</code> gives the percentage of misclassified labels, when it’s multiplied with the <code>Root node error</code> (0.40845 x 0.24138 = 0.0986). This is the error rate when the fitted model applied to the training sets used by <code>rpart</code>’s CV.</p>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="regression-trees.html#cb355-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">predict</span>(tree, <span class="at">type=</span><span class="st">&quot;class&quot;</span>)<span class="sc">!=</span>myocarde<span class="sc">$</span>PRONO)<span class="sc">/</span><span class="fu">nrow</span>(myocarde)</span></code></pre></div>
<pre><code>## [1] 0.09859155</code></pre>
<p>The <code>xerror</code> also provides the same information. But since it is applied to test sets, it shows the cross-validation error.</p>
<p>The <strong>relative</strong> improvement, or gain, due to a split is given by <code>CP</code> (cost complexity pruning), which is 0.724138 in the first split on <code>INSYS</code>. Therefore, the first split on <code>INSYS</code> reduces (improves) this error to 27.5862% (<code>rel error</code>).</p>
<p>This relative gain (<code>CP</code>) can be calculated as follows:</p>
<p><span class="math display">\[
\frac{\Delta}{G(\mathcal{N})}=\frac{G(\mathcal{N})-G\left(\mathcal{N}_{L}, \mathcal{N}_{R}\right)}{G(\mathcal{N})}.
\]</span></p>
<p>If this gain exceeds 1%, which is the default value, <code>rpart()</code> splits a variable. As you can see from the table above, since there is no significant relative gain at the <span class="math inline">\(3^{rd}\)</span> split more than the default parameter 0.01, <code>rpart()</code> decides to stop growing the tree after the <span class="math inline">\(2^{nd}\)</span> split.</p>
<p>Note that, we also calculated both the nominator and the denominator in our own algorithm: <span class="math inline">\(\Delta = 0.2832801\)</span> and <span class="math inline">\(G(\mathcal{N}) = 0.4832375\)</span>. Hence the relative gain was <span class="math inline">\(\frac{\Delta}{G(\mathcal{N})}=0.586213\)</span> in our case. We can replicate the same results if we change our outcome from factor to numeric:</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="regression-trees.html#cb357-1" tabindex="-1"></a>myocarde_v2 <span class="ot">&lt;-</span> myocarde</span>
<span id="cb357-2"><a href="regression-trees.html#cb357-2" tabindex="-1"></a>myocarde_v2<span class="sc">$</span>PRONO <span class="ot">=</span> <span class="fu">ifelse</span>(myocarde<span class="sc">$</span>PRONO <span class="sc">==</span> <span class="st">&quot;SURVIE&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb357-3"><a href="regression-trees.html#cb357-3" tabindex="-1"></a>cart <span class="ot">=</span> <span class="fu">rpart</span>(PRONO <span class="sc">~</span> ., <span class="at">data =</span> myocarde_v2)</span>
<span id="cb357-4"><a href="regression-trees.html#cb357-4" tabindex="-1"></a><span class="fu">printcp</span>(cart)</span></code></pre></div>
<pre><code>## 
## Regression tree:
## rpart(formula = PRONO ~ ., data = myocarde_v2)
## 
## Variables actually used in tree construction:
## [1] INSYS REPUL
## 
## Root node error: 17.155/71 = 0.24162
## 
## n= 71 
## 
##         CP nsplit rel error  xerror     xstd
## 1 0.586213      0   1.00000 1.02656 0.045968
## 2 0.101694      1   0.41379 0.94673 0.170680
## 3 0.028263      2   0.31209 0.78357 0.154050
## 4 0.010000      3   0.28383 0.73456 0.152971</code></pre>
<p>As you see, when the outcome is not a factor variable, <code>rpart</code> applies a <strong>regression tree</strong> method, which minimizes the sum of squares, <span class="math inline">\(\sum_{i=1}^{n}\left(y_i-f(x_i)\right)^2\)</span>. However, when <span class="math inline">\(y_i\)</span> is a binary number with two values 0 and 1, the sum of squares becomes <span class="math inline">\(np(1-p)\)</span>, which gives the same relative gain as Gini. This is clear as both relative gains (our calculation and the calculation by <code>rpart</code> above) are the same.</p>
<p>What’s the variable importance of <code>rpart()</code>?</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="regression-trees.html#cb359-1" tabindex="-1"></a><span class="co"># Variable Importance</span></span>
<span id="cb359-2"><a href="regression-trees.html#cb359-2" tabindex="-1"></a>vi <span class="ot">&lt;-</span> tree<span class="sc">$</span>variable.importance</span>
<span id="cb359-3"><a href="regression-trees.html#cb359-3" tabindex="-1"></a>vi <span class="ot">&lt;-</span> vi[<span class="fu">order</span>(vi)]</span>
<span id="cb359-4"><a href="regression-trees.html#cb359-4" tabindex="-1"></a><span class="fu">barplot</span>(</span>
<span id="cb359-5"><a href="regression-trees.html#cb359-5" tabindex="-1"></a>  vi <span class="sc">/</span> <span class="dv">100</span>,</span>
<span id="cb359-6"><a href="regression-trees.html#cb359-6" tabindex="-1"></a>  <span class="at">horiz =</span> <span class="cn">TRUE</span>,</span>
<span id="cb359-7"><a href="regression-trees.html#cb359-7" tabindex="-1"></a>  <span class="at">col =</span> <span class="st">&quot;lightgreen&quot;</span>,</span>
<span id="cb359-8"><a href="regression-trees.html#cb359-8" tabindex="-1"></a>  <span class="at">cex.names =</span> <span class="fl">0.5</span>,</span>
<span id="cb359-9"><a href="regression-trees.html#cb359-9" tabindex="-1"></a>  <span class="at">cex.axis =</span> <span class="fl">0.8</span>,</span>
<span id="cb359-10"><a href="regression-trees.html#cb359-10" tabindex="-1"></a>  <span class="at">main =</span> <span class="st">&quot;Variable Importance - rpart()&quot;</span></span>
<span id="cb359-11"><a href="regression-trees.html#cb359-11" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr20-1.png" width="672" /></p>
<p>It seems that the order of variables are similar, but magnitudes are slightly different due to the differences in calculating methods. In <code>rpart</code>, the value is calculated as the sum of the decrease in impurity both when the variable appear as a primary split and when it appears as a surrogate.</p>
</div>
<div id="pruning" class="section level2 hasAnchor" number="16.3">
<h2><span class="header-section-number">16.3</span> Pruning<a href="regression-trees.html#pruning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can now apply the second method to our case by removing the default limits in growing our tree. We can do it by changing the parameters of the <code>rpart</code> fit. Let’s see what happens if we override these parameters.</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="regression-trees.html#cb360-1" tabindex="-1"></a><span class="co"># let&#39;s change the minsplit and minbucket</span></span>
<span id="cb360-2"><a href="regression-trees.html#cb360-2" tabindex="-1"></a>tree2 <span class="ot">=</span> <span class="fu">rpart</span>(</span>
<span id="cb360-3"><a href="regression-trees.html#cb360-3" tabindex="-1"></a>  PRONO <span class="sc">~</span> .,</span>
<span id="cb360-4"><a href="regression-trees.html#cb360-4" tabindex="-1"></a>  <span class="at">data =</span> myocarde,</span>
<span id="cb360-5"><a href="regression-trees.html#cb360-5" tabindex="-1"></a>  <span class="at">control =</span> <span class="fu">rpart.control</span>(</span>
<span id="cb360-6"><a href="regression-trees.html#cb360-6" tabindex="-1"></a>    <span class="at">minsplit =</span> <span class="dv">2</span>,</span>
<span id="cb360-7"><a href="regression-trees.html#cb360-7" tabindex="-1"></a>    <span class="at">minbucket =</span> <span class="dv">1</span>,</span>
<span id="cb360-8"><a href="regression-trees.html#cb360-8" tabindex="-1"></a>    <span class="at">cp =</span> <span class="dv">0</span></span>
<span id="cb360-9"><a href="regression-trees.html#cb360-9" tabindex="-1"></a>  ),</span>
<span id="cb360-10"><a href="regression-trees.html#cb360-10" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;class&quot;</span></span>
<span id="cb360-11"><a href="regression-trees.html#cb360-11" tabindex="-1"></a>)</span>
<span id="cb360-12"><a href="regression-trees.html#cb360-12" tabindex="-1"></a></span>
<span id="cb360-13"><a href="regression-trees.html#cb360-13" tabindex="-1"></a><span class="fu">library</span>(rattle)</span>
<span id="cb360-14"><a href="regression-trees.html#cb360-14" tabindex="-1"></a><span class="co"># You can use plot() but prp() is an alternative</span></span>
<span id="cb360-15"><a href="regression-trees.html#cb360-15" tabindex="-1"></a><span class="fu">fancyRpartPlot</span>(tree2, <span class="at">caption =</span> <span class="cn">NULL</span>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr21-1.png" width="672" /></p>
<p>This is our <strong>fully grown tree</strong> with a “perfect” fit, because it identifies every outcome (<code>DECES</code> and <code>SURVIE</code>) correctly at the terminal nodes (%’s give proportion of observations). Obviously, this is not a good idea as it overfits.</p>
<p>Let’s summarize what we have seen so far: we can either go with the first strategy and <strong>limit</strong> the growth of the tree or we can have a fully developed tree then we can <code>prune</code> it.</p>
<p>The general idea in pruning is to reduce the tree’s complexity by keeping only the most important splits. When we grow a tree, <code>rpart</code> performs 10-fold cross-validation on the data. We can see the cross-validation result by <code>printcp()</code>.</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="regression-trees.html#cb361-1" tabindex="-1"></a><span class="fu">printcp</span>(tree2)</span></code></pre></div>
<pre><code>## 
## Classification tree:
## rpart(formula = PRONO ~ ., data = myocarde, method = &quot;class&quot;, 
##     control = rpart.control(minsplit = 2, minbucket = 1, cp = 0))
## 
## Variables actually used in tree construction:
## [1] FRCAR INCAR INSYS PVENT REPUL
## 
## Root node error: 29/71 = 0.40845
## 
## n= 71 
## 
##         CP nsplit rel error  xerror    xstd
## 1 0.724138      0  1.000000 1.00000 0.14282
## 2 0.103448      1  0.275862 0.48276 0.11560
## 3 0.034483      2  0.172414 0.41379 0.10889
## 4 0.017241      6  0.034483 0.51724 0.11861
## 5 0.000000      8  0.000000 0.51724 0.11861</code></pre>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="regression-trees.html#cb363-1" tabindex="-1"></a><span class="fu">plotcp</span>(tree2)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr22-1.png" width="672" /></p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="regression-trees.html#cb364-1" tabindex="-1"></a>min_cp <span class="ot">=</span> tree2<span class="sc">$</span>cptable[<span class="fu">which.min</span>(tree2<span class="sc">$</span>cptable[,<span class="st">&quot;xerror&quot;</span>]),<span class="st">&quot;CP&quot;</span>]</span>
<span id="cb364-2"><a href="regression-trees.html#cb364-2" tabindex="-1"></a>min_cp</span></code></pre></div>
<pre><code>## [1] 0.03448276</code></pre>
<p>Remember <code>rpart</code> has a built-in process for cross-validation. The <code>xerror</code> is the cross-validation error, the classification error that is calculated on the test data with a cross-validation process. In general, the cross-validation error grows as the tree gets more levels (each row represents a different height of the tree).</p>
<p>There are two common ways to prune a tree by <code>rpart</code>:</p>
<ol style="list-style-type: decimal">
<li>Use the first level (i.e. least <code>nsplit</code>) with minimum <code>xerror</code>. The first level only kicks in when there are multiple levels having the same, minimum <code>xerror</code>. This is the most common used method.</li>
<li>Use the first level where <code>xerror</code> &lt; min(<code>xerror</code>) + <code>xstd</code>, the level whose <code>xerror</code> is at or below horizontal line. This method takes into account the variability of <code>xerror</code> resulting from cross-validation.</li>
</ol>
<p>If we decide to prune our tree at the minimum <code>cp</code>:</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="regression-trees.html#cb366-1" tabindex="-1"></a>ptree2 <span class="ot">&lt;-</span> <span class="fu">prune</span>(tree2, <span class="at">cp =</span> min_cp)</span>
<span id="cb366-2"><a href="regression-trees.html#cb366-2" tabindex="-1"></a><span class="fu">printcp</span>(ptree2)</span></code></pre></div>
<pre><code>## 
## Classification tree:
## rpart(formula = PRONO ~ ., data = myocarde, method = &quot;class&quot;, 
##     control = rpart.control(minsplit = 2, minbucket = 1, cp = 0))
## 
## Variables actually used in tree construction:
## [1] INSYS PVENT
## 
## Root node error: 29/71 = 0.40845
## 
## n= 71 
## 
##         CP nsplit rel error  xerror    xstd
## 1 0.724138      0   1.00000 1.00000 0.14282
## 2 0.103448      1   0.27586 0.48276 0.11560
## 3 0.034483      2   0.17241 0.41379 0.10889</code></pre>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="regression-trees.html#cb368-1" tabindex="-1"></a><span class="fu">fancyRpartPlot</span>(ptree2)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr23-1.png" width="672" /></p>
<p>Now we have applied two approaches: limiting the tree’s growth and pruning a fully grown tree. Hence, we have two different trees: <code>tree</code> and <code>ptree2</code>. In the first case, we can use <code>cp</code> or other control parameters in <code>rpart.control</code> as hyperparameters and tune them on the test set. In the second case, we can grow the tree to its maximum capacity and tune its pruning as to maximize the prediction accuracy on the test set. We will not show the tuning of a tree here. Instead, we will see many improved tree-based models and tuned them in this section.</p>
</div>
<div id="classification-with-titanic" class="section level2 hasAnchor" number="16.4">
<h2><span class="header-section-number">16.4</span> Classification with Titanic<a href="regression-trees.html#classification-with-titanic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can use <code>rpart</code> to predict survival on the Titanic.</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="regression-trees.html#cb369-1" tabindex="-1"></a><span class="fu">library</span>(PASWR)</span>
<span id="cb369-2"><a href="regression-trees.html#cb369-2" tabindex="-1"></a><span class="fu">data</span>(titanic3)</span>
<span id="cb369-3"><a href="regression-trees.html#cb369-3" tabindex="-1"></a><span class="fu">str</span>(titanic3)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    1309 obs. of  14 variables:
##  $ pclass   : Factor w/ 3 levels &quot;1st&quot;,&quot;2nd&quot;,&quot;3rd&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ survived : int  1 1 0 0 0 1 1 0 1 0 ...
##  $ name     : Factor w/ 1307 levels &quot;Abbing, Mr. Anthony&quot;,..: 22 24 25 26 27 31 46 47 51 55 ...
##  $ sex      : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 1 2 1 2 1 2 1 2 ...
##  $ age      : num  29 0.917 2 30 25 ...
##  $ sibsp    : int  0 1 1 1 1 0 1 0 2 0 ...
##  $ parch    : int  0 2 2 2 2 0 0 0 0 0 ...
##  $ ticket   : Factor w/ 929 levels &quot;110152&quot;,&quot;110413&quot;,..: 188 50 50 50 50 125 93 16 77 826 ...
##  $ fare     : num  211 152 152 152 152 ...
##  $ cabin    : Factor w/ 187 levels &quot;&quot;,&quot;A10&quot;,&quot;A11&quot;,..: 45 81 81 81 81 151 147 17 63 1 ...
##  $ embarked : Factor w/ 4 levels &quot;&quot;,&quot;Cherbourg&quot;,..: 4 4 4 4 4 4 4 4 4 2 ...
##  $ boat     : Factor w/ 28 levels &quot;&quot;,&quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 13 4 1 1 1 14 3 1 28 1 ...
##  $ body     : int  NA NA NA 135 NA NA NA NA NA 22 ...
##  $ home.dest: Factor w/ 369 levels &quot;&quot;,&quot;?Havana, Cuba&quot;,..: 309 231 231 231 231 237 163 25 23 229 ...</code></pre>
<p>We will use the following variables:</p>
<p><code>survived</code> - 1 if true, 0 otherwise;<br />
<code>sex</code> - the gender of the passenger;<br />
<code>age</code> - age of the passenger in years;<br />
<code>pclass</code> - the passengers class of passage;<br />
<code>sibsp</code> - the number of siblings/spouses aboard;<br />
<code>parch</code> - the number of parents/children aboard.</p>
<p>What predictors are associated with those who perished compared to those who survived?</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="regression-trees.html#cb371-1" tabindex="-1"></a>titan <span class="ot">&lt;-</span></span>
<span id="cb371-2"><a href="regression-trees.html#cb371-2" tabindex="-1"></a>  <span class="fu">rpart</span>(survived <span class="sc">~</span> sex <span class="sc">+</span> age <span class="sc">+</span> pclass <span class="sc">+</span> sibsp <span class="sc">+</span> parch,</span>
<span id="cb371-3"><a href="regression-trees.html#cb371-3" tabindex="-1"></a>        <span class="at">data =</span> titanic3,</span>
<span id="cb371-4"><a href="regression-trees.html#cb371-4" tabindex="-1"></a>        <span class="at">method =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb371-5"><a href="regression-trees.html#cb371-5" tabindex="-1"></a></span>
<span id="cb371-6"><a href="regression-trees.html#cb371-6" tabindex="-1"></a><span class="fu">prp</span>(</span>
<span id="cb371-7"><a href="regression-trees.html#cb371-7" tabindex="-1"></a>  titan,</span>
<span id="cb371-8"><a href="regression-trees.html#cb371-8" tabindex="-1"></a>  <span class="at">extra =</span> <span class="dv">1</span>,</span>
<span id="cb371-9"><a href="regression-trees.html#cb371-9" tabindex="-1"></a>  <span class="at">faclen =</span> <span class="dv">5</span>,</span>
<span id="cb371-10"><a href="regression-trees.html#cb371-10" tabindex="-1"></a>  <span class="at">box.col =</span> <span class="fu">c</span>(<span class="st">&quot;indianred1&quot;</span>, <span class="st">&quot;aquamarine&quot;</span>)[tree<span class="sc">$</span>frame<span class="sc">$</span>yval]</span>
<span id="cb371-11"><a href="regression-trees.html#cb371-11" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr25-1.png" width="672" /></p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="regression-trees.html#cb372-1" tabindex="-1"></a><span class="fu">barplot</span>(</span>
<span id="cb372-2"><a href="regression-trees.html#cb372-2" tabindex="-1"></a>  titan<span class="sc">$</span>variable.importance,</span>
<span id="cb372-3"><a href="regression-trees.html#cb372-3" tabindex="-1"></a>  <span class="at">horiz =</span> <span class="cn">TRUE</span>,</span>
<span id="cb372-4"><a href="regression-trees.html#cb372-4" tabindex="-1"></a>  <span class="at">col =</span> <span class="st">&quot;yellow3&quot;</span>,</span>
<span id="cb372-5"><a href="regression-trees.html#cb372-5" tabindex="-1"></a>  <span class="at">cex.axis =</span> <span class="fl">0.7</span>,</span>
<span id="cb372-6"><a href="regression-trees.html#cb372-6" tabindex="-1"></a>  <span class="at">cex.names =</span> <span class="fl">0.7</span></span>
<span id="cb372-7"><a href="regression-trees.html#cb372-7" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr25-2.png" width="672" /></p>
<p>If we want to see the cross-validation error and the <code>cp</code> table:</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="regression-trees.html#cb373-1" tabindex="-1"></a><span class="fu">printcp</span>(titan)</span></code></pre></div>
<pre><code>## 
## Classification tree:
## rpart(formula = survived ~ sex + age + pclass + sibsp + parch, 
##     data = titanic3, method = &quot;class&quot;)
## 
## Variables actually used in tree construction:
## [1] age    parch  pclass sex    sibsp 
## 
## Root node error: 500/1309 = 0.38197
## 
## n= 1309 
## 
##         CP nsplit rel error xerror     xstd
## 1 0.424000      0     1.000  1.000 0.035158
## 2 0.021000      1     0.576  0.576 0.029976
## 3 0.015000      3     0.534  0.548 0.029438
## 4 0.011333      5     0.504  0.544 0.029359
## 5 0.010000      9     0.458  0.544 0.029359</code></pre>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="regression-trees.html#cb375-1" tabindex="-1"></a><span class="fu">plotcp</span>(titan)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr26-1.png" width="672" /></p>
<p>Of course, we would like to see the tree’s prediction accuracy by using a test dataset and the confusion table metrics.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="regression-trees.html#cb376-1" tabindex="-1"></a><span class="fu">library</span>(ROCR)</span>
<span id="cb376-2"><a href="regression-trees.html#cb376-2" tabindex="-1"></a></span>
<span id="cb376-3"><a href="regression-trees.html#cb376-3" tabindex="-1"></a><span class="co">#test/train split</span></span>
<span id="cb376-4"><a href="regression-trees.html#cb376-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb376-5"><a href="regression-trees.html#cb376-5" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(titanic3), <span class="fu">nrow</span>(titanic3) <span class="sc">*</span> <span class="fl">0.7</span>)</span>
<span id="cb376-6"><a href="regression-trees.html#cb376-6" tabindex="-1"></a>train <span class="ot">&lt;-</span> titanic3[ind,]</span>
<span id="cb376-7"><a href="regression-trees.html#cb376-7" tabindex="-1"></a>test <span class="ot">&lt;-</span> titanic3[<span class="sc">-</span>ind,]</span>
<span id="cb376-8"><a href="regression-trees.html#cb376-8" tabindex="-1"></a></span>
<span id="cb376-9"><a href="regression-trees.html#cb376-9" tabindex="-1"></a><span class="co">#Tree on train</span></span>
<span id="cb376-10"><a href="regression-trees.html#cb376-10" tabindex="-1"></a>titan2 <span class="ot">&lt;-</span></span>
<span id="cb376-11"><a href="regression-trees.html#cb376-11" tabindex="-1"></a>  <span class="fu">rpart</span>(survived <span class="sc">~</span> sex <span class="sc">+</span> age <span class="sc">+</span> pclass <span class="sc">+</span> sibsp <span class="sc">+</span> parch,</span>
<span id="cb376-12"><a href="regression-trees.html#cb376-12" tabindex="-1"></a>        <span class="at">data =</span> train,</span>
<span id="cb376-13"><a href="regression-trees.html#cb376-13" tabindex="-1"></a>        <span class="at">method =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb376-14"><a href="regression-trees.html#cb376-14" tabindex="-1"></a>phat <span class="ot">&lt;-</span> <span class="fu">predict</span>(titan2, test, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb376-15"><a href="regression-trees.html#cb376-15" tabindex="-1"></a></span>
<span id="cb376-16"><a href="regression-trees.html#cb376-16" tabindex="-1"></a><span class="co">#AUC</span></span>
<span id="cb376-17"><a href="regression-trees.html#cb376-17" tabindex="-1"></a>pred_rocr <span class="ot">&lt;-</span> <span class="fu">prediction</span>(phat[, <span class="dv">2</span>], test<span class="sc">$</span>survived)</span>
<span id="cb376-18"><a href="regression-trees.html#cb376-18" tabindex="-1"></a>auc_ROCR <span class="ot">&lt;-</span> <span class="fu">performance</span>(pred_rocr, <span class="at">measure =</span> <span class="st">&quot;auc&quot;</span>)</span>
<span id="cb376-19"><a href="regression-trees.html#cb376-19" tabindex="-1"></a>auc_ROCR<span class="sc">@</span>y.values[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0.814118</code></pre>
<p>Here, we report only AUC in this simple example. We can use Moreover, we can reweigh variables so that the loss or the cost of a wrong split would be more or less important (see cost argument in <code>rpart</code>). Finally, as in every classification, we can put a different weight on the correct classifications than the wrong classifications (or vise verse). This can easily be done in <code>rpart</code> by the loss matrix.</p>
<p>Before commenting on the strengths and weaknesses of CART, let’s see a regression tree.</p>
</div>
<div id="regression-tree" class="section level2 hasAnchor" number="16.5">
<h2><span class="header-section-number">16.5</span> Regression Tree<a href="regression-trees.html#regression-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The same partitioning procedure can be applied when the outcome variable is not qualitative. For a classification problem, a splitting criterion was either the Gini or log-likelihood function. When we have numerical outcome variable, we can can use the anova method to decide which variable gives the best split:</p>
<p><span class="math display">\[
S S_{T}-\left(S S_{L}+S S_{R}\right),
\]</span>
where</p>
<p><span class="math display">\[
SS=\sum\left(y_{i}-\bar{y}\right)^{2},
\]</span></p>
<p>which is the sum of squares for the node (T), the right (R), and the left (L) splits.</p>
<p>Similar to our delta method, if <span class="math inline">\(SS_{T}-\left(SS_{L}+SS_{R}\right)\)</span> is positive and significant, we make the split on the node (the variable). After the split, the fitted value of the node is the mean of <span class="math inline">\(y\)</span> of that node.</p>
<p>The <code>anova</code> method is the default method if <span class="math inline">\(y\)</span> a simple numeric vector. However, when <span class="math inline">\(y_i \in (0,1)\)</span>,</p>
<p><span class="math display">\[
SS_{T}=\sum\left(y_{i}-\bar{y}\right)^{2}=\sum y_{i}^2 -n\bar{y}^2=\sum y_{i} -n\bar{y}^2=n\bar y -n\bar{y}^2=np(1-p)
\]</span></p>
<p>Hence, we can show that the <strong>relative gain</strong> would be the same in regression trees using <span class="math inline">\(SS_T\)</span> or Gini when <span class="math inline">\(y_i \in (0,1)\)</span>.</p>
<p>It is not hard to write a simple loop similar to our earlier algorithm, but it would be redundant. We will use <code>rpart</code> in an example:</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="regression-trees.html#cb378-1" tabindex="-1"></a><span class="co"># simulated data</span></span>
<span id="cb378-2"><a href="regression-trees.html#cb378-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb378-3"><a href="regression-trees.html#cb378-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">100</span>,<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb378-4"><a href="regression-trees.html#cb378-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">1</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="dv">4</span> <span class="sc">*</span> <span class="fu">I</span>(x <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">-</span> <span class="dv">4</span> <span class="sc">*</span> <span class="fu">I</span>(x <span class="sc">^</span> <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">6</span>)</span>
<span id="cb378-5"><a href="regression-trees.html#cb378-5" tabindex="-1"></a>dt <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;y&quot;</span> <span class="ot">=</span> y, <span class="st">&quot;x&quot;</span> <span class="ot">=</span> x)</span>
<span id="cb378-6"><a href="regression-trees.html#cb378-6" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr28-1.png" width="672" /></p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="regression-trees.html#cb379-1" tabindex="-1"></a><span class="co"># Tree</span></span>
<span id="cb379-2"><a href="regression-trees.html#cb379-2" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(y <span class="sc">~</span> x, <span class="at">minsplit =</span> <span class="dv">83</span>, dt) <span class="co"># we want to have 1 split</span></span>
<span id="cb379-3"><a href="regression-trees.html#cb379-3" tabindex="-1"></a><span class="fu">fancyRpartPlot</span>(fit1)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr28-2.png" width="672" /></p>
<p>When we have split at <span class="math inline">\(x=-0.65\)</span>, <code>rpart</code> calculates two constant <span class="math inline">\(\hat{f}(x_i)\)</span>’s both for the left and right splits:</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="regression-trees.html#cb380-1" tabindex="-1"></a><span class="fu">mean</span>(y[x <span class="sc">&lt;=</span> <span class="sc">-</span><span class="fl">0.65</span>])</span></code></pre></div>
<pre><code>## [1] 15.33681</code></pre>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="regression-trees.html#cb382-1" tabindex="-1"></a><span class="fu">mean</span>(y[x <span class="sc">&gt;</span> <span class="sc">-</span><span class="fl">0.65</span>])</span></code></pre></div>
<pre><code>## [1] 0.9205211</code></pre>
<p>Here, we see them on the plot:</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="regression-trees.html#cb384-1" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb384-2"><a href="regression-trees.html#cb384-2" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>)</span>
<span id="cb384-3"><a href="regression-trees.html#cb384-3" tabindex="-1"></a><span class="fu">lines</span>(z, <span class="fu">predict</span>(fit1, <span class="fu">data.frame</span>(<span class="at">x =</span> z)), <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb384-4"><a href="regression-trees.html#cb384-4" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="sc">-</span><span class="fl">0.65</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr30-1.png" width="672" /></p>
<p>If we reduce the <code>minsplit</code>,</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="regression-trees.html#cb385-1" tabindex="-1"></a><span class="co"># Tree</span></span>
<span id="cb385-2"><a href="regression-trees.html#cb385-2" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">rpart</span>(y <span class="sc">~</span> x, <span class="at">minsplit =</span> <span class="dv">6</span>, dt)</span>
<span id="cb385-3"><a href="regression-trees.html#cb385-3" tabindex="-1"></a><span class="fu">fancyRpartPlot</span>(fit2)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr31-1.png" width="672" /></p>
<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb386-1"><a href="regression-trees.html#cb386-1" tabindex="-1"></a><span class="co"># On the plot</span></span>
<span id="cb386-2"><a href="regression-trees.html#cb386-2" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>)</span>
<span id="cb386-3"><a href="regression-trees.html#cb386-3" tabindex="-1"></a><span class="fu">lines</span>(z, <span class="fu">predict</span>(fit2, <span class="fu">data.frame</span>(<span class="at">x =</span> z)), <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr31-2.png" width="672" /></p>
<p>We will use an example of predicting Baseball players’ salaries from the ISLR package <span class="citation">(<a href="#ref-ISLR_2021"><strong>ISLR_2021?</strong></a>)</span>. This data set is deduced from the Baseball fielding data set reflecting the fielding performance that includes the numbers of <code>Errors</code>, <code>Putouts</code> and <code>Assists</code> made by each player.</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="regression-trees.html#cb387-1" tabindex="-1"></a><span class="co"># Hitters data</span></span>
<span id="cb387-2"><a href="regression-trees.html#cb387-2" tabindex="-1"></a><span class="fu">library</span>(ISLR)</span>
<span id="cb387-3"><a href="regression-trees.html#cb387-3" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;Hitters&quot;</span>)</span>
<span id="cb387-4"><a href="regression-trees.html#cb387-4" tabindex="-1"></a><span class="fu">str</span>(Hitters)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    322 obs. of  20 variables:
##  $ AtBat    : int  293 315 479 496 321 594 185 298 323 401 ...
##  $ Hits     : int  66 81 130 141 87 169 37 73 81 92 ...
##  $ HmRun    : int  1 7 18 20 10 4 1 0 6 17 ...
##  $ Runs     : int  30 24 66 65 39 74 23 24 26 49 ...
##  $ RBI      : int  29 38 72 78 42 51 8 24 32 66 ...
##  $ Walks    : int  14 39 76 37 30 35 21 7 8 65 ...
##  $ Years    : int  1 14 3 11 2 11 2 3 2 13 ...
##  $ CAtBat   : int  293 3449 1624 5628 396 4408 214 509 341 5206 ...
##  $ CHits    : int  66 835 457 1575 101 1133 42 108 86 1332 ...
##  $ CHmRun   : int  1 69 63 225 12 19 1 0 6 253 ...
##  $ CRuns    : int  30 321 224 828 48 501 30 41 32 784 ...
##  $ CRBI     : int  29 414 266 838 46 336 9 37 34 890 ...
##  $ CWalks   : int  14 375 263 354 33 194 24 12 8 866 ...
##  $ League   : Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 1 2 1 2 2 1 2 1 2 1 ...
##  $ Division : Factor w/ 2 levels &quot;E&quot;,&quot;W&quot;: 1 2 2 1 1 2 1 2 2 1 ...
##  $ PutOuts  : int  446 632 880 200 805 282 76 121 143 0 ...
##  $ Assists  : int  33 43 82 11 40 421 127 283 290 0 ...
##  $ Errors   : int  20 10 14 3 4 25 7 9 19 0 ...
##  $ Salary   : num  NA 475 480 500 91.5 750 70 100 75 1100 ...
##  $ NewLeague: Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 1 2 1 2 2 1 1 1 2 1 ...</code></pre>
<p>What predictors are associated with baseball player’s Salary (1987 annual salary on opening day in thousands of dollars)?</p>
<p>Let’s consider 3 covariates for the sake of simplicity: <code>Years</code> (Number of years in the major leagues); <code>Hits</code> (Number of hits in 1986); <code>Atbat</code> (Number of times at bat in 1986).</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="regression-trees.html#cb389-1" tabindex="-1"></a><span class="co"># Remove NA&#39;s</span></span>
<span id="cb389-2"><a href="regression-trees.html#cb389-2" tabindex="-1"></a>df <span class="ot">&lt;-</span> Hitters[<span class="fu">complete.cases</span>(Hitters<span class="sc">$</span>Salary),]</span>
<span id="cb389-3"><a href="regression-trees.html#cb389-3" tabindex="-1"></a>dfshort <span class="ot">&lt;-</span> df[, <span class="fu">c</span>(<span class="dv">19</span>, <span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">1</span>)]</span>
<span id="cb389-4"><a href="regression-trees.html#cb389-4" tabindex="-1"></a></span>
<span id="cb389-5"><a href="regression-trees.html#cb389-5" tabindex="-1"></a><span class="co"># cp=0, so it&#39;s fully grown</span></span>
<span id="cb389-6"><a href="regression-trees.html#cb389-6" tabindex="-1"></a>tree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(<span class="fu">log</span>(Salary) <span class="sc">~</span> Years <span class="sc">+</span> Hits <span class="sc">+</span> AtBat, <span class="at">data =</span> dfshort, <span class="at">cp =</span> <span class="dv">0</span>)</span>
<span id="cb389-7"><a href="regression-trees.html#cb389-7" tabindex="-1"></a></span>
<span id="cb389-8"><a href="regression-trees.html#cb389-8" tabindex="-1"></a><span class="fu">prp</span>(tree, <span class="at">extra =</span> <span class="dv">1</span>, <span class="at">faclen =</span> <span class="dv">5</span>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr33-1.png" width="672" /></p>
<p>It works on the same principle as we described before: find terminal nodes that minimize the sum of squares. This process may give us a good prediction on the training set but not on the test set, as it overfits the data. Hence, we use a pruned tree found by <code>rpart</code> by cross-validation:</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="regression-trees.html#cb390-1" tabindex="-1"></a>ptree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(<span class="fu">log</span>(Salary) <span class="sc">~</span> Years <span class="sc">+</span> Hits <span class="sc">+</span> AtBat, <span class="at">data =</span> dfshort)</span>
<span id="cb390-2"><a href="regression-trees.html#cb390-2" tabindex="-1"></a><span class="fu">prp</span>(ptree, <span class="at">extra=</span><span class="dv">1</span>, <span class="at">faclen=</span><span class="dv">5</span>)</span></code></pre></div>
<p><img src="16-RegressionTrees_files/figure-html/tr34-1.png" width="672" /></p>
<p>We can see its prediction power similar to what we did in the Titanic data example. Since this is a regression, we can ask which one is better, a tree or a linear model? If the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span> is linear, a linear model should perform better. We can test this:</p>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb391-1"><a href="regression-trees.html#cb391-1" tabindex="-1"></a><span class="co"># Test/train split</span></span>
<span id="cb391-2"><a href="regression-trees.html#cb391-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb391-3"><a href="regression-trees.html#cb391-3" tabindex="-1"></a>ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(dfshort), <span class="fu">nrow</span>(dfshort) <span class="sc">*</span> <span class="fl">0.7</span>)</span>
<span id="cb391-4"><a href="regression-trees.html#cb391-4" tabindex="-1"></a>train <span class="ot">&lt;-</span> dfshort[ind,]</span>
<span id="cb391-5"><a href="regression-trees.html#cb391-5" tabindex="-1"></a>test <span class="ot">&lt;-</span> dfshort[<span class="sc">-</span>ind,]</span>
<span id="cb391-6"><a href="regression-trees.html#cb391-6" tabindex="-1"></a></span>
<span id="cb391-7"><a href="regression-trees.html#cb391-7" tabindex="-1"></a><span class="co"># Tree and lm() on train</span></span>
<span id="cb391-8"><a href="regression-trees.html#cb391-8" tabindex="-1"></a>ptree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(<span class="fu">log</span>(Salary) <span class="sc">~</span> Years <span class="sc">+</span> Hits <span class="sc">+</span> AtBat, <span class="at">data =</span> dfshort)</span>
<span id="cb391-9"><a href="regression-trees.html#cb391-9" tabindex="-1"></a>predtree <span class="ot">&lt;-</span> <span class="fu">predict</span>(ptree, test)</span>
<span id="cb391-10"><a href="regression-trees.html#cb391-10" tabindex="-1"></a>lin <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(Salary) <span class="sc">~</span> ., <span class="at">data =</span> dfshort)</span>
<span id="cb391-11"><a href="regression-trees.html#cb391-11" tabindex="-1"></a>predlin <span class="ot">&lt;-</span> <span class="fu">predict</span>(lin, test)</span>
<span id="cb391-12"><a href="regression-trees.html#cb391-12" tabindex="-1"></a></span>
<span id="cb391-13"><a href="regression-trees.html#cb391-13" tabindex="-1"></a><span class="co"># RMSPE</span></span>
<span id="cb391-14"><a href="regression-trees.html#cb391-14" tabindex="-1"></a>rmspe_tree <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((<span class="fu">log</span>(test<span class="sc">$</span>Salary) <span class="sc">-</span> predtree) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb391-15"><a href="regression-trees.html#cb391-15" tabindex="-1"></a>rmspe_tree</span></code></pre></div>
<pre><code>## [1] 0.4601892</code></pre>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb393-1"><a href="regression-trees.html#cb393-1" tabindex="-1"></a>rmspe_lin <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((<span class="fu">log</span>(test<span class="sc">$</span>Salary) <span class="sc">-</span> predlin) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb393-2"><a href="regression-trees.html#cb393-2" tabindex="-1"></a>rmspe_lin</span></code></pre></div>
<pre><code>## [1] 0.6026888</code></pre>
<p>In this simple example, our the tree would do a better job.</p>
<p>Trees tend to work well for problems where there are important nonlinearities and interactions. Yet, they are known to be quite sensitive to the original sample. Therefore, the models trained in one sample may have poor predictive accuracy on another sample. These problems motivate Random Forest and Boosting methods, as we will describe in following chapters.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p><a href="https://freakonometrics.hypotheses.org/52776">freakonometrics</a> <span class="citation">(<a href="#ref-Charpentier_scratch"><strong>Charpentier_scratch?</strong></a>)</span><a href="regression-trees.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shrinkage-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ensemble-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mutluyuk/machinemetrics/edit/master/16-RegressionTrees.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["machinemetrics.pdf", "machinemetrics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
