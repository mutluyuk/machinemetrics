# Decompositions

## Matrix Decomposition


  
Matrix decomposition, also known as matrix factorization, is a process of breaking down a matrix into simpler components that can be used to simplify calculations, solve systems of equations, and gain insight into the underlying structure of the matrix. 
  
Matrix decomposition plays an important role in machine learning, particularly in the areas of dimensionality reduction, data compression, and feature extraction. For example, Principal Component Analysis (PCA) is a popular method for dimensionality reduction, which involves decomposing a high-dimensional data matrix into a lower-dimensional representation while preserving the most important information. PCA achieves this by finding the eigenvectors and eigenvalues of the covariance matrix of the data and then selecting the top eigenvectors as the new basis for the data.

Singular Value Decomposition (SVD) is also commonly used in recommender systems to find latent features in user-item interaction data. SVD decomposes the user-item interaction matrix into three matrices: a left singular matrix, a diagonal matrix of singular values, and a right singular matrix. The left and right singular matrices represent user and item features, respectively, while the singular values represent the importance of those features.

Rank optimization is another method that finds a low-rank approximation of a matrix that best fits a set of observed data. In other words, it involves finding a lower-rank approximation of a given matrix that captures the most important features of the original matrix.  For example, SVD decomposes a matrix into a product of low-rank matrices, while PCA finds the principal components of a data matrix, which can be used to create a lower-dimensional representation of the data.   In machine learning, rank optimization is often used in applications such as collaborative filtering, image processing, and data compression. By finding a low-rank approximation of a matrix, it is possible to reduce the amount of memory needed to store the matrix and improve the efficiency of algorithms that work with the matrix.

We start with the eigenvalue decomposition (EVD), which is the foundation to many matrix decomposition methods

## Eigenvectors and eigenvalues  

Eigenvalues and eigenvectors have many important applications in linear algebra and beyond. For example, in machine learning, principal component analysis (PCA) involves computing the eigenvectors and eigenvalues of the covariance matrix of a data set, which can be used to reduce the dimensionality of the data while preserving its important features. 

Almost all vectors change direction, when they are multiplied by a matrix, $\mathbf{A}$, except for certain vectors ($\mathbf{v}$) that are in the same direction as $\mathbf{A} \mathbf{v}.$ Those vectors are called "eigenvectors". 

We can see how we obtain the eigenvalues and eigenvectors of a matrix $\mathbf{A}$. If

$$
\mathbf{A} \mathbf{v}=\lambda \mathbf{v}
$$
 
Then,
   
$$
\begin{aligned}
&\mathbf{A} \mathbf{v}-\lambda \mathbf{I} \mathbf{v}=0 \\
&(\mathbf{A}-\lambda \mathbf{I}) \mathbf{v}=0,
\end{aligned}
$$
where $\mathbf{I}$ is the identity matrix. It turns out that this equation is equivalent to:

$$
\operatorname{det}(\mathbf{A}-\lambda \mathbf{I})=0,
$$

because $\operatorname{det}(\mathbf{A}-\lambda \mathbf{I}) \equiv(\mathbf{A}-\lambda \mathbf{I}) \mathbf{v}=0$.  The reason is that we want a non-trivial solution to $(\mathbf{A}-\lambda \mathbf{I}) \mathbf{v}=0$.  Therefore, $(\mathbf{A}-\lambda \mathbf{I})$ should be non-invertible. Otherwise, if it is invertible, we get $\mathbf{v}=(\mathbf{A}-\lambda \mathbf{I})^{-1} \cdot 0=0$, which is a trivial solution. Since a matrix is non-invertible if its determinant is 0 . Thus, $\operatorname{det}(\mathbf{A}-\lambda \mathbf{I})=0$ for non-trivial solutions.

We start with a square matrix, $\mathbf{A}$, like

$$
A =\left[\begin{array}{cc}
1 & 2 \\
3 & -4
\end{array}\right]
$$
$$
\begin{aligned}
\det (\mathbf{A}-\lambda \mathbf{I})=
& \left|\begin{array}{cc}
1-\lambda & 2 \\
3 & -4-\lambda
\end{array}\right|=(1-\lambda)(-4-\lambda)-2 \cdot 3 \\
& =-4-\lambda+4 \lambda+\lambda^2-6 \\
& =\lambda^2+3 \lambda-10 \\
& =(\lambda-2)(\lambda+5)=0 \\
& \therefore \lambda_1=2, ~ \lambda_2=-5 \\
&
\end{aligned}
$$

We have two eigenvalues.  We now need to consider each eigenvalue indivudally

$$
\begin{gathered}
\lambda_1=2 \\
(A 1-\lambda I) \mathbf{v}=0 \\
{\left[\begin{array}{cc}
1-\lambda_1 & 2 \\
3 & -4-\lambda_1
\end{array}\right]\left[\begin{array}{l}
v_1 \\
v_2
\end{array}\right]=\left[\begin{array}{l}
0 \\
0
\end{array}\right]} \\

{\left[\begin{array}{cc}
-1 & 2 \\
3 & -6
\end{array}\right]\left[\begin{array}{l}
v_1 \\
v_2
\end{array}\right]=\left[\begin{array}{l}
0 \\
0
\end{array}\right]}
\end{gathered}
$$
Hence, 

$$
\begin{aligned}
-v_1+2 v_2=0 \\
3 v_1-6 v_2=0\\
v_1=2, ~ v_2=1
\end{aligned}
$$
And,

$$
\begin{aligned}
&  \lambda_2=-5 \\
& {\left[\begin{array}{cc}
1-\lambda_2 & 2 \\
3 & -4-\lambda_2
\end{array}\right]\left[\begin{array}{l}
v_1 \\
v_2
\end{array}\right]=\left[\begin{array}{l}
0 \\
0
\end{array}\right]} \\
& {\left[\begin{array}{cc}
6 & 2 \\
3 & 1
\end{array}\right]\left[\begin{array}{l}
v_1 \\
v_2
\end{array}\right]=\left[\begin{array}{l}
0 \\
0
\end{array}\right]} 

\end{aligned}
$$
Hence, 

$$
\begin{gathered}
6 v_1+2 v_2=0 \\
3 v_1+v_2=0 \\

v_1=-1,~ v_2=3
\end{gathered}
$$
We have two eigenvalues

$$
\begin{aligned}
& \lambda_1=2 \\
& \lambda_2=-5
\end{aligned}
$$

And two corresponding eigenvectors

$$
\left[\begin{array}{l}
2 \\
1
\end{array}\right],\left[\begin{array}{c}
-1 \\
3
\end{array}\right]
$$
for $\lambda_1=2$

$$
\left[\begin{array}{cc}
1 & 2 \\
3 & -4
\end{array}\right]\left[\begin{array}{l}
2 \\
1
\end{array}\right]=\left[\begin{array}{l}
2+2 \\
6-4
\end{array}\right]=\left[\begin{array}{l}
4 \\
2
\end{array}\right]=2\left[\begin{array}{l}
2 \\
1
\end{array}\right]
$$
Let's see the solution in R

```{r}
A <- matrix(c(1, 3, 2, -4), 2, 2)
eigen(A)
```

The eigenvectors are typically normalized by dividing by its length $\sqrt{v^{\prime} v}$, which is 5 in our case for $\lambda_1=2$.

```{r}
# For the ev (2, 1), for lambda
c(2, 1) / sqrt(5)
```

There some nice properties that we can observe in this application.

```{r ev1}
# Sum of eigenvalues = sum of diagonal terms of A (Trace of A)
ev <- eigen(A)$values
sum(ev) == sum(diag(A))

# Product of eigenvalues = determinant of A
round(prod(ev), 4) == round(det(A), 4)

# Diagonal matrix D has eigenvalues = diagonal elements
D <- matrix(c(2, 0, 0, 5), 2, 2)
eigen(D)$values == sort(diag(D), decreasing = TRUE)
```

We can see that, if one of the eigenvalues is zero for a matrix, the determinant of the matrix will be zero.  We willl return to this issue in Singluar Value Decomposition.

Let's finish this chapter with Diagonalization and Eigendecomposition.

Suppose we have $m$ linearly independent eigenvectors ($\mathbf{v_i}$ is eigenvector $i$ in a column vector in $\mathbf{V}$) of $\mathbf{A}$.

$$
\mathbf{AV}=\mathbf{A}\left[\mathbf{v_1} \mathbf{v_2} \cdots \mathbf{v_m}\right]=\left[\mathbf{A} \mathbf{v_1} \mathbf{A} \mathbf{v_2} \ldots \mathbf{A} \mathbf{v_m}\right]=\left[\begin{array}{llll}
\lambda_1 \mathbf{v_1} & \lambda_2\mathbf{v_2}  & \ldots & \lambda_m \mathbf{v_m}
\end{array}\right]
$$

because 

$$
\mathbf{A} \mathbf{v}=\lambda \mathbf{v}
$$

$$
\mathbf{AV}=\left[\mathbf{v_1} \mathbf{v_2} \cdots \mathbf{v_m}\right]\left[\begin{array}{cccc}
\lambda_1 & 0 & \cdots & 0 \\
0 & \lambda_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\

0 & 0 & \cdots & \lambda_m
\end{array}\right]=\mathbf{V}\Lambda
$$
So that

$$
\mathbf{A V=V \Lambda}
$$
Hence,   

$$
\mathbf{A}=\mathbf{V} \Lambda \mathbf{V}^{-1}
$$

Eigendecomposition (a.k.a. spectral decomposition) decomposes a matrix $\mathbf{A}$ into a multiplication of a matrix of eigenvectors $\mathbf{V}$ and a diagonal matrix of eigenvalues $\mathbf{\Lambda}$.
  
**This can only be done if a matrix is diagonalizable**. In fact, the definition of a diagonalizable matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is that it can be eigendecomposed into $n$ eigenvectors, so that $\mathbf{V}^{-1} \mathbf{A} \mathbf{V}=\Lambda$.

$$
\begin{align}
\mathbf{A}^2&=(\mathbf{V} \Lambda \mathbf{V}^{-1})(\mathbf{V} \Lambda \mathbf{V}^{-1})\\
&=\mathbf{V} \Lambda \text{I} \Lambda \mathbf{V}^{-1}\\
&=\mathbf{V} \Lambda^2 \mathbf{V}^{-1}\\
\end{align}
$$
in general

$$
\mathbf{A}^k=\mathbf{V} \Lambda^k \mathbf{V}^{-1}
$$

Example:
  
```{r ev2}
A = matrix(sample(1:100, 9), 3, 3)
A
eigen(A)
V = eigen(A)$vectors
Lam = diag(eigen(A)$values)
# Prove that AV = VLam
round(A %*% V, 4) == round(V %*% Lam, 4)
# And decomposition
A == round(V %*% Lam %*% solve(V), 4)
```

And, matrix inverse with eigendecomposition:

$$
\mathbf{A}^{-1}=\mathbf{V} \Lambda^{-1} \mathbf{V}^{-1}
$$

Example:
  
```{r ev3}
A = matrix(sample(1:100, 9), 3, 3)
A
V = eigen(A)$vectors
Lam = diag(eigen(A)$values)

# Inverse of A
solve(A)
# And
V %*% solve(Lam) %*% solve(V)
```

The inverse of $\mathbf{\Lambda}$ is just the inverse of each diagonal element (the eigenvalues).  But, this can only be done if a matrix is diagonalizable.  So if $\mathbf{A}$ is not $n \times n$, then we can use $\mathbf{A'A}$ or $\mathbf{AA'}$, both symmetric now.

Example:
$$
\mathbf{A}=\left(\begin{array}{ll}
1 & 2 \\
2 & 4
\end{array}\right)
$$

As $\det(\mathbf{A})=0,$ $\mathbf{A}$ is singular and its inverse is undefined.  In other words, since $\det(\mathbf{A})$ equals the product of the eigenvalues $\lambda_j$ of $\mathrm{A}$, the matrix $\mathbf{A}$ has an eigenvalue which is zero.

To see this, consider the spectral (eigen) decomposition of $A$ :
$$
\mathbf{A}=\sum_{j=1}^{p} \theta_{j} \mathbf{v}_{j} \mathbf{v}_{j}^{\top}
$$
where $\mathbf{v}_{\mathrm{j}}$ is the eigenvector belonging to $\theta_{\mathrm{j}}$

The inverse of $\mathbf{A}$ is then:
  
$$
\mathbf{A}^{-1}=\sum_{j=1}^{p} \theta_{j}^{-1} \mathbf{v}_{j} \mathbf{v}_{j}^{\top}
$$

A has eigenvalues 5 and 0. The inverse of $A$ via the spectral decomposition is then undefined:
  
$$
\mathbf{A}^{-1}=\frac{1}{5} \mathbf{v}_{1} \mathbf{v}_{1}^{\top}+ \frac{1}{0} \mathbf{v}_{1} \mathbf{v}_{1}^{\top}
$$


## Singular Value Decomposition

Singular Value Decomposition (SVD) is another type of decomposition. Different than eigendecomposition, which requires a square matrix, SVD allows us to decompose a rectangular matrix. This is more useful because the rectangular matrix usually represents data in practice.

For any matrix $\mathbf{A}$, both $\mathbf{A^{\top} A}$ and $\mathbf{A A^{\top}}$ are symmetric.  Therefore, they have $n$ and $m$ **orthogonal* eigenvectors, respectively. The proof is simple:

Suppose we have a 2 x 2 symmetric matrix, $\mathbf{A}$, with two distinct eigenvalues ($\lambda_1, \lambda_2$) and two corresponding eigenvectors ($\mathbf{v}_1$ and $\mathbf{v}_1$).  Following the rule, 

$$
\begin{aligned}
& \mathbf{A} \mathbf{v}_1=\lambda_1 \mathbf{v}_1, \\
& \mathbf{A} \mathbf{v}_2=\lambda_2 \mathbf{v}_2. \\
\end{aligned}
$$
Let's multiply (inner product) the first one with $\mathbf{v}_2^{\top}$:

$$
\mathbf{v}_2^{\top}\mathbf{A} \mathbf{v}_1=\lambda_1 \mathbf{v}_2^{\top} \mathbf{v}_1
$$
And, the second one with  $\mathbf{v}_1^{\top}$

$$
\mathbf{v}_1^{\top}\mathbf{A} \mathbf{v}_2=\lambda_2 \mathbf{v}_1^{\top} \mathbf{v}_2
$$
If we take the transpose of both side of $\mathbf{v}_2^{\top}\mathbf{A} \mathbf{v}_1=\lambda_1 \mathbf{v}_2^{\top} \mathbf{v}_1$, it will be

$$
\mathbf{v}_1^{\top}\mathbf{A} \mathbf{v}_2=\lambda_1 \mathbf{v}_1^{\top} \mathbf{v}_2
$$
And, subtract these last two:

$$
\begin{aligned}
&\mathbf{v}_1^{\top}\mathbf{A} \mathbf{v}_2=\lambda_2 \mathbf{v}_1^{\top} \mathbf{v}_2 \\
& \mathbf{v}_1^{\top}\mathbf{A} \mathbf{v}_2=\lambda_1 \mathbf{v}_1^{\top} \mathbf{v}_2 \\
& \hline 0=\left(\lambda_2 - \lambda_1\right)  \mathbf{v}_1^{\top} \mathbf{v}_2 
\end{aligned}
$$
Since , $\lambda_1$ and $\lambda_2$ are distinct, $\lambda_2- \lambda_1$ cannot be zero. Therefore,  $ \mathbf{v}_1^{\top} \mathbf{v}_2 = 0$.  As we saw in Chapter 15, the dot products of two vectors can be expressed geometrically

$$
\begin{aligned}
a \cdot b=\|a\|\|b\| \cos (\theta),\\
\cos (\theta)=\frac{a \cdot b}{\|a\|\|b\|}
\end{aligned}
$$
Hence, $\cos (\theta)$ has to be zero for $ \mathbf{v}_1^{\top} \mathbf{v}_2 = 0$. Since $\cos (90)=0$, the two vectors are orthogonal.

We start with the following eigendecomposition for $\mathbf{A^{\top}A}$ and $\mathbf{A A^{\top}}$:

$$
\begin{aligned}
\mathbf{A^{\top} A =V D V^{\top}} \\
\mathbf{A A^{\top} =U D^{\prime} U^{\top}}
\end{aligned}
$$

where $\mathbf{V}$ is an $n \times n$ **orthogonal** matrix consisting of the eigenvectors of $\mathbf{A}^{\top}\mathbf{A},$ and, $\mathbf{D}$ is an $n \times n$ diagonal matrix with the eigenvalues of $\mathbf{A^{\top} A}$ on the diagonal.  The same decomposition for $\mathbf{A A^{\top}}$, now $\mathbf{U}$ is an $m \times m$ **orthogonal** matrix consisting of the eigenvectors of $\mathbf{A A^{\top}}$, and $\mathbf{D^{\prime}}$ is an $m \times m$ diagonal matrix with the eigenvalues of $\mathbf{A A^{\top}}$ on the diagonal.
  
It turns out that $\mathbf{D}$ and $\mathbf{D^{\prime}}$ have the same non-zero diagonal entries except that the order might be different.

We can write SVD for any real $m \times n$ matrix as  

$$
\mathbf{A=U \Sigma V^{\top}}
$$
  
where $\mathbf{U}$ is an $m \times m$ orthogonal matrix whose columns are the eigenvectors of $\mathbf{A A^{\top}}$, $\mathbf{V}$ is an $n \times n$ orthogonal matrix whose columns are the eigenvectors of $\mathbf{A^{\top} A}$, and $\mathbf{\Sigma}$ is an $m \times n$ diagonal matrix of the form:

$$
\mathbf{\Sigma}=\left(\begin{array}{cccc}
\sigma_{1} & & & \\
& \ddots &  \\
& & \sigma_{n} & \\
0 & 0 & 0 \\
0 & 0 &0 \\
\end{array}\right)
$$
with $\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{n}>0$ .  The number of non-zero singular values is equal to the rank of $\operatorname{rank}(\mathbf{A})$. In $\mathbf{\Sigma}$ above, $\sigma_{1}, \ldots, \sigma_{n}$ are the square roots of the eigenvalues of $\mathbf{A^{\top} A}$. They are called the **singular values** of $\mathbf{A}$.
  
One important point is that, although $\mathbf{U}$ in $\mathbf{U \Sigma V^{\top}}$ is $m \times m$, when it is multiplied by $\mathbf{\Sigma}$, it reduces to $n \times n$ due to zeros in $\mathbf{\Sigma}$.  Hence, we can actually select only those in $\mathbf{U}$ that are not going to be zeroed out due to that multiplication.  When we take only $n \times n$ from $\mathbf{U}$ matrix, it is called "Economy SVD", $\mathbf{\hat{U} \hat{\Sigma} V^{\top}}$, where all matrices will be $n \times n$.    
  
The singular value decomposition is very useful when our basic goal is to "solve" the system $\mathbf{A} x=b$ for all matrices $\mathbf{A}$ and vectors $b$ with a numerically stable algorithm. Some important applications of the SVD include computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix. We will see some of them in the following chapters

Here is an example:

```{r}
set.seed(104)
A <- matrix(sample(100, 12), 3, 4)
A
svda <- svd(A)
svda

# Singular values = sqrt(eigenvalues of t(A)%*%A))
ev <- eigen(t(A) %*% A)$values
round(sqrt(ev), 5)
```

Note that this ""Economy SVD" using only the non-zero eigenvalues and their respective eigenvectors.

```{r}
Ar <- svda$u %*% diag(svda$d) %*% t(svda$v)
Ar
```

As we use SVD in the following chapter, its usefulness will be obvious.  

## Rank(r) Approximations

One of the useful applications of singular value decomposition (SVD) is rank approximations, or matrix approximations.  

We can write $\mathbf{A=U \Sigma V^{\top}}$ as

$$
=\sigma_{1} u_{1} v_{1}^{\top}+\sigma_{2} u_{2} v_{2}^{\top}+\ldots+\sigma_{n} u_{n} v_{n}^{\top}+ 0.
$$
Each term in this equation is a Rank(1) matrix: $u_1$ is $n \times 1$ column vector and $v_1$ is $1 \times n$ row vector. Since these are the only orthogonal entries in the resulting matrix, the first term with $\sigma_1$ is a Rank(1) $n \times n$ matrix. All other terms have the same dimension. Since $\sigma$'s are ordered, the first term is the carries the most information.  So, Rank(1) approximation is taking only the first term and ignoring the others.  Here is a simple example:

```{r ra1}
#rank-one approximation
A <- matrix(c(1, 5, 4, 2), 2 , 2)
A

v1 <- matrix(eigen(t(A) %*% (A))$vector[, 1], 1, 2)
sigma <- sqrt(eigen(t(A) %*% (A))$values[1])
u1 <- matrix(eigen(A %*% t(A))$vector[, 1], 2, 1)

# Rank(1) approximation of A
Atilde <- sigma * u1 %*% v1
Atilde
```
  
And, Rank(2) approximation can be obtained by adding the first 2 terms. As we add more terms, we can get the full information in the data.  But often times, we truncate the ranks at $r$ by removing the terms with small $sigma$.  This is also called noise reduction.
  
There are many examples on the Internet for real image compression, but we apply rank approximation to a heatmap from our own work. The heatmap shows moving-window partial correlations between daily positivity rates (Covid-19) and mobility restrictions for different time delays (days, "lags")  

```{r ra2, warning=FALSE, message=FALSE}
comt <- readRDS("comt.rds")

heatmap(
  comt,
  Colv = NA,
  Rowv = NA,
  main = "Heatmap - Original",
  xlab = "Lags",
  ylab = "Starting days of 7-day rolling windows"
)

# Rank(2) with SVD
fck <- svd(comt)
r = 2
comt.re <-
  as.matrix(fck$u[, 1:r]) %*% diag(fck$d)[1:r, 1:r] %*% t(fck$v[, 1:r])

heatmap(
  comt.re,
  Colv = NA,
  Rowv = NA,
  main = "Heatmap Matrix - Rank(2) Approx",
  xlab = "Lags",
  ylab = "Startting days of 7-day rolling windows"
)
```

This Rank(2) approximation reduces the noise in the moving-window partial correlations so that we can see the clear trend about the delay in the effect of mobility restrictions on the spread. 

We change the order of correlations in the original heatmap, and make it row-wise correlations: 

```{r ra3, warning=FALSE, message=FALSE}
#XX' and X'X SVD
wtf <- comt %*% t(comt)
fck <- svd(wtf)
r = 2
comt.re2 <-
  as.matrix(fck$u[, 1:r]) %*% diag(fck$d)[1:r, 1:r] %*% t(fck$v[, 1:r])

heatmap(
  comt.re2,
  Colv = NA,
  Rowv = NA,
  main = "Row Corr. - Rank(2)",
  xlab = "Startting days of 7-day rolling windows",
  ylab = "Startting days of 7-day rolling windows"
)
```

This is now worse than the original heatmap we had ealier.  When we apply a Rank(2) approximation, however, we have a very clear picture:

```{r ra2c, warning=FALSE, message=FALSE}
wtf <- t(comt) %*% comt
fck <- svd(wtf)
r = 2
comt.re3 <-
  as.matrix(fck$u[, 1:r]) %*% diag(fck$d)[1:r, 1:r] %*% t(fck$v[, 1:r])

heatmap(
  comt.re3,
  Colv = NA,
  Rowv = NA,
  main = "Column Corr. - Rank(2)",
  xlab = "Lags",
  ylab = "Lags"
)

```
  
There is a series of great lectures on SVD and other matrix approximations by Steve Brunton at YouTube <https://www.youtube.com/watch?v=nbBvuuNVfco>.
  
## Moore-Penrose inverse 

The Singular Value Decomposition (SVD) can be used for solving Ordinary Least Squares (OLS) problems. In particular, the SVD of the design matrix $\mathbf{X}$ can be used to compute the coefficients of the linear regression model.  Here are the steps:
  
$$
\mathbf{y = X \beta}\\
\mathbf{y = U \Sigma V' \beta}\\
\mathbf{U'y = U'U \Sigma V' \beta}\\
\mathbf{U'y = \Sigma V' \beta}\\
\mathbf{\Sigma^{-1}}\mathbf{U'y =  V' \beta}\\
\mathbf{V\Sigma^{-1}}\mathbf{U'y =  \beta}\\
$$

This formula for beta is computationally efficient and numerically stable, even for ill-conditioned or singular $\mathbf{X}$ matrices. Moreover, it allows us to compute the solution to the OLS problem without explicitly computing the inverse of $\mathbf{X}^T \mathbf{X}$. 

Menawhile, the term

$$
\mathbf{V\Sigma^{-1}U' = M^+}
$$

is called **"generalized inverse" or The Moore-Penrose Pseudoinverse**.  

If $\mathbf{X}$ has full column rank, then the pseudoinverse is also the unique solution to the OLS problem. However, if $\mathbf{X}$ does not have full column rank, then its pseudoinverse may not exist or may not be unique. In this case, the OLS estimator obtained using the pseudoinverse will be a "best linear unbiased estimator" (BLUE), but it will not be the unique solution to the OLS problem.

To be more specific, the OLS estimator obtained using the pseudoinverse will minimize the sum of squared residuals subject to the constraint that the coefficients are unbiased, i.e., they have zero expected value. However, there may be other linear unbiased estimators that achieve the same minimum sum of squared residuals. These alternative estimators will differ from the OLS estimator obtained using the pseudoinverse in the values they assign to the coefficients.

In practice, the use of the pseudoinverse to estimate the OLS coefficients when $\mathbf{X}$ does not have full column rank can lead to numerical instability, especially if the singular values of $\mathbf{X}$ are very small. In such cases, it may be more appropriate to use regularization techniques such as ridge or Lasso regression to obtain stable and interpretable estimates. These methods penalize the size of the coefficients and can be used to obtain sparse or "shrunken" estimates, which can be particularly useful in high-dimensional settings where there are more predictors than observations.


Here are some application of SVD and Pseudoinverse.  

```{r mp1, message=FALSE, warning=FALSE}
library(MASS)

##Simple SVD and generalized inverse
A <- matrix(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
              0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1), 9, 4)

a.svd <- svd(A)
ds <- diag(1 / a.svd$d[1:3])
u <- a.svd$u
v <- a.svd$v
us <- as.matrix(u[, 1:3])
vs <- as.matrix(v[, 1:3])
(a.ginv <- vs %*% ds %*% t(us))
ginv(A)
```

We can use SVD for solving a regular OLS on simulated data:  

```{r mp1b, message=FALSE, warning=FALSE}
#Simulated DGP
x1 <- rep(1, 20)
x2 <- rnorm(20)
x3 <- rnorm(20)
u <- matrix(rnorm(20, mean = 0, sd = 1), nrow = 20, ncol = 1)
X <- cbind(x1, x2, x3)
beta <- matrix(c(0.5, 1.5, 2), nrow = 3, ncol = 1)
Y <- X %*% beta + u

#OLS
betahat_OLS <- solve(t(X) %*% X) %*% t(X) %*% Y
betahat_OLS

#SVD
X.svd <- svd(X)
ds <- diag(1 / X.svd$d)
u <- X.svd$u
v <- X.svd$v
us <- as.matrix(u)
vs <- as.matrix(v)
X.ginv_mine <- vs %*% ds %*% t(us)

# Compare
X.ginv <- ginv(X)
round((X.ginv_mine - X.ginv), 4)

# Now OLS
betahat_ginv <- X.ginv %*% Y
betahat_ginv
betahat_OLS
```
  